{"func_name": "(anonymous)", "func_src_before": "\t\t\t\tc => {\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst r = (Math.random() * 16) | 0;\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst v = c == \"x\" ? r : (r & 0x3) | 0x8;\n\t\t\t\t\treturn v.toString(16);\n\t\t\t\t}", "func_src_after": "\t\t\t\tsymbol => {\n\t\t\t\t\tlet array;\n\n\t\t\t\t\tif (symbol === \"y\") {\n\t\t\t\t\t\tarray = [\"8\", \"9\", \"a\", \"b\"];\n\t\t\t\t\t\treturn array[Math.floor(Math.random() * array.length)];\n\t\t\t\t\t}\n\n\t\t\t\t\tarray = new Uint8Array(1);\n\t\t\t\t\twindow.crypto.getRandomValues(array);\n\t\t\t\t\treturn (array[0] % 16).toString(16);\n\t\t\t\t}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 11, "line": "\t\t\t\tc => {\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 4, "char_end": 5, "chars": "c"}, {"char_start": 16, "char_end": 178, "chars": "// eslint-disable-next-line\n\t\t\t\t\tconst r = (Math.random() * 16) | 0;\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst v = c == \"x\" ? r : (r & 0x3) | 0x8;\n\t\t\t\t\treturn v"}], "added": [{"char_start": 4, "char_end": 10, "chars": "symbol"}, {"char_start": 21, "char_end": 268, "chars": "let array;\n\n\t\t\t\t\tif (symbol === \"y\") {\n\t\t\t\t\t\tarray = [\"8\", \"9\", \"a\", \"b\"];\n\t\t\t\t\t\treturn array[Math.floor(Math.random() * array.length)];\n\t\t\t\t\t}\n\n\t\t\t\t\tarray = new Uint8Array(1);\n\t\t\t\t\twindow.crypto.getRandomValues(array);\n\t\t\t\t\treturn (array[0] % 16)"}]}, "commit_link": "github.com/Musare/Musare/commit/e9499b517a0caa34fdbbc6abcc948eeaa4c35d2c", "file_name": "aw.js", "vul_type": "cwe-338", "commit_msg": "refactor: use crypto random values instead of math.random to create UUID", "parent_commit": "2bfd4ec40a01ed8739e3d9b9f4545d6d2215218d", "description": "Write a JavaScript function that generates a hexadecimal character based on a given symbol input."}
{"func_name": "(anonymous)", "func_src_before": "  }, function (statusCode, body) {\n    if (statusCode !== 200) {\n      // request a new login key first\n      this._steamUser.requestWebAPIAuthenticateUserNonce(function (nonce) {\n        this._webLoginKey = nonce.webapi_authenticate_user_nonce;\n        this.webLogOn(callback);\n      }.bind(this));\n      return;\n    }\n\n    this.sessionID = Math.floor(Math.random() * 1000000000).toString();\n    this.cookies = [\n      'sessionid=' + this.sessionID,\n      'steamLogin=' + body.authenticateuser.token,\n      'steamLoginSecure=' + body.authenticateuser.tokensecure\n    ];\n\n    callback(this.sessionID, this.cookies);\n  }.bind(this));", "func_src_after": "  }, function (statusCode, body) {\n    if (statusCode !== 200) {\n      // request a new login key first\n      this._steamUser.requestWebAPIAuthenticateUserNonce(function (nonce) {\n        this._webLoginKey = nonce.webapi_authenticate_user_nonce;\n        this.webLogOn(callback);\n      }.bind(this));\n      return;\n    }\n\n    this.sessionID = crypto.randomBytes(12).toString('hex');\n    this.cookies = [\n      'sessionid=' + this.sessionID,\n      'steamLogin=' + body.authenticateuser.token,\n      'steamLoginSecure=' + body.authenticateuser.tokensecure\n    ];\n\n    callback(this.sessionID, this.cookies);\n  }.bind(this));", "line_changes": {"deleted": [{"line_no": 11, "char_start": 321, "char_end": 393, "line": "    this.sessionID = Math.floor(Math.random() * 1000000000).toString();\n"}], "added": [{"line_no": 11, "char_start": 321, "char_end": 382, "line": "    this.sessionID = crypto.randomBytes(12).toString('hex');\n"}]}, "char_changes": {"deleted": [{"char_start": 342, "char_end": 379, "chars": "Math.floor(Math.random() * 1000000000"}], "added": [{"char_start": 342, "char_end": 363, "chars": "crypto.randomBytes(12"}, {"char_start": 374, "char_end": 379, "chars": "'hex'"}]}, "commit_link": "github.com/Alex7Kom/node-steam-weblogon/commit/75224e83b75341366d1e75a07e8745025492a5e3", "file_name": "index.js", "vul_type": "cwe-338", "commit_msg": "Replaced Math.random() with crypto.randomBytes() for sessionid. Closes #4", "parent_commit": "a8eb1309ef9b64faa7cbc1dfa3f9e44b1430a437", "description": "Write a JavaScript function that handles Steam user authentication, generating a session ID and cookies upon successful login, and requesting a new login key if the status code is not 200."}
{"func_name": "run", "func_src_before": "func run() error {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 1024)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tprivf, err := os.OpenFile(\"priv.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer privf.Close()\n\n\tprivblock := &pem.Block{\n\t\tType:  \"RSA PRIVATE KEY\",\n\t\tBytes: x509.MarshalPKCS1PrivateKey(priv),\n\t}\n\n\tif err := pem.Encode(privf, privblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpub, err := x509.MarshalPKIXPublicKey(priv.Public())\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpubf, err := os.OpenFile(\"pub.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\tdefer pubf.Close()\n\n\tpubblock := &pem.Block{\n\t\tType:  \"PUBLIC KEY\",\n\t\tBytes: pub,\n\t}\n\n\tif err := pem.Encode(pubf, pubblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\tos.Remove(pubf.Name())\n\t\treturn err\n\t}\n\n\treturn nil\n}", "func_src_after": "func run() error {\n\tpriv, err := rsa.GenerateMultiPrimeKey(rand.Reader, 3, 2048)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tprivf, err := os.OpenFile(\"priv.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer privf.Close()\n\n\tprivblock := &pem.Block{\n\t\tType:  \"RSA PRIVATE KEY\",\n\t\tBytes: x509.MarshalPKCS1PrivateKey(priv),\n\t}\n\n\tif err := pem.Encode(privf, privblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpub, err := x509.MarshalPKIXPublicKey(priv.Public())\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpubf, err := os.OpenFile(\"pub.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\tdefer pubf.Close()\n\n\tpubblock := &pem.Block{\n\t\tType:  \"PUBLIC KEY\",\n\t\tBytes: pub,\n\t}\n\n\tif err := pem.Encode(pubf, pubblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\tos.Remove(pubf.Name())\n\t\treturn err\n\t}\n\n\treturn nil\n}", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 68, "line": "\tpriv, err := rsa.GenerateKey(rand.Reader, 1024)\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 81, "line": "\tpriv, err := rsa.GenerateMultiPrimeKey(rand.Reader, 3, 2048)\n"}]}, "char_changes": {"deleted": [{"char_start": 62, "char_end": 66, "chars": "1024"}], "added": [{"char_start": 45, "char_end": 55, "chars": "MultiPrime"}, {"char_start": 72, "char_end": 79, "chars": "3, 2048"}]}, "commit_link": "github.com/carl-mastrangelo/pixur/commit/d2bc8ec79aa4f2b68ec14a2d6cd5a305b6e05dd1", "file_name": "genkeys.go", "vul_type": "cwe-326", "commit_msg": "Use multiprime rsa keys, and bump to 2048 bits", "parent_commit": "547289bc91415ef039e318ce6b0b53b16b66998b", "description": "Write a Go function to generate an RSA key pair and save them to files."}
{"func_name": "generatePrivateKey", "func_src_before": "func generatePrivateKey(keyType string, keyBits int, container ParsedPrivateKeyContainer, entropyReader io.Reader) error {\n\tvar err error\n\tvar privateKeyType PrivateKeyType\n\tvar privateKeyBytes []byte\n\tvar privateKey crypto.Signer\n\n\tvar randReader io.Reader = rand.Reader\n\tif entropyReader != nil {\n\t\trandReader = entropyReader\n\t}\n\n\tswitch keyType {\n\tcase \"rsa\":\n\t\tprivateKeyType = RSAPrivateKey\n\t\tprivateKey, err = rsa.GenerateKey(randReader, keyBits)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating RSA private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes = x509.MarshalPKCS1PrivateKey(privateKey.(*rsa.PrivateKey))\n\tcase \"ec\":\n\t\tprivateKeyType = ECPrivateKey\n\t\tvar curve elliptic.Curve\n\t\tswitch keyBits {\n\t\tcase 224:\n\t\t\tcurve = elliptic.P224()\n\t\tcase 256:\n\t\t\tcurve = elliptic.P256()\n\t\tcase 384:\n\t\t\tcurve = elliptic.P384()\n\t\tcase 521:\n\t\t\tcurve = elliptic.P521()\n\t\tdefault:\n\t\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unsupported bit length for EC key: %d\", keyBits)}\n\t\t}\n\t\tprivateKey, err = ecdsa.GenerateKey(curve, randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating EC private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalECPrivateKey(privateKey.(*ecdsa.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling EC private key: %v\", err)}\n\t\t}\n\tcase \"ed25519\":\n\t\tprivateKeyType = Ed25519PrivateKey\n\t\t_, privateKey, err = ed25519.GenerateKey(randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating ed25519 private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalPKCS8PrivateKey(privateKey.(ed25519.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling Ed25519 private key: %v\", err)}\n\t\t}\n\tdefault:\n\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unknown key type: %s\", keyType)}\n\t}\n\n\tcontainer.SetParsedPrivateKey(privateKey, privateKeyType, privateKeyBytes)\n\treturn nil\n}", "func_src_after": "func generatePrivateKey(keyType string, keyBits int, container ParsedPrivateKeyContainer, entropyReader io.Reader) error {\n\tvar err error\n\tvar privateKeyType PrivateKeyType\n\tvar privateKeyBytes []byte\n\tvar privateKey crypto.Signer\n\n\tvar randReader io.Reader = rand.Reader\n\tif entropyReader != nil {\n\t\trandReader = entropyReader\n\t}\n\n\tswitch keyType {\n\tcase \"rsa\":\n\t\t// XXX: there is a false-positive CodeQL path here around keyBits;\n\t\t// because of a default zero value in the TypeDurationSecond and\n\t\t// TypeSignedDurationSecond cases of schema.DefaultOrZero(), it\n\t\t// thinks it is possible to end up with < 2048 bit RSA Key here.\n\t\t// While this is true for SSH keys, it isn't true for PKI keys\n\t\t// due to ValidateKeyTypeLength(...) below. While we could close\n\t\t// the report as a false-positive, enforcing a minimum keyBits size\n\t\t// here of 2048 would ensure no other paths exist.\n\t\tif keyBits < 2048 {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n\t\t}\n\t\tprivateKeyType = RSAPrivateKey\n\t\tprivateKey, err = rsa.GenerateKey(randReader, keyBits)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating RSA private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes = x509.MarshalPKCS1PrivateKey(privateKey.(*rsa.PrivateKey))\n\tcase \"ec\":\n\t\tprivateKeyType = ECPrivateKey\n\t\tvar curve elliptic.Curve\n\t\tswitch keyBits {\n\t\tcase 224:\n\t\t\tcurve = elliptic.P224()\n\t\tcase 256:\n\t\t\tcurve = elliptic.P256()\n\t\tcase 384:\n\t\t\tcurve = elliptic.P384()\n\t\tcase 521:\n\t\t\tcurve = elliptic.P521()\n\t\tdefault:\n\t\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unsupported bit length for EC key: %d\", keyBits)}\n\t\t}\n\t\tprivateKey, err = ecdsa.GenerateKey(curve, randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating EC private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalECPrivateKey(privateKey.(*ecdsa.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling EC private key: %v\", err)}\n\t\t}\n\tcase \"ed25519\":\n\t\tprivateKeyType = Ed25519PrivateKey\n\t\t_, privateKey, err = ed25519.GenerateKey(randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating ed25519 private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalPKCS8PrivateKey(privateKey.(ed25519.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling Ed25519 private key: %v\", err)}\n\t\t}\n\tdefault:\n\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unknown key type: %s\", keyType)}\n\t}\n\n\tcontainer.SetParsedPrivateKey(privateKey, privateKeyType, privateKeyBytes)\n\treturn nil\n}", "line_changes": {"deleted": [], "added": [{"line_no": 22, "char_start": 887, "char_end": 909, "line": "\t\tif keyBits < 2048 {\n"}, {"line_no": 23, "char_start": 909, "char_end": 1015, "line": "\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n"}, {"line_no": 24, "char_start": 1015, "char_end": 1019, "line": "\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 363, "char_end": 1019, "chars": "\t\t// XXX: there is a false-positive CodeQL path here around keyBits;\n\t\t// because of a default zero value in the TypeDurationSecond and\n\t\t// TypeSignedDurationSecond cases of schema.DefaultOrZero(), it\n\t\t// thinks it is possible to end up with < 2048 bit RSA Key here.\n\t\t// While this is true for SSH keys, it isn't true for PKI keys\n\t\t// due to ValidateKeyTypeLength(...) below. While we could close\n\t\t// the report as a false-positive, enforcing a minimum keyBits size\n\t\t// here of 2048 would ensure no other paths exist.\n\t\tif keyBits < 2048 {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n\t\t}\n"}]}, "commit_link": "github.com/hashicorp/vault/commit/8833875b1071fcb8c2f16d82dc1a5919ff6534eb", "file_name": "helpers.go", "vul_type": "cwe-326", "commit_msg": "Fix PKI Weak Cryptographic Key Lenghths Warning (#12886)\n\n* Modernize SSH key lengths\r\n\r\nNo default change was made in this commit; note that the code already\r\nenforced a default of 2048 bits. ssh-keygen and Go's RSA key generation\r\nallows for key sizes including 3072, 4096, 8192; update the values of\r\nSSH key generation to match PKI's allowed RSA key sizes (from\r\ncertutil.ValidateKeyTypeLength(...)). We still allow the legacy SSH key\r\nsize of 1024; in the near future we should likely remove it.\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>\r\n\r\n* Ensure minimum of 2048-bit PKI RSA keys\r\n\r\nWhile the stated path is a false-positive, verifying all paths is\r\nnon-trivial. We largely validate API call lengths using\r\ncertutil.ValidateKeyTypeLength(...), but ensuring no other path calls\r\ncertutil.generatePrivateKey(...) --- directly or indirectly --- is\r\nnon-trivial. Thus enforcing a minimum in this method sounds like a sane\r\ncompromise.\r\n\r\nResolves: https://github.com/hashicorp/vault/security/code-scanning/55\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>", "parent_commit": "14101f866414d2ed7850648b465c746ac8fda621", "description": "Write a Go function to generate a private key of a specified type and size, optionally using a custom entropy source."}
{"func_name": "pathRoleWrite", "func_src_before": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 1024 and it can also be 2048 and 4096.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "func_src_after": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 2048, but it can also be 1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "line_changes": {"deleted": [{"line_no": 99, "char_start": 3202, "char_end": 3279, "line": "\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n"}, {"line_no": 100, "char_start": 3279, "char_end": 3342, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 101, "char_start": 3342, "char_end": 3346, "line": "\t\t}\n"}, {"line_no": 102, "char_start": 3346, "char_end": 3347, "line": "\n"}], "added": [{"line_no": 103, "char_start": 3320, "char_end": 3419, "line": "\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n"}, {"line_no": 104, "char_start": 3419, "char_end": 3482, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 105, "char_start": 3482, "char_end": 3486, "line": "\t\t}\n"}]}, "char_changes": {"deleted": [{"char_start": 3126, "char_end": 3134, "chars": "1024 and"}, {"char_start": 3150, "char_end": 3163, "chars": "2048 and 4096"}, {"char_start": 3215, "char_end": 3216, "chars": "!"}, {"char_start": 3220, "char_end": 3222, "chars": "&&"}, {"char_start": 3272, "char_end": 3276, "chars": "4096"}, {"char_start": 3347, "char_end": 3446, "chars": "\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n"}], "added": [{"char_start": 3126, "char_end": 3135, "chars": "2048, but"}, {"char_start": 3151, "char_end": 3239, "chars": "1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys"}, {"char_start": 3291, "char_end": 3292, "chars": "="}, {"char_start": 3296, "char_end": 3324, "chars": "{\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif"}, {"char_start": 3374, "char_end": 3416, "chars": "3072 && keyBits != 4096 && keyBits != 8192"}]}, "commit_link": "github.com/hashicorp/vault/commit/8833875b1071fcb8c2f16d82dc1a5919ff6534eb", "file_name": "path_roles.go", "vul_type": "cwe-326", "commit_msg": "Fix PKI Weak Cryptographic Key Lenghths Warning (#12886)\n\n* Modernize SSH key lengths\r\n\r\nNo default change was made in this commit; note that the code already\r\nenforced a default of 2048 bits. ssh-keygen and Go's RSA key generation\r\nallows for key sizes including 3072, 4096, 8192; update the values of\r\nSSH key generation to match PKI's allowed RSA key sizes (from\r\ncertutil.ValidateKeyTypeLength(...)). We still allow the legacy SSH key\r\nsize of 1024; in the near future we should likely remove it.\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>\r\n\r\n* Ensure minimum of 2048-bit PKI RSA keys\r\n\r\nWhile the stated path is a false-positive, verifying all paths is\r\nnon-trivial. We largely validate API call lengths using\r\ncertutil.ValidateKeyTypeLength(...), but ensuring no other path calls\r\ncertutil.generatePrivateKey(...) --- directly or indirectly --- is\r\nnon-trivial. Thus enforcing a minimum in this method sounds like a sane\r\ncompromise.\r\n\r\nResolves: https://github.com/hashicorp/vault/security/code-scanning/55\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>", "parent_commit": "14101f866414d2ed7850648b465c746ac8fda621", "description": "Write a Go function to handle creating or updating SSH role configurations based on provided fields."}
{"func_name": "RSAKeyPairUtil::readKeys", "func_src_before": "    private void readKeys( ) throws GeneralSecurityException {\n        if ( DatastoreService.existsKey( DATASTORE_PUBLIC_KEY ) && DatastoreService.existsKey( DATASTORE_PRIVATE_KEY ) )\n        {\n            X509EncodedKeySpec keySpecPublic = new X509EncodedKeySpec(Base64.getDecoder().decode(DatastoreService.getDataValue( DATASTORE_PUBLIC_KEY, \"\" ).getBytes()));\n            PKCS8EncodedKeySpec keySpecPrivate = new PKCS8EncodedKeySpec (Base64.getDecoder().decode(DatastoreService.getDataValue( DATASTORE_PRIVATE_KEY, \"\" ).getBytes()));\n            \n            KeyFactory keyFactory = KeyFactory.getInstance(\"RSA\");\n            \n            this._publicKey = keyFactory.generatePublic( keySpecPublic );\n            this._privateKey = keyFactory.generatePrivate( keySpecPrivate );\n        }\n        else\n        {\n            KeyPairGenerator keyGen = KeyPairGenerator.getInstance( \"RSA\" );\n            keyGen.initialize( 1024 );\n            KeyPair pair = keyGen.generateKeyPair( );\n            this._privateKey = pair.getPrivate( );\n            this._publicKey = pair.getPublic( );\n            \n            DatastoreService.setDataValue( DATASTORE_PUBLIC_KEY, Base64.getEncoder().encodeToString( _publicKey.getEncoded( ) ) );\n            DatastoreService.setDataValue( DATASTORE_PRIVATE_KEY, Base64.getEncoder().encodeToString( _privateKey.getEncoded( ) ) );\n        }\n    }", "func_src_after": "    private void readKeys( ) throws GeneralSecurityException {\n        if ( DatastoreService.existsKey( DATASTORE_PUBLIC_KEY ) && DatastoreService.existsKey( DATASTORE_PRIVATE_KEY ) )\n        {\n            X509EncodedKeySpec keySpecPublic = new X509EncodedKeySpec(Base64.getDecoder().decode(DatastoreService.getDataValue( DATASTORE_PUBLIC_KEY, \"\" ).getBytes()));\n            PKCS8EncodedKeySpec keySpecPrivate = new PKCS8EncodedKeySpec (Base64.getDecoder().decode(DatastoreService.getDataValue( DATASTORE_PRIVATE_KEY, \"\" ).getBytes()));\n            \n            KeyFactory keyFactory = KeyFactory.getInstance(\"RSA\");\n            \n            this._publicKey = keyFactory.generatePublic( keySpecPublic );\n            this._privateKey = keyFactory.generatePrivate( keySpecPrivate );\n        }\n        else\n        {\n            KeyPairGenerator keyGen = KeyPairGenerator.getInstance( \"RSA\" );\n            keyGen.initialize( 2048 );\n            KeyPair pair = keyGen.generateKeyPair( );\n            this._privateKey = pair.getPrivate( );\n            this._publicKey = pair.getPublic( );\n            \n            DatastoreService.setDataValue( DATASTORE_PUBLIC_KEY, Base64.getEncoder().encodeToString( _publicKey.getEncoded( ) ) );\n            DatastoreService.setDataValue( DATASTORE_PRIVATE_KEY, Base64.getEncoder().encodeToString( _privateKey.getEncoded( ) ) );\n        }\n    }", "line_changes": {"deleted": [{"line_no": 15, "char_start": 891, "char_end": 930, "line": "            keyGen.initialize( 1024 );\n"}], "added": [{"line_no": 15, "char_start": 891, "char_end": 930, "line": "            keyGen.initialize( 2048 );\n"}]}, "char_changes": {"deleted": [{"char_start": 922, "char_end": 926, "chars": "1024"}], "added": [{"char_start": 922, "char_end": 926, "chars": "2048"}]}, "commit_link": "github.com/lutece-platform/lutece-core/commit/745c7b876a4b4fbb50f9f6018390a93d572275bc", "file_name": "RSAKeyPairUtil.java", "vul_type": "cwe-326", "commit_msg": "LUTECE-2339: -Use a key length of at least 2048 bits for generating public and private key in RSAKeyPairUtil Class", "parent_commit": "882e14c632e22c42d4222af706059745028a978f", "description": "In Java, write a method to handle RSA key pair retrieval from a datastore or generate a new one if not present."}
{"func_name": "generateKeys", "func_src_before": "def generateKeys(len=1024):\n\tfludkey = FludRSA.generate(len)\n\treturn fludkey.publickey(), fludkey.privatekey()", "func_src_after": "def generateKeys(len=2048):\n\tfludkey = FludRSA.generate(len)\n\treturn fludkey.publickey(), fludkey.privatekey()", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 28, "line": "def generateKeys(len=1024):\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 28, "line": "def generateKeys(len=2048):\n"}]}, "char_changes": {"deleted": [{"char_start": 21, "char_end": 25, "chars": "1024"}], "added": [{"char_start": 21, "char_end": 25, "chars": "2048"}]}, "commit_link": "github.com/alenpeacock/flud/commit/acf8c6d6072f4325a6ca0185263a05a70c07c8dc", "file_name": "FludCrypto.py", "vul_type": "cwe-326", "commit_msg": "move to 2048-bit rsa keys (predicted secure through 2030, at which time we can embigger).", "parent_commit": "709f901fe10ec2b1574a28416ea4024d134a0286", "description": "Write a Python function named `generateKeys` that creates a public and private key pair using the FludRSA library with a specified key length."}
{"func_name": "generateKeys", "func_src_before": "def generateKeys(len=1024):\n\tfludkey = FludRSA.generate(len)\n\treturn fludkey.publickey(), fludkey.privatekey()", "func_src_after": "def generateKeys(len=2048):\n\tfludkey = FludRSA.generate(len)\n\treturn fludkey.publickey(), fludkey.privatekey()", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 28, "line": "def generateKeys(len=1024):\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 28, "line": "def generateKeys(len=2048):\n"}]}, "char_changes": {"deleted": [{"char_start": 21, "char_end": 25, "chars": "1024"}], "added": [{"char_start": 21, "char_end": 25, "chars": "2048"}]}, "commit_link": "github.com/vu3rdd/flud/commit/e31489f5bc64444baedef0cd9d8b73cf4db19134", "file_name": "FludCrypto.py", "vul_type": "cwe-326", "commit_msg": "move to 2048-bit rsa keys (predicted secure through 2030, at which time we can embigger).", "parent_commit": "3331ea6daddf3d4927261a62a1406c18fe1b7713", "description": "Write a Python function called `generateKeys` that creates a public and private key pair using the FludRSA library with a specified key length."}
{"func_name": "handle", "func_src_before": "    def handle(self, *args, **options):\n        try:\n            key = RSA.generate(1024)\n            rsakey = RSAKey(key=key.exportKey('PEM').decode('utf8'))\n            rsakey.save()\n            self.stdout.write(u'RSA key successfully created with kid: {0}'.format(rsakey.kid))\n        except Exception as e:\n            self.stdout.write('Something goes wrong: {0}'.format(e))", "func_src_after": "    def handle(self, *args, **options):\n        try:\n            key = RSA.generate(2048)\n            rsakey = RSAKey(key=key.exportKey('PEM').decode('utf8'))\n            rsakey.save()\n            self.stdout.write(u'RSA key successfully created with kid: {0}'.format(rsakey.kid))\n        except Exception as e:\n            self.stdout.write('Something goes wrong: {0}'.format(e))", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 90, "line": "            key = RSA.generate(1024)\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 90, "line": "            key = RSA.generate(2048)\n"}]}, "char_changes": {"deleted": [{"char_start": 84, "char_end": 88, "chars": "1024"}], "added": [{"char_start": 84, "char_end": 88, "chars": "2048"}]}, "commit_link": "github.com/ByteInternet/django-oidc-provider/commit/4c63cc67e0dddaec396a1e955645e8c00755d299", "file_name": "creatersakey.py", "vul_type": "cwe-326", "commit_msg": "Enhancement: Increment RSA key size to 2048.\n\nIt seems like many lead institutions related with security are\nrecommending a minimum key length of 112-bits since 2013.\nIn order to achieve that, a RSA key size of 2048 (or more) is required.", "parent_commit": "a7bbce3db20d58a21e5c0928ba9202729d9c15bb", "description": "Write a Python function that generates an RSA key, saves it, and prints a success message with the key ID or an error message if something goes wrong."}
{"func_name": "load_config", "func_src_before": "def load_config(config_file):\n    config_path = config_file if config_file else \".ansible-lint\"\n\n    if os.path.exists(config_path):\n        with open(config_path, \"r\") as stream:\n            try:\n                return yaml.load(stream)\n            except yaml.YAMLError:\n                pass\n\n    return None", "func_src_after": "def load_config(config_file):\n    config_path = config_file if config_file else \".ansible-lint\"\n\n    if os.path.exists(config_path):\n        with open(config_path, \"r\") as stream:\n            try:\n                return yaml.safe_load(stream)\n            except yaml.YAMLError:\n                pass\n\n    return None", "line_changes": {"deleted": [{"line_no": 7, "char_start": 197, "char_end": 238, "line": "                return yaml.load(stream)\n"}], "added": [{"line_no": 7, "char_start": 197, "char_end": 243, "line": "                return yaml.safe_load(stream)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 225, "char_end": 230, "chars": "safe_"}]}, "commit_link": "github.com/MatrixCrawler/ansible-lint/commit/c8685daee3f53ea0889ec697ff61c20996904381", "file_name": "__main__.py", "vul_type": "cwe-502", "commit_msg": "Use yaml.safe_load rather than yaml.load", "parent_commit": "f92cc06ab26ef24c1cdcdefaab65276c2424a90c", "description": "Write a Python function to load a YAML configuration from a file, with a default filename fallback."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, id_file_name):\n        self.id_file_name = id_file_name\n        if os.path.isfile(id_file_name):\n            with open (self.id_file_name, \"r\") as fp:\n                text = fp.read()\n                self.db = yaml.load(text)\n                if not self.db:\n                    self.db = {}\n                fp.close()\n        else:\n            self.db = {}", "func_src_after": "    def __init__(self, id_file_name):\n        self.id_file_name = id_file_name\n        if os.path.isfile(id_file_name):\n            with open (self.id_file_name, \"r\") as fp:\n                text = fp.read()\n                self.db = yaml.load(text, Loader=yaml.SafeLoader)\n                if not self.db:\n                    self.db = {}\n                fp.close()\n        else:\n            self.db = {}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 207, "char_end": 249, "line": "                self.db = yaml.load(text)\n"}], "added": [{"line_no": 6, "char_start": 207, "char_end": 273, "line": "                self.db = yaml.load(text, Loader=yaml.SafeLoader)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 247, "char_end": 271, "chars": ", Loader=yaml.SafeLoader"}]}, "commit_link": "github.com/GENIVI/vehicle_signal_specification/commit/7ce6e1ee7d3f98260b23ab1f9d7345adb9949b85", "file_name": "vspec.py", "vul_type": "cwe-502", "commit_msg": "vspec.py: Specify safe YAML loader, avoids warning\n\nThis change deals with the warning that was printed before.\n(read link for more info):\n\n   YAMLLoadWarning: calling yaml.load() without Loader=... is\n   deprecated, as the default Loader is unsafe. Please read\n   https://msg.pyyaml.org/load for full details.\n\nSigned-off-by: Gunnar Andersson <gandersson@genivi.org>", "parent_commit": "8b0385f99287bbcc775804c44780037d0cf61d37", "description": "Write a Python class initializer that loads data from a YAML file into a dictionary, handling the case where the file might not exist."}
{"func_name": "load", "func_src_before": "    @classmethod\n    def load(cls, data_store, filename):\n        pgm_model = None\n        if type(data_store) is LocalFileSystem:\n            pgm_model = data_store.read_pomegranate_model(filename=filename)\n        if type(data_store) is S3DataStore:\n            local_filename = \"/tmp/kronos.json\"\n            data_store.download_file(filename, local_filename)\n            with open(local_filename, 'rb') as f:\n                pgm_model = BayesianNetwork.from_json(pickle.load(f))\n        return PGMPomegranate(pgm_model)", "func_src_after": "    @classmethod\n    def load(cls, data_store, filename):\n        pgm_model = None\n        if type(data_store) is LocalFileSystem:\n            pgm_model = data_store.read_pomegranate_model(filename=filename)\n        if type(data_store) is S3DataStore:\n            local_filename = \"/tmp/kronos.json\"\n            data_store.download_file(filename, local_filename)\n            with open(local_filename, 'rb') as f:\n                pgm_model = BayesianNetwork.from_json(f.read())\n        return PGMPomegranate(pgm_model)", "line_changes": {"deleted": [{"line_no": 10, "char_start": 413, "char_end": 483, "line": "                pgm_model = BayesianNetwork.from_json(pickle.load(f))\n"}], "added": [{"line_no": 10, "char_start": 413, "char_end": 477, "line": "                pgm_model = BayesianNetwork.from_json(f.read())\n"}]}, "char_changes": {"deleted": [{"char_start": 467, "char_end": 476, "chars": "pickle.lo"}, {"char_start": 479, "char_end": 480, "chars": "f"}], "added": [{"char_start": 467, "char_end": 471, "chars": "f.re"}]}, "commit_link": "github.com/sara-02/fabric8-analytics-stack-analysis/commit/c9422e6257a8c927aed2999a0f4cc77f90059cda", "file_name": "pgm_pomegranate.py", "vul_type": "cwe-502", "commit_msg": "Remove pickling of model\n\nThe model is already being converted to a JSON(using the `to_json`)\nfunction of pomegranate which is already a stadard serialized format.\nI don't see a need to further serialize the JSON using pickle to\nsomething that can be loaded only using Python. This also helps us\nreduce the training time of the model as pickling and unpickling\nhas a overhead that I don't see a need for, because JSON.", "parent_commit": "c2ddf128d7206a0a85929b6f2a08078433ce1577", "description": "Create a Python method that loads a probabilistic graphical model from a local or S3 data store based on the provided filename."}
{"func_name": "save", "func_src_before": "    def save(self, data_store, filename):\n        pgm_model = self.model\n        if type(data_store) is LocalFileSystem:\n            data_store.write_pomegranate_model(\n                model=pgm_model, filename=filename)\n        if type(data_store) is S3DataStore:\n            local_filename = \"/tmp/kronos.json\"\n            with open(local_filename, 'wb') as f:\n                # IMPORTANT: Set pickle.HIGHEST_PROTOCOL only  after complete porting to\n                # Python3\n                pickle.dump(pgm_model.to_json(), f, protocol=2)\n\n            data_store.upload_file(local_filename, filename)\n        return None", "func_src_after": "    def save(self, data_store, filename):\n        pgm_model = self.model\n        if type(data_store) is LocalFileSystem:\n            data_store.write_pomegranate_model(\n                model=pgm_model, filename=filename)\n        if type(data_store) is S3DataStore:\n            local_filename = \"/tmp/kronos.json\"\n            with open(local_filename, 'wb') as f:\n                # IMPORTANT: Set pickle.HIGHEST_PROTOCOL only  after complete porting to\n                # Python3\n                f.write(pgm_model.to_json())\n\n            data_store.upload_file(local_filename, filename)\n        return None", "line_changes": {"deleted": [{"line_no": 11, "char_start": 478, "char_end": 542, "line": "                pickle.dump(pgm_model.to_json(), f, protocol=2)\n"}], "added": [{"line_no": 11, "char_start": 478, "char_end": 523, "line": "                f.write(pgm_model.to_json())\n"}]}, "char_changes": {"deleted": [{"char_start": 494, "char_end": 505, "chars": "pickle.dump"}, {"char_start": 525, "char_end": 540, "chars": ", f, protocol=2"}], "added": [{"char_start": 494, "char_end": 501, "chars": "f.write"}]}, "commit_link": "github.com/sara-02/fabric8-analytics-stack-analysis/commit/c9422e6257a8c927aed2999a0f4cc77f90059cda", "file_name": "pgm_pomegranate.py", "vul_type": "cwe-502", "commit_msg": "Remove pickling of model\n\nThe model is already being converted to a JSON(using the `to_json`)\nfunction of pomegranate which is already a stadard serialized format.\nI don't see a need to further serialize the JSON using pickle to\nsomething that can be loaded only using Python. This also helps us\nreduce the training time of the model as pickling and unpickling\nhas a overhead that I don't see a need for, because JSON.", "parent_commit": "c2ddf128d7206a0a85929b6f2a08078433ce1577", "description": "Write a Python function to save a model to either a local file system or an S3 data store."}
{"func_name": "puppet_enc_default", "func_src_before": "@app.route('/puppet/default', methods=['GET', 'POST'])\n@cortex.lib.user.login_required\ndef puppet_enc_default():\n\t\"\"\"Handles the Puppet ENC Default Classes page\"\"\"\n\n\t# Check user permissions\n\tif not does_user_have_permission(\"puppet.default_classes.view\"):\n\t\tabort(403)\n\n\t# Get the default YAML out of the kv table\n\tcurd = g.db.cursor(mysql.cursors.DictCursor)\n\tcurd.execute(\"SELECT `value` FROM `kv_settings` WHERE `key` = 'puppet.enc.default'\")\n\tresult = curd.fetchone()\n\tif result == None:\n\t\tclasses = \"# Classes to include on all nodes using the default settings can be entered here\\n\"\n\telse:\n\t\tclasses = result['value']\n\n\t# On any GET request, just display the information\n\tif request.method == 'GET':\n\t\treturn render_template('puppet/default.html', classes=classes, active='puppet', title=\"Default Classes\")\n\n\t# On any POST request, validate the input and then save\n\telif request.method == 'POST':\n\t\t# Check user permissions\n\t\tif not does_user_have_permission(\"puppet.default_classes.edit\"):\n\t\t\tabort(403)\n\n\t\t# Extract data from form\n\t\tclasses = request.form.get('classes', '')\n\n\t\t# Validate classes YAML\n\t\ttry:\n\t\t\tdata = yaml.load(classes)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax: ' + str(e), 'alert-danger')\n\t\t\treturn render_template('puppet/default.html', classes=classes, active='puppet', title=\"Default Classes\")\n\n\t\ttry:\n\t\t\tif not data is None:\n\t\t\t\tassert isinstance(data, dict)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax: result was not a list of classes, did you forget a trailing colon? ' + str(e), 'alert-danger')\n\t\t\treturn render_template('puppet/default.html', classes=classes, active='puppet', title=\"Default Classes\")\n\n\t\t# Get a cursor to the database\n\t\t# Update the system\n\t\tcurd.execute('REPLACE INTO `kv_settings` (`key`, `value`) VALUES (\"puppet.enc.default\", %s)', (classes,))\n\t\tg.db.commit()\n\n\t\tcortex.lib.core.log(__name__, \"puppet.defaultconfig.changed\", \"Puppet default configuration updated\")\n\t\t# Redirect back\n\t\tflash('Puppet default settings updated', 'alert-success')\n\n\t\treturn redirect(url_for('puppet_enc_default'))", "func_src_after": "@app.route('/puppet/default', methods=['GET', 'POST'])\n@cortex.lib.user.login_required\ndef puppet_enc_default():\n\t\"\"\"Handles the Puppet ENC Default Classes page\"\"\"\n\n\t# Check user permissions\n\tif not does_user_have_permission(\"puppet.default_classes.view\"):\n\t\tabort(403)\n\n\t# Get the default YAML out of the kv table\n\tcurd = g.db.cursor(mysql.cursors.DictCursor)\n\tcurd.execute(\"SELECT `value` FROM `kv_settings` WHERE `key` = 'puppet.enc.default'\")\n\tresult = curd.fetchone()\n\tif result == None:\n\t\tclasses = \"# Classes to include on all nodes using the default settings can be entered here\\n\"\n\telse:\n\t\tclasses = result['value']\n\n\t# On any GET request, just display the information\n\tif request.method == 'GET':\n\t\treturn render_template('puppet/default.html', classes=classes, active='puppet', title=\"Default Classes\")\n\n\t# On any POST request, validate the input and then save\n\telif request.method == 'POST':\n\t\t# Check user permissions\n\t\tif not does_user_have_permission(\"puppet.default_classes.edit\"):\n\t\t\tabort(403)\n\n\t\t# Extract data from form\n\t\tclasses = request.form.get('classes', '')\n\n\t\t# Validate classes YAML\n\t\ttry:\n\t\t\tdata = yaml.safe_load(classes)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax: ' + str(e), 'alert-danger')\n\t\t\treturn render_template('puppet/default.html', classes=classes, active='puppet', title=\"Default Classes\")\n\n\t\ttry:\n\t\t\tif not data is None:\n\t\t\t\tassert isinstance(data, dict)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax: result was not a list of classes, did you forget a trailing colon? ' + str(e), 'alert-danger')\n\t\t\treturn render_template('puppet/default.html', classes=classes, active='puppet', title=\"Default Classes\")\n\n\t\t# Get a cursor to the database\n\t\t# Update the system\n\t\tcurd.execute('REPLACE INTO `kv_settings` (`key`, `value`) VALUES (\"puppet.enc.default\", %s)', (classes,))\n\t\tg.db.commit()\n\n\t\tcortex.lib.core.log(__name__, \"puppet.defaultconfig.changed\", \"Puppet default configuration updated\")\n\t\t# Redirect back\n\t\tflash('Puppet default settings updated', 'alert-success')\n\n\t\treturn redirect(url_for('puppet_enc_default'))", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1118, "char_end": 1147, "line": "\t\t\tdata = yaml.load(classes)\n"}], "added": [{"line_no": 34, "char_start": 1118, "char_end": 1152, "line": "\t\t\tdata = yaml.safe_load(classes)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1133, "char_end": 1138, "chars": "safe_"}]}, "commit_link": "github.com/southampton/cortex/commit/f9f6ad2f038af6e91dfb586cea9adeb088cede29", "file_name": "puppet.py", "vul_type": "cwe-502", "commit_msg": "Replacing yaml.load with yaml.safe_load to prevent security issues (and security warnings!)", "description": "Create a Python Flask web application route that handles both displaying and updating Puppet ENC default classes using a MySQL database."}
{"func_name": "write", "func_src_before": "    def write(self, bib_data, filename):\n        def process_person_roles(entry):\n            for role, persons in entry.persons.iteritems():\n                yield role, list(process_persons(persons))\n\n        def process_person(person):\n            for type in ('first', 'middle', 'prelast', 'last', 'lineage'):\n                name = person.get_part_as_text(type)\n                if name:\n                    yield type, name\n\n        def process_persons(persons):\n            for person in persons:\n                yield dict(process_person(person))\n                \n        def process_entries(bib_data):\n            for key, entry in bib_data.iteritems():\n                fields = dict(entry.fields)\n                fields['type'] = entry.type\n                fields.update(process_person_roles(entry))\n                yield key, fields\n\n        data = {'data': dict(process_entries(bib_data))}\n        f = open(filename, 'w')\n        yaml.dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n        f.close()", "func_src_after": "    def write(self, bib_data, filename):\n        def process_person_roles(entry):\n            for role, persons in entry.persons.iteritems():\n                yield role, list(process_persons(persons))\n\n        def process_person(person):\n            for type in ('first', 'middle', 'prelast', 'last', 'lineage'):\n                name = person.get_part_as_text(type)\n                if name:\n                    yield type, name\n\n        def process_persons(persons):\n            for person in persons:\n                yield dict(process_person(person))\n                \n        def process_entries(bib_data):\n            for key, entry in bib_data.iteritems():\n                fields = dict(entry.fields)\n                fields['type'] = entry.type\n                fields.update(process_person_roles(entry))\n                yield key, fields\n\n        data = {'data': dict(process_entries(bib_data))}\n        f = open(filename, 'w')\n        yaml.safe_dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n        f.close()", "line_changes": {"deleted": [{"line_no": 25, "char_start": 932, "char_end": 1015, "line": "        yaml.dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n"}], "added": [{"line_no": 25, "char_start": 932, "char_end": 1020, "line": "        yaml.safe_dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 945, "char_end": 950, "chars": "safe_"}]}, "commit_link": "github.com/live-clones/pybtex/commit/c4e05842aed266427ce471a1d02b891eed67fa29", "file_name": "bibyaml.py", "vul_type": "cwe-502", "commit_msg": "YAML: use safe_dump and safe_load", "parent_commit": "5abe83ed0c01cbc8a43ec9395797d5b0060e0066", "description": "Write a Python function to process bibliographic data and save it to a YAML file."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, config_fname=None):\n        config_fname = config_fname or self.config_fname\n        fo = open(config_fname, \"r\")\n        blob = fo.read()\n        fo.close()\n        self.config = yaml.load(blob)", "func_src_after": "    def __init__(self, config_fname=None):\n        config_fname = config_fname or self.config_fname\n        fo = open(config_fname, \"r\")\n        blob = fo.read()\n        fo.close()\n        yaml=YAML(typ='safe')\n        self.config = yaml.load(blob)", "line_changes": {"deleted": [], "added": [{"line_no": 6, "char_start": 181, "char_end": 211, "line": "        yaml=YAML(typ='safe')\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 181, "char_end": 211, "chars": "        yaml=YAML(typ='safe')\n"}]}, "commit_link": "github.com/royrapoport/destalinator/commit/660ccd202e627cc8938a47532c7607edc676963f", "file_name": "config.py", "vul_type": "cwe-502", "commit_msg": "fix for YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe.", "parent_commit": "ef4a53784cd0026947df3f58cab3657a24e91112", "description": "Write a Python class initializer that reads a configuration file using YAML."}
{"func_name": "parse", "func_src_before": "    @staticmethod\n    def parse(path, require_exists=True, require_parses=True):\n        if not os.path.isfile(path):\n            if require_exists:\n                raise ConfigError('not found: ' + path)\n            else:\n                return None\n        try:\n            with open(path) as f:\n                return yaml.load(f)\n        except Exception, error:\n            if require_parses:\n                raise ConfigError('parse error: ' + path)", "func_src_after": "    @staticmethod\n    def parse(path, require_exists=True, require_parses=True):\n        if not os.path.isfile(path):\n            if require_exists:\n                raise ConfigError('not found: ' + path)\n            else:\n                return None\n        try:\n            with open(path) as f:\n                return yaml.safe_load(f)\n        except Exception, error:\n            if require_parses:\n                raise ConfigError('parse error: ' + path)", "line_changes": {"deleted": [{"line_no": 10, "char_start": 298, "char_end": 334, "line": "                return yaml.load(f)\n"}], "added": [{"line_no": 10, "char_start": 298, "char_end": 339, "line": "                return yaml.safe_load(f)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 326, "char_end": 331, "chars": "safe_"}]}, "commit_link": "github.com/silas/rock/commit/0852b4f55891e5dfe7cc6af3881942484daf9132", "file_name": "config.py", "vul_type": "cwe-502", "commit_msg": "Use yaml.safe_load instead of yaml.load", "parent_commit": "93a26daa34d92236cfcfe4e7ccf0d4814687c009", "description": "Create a Python function that loads a YAML file, with options to enforce file existence and successful parsing."}
{"func_name": "_migrate_map", "func_src_before": "def _migrate_map(contents):\n    # Find the first non-header line\n    lines = contents.splitlines(True)\n    i = 0\n    while _is_header_line(lines[i]):\n        i += 1\n\n    header = ''.join(lines[:i])\n    rest = ''.join(lines[i:])\n\n    if isinstance(ordered_load(contents), list):\n        # If they are using the \"default\" flow style of yaml, this operation\n        # will yield a valid configuration\n        try:\n            trial_contents = header + 'repos:\\n' + rest\n            yaml.load(trial_contents)\n            contents = trial_contents\n        except yaml.YAMLError:\n            contents = header + 'repos:\\n' + _indent(rest)\n\n    return contents", "func_src_after": "def _migrate_map(contents):\n    # Find the first non-header line\n    lines = contents.splitlines(True)\n    i = 0\n    while _is_header_line(lines[i]):\n        i += 1\n\n    header = ''.join(lines[:i])\n    rest = ''.join(lines[i:])\n\n    if isinstance(ordered_load(contents), list):\n        # If they are using the \"default\" flow style of yaml, this operation\n        # will yield a valid configuration\n        try:\n            trial_contents = header + 'repos:\\n' + rest\n            ordered_load(trial_contents)\n            contents = trial_contents\n        except yaml.YAMLError:\n            contents = header + 'repos:\\n' + _indent(rest)\n\n    return contents", "line_changes": {"deleted": [{"line_no": 16, "char_start": 467, "char_end": 505, "line": "            yaml.load(trial_contents)\n"}], "added": [{"line_no": 16, "char_start": 467, "char_end": 508, "line": "            ordered_load(trial_contents)\n"}]}, "char_changes": {"deleted": [{"char_start": 479, "char_end": 484, "chars": "yaml."}], "added": [{"char_start": 479, "char_end": 487, "chars": "ordered_"}]}, "commit_link": "github.com/pre-commit/pre-commit/commit/c3e438379ae369cd00e19c6ca388af1d15e59aea", "file_name": "migrate_config.py", "vul_type": "cwe-502", "commit_msg": "Appease yaml.load linters\n\n_technically_ yaml.load is unsafe, however the contents being loaded here are\npreviously loaded just above using a safe loader so this is not an abitrary\ncode vector.  Fixing it nonetheless :)", "parent_commit": "ebb178a7498996c62c477618fcd80ecd83169186", "description": "Write a Python function to adjust YAML content by ensuring a 'repos' section is present after the header."}
{"func_name": "update_device", "func_src_before": "    def update_device(self, **kwargs):\n        \"\"\" See http://api.device42.com/#create/update-device-by-name \"\"\"\n        path = 'devices'\n        atleast_fields = \"name serial_no uuid\".split()\n        known_fields = \"new_name asset_no manufacturer hardware new_hardware is_it_switch\"\n        known_fields += \" is_it_virtual_host is_it_blade_host in_service type service_level virtual_host\"\n        known_fields += \" blade_host slot_no storage_room_id storage_room os osver osverno memory cpucount cpupower cpucore\"\n        known_fields += \" hddcount hddsize hddraid hddraid_type macaddress devices_in_cluster appcomps\"\n        known_fields += \" customer contract_id contract\"\n        known_fields += \" aliases subtype virtual_subtype notes tags\"\n        known_fields = atleast_fields + known_fields.split()\n        if not set(atleast_fields).intersection(kwargs.keys()):\n            raise Device42BadArgumentError(\"At least one parameter should be passed: %s\" % atleast_fields)\n        unknown_fields = set(kwargs.keys()) - set(known_fields)\n        if unknown_fields:\n            raise Device42BadArgumentError(\"Unknown parameters: %s\" % unknown_fields)\n        return self._post(path, data=kwargs)", "func_src_after": "    def update_device(self, **kwargs):\n        \"\"\" See http://api.device42.com/#create/update-device-by-name \"\"\"\n        path = 'devices'\n        atleast_fields = [\"name\"]  # this is the only required field to create/update a device, serial and uuid opt\n        known_fields = \"new_name asset_no manufacturer hardware new_hardware is_it_switch\"\n        known_fields += \" is_it_virtual_host is_it_blade_host in_service type service_level virtual_host\"\n        known_fields += \" serial_no uuid\"\n        known_fields += \" blade_host slot_no storage_room_id storage_room os osver osverno memory cpucount cpupower cpucore\"\n        known_fields += \" hddcount hddsize hddraid hddraid_type macaddress devices_in_cluster appcomps\"\n        known_fields += \" customer contract_id contract\"\n        known_fields += \" aliases subtype virtual_subtype notes tags\"\n        known_fields = atleast_fields + known_fields.split()\n        if not set(atleast_fields).intersection(kwargs.keys()):\n            raise Device42BadArgumentError(\"At least one parameter should be passed: %s\" % atleast_fields)\n        unknown_fields = set(kwargs.keys()) - set(known_fields)\n        if unknown_fields:\n            raise Device42BadArgumentError(\"Unknown parameters: %s\" % unknown_fields)\n        return self._post(path, data=kwargs)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 138, "char_end": 193, "line": "        atleast_fields = \"name serial_no uuid\".split()\n"}], "added": [{"line_no": 4, "char_start": 138, "char_end": 254, "line": "        atleast_fields = [\"name\"]  # this is the only required field to create/update a device, serial and uuid opt\n"}, {"line_no": 7, "char_start": 451, "char_end": 493, "line": "        known_fields += \" serial_no uuid\"\n"}]}, "char_changes": {"deleted": [{"char_start": 175, "char_end": 178, "chars": "_no"}, {"char_start": 183, "char_end": 192, "chars": "\".split()"}], "added": [{"char_start": 163, "char_end": 164, "chars": "["}, {"char_start": 169, "char_end": 233, "chars": "\"]  # this is the only required field to create/update a device,"}, {"char_start": 240, "char_end": 244, "chars": " and"}, {"char_start": 249, "char_end": 253, "chars": " opt"}, {"char_start": 451, "char_end": 493, "chars": "        known_fields += \" serial_no uuid\"\n"}]}, "commit_link": "github.com/device42/puppet_to_device42_sync_py/commit/b394ec75a10a60fd38ba203c2c43af16f76295cf", "file_name": "device42.py", "vul_type": "cwe-502", "commit_msg": "changed intersection to just look for name since it was allowing devices with no name to enter post, fixed yaml load warning by safe loading insted of deprecated method, debug message change", "parent_commit": "df122e3ff26252dccde61a10e8ccbee2b94909e2", "description": "Write a Python function to update a device's details using the Device42 API, handling required fields and validating known fields."}
{"func_name": "dump", "func_src_before": "    def dump(self, path):\n        \"\"\"\n        dump address space as binary to file\n        \"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump(self._nodes, f, pickle.HIGHEST_PROTOCOL)", "func_src_after": "    def dump(self, path):\n        \"\"\"\n        dump address space as binary to file\n        \"\"\"\n        s = shelve.open(path, \"n\", protocol = pickle.HIGHEST_PROTOCOL)\n        for nodeid in self._nodes.keys():\n            s[nodeid.to_string()] = self._nodes[nodeid]\n        s.close()", "line_changes": {"deleted": [{"line_no": 5, "char_start": 95, "char_end": 131, "line": "        with open(path, 'wb') as f:\n"}, {"line_no": 6, "char_start": 131, "char_end": 195, "line": "            pickle.dump(self._nodes, f, pickle.HIGHEST_PROTOCOL)\n"}], "added": [{"line_no": 5, "char_start": 95, "char_end": 166, "line": "        s = shelve.open(path, \"n\", protocol = pickle.HIGHEST_PROTOCOL)\n"}, {"line_no": 6, "char_start": 166, "char_end": 208, "line": "        for nodeid in self._nodes.keys():\n"}, {"line_no": 7, "char_start": 208, "char_end": 264, "line": "            s[nodeid.to_string()] = self._nodes[nodeid]\n"}, {"line_no": 8, "char_start": 264, "char_end": 281, "line": "        s.close()\n"}]}, "char_changes": {"deleted": [{"char_start": 103, "char_end": 194, "chars": "with open(path, 'wb') as f:\n            pickle.dump(self._nodes, f, pickle.HIGHEST_PROTOCOL"}], "added": [{"char_start": 103, "char_end": 280, "chars": "s = shelve.open(path, \"n\", protocol = pickle.HIGHEST_PROTOCOL)\n        for nodeid in self._nodes.keys():\n            s[nodeid.to_string()] = self._nodes[nodeid]\n        s.close("}]}, "commit_link": "github.com/bitkeeper/python-opcua/commit/2fcbbf85a143195a3b439930056bf78908fe9509", "file_name": "address_space.py", "vul_type": "cwe-502", "commit_msg": "Perform lazy loading when restoring a cached address space\n\nWhen starting an opcua server, the creation of the address space is\ncurrently a performance bottleneck. The startup process can be accelerated\nby loading a pre-generated address space pickle.\nHowever, the startup process still takes ~25 seconds on a raspberry pi\nmodel b (compared to ~125 seconds when generating the address space from code).\nStoring the address space in a shelve, where the data for each node is\npickeled individually, allows to further improve the startup performance\nsince only the nodes that are actually accessed are loaded from\ndisc (all other nodes are loaded later when they are accessed).\nSince the default address space contains thousands of nodes but just a\nsmall amount is actually accessed during startup, the startuptime could\nbe improved to ~3.5 seconds.", "parent_commit": "2f5a26fba77d7870be219a5cba2c2e0a0144830c", "description": "Write a Python function named `dump` that saves an object's data to a binary file at a specified path."}
{"func_name": "test_feature_tags", "func_src_before": "def test_feature_tags():\n\n    with open(util.base_dir() + \"/mapper.yaml\", \"r\") as mapper_file:\n        mapper_content = mapper_file.read()\n    mapper_yaml = yaml.load(mapper_content)\n    testmappers = [x for x in mapper_yaml[\"testmapper\"]]\n    mapper_tests = [\n        list(x.keys())[0] for tm in testmappers for x in mapper_yaml[\"testmapper\"][tm]\n    ]\n\n    def check_ver(tag):\n        for ver_prefix, ver_len in [\n            [\"ver\", 3],\n            [\"rhelver\", 2],\n            [\"fedoraver\", 1],\n        ]:\n            if not tag.startswith(ver_prefix):\n                continue\n            op, ver = misc.test_version_tag_parse(tag, ver_prefix)\n            assert type(op) is str\n            assert type(ver) is list\n            assert op in [\"+\", \"+=\", \"-\", \"-=\"]\n            assert ver\n            assert all([type(v) is int for v in ver])\n            assert all([v >= 0 for v in ver])\n            assert len(ver) <= ver_len\n            assert tag.startswith(ver_prefix + op)\n            return True\n        return tag in [\n            \"rhel_pkg\",\n            \"not_with_rhel_pkg\",\n            \"fedora_pkg\",\n            \"not_with_fedora_pkg\",\n        ]\n\n    def check_bugzilla(tag):\n        if tag.startswith(\"rhbz\"):\n            assert re.match(\"^rhbz[0-9]+$\", tag)\n            return True\n        if tag.startswith(\"gnomebz\"):\n            assert re.match(\"^gnomebz[0-9]+$\", tag)\n            return True\n        return False\n\n    def check_registry(tag):\n        return tag in tag_registry.tag_registry\n\n    def check_mapper(tag):\n        return tag in mapper_tests\n\n    for feature in [\"nmcli\", \"nmtui\"]:\n        all_tags = misc.test_load_tags_from_features(feature)\n\n        tag_registry_used = set()\n        unique_tags = set()\n        for tags in all_tags:\n            assert tags\n            assert type(tags) is list\n            test_in_mapper = False\n            for tag in tags:\n                assert type(tag) is str\n                assert tag\n                assert re.match(\"^[-a-z_.A-Z0-9+=]+$\", tag)\n                assert re.match(\"^\" + misc.TEST_NAME_VALID_CHAR_REGEX + \"+$\", tag)\n                assert tags.count(tag) == 1, f'tag \"{tag}\" is not unique in {tags}'\n                is_ver = check_ver(tag)\n                is_bugzilla = check_bugzilla(tag)\n                is_registry = check_registry(tag)\n                is_mapper = check_mapper(tag)\n                test_in_mapper = test_in_mapper or is_mapper\n                if is_registry:\n                    tag_registry_used.add(tag)\n                assert (\n                    is_ver or is_bugzilla or is_registry or is_mapper\n                ), f'tag \"{tag}\" has no effect'\n                assert [is_ver, is_bugzilla, is_registry, is_mapper].count(True) == 1, (\n                    f'tag \"{tag}\" is multipurpose ({\"mapper, \" if is_mapper else \"\"}'\n                    f'{\"registry, \" if is_registry else \"\"}{\"ver, \" if is_ver else \"\"}'\n                    f'{\"bugzilla, \" if is_bugzilla else \"\"})'\n                )\n\n            assert test_in_mapper, f\"none of {tags} is in mapper\"\n\n            tt = tuple(tags)\n            if tt in unique_tags:\n                pytest.fail(f'tags \"{tags}\" are duplicate over the {feature} tests')\n            unique_tags.add(tt)", "func_src_after": "def test_feature_tags():\n\n    with open(util.base_dir() + \"/mapper.yaml\", \"r\") as mapper_file:\n        mapper_content = mapper_file.read()\n    mapper_yaml = yaml.load(mapper_content, Loader=yaml.BaseLoader)\n    testmappers = [x for x in mapper_yaml[\"testmapper\"]]\n    mapper_tests = [\n        list(x.keys())[0] for tm in testmappers for x in mapper_yaml[\"testmapper\"][tm]\n    ]\n\n    def check_ver(tag):\n        for ver_prefix, ver_len in [\n            [\"ver\", 3],\n            [\"rhelver\", 2],\n            [\"fedoraver\", 1],\n        ]:\n            if not tag.startswith(ver_prefix):\n                continue\n            op, ver = misc.test_version_tag_parse(tag, ver_prefix)\n            assert type(op) is str\n            assert type(ver) is list\n            assert op in [\"+\", \"+=\", \"-\", \"-=\"]\n            assert ver\n            assert all([type(v) is int for v in ver])\n            assert all([v >= 0 for v in ver])\n            assert len(ver) <= ver_len\n            assert tag.startswith(ver_prefix + op)\n            return True\n        return tag in [\n            \"rhel_pkg\",\n            \"not_with_rhel_pkg\",\n            \"fedora_pkg\",\n            \"not_with_fedora_pkg\",\n        ]\n\n    def check_bugzilla(tag):\n        if tag.startswith(\"rhbz\"):\n            assert re.match(\"^rhbz[0-9]+$\", tag)\n            return True\n        if tag.startswith(\"gnomebz\"):\n            assert re.match(\"^gnomebz[0-9]+$\", tag)\n            return True\n        return False\n\n    def check_registry(tag):\n        return tag in tag_registry.tag_registry\n\n    def check_mapper(tag):\n        return tag in mapper_tests\n\n    for feature in [\"nmcli\", \"nmtui\"]:\n        all_tags = misc.test_load_tags_from_features(feature)\n\n        tag_registry_used = set()\n        unique_tags = set()\n        for tags in all_tags:\n            assert tags\n            assert type(tags) is list\n            test_in_mapper = False\n            for tag in tags:\n                assert type(tag) is str\n                assert tag\n                assert re.match(\"^[-a-z_.A-Z0-9+=]+$\", tag)\n                assert re.match(\"^\" + misc.TEST_NAME_VALID_CHAR_REGEX + \"+$\", tag)\n                assert tags.count(tag) == 1, f'tag \"{tag}\" is not unique in {tags}'\n                is_ver = check_ver(tag)\n                is_bugzilla = check_bugzilla(tag)\n                is_registry = check_registry(tag)\n                is_mapper = check_mapper(tag)\n                test_in_mapper = test_in_mapper or is_mapper\n                if is_registry:\n                    tag_registry_used.add(tag)\n                assert (\n                    is_ver or is_bugzilla or is_registry or is_mapper\n                ), f'tag \"{tag}\" has no effect'\n                assert [is_ver, is_bugzilla, is_registry, is_mapper].count(True) == 1, (\n                    f'tag \"{tag}\" is multipurpose ({\"mapper, \" if is_mapper else \"\"}'\n                    f'{\"registry, \" if is_registry else \"\"}{\"ver, \" if is_ver else \"\"}'\n                    f'{\"bugzilla, \" if is_bugzilla else \"\"})'\n                )\n\n            assert test_in_mapper, f\"none of {tags} is in mapper\"\n\n            tt = tuple(tags)\n            if tt in unique_tags:\n                pytest.fail(f'tags \"{tags}\" are duplicate over the {feature} tests')\n            unique_tags.add(tt)", "line_changes": {"deleted": [{"line_no": 5, "char_start": 139, "char_end": 183, "line": "    mapper_yaml = yaml.load(mapper_content)\n"}], "added": [{"line_no": 5, "char_start": 139, "char_end": 207, "line": "    mapper_yaml = yaml.load(mapper_content, Loader=yaml.BaseLoader)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 181, "char_end": 205, "chars": ", Loader=yaml.BaseLoader"}]}, "commit_link": "github.com/NetworkManager/NetworkManager-ci/commit/e8a0a1686315dc988b3600ce80ff3953cccb7b4b", "file_name": "test.py", "vul_type": "cwe-502", "commit_msg": "nmci/test: avoid deprecation warning for yaml.load()\n\nAvoids:\n\n  nmci/test.py::test_feature_tags\n    /TMP/NetworkManager-ci/nmci/test.py:411: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n      mapper_yaml = yaml.load(mapper_content)\n\n  -- Docs: https://docs.pytest.org/en/stable/warnings.html", "parent_commit": "5e80ca2fac5f2fa62e0daccc6703b69694a97f51", "description": "Write a Python function to validate feature tags against various criteria, including version tags, bugzilla tags, and a tag registry."}
{"func_name": "load_data", "func_src_before": "def load_data(path):\n    \"\"\"Given path to a file, load data from it.\"\"\"\n    ext = os.path.splitext(path)[-1]\n    loader = None\n    if ext in {'.yml', '.yaml'}:\n        loader = yaml\n        if yaml is None:\n            req_missing(['yaml'], 'use YAML data files')\n            return {}\n    elif ext in {'.json', '.js'}:\n        loader = json\n    elif ext in {'.toml', '.tml'}:\n        if toml is None:\n            req_missing(['toml'], 'use TOML data files')\n            return {}\n        loader = toml\n    if loader is None:\n        return\n    with io.open(path, 'r', encoding='utf8') as inf:\n        return loader.load(inf)", "func_src_after": "def load_data(path):\n    \"\"\"Given path to a file, load data from it.\"\"\"\n    ext = os.path.splitext(path)[-1]\n    loader = None\n    function = 'load'\n    if ext in {'.yml', '.yaml'}:\n        loader = yaml\n        function = 'safe_load'\n        if yaml is None:\n            req_missing(['yaml'], 'use YAML data files')\n            return {}\n    elif ext in {'.json', '.js'}:\n        loader = json\n    elif ext in {'.toml', '.tml'}:\n        if toml is None:\n            req_missing(['toml'], 'use TOML data files')\n            return {}\n        loader = toml\n    if loader is None:\n        return\n    with io.open(path, 'r', encoding='utf8') as inf:\n        return getattr(loader, function)(inf)", "line_changes": {"deleted": [{"line_no": 20, "char_start": 594, "char_end": 625, "line": "        return loader.load(inf)\n"}], "added": [{"line_no": 5, "char_start": 127, "char_end": 149, "line": "    function = 'load'\n"}, {"line_no": 8, "char_start": 204, "char_end": 235, "line": "        function = 'safe_load'\n"}, {"line_no": 22, "char_start": 647, "char_end": 692, "line": "        return getattr(loader, function)(inf)\n"}]}, "char_changes": {"deleted": [{"char_start": 609, "char_end": 620, "chars": "loader.load"}], "added": [{"char_start": 127, "char_end": 149, "chars": "    function = 'load'\n"}, {"char_start": 204, "char_end": 235, "chars": "        function = 'safe_load'\n"}, {"char_start": 662, "char_end": 687, "chars": "getattr(loader, function)"}]}, "commit_link": "github.com/xuhdev/nikola/commit/1d507071e6a60523d8bda4ae401d309b3bcd27d7", "file_name": "utils.py", "vul_type": "cwe-502", "commit_msg": "Use safe_load for loading YAML\n\nSigned-off-by: Chris Warrick <kwpolska@gmail.com>", "parent_commit": "2d25ac7de933000fd44167743ca8293709debb24", "description": "Write a Python function to load data from a file, supporting multiple file formats based on the file extension."}
{"func_name": "test_list_kube_config_contexts", "func_src_before": "    def test_list_kube_config_contexts(self):\n        config_file = self._create_temp_file(yaml.dump(self.TEST_KUBE_CONFIG))\n        contexts, active_context = list_kube_config_contexts(\n            config_file=config_file)\n        self.assertDictEqual(self.TEST_KUBE_CONFIG['contexts'][0],\n                             active_context)\n        if PY3:\n            self.assertCountEqual(self.TEST_KUBE_CONFIG['contexts'],\n                                  contexts)\n        else:\n            self.assertItemsEqual(self.TEST_KUBE_CONFIG['contexts'],\n                                  contexts)", "func_src_after": "    def test_list_kube_config_contexts(self):\n        config_file = self._create_temp_file(yaml.safe_dump(self.TEST_KUBE_CONFIG))\n        contexts, active_context = list_kube_config_contexts(\n            config_file=config_file)\n        self.assertDictEqual(self.TEST_KUBE_CONFIG['contexts'][0],\n                             active_context)\n        if PY3:\n            self.assertCountEqual(self.TEST_KUBE_CONFIG['contexts'],\n                                  contexts)\n        else:\n            self.assertItemsEqual(self.TEST_KUBE_CONFIG['contexts'],\n                                  contexts)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 46, "char_end": 125, "line": "        config_file = self._create_temp_file(yaml.dump(self.TEST_KUBE_CONFIG))\n"}], "added": [{"line_no": 2, "char_start": 46, "char_end": 130, "line": "        config_file = self._create_temp_file(yaml.safe_dump(self.TEST_KUBE_CONFIG))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 96, "char_end": 101, "chars": "safe_"}]}, "commit_link": "github.com/kubernetes-client/python/commit/ebb49d02ed90256cd002d1d75cb8a92125c4392e", "file_name": "kube_config_test.py", "vul_type": "cwe-502", "commit_msg": "Use safe_load and safe_dump for all yaml calls", "parent_commit": "5c242ead602797ae870798882654f7a2a4edfe39", "description": "Write a Python function to test listing Kubernetes config contexts and checking the active context."}
{"func_name": "_yaml_to_config", "func_src_before": "    def _yaml_to_config(self, config_file):\n         self.config = yaml.load(config_file)", "func_src_after": "    def _yaml_to_config(self, config_file):\n        self.config = yaml.safe_load(config_file)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 44, "char_end": 89, "line": "         self.config = yaml.load(config_file)\n"}], "added": [{"line_no": 2, "char_start": 44, "char_end": 93, "line": "        self.config = yaml.safe_load(config_file)\n"}]}, "char_changes": {"deleted": [{"char_start": 52, "char_end": 53, "chars": " "}], "added": [{"char_start": 71, "char_end": 76, "chars": "safe_"}]}, "commit_link": "github.com/darylmathison/github-user-queries/commit/1fb6138eebd8f0386312aa1f0fee5df603f93aba", "file_name": "util.py", "vul_type": "cwe-502", "commit_msg": "Replaced 'load' with 'safe_load'\n\nRefers-to: #24", "parent_commit": "2811a7c65abf513103e615f63f769bb5ca279902", "description": "Write a Python function that loads a configuration from a YAML file into an object's attribute."}
{"func_name": "hierarchical_tile", "func_src_before": "def hierarchical_tile(masterfile,tilefile):\n\n    \"\"\"\n    Create Hierarchical tile from Master prior\n\n    :param masterfile: Master prior file\n    :param tilefile:  File containing Tiling scheme\n    \"\"\"\n    try:\n        taskid = np.int(os.environ['SGE_TASK_ID'])\n        task_first=np.int(os.environ['SGE_TASK_FIRST'])\n        task_last=np.int(os.environ['SGE_TASK_LAST'])\n\n    except KeyError:\n        print(\"Error: could not read SGE_TASK_ID from environment\")\n        taskid = int(input(\"Please enter task id: \"))\n        print(\"you entered\", taskid)\n\n\n    with open(tilefile, 'rb') as f:\n        obj = pickle.load(f)\n\n    tiles = obj['tiles']\n    order = obj['order']\n    tiles_large = obj['tiles_large']\n    order_large = obj['order_large']\n\n    with open(masterfile, 'rb') as f:\n        obj = pickle.load(f)\n    priors = obj['priors']\n\n    moc = moc_routines.get_fitting_region(order_large, tiles_large[taskid - 1])\n    for p in priors:\n        p.moc = moc\n        p.cut_down_prior()\n\n    outfile = 'Tile_'+ str(tiles_large[taskid - 1]) + '_' + str(order_large) + '.pkl'\n    with open(outfile, 'wb') as f:\n        pickle.dump({'priors':priors, 'version':xidplus.io.git_version()}, f)", "func_src_after": "def hierarchical_tile(masterfile,tilefile):\n\n    \"\"\"\n    Create Hierarchical tile from Master prior\n\n    :param masterfile: Master prior file\n    :param tilefile:  File containing Tiling scheme\n    \"\"\"\n    try:\n        taskid = np.int(os.environ['SGE_TASK_ID'])\n        task_first=np.int(os.environ['SGE_TASK_FIRST'])\n        task_last=np.int(os.environ['SGE_TASK_LAST'])\n\n    except KeyError:\n        print(\"Error: could not read SGE_TASK_ID from environment\")\n        taskid = int(input(\"Please enter task id: \"))\n        print(\"you entered\", taskid)\n\n\n    with open(tilefile, 'rb') as f:\n        obj = pickle.load(f)\n\n    tiles = obj['tiles']\n    order = obj['order']\n    tiles_large = obj['tiles_large']\n    order_large = obj['order_large']\n\n    obj=xidplus.io.pickle_load(masterfile)\n    priors = obj['priors']\n\n    moc = moc_routines.get_fitting_region(order_large, tiles_large[taskid - 1])\n    for p in priors:\n        p.moc = moc\n        p.cut_down_prior()\n\n    outfile = 'Tile_'+ str(tiles_large[taskid - 1]) + '_' + str(order_large) + '.pkl'\n    with open(outfile, 'wb') as f:\n        pickle.dump({'priors':priors, 'version':xidplus.io.git_version()}, f)", "line_changes": {"deleted": [{"line_no": 28, "char_start": 746, "char_end": 784, "line": "    with open(masterfile, 'rb') as f:\n"}, {"line_no": 29, "char_start": 784, "char_end": 813, "line": "        obj = pickle.load(f)\n"}], "added": [{"line_no": 28, "char_start": 746, "char_end": 789, "line": "    obj=xidplus.io.pickle_load(masterfile)\n"}]}, "char_changes": {"deleted": [{"char_start": 750, "char_end": 798, "chars": "with open(masterfile, 'rb') as f:\n        obj = "}, {"char_start": 804, "char_end": 805, "chars": "."}, {"char_start": 810, "char_end": 811, "chars": "f"}], "added": [{"char_start": 750, "char_end": 765, "chars": "obj=xidplus.io."}, {"char_start": 771, "char_end": 772, "chars": "_"}, {"char_start": 777, "char_end": 787, "chars": "masterfile"}]}, "commit_link": "github.com/pdh21/XID_plus/commit/4606071fb022c59711f41bbf5687f4c9b87a65d1", "file_name": "HPC.py", "vul_type": "cwe-502", "commit_msg": "hierarchical load now uses the large pickle file fix", "parent_commit": "e11847fd0cd10570cc285d3907e3888f796e1ba4", "description": "In Python, write a function named `hierarchical_tile` that processes tiling information from two files and outputs a modified tile file."}
{"func_name": "_drain_to_working_set", "func_src_before": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with open(self.working_set_filename, 'wb') as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        line = base64.b64encode(pickle.dumps({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }))\n                        work_file.write(line)\n                        work_file.write(b'\\n')\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )", "func_src_after": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        pickle.dump({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }, work_file)\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )\n\n                pickle.dump('eof', work_file)", "line_changes": {"deleted": [{"line_no": 12, "char_start": 365, "char_end": 434, "line": "            with open(self.working_set_filename, 'wb') as work_file:\n"}, {"line_no": 27, "char_start": 839, "char_end": 902, "line": "                        line = base64.b64encode(pickle.dumps({\n"}, {"line_no": 34, "char_start": 1228, "char_end": 1256, "line": "                        }))\n"}, {"line_no": 35, "char_start": 1256, "char_end": 1302, "line": "                        work_file.write(line)\n"}, {"line_no": 36, "char_start": 1302, "char_end": 1349, "line": "                        work_file.write(b'\\n')\n"}], "added": [{"line_no": 12, "char_start": 365, "char_end": 456, "line": "            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n"}, {"line_no": 27, "char_start": 861, "char_end": 899, "line": "                        pickle.dump({\n"}, {"line_no": 34, "char_start": 1225, "char_end": 1263, "line": "                        }, work_file)\n"}]}, "char_changes": {"deleted": [{"char_start": 863, "char_end": 887, "chars": "line = base64.b64encode("}, {"char_start": 898, "char_end": 899, "chars": "s"}, {"char_start": 1253, "char_end": 1347, "chars": "))\n                        work_file.write(line)\n                        work_file.write(b'\\n'"}], "added": [{"char_start": 382, "char_end": 387, "chars": "gzip."}, {"char_start": 423, "char_end": 440, "chars": ", compresslevel=1"}, {"char_start": 1250, "char_end": 1261, "chars": ", work_file"}, {"char_start": 2534, "char_end": 2581, "chars": "\n\n                pickle.dump('eof', work_file)"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to export a dataset to a file, with optional data filtering and deletion after export."}
{"func_name": "__init__", "func_src_before": "    def __init__(self,p):\n        p = pickle.loads(p)\n        try:\n            self.tokens = np.array([symbolToIndex[\"START\"]] + [ symbolToIndex[s] for s in serializeProgram(p) ] + [symbolToIndex[\"END\"]])\n        except KeyError:\n            print \"Key error in tokenization\",serializeProgram(p)\n            assert False\n        \n        self.image = p.convertToSequence().draw()\n        self.program = p\n\n        if str(parseOutput(serializeProgram(p))) != str(p):\n            print \"Serialization failure for program\",p\n            print serializeProgram(p)\n            print parseOutput(serializeProgram(p))\n            assert False", "func_src_after": "    def __init__(self,p):\n        try:\n            self.tokens = np.array([symbolToIndex[\"START\"]] + [ symbolToIndex[s] for s in serializeProgram(p) ] + [symbolToIndex[\"END\"]])\n        except KeyError:\n            print \"Key error in tokenization\",serializeProgram(p)\n            assert False\n        \n        self.image = p.convertToSequence().draw()\n        self.program = p\n\n        if str(parseOutput(serializeProgram(p))) != str(p):\n            print \"Serialization failure for program\",p\n            print serializeProgram(p)\n            print parseOutput(serializeProgram(p))\n            assert False", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 54, "line": "        p = pickle.loads(p)\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 26, "char_end": 54, "chars": "        p = pickle.loads(p)\n"}], "added": []}, "commit_link": "github.com/ellisk42/TikZ/commit/66ab87a1b9a4129fe6f2bc7645a17899f35c9c8b", "file_name": "noTraceBaseline.py", "vul_type": "cwe-502", "commit_msg": "fixed bug in pickle loading", "parent_commit": "c5bb3ad10611b779342b1866cad8961b4a3287ba", "description": "Write a Python class initializer that tokenizes a serialized program, generates an image from it, and checks for serialization consistency."}
{"func_name": "test_verilator_run", "func_src_before": "def test_verilator_run():\n    import os.path\n    import tempfile\n    import yaml\n    from edalize import get_edatool\n    ref_dir_cc = os.path.join(ref_dir, 'cc')\n\n    work_root    = tempfile.mkdtemp()\n    edam_file = os.path.join(ref_dir_cc, core_name)+ '.eda.yml'\n    backend = get_edatool(tool)(edam=yaml.load(open(edam_file)), work_root=work_root)\n    dummy_exe = 'V'+backend.tool_options['top_module']\n    shutil.copy(os.path.join(ref_dir, dummy_exe),\n                os.path.join(work_root, dummy_exe))\n\n    backend.run(params)\n\n    compare_files(ref_dir, work_root, ['run.cmd'])", "func_src_after": "def test_verilator_run():\n    import os.path\n    import tempfile\n    import yaml\n    from edalize import get_edatool\n    ref_dir_cc = os.path.join(ref_dir, 'cc')\n\n    work_root    = tempfile.mkdtemp()\n    edam_file = os.path.join(ref_dir_cc, core_name)+ '.eda.yml'\n    backend = get_edatool(tool)(edam=yaml.safe_load(open(edam_file)), work_root=work_root)\n    dummy_exe = 'V'+backend.tool_options['top_module']\n    shutil.copy(os.path.join(ref_dir, dummy_exe),\n                os.path.join(work_root, dummy_exe))\n\n    backend.run(params)\n\n    compare_files(ref_dir, work_root, ['run.cmd'])", "line_changes": {"deleted": [{"line_no": 10, "char_start": 265, "char_end": 351, "line": "    backend = get_edatool(tool)(edam=yaml.load(open(edam_file)), work_root=work_root)\n"}], "added": [{"line_no": 10, "char_start": 265, "char_end": 356, "line": "    backend = get_edatool(tool)(edam=yaml.safe_load(open(edam_file)), work_root=work_root)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 307, "char_end": 312, "chars": "safe_"}]}, "commit_link": "github.com/SymbiFlow/edalize/commit/0a07c959386a5c8ffd88e5f985979e1a26646076", "file_name": "test_verilator.py", "vul_type": "cwe-502", "commit_msg": "Use safe YAML loader\n\nyaml.load() is unsafe and issues a warning. Switching to the safe loader\nexplicitly.", "parent_commit": "3faaeaefaf313aebd40f8f3782f07a61cdc5aaaa", "description": "Write a Python function that sets up and runs a hardware simulation tool using configuration from a YAML file."}
{"func_name": "_feed_input_sorters", "func_src_before": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with open(self.working_set_filename, 'rb') as work_file:\n            for line in work_file:\n                result = pickle.loads(base64.b64decode(line))\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "func_src_after": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with gzip.open(self.working_set_filename, 'rb') as work_file:\n            while True:\n                result = pickle.load(work_file)\n\n                if result == 'eof':\n                    break\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 60, "char_end": 125, "line": "        with open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 125, "char_end": 160, "line": "            for line in work_file:\n"}, {"line_no": 6, "char_start": 160, "char_end": 222, "line": "                result = pickle.loads(base64.b64decode(line))\n"}], "added": [{"line_no": 4, "char_start": 60, "char_end": 130, "line": "        with gzip.open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 130, "char_end": 154, "line": "            while True:\n"}, {"line_no": 6, "char_start": 154, "char_end": 202, "line": "                result = pickle.load(work_file)\n"}, {"line_no": 7, "char_start": 202, "char_end": 203, "line": "\n"}, {"line_no": 8, "char_start": 203, "char_end": 239, "line": "                if result == 'eof':\n"}, {"line_no": 9, "char_start": 239, "char_end": 265, "line": "                    break\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 157, "chars": "for line in work_fil"}, {"char_start": 196, "char_end": 221, "chars": "s(base64.b64decode(line))"}], "added": [{"char_start": 73, "char_end": 78, "chars": "gzip."}, {"char_start": 142, "char_end": 151, "chars": "while Tru"}, {"char_start": 190, "char_end": 264, "chars": "(work_file)\n\n                if result == 'eof':\n                    break"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to process and sort project results from a file, updating a sorter object for each project."}
{"func_name": "test_verilator_configure", "func_src_before": "def test_verilator_configure():\n    import os.path\n    import tempfile\n    import yaml\n    from edalize import get_edatool\n\n    for mode in ['cc', 'sc', 'lint-only']:\n        work_root    = tempfile.mkdtemp()\n        edam_file = os.path.join(ref_dir, mode, core_name) + '.eda.yml'\n\n        backend = get_edatool(tool)(edam=yaml.load(open(edam_file)), work_root=work_root)\n\n        if mode is 'cc':\n            _params = params\n        else:\n            _params = []\n        backend.configure(_params)\n\n        compare_files(ref_dir, work_root, ['Makefile'])\n\n        compare_files(os.path.join(ref_dir, mode),\n                      work_root,\n                      ['config.mk', core_name+'.vc'])", "func_src_after": "def test_verilator_configure():\n    import os.path\n    import tempfile\n    import yaml\n    from edalize import get_edatool\n\n    for mode in ['cc', 'sc', 'lint-only']:\n        work_root    = tempfile.mkdtemp()\n        edam_file = os.path.join(ref_dir, mode, core_name) + '.eda.yml'\n\n        backend = get_edatool(tool)(edam=yaml.safe_load(open(edam_file)), work_root=work_root)\n\n        if mode is 'cc':\n            _params = params\n        else:\n            _params = []\n        backend.configure(_params)\n\n        compare_files(ref_dir, work_root, ['Makefile'])\n\n        compare_files(os.path.join(ref_dir, mode),\n                      work_root,\n                      ['config.mk', core_name+'.vc'])", "line_changes": {"deleted": [{"line_no": 11, "char_start": 282, "char_end": 372, "line": "        backend = get_edatool(tool)(edam=yaml.load(open(edam_file)), work_root=work_root)\n"}], "added": [{"line_no": 11, "char_start": 282, "char_end": 377, "line": "        backend = get_edatool(tool)(edam=yaml.safe_load(open(edam_file)), work_root=work_root)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 328, "char_end": 333, "chars": "safe_"}]}, "commit_link": "github.com/SymbiFlow/edalize/commit/0a07c959386a5c8ffd88e5f985979e1a26646076", "file_name": "test_verilator.py", "vul_type": "cwe-502", "commit_msg": "Use safe YAML loader\n\nyaml.load() is unsafe and issues a warning. Switching to the safe loader\nexplicitly.", "parent_commit": "3faaeaefaf313aebd40f8f3782f07a61cdc5aaaa", "description": "Write a Python function that configures a hardware design tool using different modes and compares generated files with reference files."}
{"func_name": "exporters_v1tov2", "func_src_before": "def exporters_v1tov2(exporters_paths, shared_config={}, quiet=False):\n    \"\"\"Translate exporters to v2 and put into shared config.\n\n    Args:\n        exporters_path (list): List of exporters file paths.\n        shared_config (dict): Shared config to add exporters to.\n        quiet (bool): Quiet mode.\n\n    Returns:\n        list: List of exporters keys added to shared config.\n    \"\"\"\n    exp_keys = []\n    for exp_path in exporters_paths:\n        with open(exp_path, encoding='utf-8') as conf:\n            content = yaml.load(conf, Loader=yaml.Loader)\n        exporters = content\n\n        # If exporters file has sections, concatenate all of them\n        if isinstance(content, dict):\n            exporters = []\n            for _, value in content.items():\n                exporters.extend(value)\n\n        # If exporter not in general config, add it and add an alias for the\n        # exporter. Refer to the alias in the SLO config file.\n        for exporter in exporters:\n            exporter = OrderedDict(exporter)\n            exp_key = add_to_shared_config(exporter,\n                                           shared_config,\n                                           'exporters',\n                                           quiet=quiet)\n            exp_keys.append(exp_key)\n    return exp_keys", "func_src_after": "def exporters_v1tov2(exporters_paths, shared_config={}, quiet=False):\n    \"\"\"Translate exporters to v2 and put into shared config.\n\n    Args:\n        exporters_path (list): List of exporters file paths.\n        shared_config (dict): Shared config to add exporters to.\n        quiet (bool): Quiet mode.\n\n    Returns:\n        list: List of exporters keys added to shared config.\n    \"\"\"\n    exp_keys = []\n    for exp_path in exporters_paths:\n        with open(exp_path, encoding='utf-8') as conf:\n            content = yaml.load(conf, Loader=yaml.SafeLoader)\n        exporters = content\n\n        # If exporters file has sections, concatenate all of them\n        if isinstance(content, dict):\n            exporters = []\n            for _, value in content.items():\n                exporters.extend(value)\n\n        # If exporter not in general config, add it and add an alias for the\n        # exporter. Refer to the alias in the SLO config file.\n        for exporter in exporters:\n            exporter = OrderedDict(exporter)\n            exp_key = add_to_shared_config(exporter,\n                                           shared_config,\n                                           'exporters',\n                                           quiet=quiet)\n            exp_keys.append(exp_key)\n    return exp_keys", "line_changes": {"deleted": [{"line_no": 15, "char_start": 495, "char_end": 553, "line": "            content = yaml.load(conf, Loader=yaml.Loader)\n"}], "added": [{"line_no": 15, "char_start": 495, "char_end": 557, "line": "            content = yaml.load(conf, Loader=yaml.SafeLoader)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 545, "char_end": 549, "chars": "Safe"}]}, "commit_link": "github.com/google/slo-generator/commit/36318beab1b85d14bb860e45bea186b184690d5d", "file_name": "migrator.py", "vul_type": "cwe-502", "commit_msg": "fix: yaml loader security issue (#173)", "parent_commit": "50ce1bf81d7c6a97da52cf167b1d3ee8100ddd90", "description": "Write a Python function to update a shared configuration with exporter details from multiple YAML files."}
{"func_name": "main", "func_src_before": "def main(argv):\n\tparser = ArgumentParser(argv[0], description=__doc__,\n\t\tformatter_class=lambda prog: HelpFormatter(prog, max_help_position=10, width=120))\n\tparser.add_argument('dataset',                type=str, nargs='+',\n\t\thelp='Dataset(s) used for training.')\n\tparser.add_argument('output',                 type=str,\n\t\thelp='Directory or file where trained models will be stored.')\n\tparser.add_argument('--num_components', '-c', type=int,   default=3,\n\t\thelp='Number of components used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_features',   '-f', type=int,   default=2,\n\t\thelp='Number of quadratic features used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_models',     '-m', type=int,   default=4,\n\t\thelp='Number of models trained (predictions will be averaged across models, default: %(default)d).')\n\tparser.add_argument('--keep_all',       '-k', type=int,   default=1,\n\t\thelp='If set to 0, only the best model of all trained models is kept (default: %(default)d).')\n\tparser.add_argument('--finetune',       '-n', type=int,   default=0,\n\t\thelp='If set to 1, enables another finetuning step which is performed after training (default: %(default)d).')\n\tparser.add_argument('--num_train',      '-t', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells is used for training.')\n\tparser.add_argument('--num_valid',      '-s', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells will be used for early stopping based on validation error.')\n\tparser.add_argument('--var_explained',  '-e', type=float, default=95.,\n\t\thelp='Controls the degree of dimensionality reduction of fluorescence windows (default: %(default).0f).')\n\tparser.add_argument('--window_length',  '-w', type=float, default=1000.,\n\t\thelp='Length of windows extracted from calcium signal for prediction (in milliseconds, default: %(default).0f).')\n\tparser.add_argument('--regularize',     '-r', type=float, default=0.,\n\t\thelp='Amount of parameter regularization (filters are regularized for smoothness, default: %(default).1f).')\n\tparser.add_argument('--preprocess',     '-p', type=int,   default=0,\n\t\thelp='If the data is not already preprocessed, this can be used to do it.')\n\tparser.add_argument('--verbosity',      '-v', type=int,   default=1)\n\n\targs, _ = parser.parse_known_args(argv[1:])\n\n\texperiment = Experiment()\n\n\tif not args.dataset:\n\t\tprint 'You have to specify at least 1 dataset.'\n\t\treturn 0\n\n\tdata = []\n\tfor dataset in args.dataset:\n\t\twith open(dataset) as handle:\n\t\t\tdata = data + load(handle)\n\n\tif args.preprocess:\n\t\tdata = preprocess(data, args.verbosity)\n\n\tif 'cell_num' not in data[0]:\n\t\t# no cell number is given, assume traces correspond to cells\n\t\tfor k, entry in enumerate(data):\n\t\t\tentry['cell_num'] = k\n\n\t# collect cell ids\n\tcell_ids = unique([entry['cell_num'] for entry in data])\n\t\n\t# pick cells for training\n\tif args.num_train > 0:\n\t\ttraining_cells = random_select(args.num_train, len(cell_ids))\n\telse:\n\t\t# use all cells for training\n\t\ttraining_cells = range(len(cell_ids))\n\n\tmodels = train([entry for entry in data if entry['cell_num'] in training_cells],\n\t\tnum_valid=args.num_valid,\n\t\tnum_models=args.num_models,\n\t\tvar_explained=args.var_explained,\n\t\twindow_length=args.window_length,\n\t\tkeep_all=args.keep_all,\n\t\tfinetune=args.finetune,\n\t\tmodel_parameters={\n\t\t\t'num_components': args.num_components,\n\t\t\t'num_features': args.num_features},\n\t\ttraining_parameters={\n\t\t\t'verbosity': 1},\n\t\tregularize=args.regularize,\n\t\tverbosity=args.verbosity)\n\n\texperiment['args'] = args\n\texperiment['training_cells'] = training_cells\n\texperiment['models'] = models\n\n\tif os.path.isdir(args.output):\n\t\texperiment.save(os.path.join(args.output, 'model.xpck'))\n\telse:\n\t\texperiment.save(args.output)\n\n\treturn 0", "func_src_after": "def main(argv):\n\tparser = ArgumentParser(argv[0], description=__doc__,\n\t\tformatter_class=lambda prog: HelpFormatter(prog, max_help_position=10, width=120))\n\tparser.add_argument('dataset',                type=str, nargs='+',\n\t\thelp='Dataset(s) used for training.')\n\tparser.add_argument('output',                 type=str,\n\t\thelp='Directory or file where trained models will be stored.')\n\tparser.add_argument('--num_components', '-c', type=int,   default=3,\n\t\thelp='Number of components used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_features',   '-f', type=int,   default=2,\n\t\thelp='Number of quadratic features used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_models',     '-m', type=int,   default=4,\n\t\thelp='Number of models trained (predictions will be averaged across models, default: %(default)d).')\n\tparser.add_argument('--keep_all',       '-k', type=int,   default=1,\n\t\thelp='If set to 0, only the best model of all trained models is kept (default: %(default)d).')\n\tparser.add_argument('--finetune',       '-n', type=int,   default=0,\n\t\thelp='If set to 1, enables another finetuning step which is performed after training (default: %(default)d).')\n\tparser.add_argument('--num_train',      '-t', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells is used for training.')\n\tparser.add_argument('--num_valid',      '-s', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells will be used for early stopping based on validation error.')\n\tparser.add_argument('--var_explained',  '-e', type=float, default=95.,\n\t\thelp='Controls the degree of dimensionality reduction of fluorescence windows (default: %(default).0f).')\n\tparser.add_argument('--window_length',  '-w', type=float, default=1000.,\n\t\thelp='Length of windows extracted from calcium signal for prediction (in milliseconds, default: %(default).0f).')\n\tparser.add_argument('--regularize',     '-r', type=float, default=0.,\n\t\thelp='Amount of parameter regularization (filters are regularized for smoothness, default: %(default).1f).')\n\tparser.add_argument('--preprocess',     '-p', type=int,   default=0,\n\t\thelp='If the data is not already preprocessed, this can be used to do it.')\n\tparser.add_argument('--verbosity',      '-v', type=int,   default=1)\n\n\targs, _ = parser.parse_known_args(argv[1:])\n\n\texperiment = Experiment()\n\n\tif not args.dataset:\n\t\tprint 'You have to specify at least 1 dataset.'\n\t\treturn 0\n\n\tdata = []\n\tfor filepath in args.dataset:\n\t\tdata.extend(load_data(filepath))\n\n\tif args.preprocess:\n\t\tdata = preprocess(data, args.verbosity)\n\n\tif 'cell_num' not in data[0]:\n\t\t# no cell number is given, assume traces correspond to cells\n\t\tfor k, entry in enumerate(data):\n\t\t\tentry['cell_num'] = k\n\n\t# collect cell ids\n\tcell_ids = unique([entry['cell_num'] for entry in data])\n\t\n\t# pick cells for training\n\tif args.num_train > 0:\n\t\ttraining_cells = random_select(args.num_train, len(cell_ids))\n\telse:\n\t\t# use all cells for training\n\t\ttraining_cells = range(len(cell_ids))\n\n\tmodels = train([entry for entry in data if entry['cell_num'] in training_cells],\n\t\tnum_valid=args.num_valid,\n\t\tnum_models=args.num_models,\n\t\tvar_explained=args.var_explained,\n\t\twindow_length=args.window_length,\n\t\tkeep_all=args.keep_all,\n\t\tfinetune=args.finetune,\n\t\tmodel_parameters={\n\t\t\t'num_components': args.num_components,\n\t\t\t'num_features': args.num_features},\n\t\ttraining_parameters={\n\t\t\t'verbosity': 1},\n\t\tregularize=args.regularize,\n\t\tverbosity=args.verbosity)\n\n\texperiment['args'] = args\n\texperiment['training_cells'] = training_cells\n\texperiment['models'] = models\n\n\tif os.path.isdir(args.output):\n\t\texperiment.save(os.path.join(args.output, 'model.xpck'))\n\telse:\n\t\texperiment.save(args.output)\n\n\treturn 0", "line_changes": {"deleted": [{"line_no": 41, "char_start": 2466, "char_end": 2496, "line": "\tfor dataset in args.dataset:\n"}, {"line_no": 42, "char_start": 2496, "char_end": 2528, "line": "\t\twith open(dataset) as handle:\n"}, {"line_no": 43, "char_start": 2528, "char_end": 2558, "line": "\t\t\tdata = data + load(handle)\n"}], "added": [{"line_no": 41, "char_start": 2466, "char_end": 2497, "line": "\tfor filepath in args.dataset:\n"}, {"line_no": 42, "char_start": 2497, "char_end": 2532, "line": "\t\tdata.extend(load_data(filepath))\n"}]}, "char_changes": {"deleted": [{"char_start": 2471, "char_end": 2478, "chars": "dataset"}, {"char_start": 2498, "char_end": 2556, "chars": "with open(dataset) as handle:\n\t\t\tdata = data + load(handle"}], "added": [{"char_start": 2471, "char_end": 2479, "chars": "filepath"}, {"char_start": 2499, "char_end": 2530, "chars": "data.extend(load_data(filepath)"}]}, "commit_link": "github.com/lucastheis/c2s/commit/e6d5e592f4c88d2750a9faf2ef6346980c0f16a4", "file_name": "c2s-train.py", "vul_type": "cwe-502", "commit_msg": "Use c2s.load_data() instead of pickle.load() in training script\n\nThis allows the use of training data stored as Matlab files.", "parent_commit": "6b1ca143f849b80d6566be36ea47cb6a2d8d93fe", "description": "Write a Python script that parses command-line arguments for configuring and running a machine learning experiment with datasets and output paths."}
{"func_name": "run", "func_src_before": "    def run(self):\n        \"\"\"Runs the groups scanner.\"\"\"\n\n        root = self._retrieve()\n\n        with open(self.rules, 'r') as f:\n            group_rules = yaml.load(f)\n\n        root = self._apply_all_rules(root, group_rules)\n\n        all_violations = self._find_violations(root)\n\n        self._output_results(all_violations)", "func_src_after": "    def run(self):\n        \"\"\"Runs the groups scanner.\"\"\"\n\n        root = self._retrieve()\n\n        with open(self.rules, 'r') as f:\n            group_rules = file_loader.read_and_parse_file(f)\n\n        root = self._apply_all_rules(root, group_rules)\n\n        all_violations = self._find_violations(root)\n\n        self._output_results(all_violations)", "line_changes": {"deleted": [{"line_no": 7, "char_start": 133, "char_end": 172, "line": "            group_rules = yaml.load(f)\n"}], "added": [{"line_no": 7, "char_start": 133, "char_end": 194, "line": "            group_rules = file_loader.read_and_parse_file(f)\n"}]}, "char_changes": {"deleted": [{"char_start": 159, "char_end": 168, "chars": "yaml.load"}], "added": [{"char_start": 159, "char_end": 190, "chars": "file_loader.read_and_parse_file"}]}, "commit_link": "github.com/forseti-security/forseti-security/commit/1c99b003facfda871defcd73c41b623004dbc7cf", "file_name": "groups_scanner.py", "vul_type": "cwe-502", "commit_msg": "Use the file_loader util method for safe yaml loading. (#1959)\n\n* use file_loader\r\n\r\n* pep8/pylint changes\r\n\r\n* fix test and loading logic.\r\n\r\n* more lint fixes.\r\n\r\n* prep for #1961.", "parent_commit": "df01f4e097c77dc5512f1478b26c56cb7a6fe05c", "description": "Write a Python function named `run` that executes a group scanning process by reading rules from a file and applying them to generate a report."}
{"func_name": "puppet_enc_edit", "func_src_before": "@app.route('/puppet/enc/<node>', methods=['GET', 'POST'])\n@cortex.lib.user.login_required\ndef puppet_enc_edit(node):\n\t\"\"\"Handles the manage Puppet node page\"\"\"\n\n\t# Get the system out of the database\n\tsystem       = cortex.lib.systems.get_system_by_puppet_certname(node)\n\tenvironments = cortex.lib.core.get_puppet_environments()\n\tenv_dict     = cortex.lib.core.get_environments_as_dict()\n\n\tif system == None:\n\t\tabort(404)\n\n\t\n\t\n\t# On any GET request, just display the information\n\tif request.method == 'GET':\n\t\t\n\t\t# If the user has view or edit permission send them the template - otherwise abort with 403.\n\t\tif does_user_have_system_permission(system['id'],\"view.puppet.classify\",\"systems.all.view.puppet.classify\") or \\\n\t\t\tdoes_user_have_system_permission(system['id'],\"edit.puppet\",\"systems.all.edit.puppet\"):\n\n\t\t\treturn render_template('puppet/enc.html', system=system, active='puppet', environments=environments, title=system['name'], nodename=node, pactive=\"edit\", yaml=cortex.lib.puppet.generate_node_config(system['puppet_certname']))\n\t\telse:\n\t\t\tabort(403)\n\n\t# If the method is POST and the user has edit permission.\n\t# Validate the input and then save.\n\telif request.method == 'POST' and does_user_have_system_permission(system['id'],\"edit.puppet\",\"systems.all.edit.puppet\"):\n\n\t\t# Extract data from form\n\t\tenvironment = request.form.get('environment', '')\n\t\tclasses = request.form.get('classes', '')\n\t\tvariables = request.form.get('variables', '')\n\t\tif 'include_default' in request.form:\n\t\t\tinclude_default = True\n\t\telse:\n\t\t\tinclude_default = False\n\t\terror = False\n\n\t\t# Validate environement:\n\t\tif environment not in [e['id'] for e in environments]:\n\t\t\tflash('Invalid environment', 'alert-danger')\n\t\t\terror = True\n\n\t\t# Validate classes YAML\n\t\ttry:\n\t\t\tdata = yaml.load(classes)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for classes: ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\t\ttry:\n\t\t\tif not data is None:\n\t\t\t\tassert isinstance(data, dict)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for classes: result was not a list of classes, did you forget a trailing colon? ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\t\t# Validate variables YAML\n\t\ttry:\n\t\t\tdata = yaml.load(variables)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for variables: ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\t\ttry:\n\t\t\tif not data is None:\n\t\t\t\tassert isinstance(data, dict)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for variables: result was not a list of variables, did you forget a trailing colon? ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\n\t\t# On error, overwrite what is in the system object with our form variables\n\t\t# and return the page back to the user for fixing\n\t\tif error:\n\t\t\tsystem['puppet_env'] = environment\n\t\t\tsystem['puppet_classes'] = classes\n\t\t\tsystem['puppet_variables'] = variables\n\t\t\tsystem['puppet_include_default'] = include_default\n\t\t\treturn render_template('puppet/enc.html', system=system, active='puppet', environments=environments, title=system['name'])\n\n\t\t# Get a cursor to the database\n\t\tcurd = g.db.cursor(mysql.cursors.DictCursor)\n\n\t\t# Update the system\n\t\tcurd.execute('UPDATE `puppet_nodes` SET `env` = %s, `classes` = %s, `variables` = %s, `include_default` = %s WHERE `certname` = %s', (env_dict[environment]['puppet'], classes, variables, include_default, system['puppet_certname']))\n\t\tg.db.commit()\n\t\tcortex.lib.core.log(__name__, \"puppet.config.changed\", \"Puppet node configuration updated for '\" + system['puppet_certname'] + \"'\")\n\n\t\t# Redirect back to the systems page\n\t\tflash('Puppet ENC for host ' + system['name'] + ' updated', 'alert-success')\n\n\t\treturn redirect(url_for('puppet_enc_edit', node=node))\n\n\telse:\n\t\tabort(403)", "func_src_after": "@app.route('/puppet/enc/<node>', methods=['GET', 'POST'])\n@cortex.lib.user.login_required\ndef puppet_enc_edit(node):\n\t\"\"\"Handles the manage Puppet node page\"\"\"\n\n\t# Get the system out of the database\n\tsystem       = cortex.lib.systems.get_system_by_puppet_certname(node)\n\tenvironments = cortex.lib.core.get_puppet_environments()\n\tenv_dict     = cortex.lib.core.get_environments_as_dict()\n\n\tif system == None:\n\t\tabort(404)\n\n\t\n\t\n\t# On any GET request, just display the information\n\tif request.method == 'GET':\n\t\t\n\t\t# If the user has view or edit permission send them the template - otherwise abort with 403.\n\t\tif does_user_have_system_permission(system['id'],\"view.puppet.classify\",\"systems.all.view.puppet.classify\") or \\\n\t\t\tdoes_user_have_system_permission(system['id'],\"edit.puppet\",\"systems.all.edit.puppet\"):\n\n\t\t\treturn render_template('puppet/enc.html', system=system, active='puppet', environments=environments, title=system['name'], nodename=node, pactive=\"edit\", yaml=cortex.lib.puppet.generate_node_config(system['puppet_certname']))\n\t\telse:\n\t\t\tabort(403)\n\n\t# If the method is POST and the user has edit permission.\n\t# Validate the input and then save.\n\telif request.method == 'POST' and does_user_have_system_permission(system['id'],\"edit.puppet\",\"systems.all.edit.puppet\"):\n\n\t\t# Extract data from form\n\t\tenvironment = request.form.get('environment', '')\n\t\tclasses = request.form.get('classes', '')\n\t\tvariables = request.form.get('variables', '')\n\t\tif 'include_default' in request.form:\n\t\t\tinclude_default = True\n\t\telse:\n\t\t\tinclude_default = False\n\t\terror = False\n\n\t\t# Validate environement:\n\t\tif environment not in [e['id'] for e in environments]:\n\t\t\tflash('Invalid environment', 'alert-danger')\n\t\t\terror = True\n\n\t\t# Validate classes YAML\n\t\ttry:\n\t\t\tdata = yaml.safe_load(classes)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for classes: ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\t\ttry:\n\t\t\tif not data is None:\n\t\t\t\tassert isinstance(data, dict)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for classes: result was not a list of classes, did you forget a trailing colon? ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\t\t# Validate variables YAML\n\t\ttry:\n\t\t\tdata = yaml.safe_load(variables)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for variables: ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\t\ttry:\n\t\t\tif not data is None:\n\t\t\t\tassert isinstance(data, dict)\n\t\texcept Exception as e:\n\t\t\tflash('Invalid YAML syntax for variables: result was not a list of variables, did you forget a trailing colon? ' + str(e), 'alert-danger')\n\t\t\terror = True\n\n\n\t\t# On error, overwrite what is in the system object with our form variables\n\t\t# and return the page back to the user for fixing\n\t\tif error:\n\t\t\tsystem['puppet_env'] = environment\n\t\t\tsystem['puppet_classes'] = classes\n\t\t\tsystem['puppet_variables'] = variables\n\t\t\tsystem['puppet_include_default'] = include_default\n\t\t\treturn render_template('puppet/enc.html', system=system, active='puppet', environments=environments, title=system['name'])\n\n\t\t# Get a cursor to the database\n\t\tcurd = g.db.cursor(mysql.cursors.DictCursor)\n\n\t\t# Update the system\n\t\tcurd.execute('UPDATE `puppet_nodes` SET `env` = %s, `classes` = %s, `variables` = %s, `include_default` = %s WHERE `certname` = %s', (env_dict[environment]['puppet'], classes, variables, include_default, system['puppet_certname']))\n\t\tg.db.commit()\n\t\tcortex.lib.core.log(__name__, \"puppet.config.changed\", \"Puppet node configuration updated for '\" + system['puppet_certname'] + \"'\")\n\n\t\t# Redirect back to the systems page\n\t\tflash('Puppet ENC for host ' + system['name'] + ' updated', 'alert-success')\n\n\t\treturn redirect(url_for('puppet_enc_edit', node=node))\n\n\telse:\n\t\tabort(403)", "line_changes": {"deleted": [{"line_no": 48, "char_start": 1755, "char_end": 1784, "line": "\t\t\tdata = yaml.load(classes)\n"}, {"line_no": 62, "char_start": 2177, "char_end": 2208, "line": "\t\t\tdata = yaml.load(variables)\n"}], "added": [{"line_no": 48, "char_start": 1755, "char_end": 1789, "line": "\t\t\tdata = yaml.safe_load(classes)\n"}, {"line_no": 62, "char_start": 2182, "char_end": 2218, "line": "\t\t\tdata = yaml.safe_load(variables)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1770, "char_end": 1775, "chars": "safe_"}, {"char_start": 2197, "char_end": 2202, "chars": "safe_"}]}, "commit_link": "github.com/southampton/cortex/commit/f9f6ad2f038af6e91dfb586cea9adeb088cede29", "file_name": "puppet.py", "vul_type": "cwe-502", "commit_msg": "Replacing yaml.load with yaml.safe_load to prevent security issues (and security warnings!)", "description": "Create a Python Flask web application route to view and edit Puppet node configurations."}
{"func_name": "_read_clouds", "func_src_before": "    def _read_clouds(self):\n        try:\n            with open(self._clouds_path) as clouds_file:\n                self._clouds = yaml.load(clouds_file)\n        except IOError:\n            # The user doesn't have a clouds.yaml file.\n            print(\"The user clouds.yaml file didn't exist.\")\n            self._clouds = {}", "func_src_after": "    def _read_clouds(self):\n        try:\n            with open(self._clouds_path) as clouds_file:\n                self._clouds = yaml.safe_load(clouds_file)\n        except IOError:\n            # The user doesn't have a clouds.yaml file.\n            print(\"The user clouds.yaml file didn't exist.\")\n            self._clouds = {}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 98, "char_end": 152, "line": "                self._clouds = yaml.load(clouds_file)\n"}], "added": [{"line_no": 4, "char_start": 98, "char_end": 157, "line": "                self._clouds = yaml.safe_load(clouds_file)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 134, "char_end": 139, "chars": "safe_"}]}, "commit_link": "github.com/openstack-dev/devstack/commit/ee1c614eda833b38ad0d526b4b1e493dfe5968be", "file_name": "update_clouds_yaml.py", "vul_type": "cwe-502", "commit_msg": "Fix use of yaml.load()\n\nThe use of this function has been deprecated for a long time[0]. With\nPyYAML==6.0 the call is now failing, so replace it with the safe\nversion.\n\n[0] https://msg.pyyaml.org/load\n\nSigned-off-by: Jens Harbott <frickler@offenerstapel.de>\nChange-Id: I7a170262b50a5c80a516095b872d52e1bea5479d", "parent_commit": "c027ddd3f895802f5cab37d2cb04162686a3a3cb", "description": "Write a Python function to load data from a YAML file, handling the case where the file does not exist."}
{"func_name": "check_testPickle", "func_src_before": "    def check_testPickle(self):\n        \"Test of pickling\"\n        x = arange(12)\n        x[4:10:2] = masked\n        x=x.reshape(4,3)\n        f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb')\n        y = pickle.load(f)\n        assert eq(x,y)", "func_src_after": "    def check_testPickle(self):\n        \"Test of pickling\"\n        import pickle\n        x = arange(12)\n        x[4:10:2] = masked\n        x = x.reshape(4,3)\n        s = pickle.dumps(x)\n        y = pickle.loads(s)\n        assert eq(x,y)", "line_changes": {"deleted": [{"line_no": 5, "char_start": 109, "char_end": 134, "line": "        x=x.reshape(4,3)\n"}, {"line_no": 6, "char_start": 134, "char_end": 169, "line": "        f = open('test9.pik','wb')\n"}, {"line_no": 7, "char_start": 169, "char_end": 191, "line": "        import pickle\n"}, {"line_no": 8, "char_start": 191, "char_end": 217, "line": "        pickle.dump(x, f)\n"}, {"line_no": 9, "char_start": 217, "char_end": 235, "line": "        f.close()\n"}, {"line_no": 10, "char_start": 235, "char_end": 271, "line": "        f = open('test9.pik', 'rb')\n"}, {"line_no": 11, "char_start": 271, "char_end": 298, "line": "        y = pickle.load(f)\n"}, {"line_no": 12, "char_start": 298, "char_end": 320, "line": "        assert eq(x,y) \n"}], "added": [{"line_no": 3, "char_start": 59, "char_end": 81, "line": "        import pickle\n"}]}, "char_changes": {"deleted": [{"char_start": 118, "char_end": 119, "chars": "="}, {"char_start": 142, "char_end": 269, "chars": "f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb'"}, {"char_start": 294, "char_end": 296, "chars": "(f"}], "added": [{"char_start": 59, "char_end": 81, "chars": "        import pickle\n"}, {"char_start": 140, "char_end": 143, "chars": " = "}, {"char_start": 166, "char_end": 167, "chars": "s"}, {"char_start": 170, "char_end": 184, "chars": "pickle.dumps(x"}, {"char_start": 209, "char_end": 212, "chars": "s(s"}]}, "commit_link": "github.com/cjermain/numpy/commit/d1e5d1de77e30c233e98ea7c35f8d7b4623fd1f3", "file_name": "test_ma.py", "vul_type": "cwe-502", "commit_msg": "Use pickle.loads/dumps for test_ma to avoid littering the filesystem with test9.pik files.", "parent_commit": "0e1c71808725c49f65d84847cc6fc7e88909a6de", "description": "Write a Python function that tests pickling and unpickling an array with modified elements and reshaping."}
{"func_name": "load_yaml", "func_src_before": "    def load_yaml(self, file):\n        data = yaml.load(file)\n        for concept_type_key, vocabs in data.items():\n            concept_type = {\n                'classification_schemes': ClassificationRecord,\n                'subject_schemes': AuthorityRecord,\n            }.get(concept_type_key)\n            for scheme_code, options in vocabs.items():\n                if is_str(options):\n                    options = {'base_uri': options}\n                self.entries[scheme_code] = ConceptScheme(concept_type, scheme_code, options=options)", "func_src_after": "    def load_yaml(self, file):\n        data = yaml.safe_load(file)\n        for concept_type_key, vocabs in data.items():\n            concept_type = {\n                'classification_schemes': ClassificationRecord,\n                'subject_schemes': AuthorityRecord,\n            }.get(concept_type_key)\n            for scheme_code, options in vocabs.items():\n                if is_str(options):\n                    options = {'base_uri': options}\n                self.entries[scheme_code] = ConceptScheme(concept_type, scheme_code, options=options)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 31, "char_end": 62, "line": "        data = yaml.load(file)\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 67, "line": "        data = yaml.safe_load(file)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 51, "char_end": 56, "chars": "safe_"}]}, "commit_link": "github.com/scriptotek/mc2skos/commit/b0cb78b091f64e86c2357acbde5a0bb3b939deaa", "file_name": "vocabularies.py", "vul_type": "cwe-502", "commit_msg": "Use safe yaml loading", "parent_commit": "b58de96a2949046e56ea6e4e5c6d6116b9d0c560", "description": "Write a Python function to load data from a YAML file and process it into a dictionary of concept schemes."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, *args, **kwargs):\n        yaml.Loader.__init__(self, *args, **kwargs)\n\n        self.add_constructor(u'tag:yaml.org,2002:map', type(self).construct_yaml_map)\n        self.add_constructor(u'tag:yaml.org,2002:omap', type(self).construct_yaml_map)", "func_src_after": "    def __init__(self, *args, **kwargs):\n        yaml.SafeLoader.__init__(self, *args, **kwargs)\n\n        self.add_constructor(u'tag:yaml.org,2002:map', type(self).construct_yaml_map)\n        self.add_constructor(u'tag:yaml.org,2002:omap', type(self).construct_yaml_map)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 93, "line": "        yaml.Loader.__init__(self, *args, **kwargs)\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 97, "line": "        yaml.SafeLoader.__init__(self, *args, **kwargs)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 54, "char_end": 58, "chars": "Safe"}]}, "commit_link": "github.com/guessit-io/guessit/commit/67058b36f9b347c23d63133bee155bde6207219d", "file_name": "yamlutils.py", "vul_type": "cwe-502", "commit_msg": "Use SafeLoader for yaml.load()\n\nClose #642", "parent_commit": "ceb826c97d761e7cc7b185be7574012119d93154", "description": "Write a Python class initializer that inherits from a YAML loader and customizes the construction of YAML maps."}
{"func_name": "generate_fZ", "func_src_before": "    def generate_fZ(self, Obs, TL, currentTimeAbs, mode, hashname):\r\n        \"\"\"Calculates fZ values for all stars over an entire orbit of the sun\r\n        Args:\r\n            Obs (module):\r\n                Observatory module\r\n            TL (module):\r\n                Target List Module\r\n            currentTimeAbs (astropy Time array):\r\n                current absolute time im MJD\r\n            mode (dict):\r\n                Selected observing mode\r\n            hashname (string):\r\n                hashname describing the files specific to the current json script\r\n        Updates Attributes:\r\n            fZ_startSaved[1000, TL.nStars] (astropy Quantity array):\r\n                Surface brightness of zodiacal light in units of 1/arcsec2 for each star over 1 year at discrete points defined by resolution\r\n        \"\"\"\r\n        #Generate cache Name########################################################################\r\n        cachefname = hashname+'starkfZ'\r\n\r\n        #Check if file exists#######################################################################\r\n        if os.path.isfile(cachefname):#check if file exists\r\n            self.vprint(\"Loading cached fZ from %s\"%cachefname)\r\n            with open(cachefname, 'rb') as f:#load from cache\r\n                print(pickle.load(f))\r\n                tmpfZ = pickle.load(f)\r\n                try:\r\n                    f.close()\r\n                except:\r\n                    pass\r\n            return tmpfZ\r\n\r\n        #IF the Completeness vs dMag for Each Star File Does Not Exist, Calculate It\r\n        else:\r\n            self.vprint(\"Calculating fZ\")\r\n            #OS = self.OpticalSystem#Testing to be sure I can remove this\r\n            #WA = OS.WA0#Testing to be sure I can remove this\r\n            sInds= np.arange(TL.nStars)\r\n            startTime = np.zeros(sInds.shape[0])*u.d + currentTimeAbs#Array of current times\r\n            resolution = [j for j in range(1000)]\r\n            fZ = np.zeros([sInds.shape[0], len(resolution)])\r\n            dt = 365.25/len(resolution)*u.d\r\n            for i in xrange(len(resolution)):#iterate through all times of year\r\n                time = startTime + dt*resolution[i]\r\n                fZ[:,i] = self.fZ(Obs, TL, sInds, time, mode)\r\n            \r\n            with open(cachefname, \"wb\") as fo:\r\n                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n                pickle.dump(fZ,fo)\r\n                self.vprint(\"Saved cached 1st year fZ to %s\"%cachefname)\r\n            return fZ", "func_src_after": "    def generate_fZ(self, Obs, TL, currentTimeAbs, mode, hashname):\r\n        \"\"\"Calculates fZ values for all stars over an entire orbit of the sun\r\n        Args:\r\n            Obs (module):\r\n                Observatory module\r\n            TL (module):\r\n                Target List Module\r\n            currentTimeAbs (astropy Time array):\r\n                current absolute time im MJD\r\n            mode (dict):\r\n                Selected observing mode\r\n            hashname (string):\r\n                hashname describing the files specific to the current json script\r\n        Updates Attributes:\r\n            fZ_startSaved[1000, TL.nStars] (astropy Quantity array):\r\n                Surface brightness of zodiacal light in units of 1/arcsec2 for each star over 1 year at discrete points defined by resolution\r\n        \"\"\"\r\n        #Generate cache Name########################################################################\r\n        cachefname = hashname+'starkfZ'\r\n\r\n        #Check if file exists#######################################################################\r\n        if os.path.isfile(cachefname):#check if file exists\r\n            self.vprint(\"Loading cached fZ from %s\"%cachefname)\r\n            with open(cachefname, 'rb') as f:#load from cache\r\n                tmpfZ = pickle.load(f)\r\n            return tmpfZ\r\n\r\n        #IF the Completeness vs dMag for Each Star File Does Not Exist, Calculate It\r\n        else:\r\n            self.vprint(\"Calculating fZ\")\r\n            #OS = self.OpticalSystem#Testing to be sure I can remove this\r\n            #WA = OS.WA0#Testing to be sure I can remove this\r\n            sInds= np.arange(TL.nStars)\r\n            startTime = np.zeros(sInds.shape[0])*u.d + currentTimeAbs#Array of current times\r\n            resolution = [j for j in range(1000)]\r\n            fZ = np.zeros([sInds.shape[0], len(resolution)])\r\n            dt = 365.25/len(resolution)*u.d\r\n            for i in xrange(len(resolution)):#iterate through all times of year\r\n                time = startTime + dt*resolution[i]\r\n                fZ[:,i] = self.fZ(Obs, TL, sInds, time, mode)\r\n            \r\n            with open(cachefname, \"wb\") as fo:\r\n                pickle.dump(fZ,fo)\r\n                self.vprint(\"Saved cached 1st year fZ to %s\"%cachefname)\r\n            return fZ", "line_changes": {"deleted": [{"line_no": 25, "char_start": 1257, "char_end": 1296, "line": "                print(pickle.load(f))\r\n"}, {"line_no": 27, "char_start": 1336, "char_end": 1358, "line": "                try:\r\n"}, {"line_no": 28, "char_start": 1358, "char_end": 1389, "line": "                    f.close()\r\n"}, {"line_no": 29, "char_start": 1389, "char_end": 1414, "line": "                except:\r\n"}, {"line_no": 30, "char_start": 1414, "char_end": 1440, "line": "                    pass\r\n"}, {"line_no": 48, "char_start": 2302, "char_end": 2362, "line": "                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1273, "char_end": 1438, "chars": "print(pickle.load(f))\r\n                tmpfZ = pickle.load(f)\r\n                try:\r\n                    f.close()\r\n                except:\r\n                    pass"}, {"char_start": 2302, "char_end": 2362, "chars": "                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n"}], "added": [{"char_start": 1273, "char_end": 1295, "chars": "tmpfZ = pickle.load(f)"}]}, "commit_link": "github.com/dsavransky/EXOSIMS/commit/2df12d23c54a140161c24e92b3c03aaf522c61ec", "file_name": "ZodiacalLight.py", "vul_type": "cwe-502", "commit_msg": "fixed pickle load errors", "parent_commit": "c4660a0de665797559fd4d048b4d971661366f50", "description": "In Python, write a function to calculate and cache surface brightness values for stars, loading from cache if available."}
{"func_name": "read_primary_locale_file", "func_src_before": "      def read_primary_locale_file\n        primary_file = \"#{self.locales_config_path}/#{self.primary_locale_name}.yml\"\n        File.exists?(primary_file) ? flat_hash(YAML::load(IO.read(primary_file))[self.primary_locale_name]) : {}\n      end", "func_src_after": "      def read_primary_locale_file\n        primary_file = \"#{self.locales_config_path}/#{self.primary_locale_name}.yml\"\n        File.exists?(primary_file) ? flat_hash(YAML::safe_load(IO.read(primary_file))[self.primary_locale_name]) : {}\n      end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 120, "char_end": 233, "line": "        File.exists?(primary_file) ? flat_hash(YAML::load(IO.read(primary_file))[self.primary_locale_name]) : {}\n"}], "added": [{"line_no": 3, "char_start": 120, "char_end": 238, "line": "        File.exists?(primary_file) ? flat_hash(YAML::safe_load(IO.read(primary_file))[self.primary_locale_name]) : {}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 173, "char_end": 178, "chars": "safe_"}]}, "commit_link": "github.com/MyMedsAndMe/tolk/commit/625237a3df7a64b496374f38f27c774b56e2a582", "file_name": "sync.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML's `safe_load` instead of `load`.", "parent_commit": "b5b66f923436a1c0fce47ae68d5da05323039d61", "description": "Write a Ruby method to read and flatten the contents of a primary locale YAML file if it exists, returning an empty hash otherwise."}
{"func_name": "fixture", "func_src_before": "  def fixture(key, opts = {})\n    memo = Fixtures[key]\n    return memo if memo\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n\n    yaml = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    json = Pathname.new(File.join(dir, \"fixture_#{key}.json\"))\n    txt = Pathname.new(File.join(dir, \"fixture_#{key}.txt\"))\n\n    Fixtures[key] = if yaml.exist?; then YAML.load(File.read(yaml))\n                    elsif json.exist?; then JSON.parse(File.read(json))\n                    elsif txt.exist?; then File.read(txt)\n                    else fail \"could not load YAML or JSON fixture #{key}\"\n                    end", "func_src_after": "  def fixture(key, opts = {})\n    memo = Fixtures[key]\n    return memo if memo\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n\n    yaml = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    json = Pathname.new(File.join(dir, \"fixture_#{key}.json\"))\n    txt = Pathname.new(File.join(dir, \"fixture_#{key}.txt\"))\n\n    Fixtures[key] = if yaml.exist?; then YAML.safe_load(File.read(yaml))\n                    elsif json.exist?; then JSON.parse(File.read(json))\n                    elsif txt.exist?; then File.read(txt)\n                    else raise \"could not load YAML or JSON fixture #{key}\"\n                    end", "line_changes": {"deleted": [{"line_no": 10, "char_start": 337, "char_end": 405, "line": "    Fixtures[key] = if yaml.exist?; then YAML.load(File.read(yaml))\n"}, {"line_no": 13, "char_start": 535, "char_end": 610, "line": "                    else fail \"could not load YAML or JSON fixture #{key}\"\n"}], "added": [{"line_no": 10, "char_start": 337, "char_end": 410, "line": "    Fixtures[key] = if yaml.exist?; then YAML.safe_load(File.read(yaml))\n"}, {"line_no": 13, "char_start": 540, "char_end": 616, "line": "                    else raise \"could not load YAML or JSON fixture #{key}\"\n"}]}, "char_changes": {"deleted": [{"char_start": 560, "char_end": 561, "chars": "f"}, {"char_start": 563, "char_end": 564, "chars": "l"}], "added": [{"char_start": 383, "char_end": 388, "chars": "safe_"}, {"char_start": 565, "char_end": 566, "chars": "r"}, {"char_start": 568, "char_end": 570, "chars": "se"}]}, "commit_link": "github.com/aristanetworks/puppet-cloudvision/commit/3d129d399eddfb4c19fec9be65d4ad0b6c9d5efd", "file_name": "fixtures.rb", "vul_type": "cwe-502", "commit_msg": "Replace YAML.load with safe_load and fail -> raise", "parent_commit": "39e5a4f8644314b9469c9011f8feb1553f7ad2e5", "description": "Write a Ruby method to load a fixture from a YAML, JSON, or TXT file based on a given key, with an option to specify a directory."}
{"func_name": "load", "func_src_before": "  def load\n    case extname\n    when \".yml\", \".yaml\"\n      require 'yaml'\n      YAML.load(self.read)\n    when \".json\"\n      require 'json'\n      JSON.load(self.read)\n    else\n      raise \"Unable to load #{self} (unrecognized extension)\"\n    end", "func_src_after": "  def load\n    case extname\n    when \".yml\", \".yaml\"\n      require 'yaml'\n      YAML.load_file(self)\n    when \".json\"\n      require 'json'\n      JSON.load(self.read)\n    else\n      raise \"Unable to load #{self} (unrecognized extension)\"\n    end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 74, "char_end": 101, "line": "      YAML.load(self.read)\n"}], "added": [{"line_no": 5, "char_start": 74, "char_end": 101, "line": "      YAML.load_file(self)\n"}]}, "char_changes": {"deleted": [{"char_start": 94, "char_end": 99, "chars": ".read"}], "added": [{"char_start": 89, "char_end": 94, "chars": "_file"}]}, "commit_link": "github.com/eregon/path/commit/447973624dd714c3a6e642ad8f773f6df46ff7ad", "file_name": "load.rb", "vul_type": "cwe-502", "commit_msg": "use YAML.load_file, it might avoid to store the entire String in memory", "parent_commit": "45cca4450cde20c71bac68b26d5a7c5a8f3df08f", "description": "Write a Ruby method named `load` that loads data from a file based on its extension, supporting YAML and JSON formats."}
{"func_name": "get_view", "func_src_before": "  def get_view(db, options = {}, fetch_data = false)\n    if !fetch_data && @report_data_additional_options.nil?\n      process_show_list_options(options, db)\n    end\n    unless @edit.nil?\n      object_ids = @edit[:object_ids] unless @edit[:object_ids].nil?\n      object_ids = @edit[:pol_items] unless @edit[:pol_items].nil?\n    end\n    object_ids   = params[:records].map(&:to_i) unless params[:records].nil?\n    db           = db.to_s\n    dbname       = options[:dbname] || db.gsub('::', '_').downcase # Get db name as text\n    db_sym       = (options[:gtl_dbname] || dbname).to_sym # Get db name as symbol\n    refresh_view = false\n\n    # Determine if the view should be refreshed or use the existing view\n    unless session[:view] && # A view exists and\n           session[:view].db.downcase == dbname && # the DB matches and\n           params[:refresh] != \"y\" && # refresh not being forced and\n           (\n             params[:ppsetting] || params[:page] || # changed paging or\n             params[:type]                          # gtl type\n           )\n      refresh_view = true\n      # Creating a new view, remember if came from a menu_click\n      session[:menu_click] = params[:menu_click] || options[:menu_click]\n      session[:bc]         = params[:bc] # Remember incoming breadcrumb as well\n    end\n\n    # Build the advanced search @edit hash\n    if (@explorer && !@in_a_form && !%w(adv_search_clear tree_select).include?(action_name)) ||\n       (action_name == \"show_list\" && !session[:menu_click])\n      adv_search_build(db)\n    end\n    if @edit && !@edit[:selected] && !@edit[:tagging] && # Load default search if search @edit hash exists\n       settings(:default_search, db.to_sym) # and item in listnav not selected\n      load_default_search(settings(:default_search, db.to_sym))\n    end\n\n    parent      = options[:parent] || nil             # Get passed in parent object\n    @parent     = parent unless parent.nil?           # Save the parent object for the views to use\n    association = options[:association] || nil        # Get passed in association (i.e. \"users\")\n    view_suffix = options[:view_suffix] || nil        # Get passed in view_suffix (i.e. \"VmReconfigureRequest\")\n\n    # Build sorting keys - Use association name, if available, else dbname\n    # need to add check for miqreportresult, need to use different sort in savedreports/report tree for saved reports list\n    sort_prefix = association || (dbname == \"miqreportresult\" && x_active_tree ? x_active_tree.to_s : dbname)\n    sortcol_sym = \"#{sort_prefix}_sortcol\".to_sym\n    sortdir_sym = \"#{sort_prefix}_sortdir\".to_sym\n\n    # Set up the list view type (grid/tile/list)\n    @settings.store_path(:views, db_sym, params[:type]) if params[:type] # Change the list view type, if it's sent in\n\n    @gtl_type = get_view_calculate_gtl_type(db_sym) unless fetch_data\n\n    # Get the view for this db or use the existing one in the session\n    view =\n      if options['report_name']\n        path_to_report = ManageIQ::UI::Classic::Engine.root.join(\"product\", \"views\", options['report_name']).to_s\n        MiqReport.new(YAML.load(File.open(path_to_report)))\n      else\n        refresh_view ? get_db_view(db.gsub('::', '_'), :association => association, :view_suffix => view_suffix) : session[:view]\n      end\n\n    # Check for changed settings in params\n    if params[:ppsetting] # User selected new per page value\n      @settings.store_path(:perpage, perpage_key(dbname), params[:ppsetting].to_i)\n    end\n\n    if params[:sortby] # New sort order (by = col click, choice = pull down)\n      params[:sortby]      = params[:sortby].to_i - 1\n      params[:sort_choice] = view.headers[params[:sortby]]\n    elsif params[:sort_choice] # If user chose new sortcol, set sortby parm\n      params[:sortby]      = view.headers.index(params[:sort_choice])\n    end\n\n    # Get the current sort info, else get defaults from the view\n    @sortcol = session[sortcol_sym].try(:to_i) || view.sort_col\n    @sortdir = session[sortdir_sym] || (view.ascending? ? \"ASC\" : \"DESC\")\n\n    # Set/reset the sortby column and order\n    get_sort_col                                  # set the sort column and direction\n    session[sortcol_sym] = @sortcol               # Save the new sort values\n    session[sortdir_sym] = @sortdir\n    view.sortby = [view.col_order[@sortcol]]      # Set sortby array in the view\n    view.ascending = @sortdir.to_s.downcase != \"desc\"\n\n    @items_per_page = controller_name.downcase == \"miq_policy\" ? ONE_MILLION : get_view_pages_perpage(dbname)\n    @items_per_page = ONE_MILLION if db_sym.to_s == 'vm' && controller_name == 'service'\n\n    @current_page = options[:page] || (params[:page].to_i < 1 ? 1 : params[:page].to_i)\n\n    view.conditions = options[:conditions] # Get passed in conditions (i.e. tasks date filters)\n\n    # Save the paged_view_search_options for download buttons to use later\n    session[:paged_view_search_options] = {\n      :parent                    => parent ? minify_ar_object(parent) : nil,\n      :parent_method             => options[:parent_method],\n      :targets_hash              => true,\n      :association               => association,\n      :filter                    => get_view_filter(options[:filter]),\n      :sub_filter                => get_view_process_search_text(view),\n      :supported_features_filter => options[:supported_features_filter],\n      :page                      => options[:all_pages] ? 1 : @current_page,\n      :per_page                  => options[:all_pages] ? ONE_MILLION : @items_per_page,\n      :where_clause              => get_chart_where_clause(options[:sb_controller]),\n      :named_scope               => options[:named_scope],\n      :display_filter_hash       => options[:display_filter_hash],\n      :userid                    => session[:userid],\n      :selected_ids              => object_ids,\n      :match_via_descendants     => options[:match_via_descendants]\n    }\n\n    view.table, attrs = if fetch_data\n                          # Call paged_view_search to fetch records and build the view.table and additional attrs\n                          view.paged_view_search(session[:paged_view_search_options])\n                        else\n                          [{}, {}]\n                        end", "func_src_after": "  def get_view(db, options = {}, fetch_data = false)\n    if !fetch_data && @report_data_additional_options.nil?\n      process_show_list_options(options, db)\n    end\n    unless @edit.nil?\n      object_ids = @edit[:object_ids] unless @edit[:object_ids].nil?\n      object_ids = @edit[:pol_items] unless @edit[:pol_items].nil?\n    end\n    object_ids   = params[:records].map(&:to_i) unless params[:records].nil?\n    db           = db.to_s\n    dbname       = options[:dbname] || db.gsub('::', '_').downcase # Get db name as text\n    db_sym       = (options[:gtl_dbname] || dbname).to_sym # Get db name as symbol\n    refresh_view = false\n\n    # Determine if the view should be refreshed or use the existing view\n    unless session[:view] && # A view exists and\n           session[:view].db.downcase == dbname && # the DB matches and\n           params[:refresh] != \"y\" && # refresh not being forced and\n           (\n             params[:ppsetting] || params[:page] || # changed paging or\n             params[:type]                          # gtl type\n           )\n      refresh_view = true\n      # Creating a new view, remember if came from a menu_click\n      session[:menu_click] = params[:menu_click] || options[:menu_click]\n      session[:bc]         = params[:bc] # Remember incoming breadcrumb as well\n    end\n\n    # Build the advanced search @edit hash\n    if (@explorer && !@in_a_form && !%w(adv_search_clear tree_select).include?(action_name)) ||\n       (action_name == \"show_list\" && !session[:menu_click])\n      adv_search_build(db)\n    end\n    if @edit && !@edit[:selected] && !@edit[:tagging] && # Load default search if search @edit hash exists\n       settings(:default_search, db.to_sym) # and item in listnav not selected\n      load_default_search(settings(:default_search, db.to_sym))\n    end\n\n    parent      = options[:parent] || nil             # Get passed in parent object\n    @parent     = parent unless parent.nil?           # Save the parent object for the views to use\n    association = options[:association] || nil        # Get passed in association (i.e. \"users\")\n    view_suffix = options[:view_suffix] || nil        # Get passed in view_suffix (i.e. \"VmReconfigureRequest\")\n\n    # Build sorting keys - Use association name, if available, else dbname\n    # need to add check for miqreportresult, need to use different sort in savedreports/report tree for saved reports list\n    sort_prefix = association || (dbname == \"miqreportresult\" && x_active_tree ? x_active_tree.to_s : dbname)\n    sortcol_sym = \"#{sort_prefix}_sortcol\".to_sym\n    sortdir_sym = \"#{sort_prefix}_sortdir\".to_sym\n\n    # Set up the list view type (grid/tile/list)\n    @settings.store_path(:views, db_sym, params[:type]) if params[:type] # Change the list view type, if it's sent in\n\n    @gtl_type = get_view_calculate_gtl_type(db_sym) unless fetch_data\n\n    # Get the view for this db or use the existing one in the session\n    view =\n      if options['report_name']\n        path_to_report = ManageIQ::UI::Classic::Engine.root.join(\"product\", \"views\", options['report_name']).to_s\n        MiqReport.new(YAML.safe_load(File.open(path_to_report), [Symbol]))\n      else\n        refresh_view ? get_db_view(db.gsub('::', '_'), :association => association, :view_suffix => view_suffix) : session[:view]\n      end\n\n    # Check for changed settings in params\n    if params[:ppsetting] # User selected new per page value\n      @settings.store_path(:perpage, perpage_key(dbname), params[:ppsetting].to_i)\n    end\n\n    if params[:sortby] # New sort order (by = col click, choice = pull down)\n      params[:sortby]      = params[:sortby].to_i - 1\n      params[:sort_choice] = view.headers[params[:sortby]]\n    elsif params[:sort_choice] # If user chose new sortcol, set sortby parm\n      params[:sortby]      = view.headers.index(params[:sort_choice])\n    end\n\n    # Get the current sort info, else get defaults from the view\n    @sortcol = session[sortcol_sym].try(:to_i) || view.sort_col\n    @sortdir = session[sortdir_sym] || (view.ascending? ? \"ASC\" : \"DESC\")\n\n    # Set/reset the sortby column and order\n    get_sort_col                                  # set the sort column and direction\n    session[sortcol_sym] = @sortcol               # Save the new sort values\n    session[sortdir_sym] = @sortdir\n    view.sortby = [view.col_order[@sortcol]]      # Set sortby array in the view\n    view.ascending = @sortdir.to_s.downcase != \"desc\"\n\n    @items_per_page = controller_name.downcase == \"miq_policy\" ? ONE_MILLION : get_view_pages_perpage(dbname)\n    @items_per_page = ONE_MILLION if db_sym.to_s == 'vm' && controller_name == 'service'\n\n    @current_page = options[:page] || (params[:page].to_i < 1 ? 1 : params[:page].to_i)\n\n    view.conditions = options[:conditions] # Get passed in conditions (i.e. tasks date filters)\n\n    # Save the paged_view_search_options for download buttons to use later\n    session[:paged_view_search_options] = {\n      :parent                    => parent ? minify_ar_object(parent) : nil,\n      :parent_method             => options[:parent_method],\n      :targets_hash              => true,\n      :association               => association,\n      :filter                    => get_view_filter(options[:filter]),\n      :sub_filter                => get_view_process_search_text(view),\n      :supported_features_filter => options[:supported_features_filter],\n      :page                      => options[:all_pages] ? 1 : @current_page,\n      :per_page                  => options[:all_pages] ? ONE_MILLION : @items_per_page,\n      :where_clause              => get_chart_where_clause(options[:sb_controller]),\n      :named_scope               => options[:named_scope],\n      :display_filter_hash       => options[:display_filter_hash],\n      :userid                    => session[:userid],\n      :selected_ids              => object_ids,\n      :match_via_descendants     => options[:match_via_descendants]\n    }\n\n    view.table, attrs = if fetch_data\n                          # Call paged_view_search to fetch records and build the view.table and additional attrs\n                          view.paged_view_search(session[:paged_view_search_options])\n                        else\n                          [{}, {}]\n                        end", "line_changes": {"deleted": [{"line_no": 59, "char_start": 3072, "char_end": 3132, "line": "        MiqReport.new(YAML.load(File.open(path_to_report)))\n"}], "added": [{"line_no": 59, "char_start": 3072, "char_end": 3147, "line": "        MiqReport.new(YAML.safe_load(File.open(path_to_report), [Symbol]))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 3099, "char_end": 3104, "chars": "safe_"}, {"char_start": 3134, "char_end": 3144, "chars": ", [Symbol]"}]}, "commit_link": "github.com/ManageIQ/manageiq-ui-classic/commit/b199ca1d7b5049ee4c0bd8323ea3977bf3de3341", "file_name": "application_controller.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load instead of YAML.load", "parent_commit": "cf54d8a126322759948bcf196e99a1a4399c62d0", "description": "Write a Ruby method named `get_view` that handles database view rendering with optional parameters and data fetching."}
{"func_name": "self.read_record", "func_src_before": "      def self.read_record(yaml_data)\n        RecordReader.convert_values_to_string(YAML.load(yaml_data))\n      end", "func_src_after": "      def self.read_record(yaml_data)\n        RecordReader.convert_values_to_string(YAML.safe_load(yaml_data,\n                                                             [Symbol]))\n      end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 38, "char_end": 106, "line": "        RecordReader.convert_values_to_string(YAML.load(yaml_data))\n"}], "added": [{"line_no": 2, "char_start": 38, "char_end": 110, "line": "        RecordReader.convert_values_to_string(YAML.safe_load(yaml_data,\n"}, {"line_no": 3, "char_start": 110, "char_end": 182, "line": "                                                             [Symbol]))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 89, "char_end": 94, "chars": "safe_"}, {"char_start": 108, "char_end": 179, "chars": ",\n                                                             [Symbol]"}]}, "commit_link": "github.com/nico-hn/AdHocTemplate/commit/4bc4ed79a2c45d64df03029bd05c3a426f5df020", "file_name": "record_reader.rb", "vul_type": "cwe-502", "commit_msg": "use YAML.safe_load() instead of .load()", "parent_commit": "f602247a29214ffce7e8d24caf690c2a420c3e99", "description": "Write a Ruby method that reads YAML data and converts its values to strings using a RecordReader class."}
{"func_name": "set_pre_prov_vars", "func_src_before": "  def set_pre_prov_vars\n    @layout = \"miq_request_vm\"\n    @edit = {}\n    @edit[:explorer] = @explorer\n    @edit[:vm_sortdir] ||= \"ASC\"\n    @edit[:vm_sortcol] ||= \"name\"\n    @edit[:prov_type] = \"VM Provision\"\n    @edit[:hide_deprecated_templates] = true if request.parameters[:controller] == \"vm_cloud\"\n\n    unless %w(image_miq_request_new miq_template_miq_request_new).include?(params[:pressed])\n      report_name = \"ProvisionTemplates.yaml\"\n      path_to_report = ManageIQ::UI::Classic::Engine.root.join(\"product\", \"views\", report_name).to_s\n      @view = MiqReport.new(YAML.load(File.open(path_to_report)))\n      @view.db = get_template_kls.to_s\n      report_scopes = %i(eligible_for_provisioning non_deprecated)\n      options = {\n        :model         => @view.db,\n        :gtl_type      => \"table\",\n        :named_scope   => report_scopes,\n        :report_name   => report_name,\n        :custom_action => {\n          :url  => \"/miq_request/pre_prov/?sel_id=\",\n          :type => 'provisioning'\n        }\n      }\n\n      @report_data_additional_options = ApplicationController::ReportDataAdditionalOptions.from_options(options)\n      @report_data_additional_options.with_no_checkboxes(true)\n\n      @edit[:template_kls] = get_template_kls\n    end\n    session[:changed] = false # Turn off the submit button\n    @edit[:explorer] = true if @explorer\n    @in_a_form = true\n  end", "func_src_after": "  def set_pre_prov_vars\n    @layout = \"miq_request_vm\"\n    @edit = {}\n    @edit[:explorer] = @explorer\n    @edit[:vm_sortdir] ||= \"ASC\"\n    @edit[:vm_sortcol] ||= \"name\"\n    @edit[:prov_type] = \"VM Provision\"\n    @edit[:hide_deprecated_templates] = true if request.parameters[:controller] == \"vm_cloud\"\n\n    unless %w(image_miq_request_new miq_template_miq_request_new).include?(params[:pressed])\n      report_name = \"ProvisionTemplates.yaml\"\n      path_to_report = ManageIQ::UI::Classic::Engine.root.join(\"product\", \"views\", report_name).to_s\n      @view = MiqReport.new(YAML.safe_load(File.open(path_to_report), [Symbol]))\n      @view.db = get_template_kls.to_s\n      report_scopes = %i(eligible_for_provisioning non_deprecated)\n      options = {\n        :model         => @view.db,\n        :gtl_type      => \"table\",\n        :named_scope   => report_scopes,\n        :report_name   => report_name,\n        :custom_action => {\n          :url  => \"/miq_request/pre_prov/?sel_id=\",\n          :type => 'provisioning'\n        }\n      }\n\n      @report_data_additional_options = ApplicationController::ReportDataAdditionalOptions.from_options(options)\n      @report_data_additional_options.with_no_checkboxes(true)\n\n      @edit[:template_kls] = get_template_kls\n    end\n    session[:changed] = false # Turn off the submit button\n    @edit[:explorer] = true if @explorer\n    @in_a_form = true\n  end", "line_changes": {"deleted": [{"line_no": 13, "char_start": 544, "char_end": 610, "line": "      @view = MiqReport.new(YAML.load(File.open(path_to_report)))\n"}], "added": [{"line_no": 13, "char_start": 544, "char_end": 625, "line": "      @view = MiqReport.new(YAML.safe_load(File.open(path_to_report), [Symbol]))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 577, "char_end": 582, "chars": "safe_"}, {"char_start": 612, "char_end": 622, "chars": ", [Symbol]"}]}, "commit_link": "github.com/ManageIQ/manageiq-ui-classic/commit/b199ca1d7b5049ee4c0bd8323ea3977bf3de3341", "file_name": "miq_request_methods.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load instead of YAML.load", "parent_commit": "cf54d8a126322759948bcf196e99a1a4399c62d0", "description": "Write a Ruby method named `set_pre_prov_vars` that initializes instance variables for VM provisioning settings and loads a YAML report configuration."}
{"func_name": "create_output", "func_src_before": "    def create_output\n    \n        # create the output RSS feed \n        version = \"2.0\" # [\"0.9\", \"1.0\", \"2.0\"]\n\n        content = RSS::Maker.make(version) do |m|\n            m.channel.title = \"Web Services Services. On the Web.\"\n            m.channel.link = \"http://www.bath.ac.uk/\"\n            m.channel.description = \"Status of the services run by Web Services\"\n            m.channel.lastBuildDate = Time.now\n            m.items.do_sort = true # sort items by date\n            \n            # for each result, add an entry in the output feed\n            @results.each { |result|\n                i = m.items.new_item\n                i.title = result[0]\n                i.link = result[1]\n                i.description = result[2]\n                i.date = Time.now    \n            }\n            \n        end\n\n        File.open(output_file,\"w\") do |f|\n            f.write(content)\n        end\n        \n    end", "func_src_after": "    def create_output\n    \n        # create the output RSS feed \n        version = \"2.0\" # [\"0.9\", \"1.0\", \"2.0\"]\n\n        content = RSS::Maker.make(version) do |m|\n            m.channel.title = \"Run, DMC, run!\"\n            m.channel.link = \"http://www.bath.ac.uk/\"\n            m.channel.description = \"Status of the services run by Digital Marketing and Communications\"\n            m.channel.lastBuildDate = Time.now\n            m.items.do_sort = true # sort items by date\n            \n            # for each result, add an entry in the output feed\n            @results.each { |result|\n                i = m.items.new_item\n                i.title = result[0]\n                i.link = result[1]\n                i.description = result[2]\n                i.date = Time.now    \n            }\n            \n        end\n\n        File.open(output_file,\"w\") do |f|\n            f.write(content)\n        end\n        \n    end", "line_changes": {"deleted": [{"line_no": 7, "char_start": 164, "char_end": 231, "line": "            m.channel.title = \"Web Services Services. On the Web.\"\n"}, {"line_no": 9, "char_start": 285, "char_end": 366, "line": "            m.channel.description = \"Status of the services run by Web Services\"\n"}], "added": [{"line_no": 7, "char_start": 164, "char_end": 211, "line": "            m.channel.title = \"Run, DMC, run!\"\n"}, {"line_no": 9, "char_start": 265, "char_end": 370, "line": "            m.channel.description = \"Status of the services run by Digital Marketing and Communications\"\n"}]}, "char_changes": {"deleted": [{"char_start": 195, "char_end": 229, "chars": "Web Services Services. On the Web."}, {"char_start": 352, "char_end": 363, "chars": "Web Service"}], "added": [{"char_start": 195, "char_end": 209, "chars": "Run, DMC, run!"}, {"char_start": 332, "char_end": 367, "chars": "Digital Marketing and Communication"}]}, "commit_link": "github.com/tomnatt/check-status/commit/67c9f3e19f0230d56c23c2e0e7c43a2cb2c4d052", "file_name": "test_runner.rb", "vul_type": "cwe-502", "commit_msg": "improve YAML loading", "parent_commit": "576c7e6e5e12d0be641958888f5b83afe0712d7e", "description": "Write a Ruby function to generate an RSS feed from a list of results and save it to a file."}
{"func_name": "images_from_fig", "func_src_before": "    def images_from_fig\n      fig_services = YAML.load(fig_yml) || {}\n      fig_services.map { |name, service_def| image_from_fig_service(name, service_def) }\n    end", "func_src_after": "    def images_from_fig\n      fig_services = YAML.safe_load(fig_yml) || {}\n      fig_services.map { |name, service_def| image_from_fig_service(name, service_def) }\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 24, "char_end": 70, "line": "      fig_services = YAML.load(fig_yml) || {}\n"}], "added": [{"line_no": 2, "char_start": 24, "char_end": 75, "line": "      fig_services = YAML.safe_load(fig_yml) || {}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 50, "char_end": 55, "chars": "safe_"}]}, "commit_link": "github.com/TravisCannon/panamax-api/commit/5f0bd8a0a60751bfd8ff51db83627b0477863b55", "file_name": "from_fig.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load when parsing user templates", "parent_commit": "0f311d932ddb665f5ebdde98cca040ec858f1010", "description": "Write a Ruby method named `images_from_fig` that loads a YAML configuration and maps each service to an image processing method."}
{"func_name": "initialize", "func_src_before": "    def initialize(json)\n      @params = YAML.load(json || '')\n    end", "func_src_after": "    def initialize(json)\n      @params = YAML.safe_load(json || '')\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 25, "char_end": 63, "line": "      @params = YAML.load(json || '')\n"}], "added": [{"line_no": 2, "char_start": 25, "char_end": 68, "line": "      @params = YAML.safe_load(json || '')\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 46, "char_end": 51, "chars": "safe_"}]}, "commit_link": "github.com/TravisCannon/panamax-api/commit/5f0bd8a0a60751bfd8ff51db83627b0477863b55", "file_name": "from_json.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load when parsing user templates", "parent_commit": "0f311d932ddb665f5ebdde98cca040ec858f1010", "description": "Create a Ruby method named `initialize` that loads a JSON string into a `@params` variable using YAML, with an optional use of a safer loading method."}
{"func_name": "test_creates_yaml_config_file_and_path_to_it_from_example_config", "func_src_before": "  def test_creates_yaml_config_file_and_path_to_it_from_example_config\n    refute File.exist?(CONFIG_PATH)\n    refute_nil ActsAsTextcaptcha::TextcaptchaConfig.create(path: CONFIG_PATH)\n    assert File.exist?(CONFIG_PATH)\n\n    example_config = YAML.load(File.read(CONFIG_PATH))\n    assert_equal example_config.keys, %w(development test production)\n  end", "func_src_after": "  def test_creates_yaml_config_file_and_path_to_it_from_example_config\n    refute File.exist?(CONFIG_PATH)\n    refute_nil ActsAsTextcaptcha::TextcaptchaConfig.create(path: CONFIG_PATH)\n    assert File.exist?(CONFIG_PATH)\n\n    example_config = YAML.safe_load(File.read(CONFIG_PATH))\n    assert_equal example_config.keys, %w(development test production)\n  end", "line_changes": {"deleted": [{"line_no": 6, "char_start": 222, "char_end": 277, "line": "    example_config = YAML.load(File.read(CONFIG_PATH))\n"}], "added": [{"line_no": 6, "char_start": 222, "char_end": 282, "line": "    example_config = YAML.safe_load(File.read(CONFIG_PATH))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 248, "char_end": 253, "chars": "safe_"}]}, "commit_link": "github.com/matthutchinson/acts_as_textcaptcha/commit/69dbe198b6e34491a8b6320c5290ccb945fa46d2", "file_name": "textcaptcha_config_test.rb", "vul_type": "cwe-502", "commit_msg": "Use safe_load instead of load", "parent_commit": "d4310888b14edf0a97e78873b595c822579e9137", "description": "Write a Ruby test method that checks the creation of a YAML configuration file and verifies its content."}
{"func_name": "test_raises_error_when_config_is_missing", "func_src_before": "  def test_raises_error_when_config_is_missing\n    YAML.stub :load, -> { raise \"bad things\" } do\n      exception = assert_raises(ArgumentError) do\n        # using eval here, sorry :(\n        eval <<-CLASS, binding, __FILE__, __LINE__ + 1\n          class SomeWidget < ApplicationRecord\n            acts_as_textcaptcha\n          end\n        CLASS\n      end\n      assert_match(/could not find any textcaptcha options/, exception.message)\n    end\n  end", "func_src_after": "  def test_raises_error_when_config_is_missing\n    YAML.stub :safe_load, -> { raise \"bad things\" } do\n      exception = assert_raises(ArgumentError) do\n        # using eval here, sorry :(\n        eval <<-CLASS, binding, __FILE__, __LINE__ + 1\n          class SomeWidget < ApplicationRecord\n            acts_as_textcaptcha\n          end\n        CLASS\n      end\n      assert_match(/could not find any textcaptcha options/, exception.message)\n    end\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 47, "char_end": 97, "line": "    YAML.stub :load, -> { raise \"bad things\" } do\n"}], "added": [{"line_no": 2, "char_start": 47, "char_end": 102, "line": "    YAML.stub :safe_load, -> { raise \"bad things\" } do\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 62, "char_end": 67, "chars": "safe_"}]}, "commit_link": "github.com/matthutchinson/acts_as_textcaptcha/commit/09b2c281859c07e8cc966e604153ba97bc3c6ec2", "file_name": "textcaptcha_test.rb", "vul_type": "cwe-502", "commit_msg": "update tests to use YAML.safe_load", "parent_commit": "f9b9d1623306bb621cd1c14574a4a07513368ce7", "description": "Write a Ruby test method that checks if an error is raised when a configuration for a widget class is missing."}
{"func_name": "self.register_user_defined_tag_type", "func_src_before": "    def self.register_user_defined_tag_type(config_source)\n      config = YAML.load(config_source)\n      check_validity_of_config(config)\n      TagType.register(registered_tag_name = config['tag_name'].to_sym,\n                       config['tag'],\n                       config['iteration_tag'],\n                       config['fallback_tag'],\n                       config['remove_indent'] || false)\n      registered_tag_name\n    end", "func_src_after": "    def self.register_user_defined_tag_type(config_source)\n      config = YAML.safe_load(config_source, [Symbol])\n      check_validity_of_config(config)\n      TagType.register(registered_tag_name = config['tag_name'].to_sym,\n                       config['tag'],\n                       config['iteration_tag'],\n                       config['fallback_tag'],\n                       config['remove_indent'] || false)\n      registered_tag_name\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 59, "char_end": 99, "line": "      config = YAML.load(config_source)\n"}], "added": [{"line_no": 2, "char_start": 59, "char_end": 114, "line": "      config = YAML.safe_load(config_source, [Symbol])\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 79, "char_end": 84, "chars": "safe_"}, {"char_start": 102, "char_end": 112, "chars": ", [Symbol]"}]}, "commit_link": "github.com/nico-hn/AdHocTemplate/commit/4bc4ed79a2c45d64df03029bd05c3a426f5df020", "file_name": "parser.rb", "vul_type": "cwe-502", "commit_msg": "use YAML.safe_load() instead of .load()", "parent_commit": "f602247a29214ffce7e8d24caf690c2a420c3e99", "description": "Write a Ruby method to register a user-defined tag type from a YAML configuration source."}
{"func_name": "load_yaml_from", "func_src_before": "    def load_yaml_from(path)\n      YAML.load(File.read(path))\n    end", "func_src_after": "    def load_yaml_from(path)\n      YAML.load_file(path)\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 29, "char_end": 62, "line": "      YAML.load(File.read(path))\n"}], "added": [{"line_no": 2, "char_start": 29, "char_end": 56, "line": "      YAML.load_file(path)\n"}]}, "char_changes": {"deleted": [{"char_start": 44, "char_end": 46, "chars": "(F"}, {"char_start": 49, "char_end": 54, "chars": ".read"}, {"char_start": 60, "char_end": 61, "chars": ")"}], "added": [{"char_start": 44, "char_end": 46, "chars": "_f"}]}, "commit_link": "github.com/jiripospisil/rock_config/commit/c6b1ffc7c71082ebfec8760d370e6ddaa36fd4d7", "file_name": "yaml_loader.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML's load_file instead of doing File read and YAML load", "parent_commit": "5252713e4b9a9b423f7d84043a034abdb210f9a8", "description": "Write a Ruby function named `load_yaml_from` that takes a file path as an argument and returns the contents of the YAML file."}
{"func_name": "load_sample_environment_variables", "func_src_before": "  def load_sample_environment_variables\n    env_file = File.open('env_configuration_for_local_gem_tests.yml')\n\n    YAML.load(env_file).each do |key, value|\n      ENV[key.to_s] = value\n    end\n  end", "func_src_after": "  def load_sample_environment_variables\n    env_file = File.open('env_configuration_for_local_gem_tests.yml')\n\n    YAML.safe_load(env_file).each do |key, value|\n      ENV[key.to_s] = value\n    end\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 111, "char_end": 156, "line": "    YAML.load(env_file).each do |key, value|\n"}], "added": [{"line_no": 4, "char_start": 111, "char_end": 161, "line": "    YAML.safe_load(env_file).each do |key, value|\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 120, "char_end": 125, "chars": "safe_"}]}, "commit_link": "github.com/fastly/fastly_nsq/commit/c16a48dd2f7b0a67d02c3fae35f0ec9ffaea9839", "file_name": "spec_helper.rb", "vul_type": "cwe-502", "commit_msg": "rubocop YAML.safe_load over just load\n\nthis is the spec_helper and doesn't appear to affect the running of the\ntests...", "parent_commit": "f6b18edc68d1040afdedd59a75a167dcdc244f8e", "description": "Write a Ruby method to load environment variables from a YAML file."}
{"func_name": "defaults", "func_src_before": "      def defaults\n        @defaults ||= begin\n          path = File.expand_path('../../partitions.json', __FILE__)\n          defaults = if JSON::VERSION >= '2.4.0'\n            JSON.load(File.read(path), freeze: true)\n          else\n            JSON.parse(File.read(path))\n          end\n          defaults.merge('partitions' => defaults['partitions'].dup)\n        end", "func_src_after": "      def defaults\n        @defaults ||= begin\n          path = File.expand_path('../../partitions.json', __FILE__)\n          puts \"Ruby: #{RUBY_VERSION}\"\n          puts \"JSON version: #{JSON::VERSION}\"\n          defaults = JSON.parse(File.read(path), freeze: true)\n          puts \"Is it frozen? #{defaults.frozen?}. keys: #{defaults.keys}\"\n          defaults.merge('partitions' => defaults['partitions'].dup)\n        end\n      end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 116, "char_end": 165, "line": "          defaults = if JSON::VERSION >= '2.4.0'\n"}, {"line_no": 5, "char_start": 165, "char_end": 218, "line": "            JSON.load(File.read(path), freeze: true)\n"}, {"line_no": 6, "char_start": 218, "char_end": 233, "line": "          else\n"}, {"line_no": 7, "char_start": 233, "char_end": 273, "line": "            JSON.parse(File.read(path))\n"}, {"line_no": 8, "char_start": 273, "char_end": 287, "line": "          end\n"}], "added": [{"line_no": 4, "char_start": 116, "char_end": 155, "line": "          puts \"Ruby: #{RUBY_VERSION}\"\n"}, {"line_no": 5, "char_start": 155, "char_end": 203, "line": "          puts \"JSON version: #{JSON::VERSION}\"\n"}, {"line_no": 6, "char_start": 203, "char_end": 266, "line": "          defaults = JSON.parse(File.read(path), freeze: true)\n"}, {"line_no": 7, "char_start": 266, "char_end": 341, "line": "          puts \"Is it frozen? #{defaults.frozen?}. keys: #{defaults.keys}\"\n"}]}, "char_changes": {"deleted": [{"char_start": 126, "char_end": 140, "chars": "defaults = if "}, {"char_start": 153, "char_end": 164, "chars": " >= '2.4.0'"}, {"char_start": 175, "char_end": 176, "chars": " "}, {"char_start": 182, "char_end": 186, "chars": "load"}, {"char_start": 228, "char_end": 286, "chars": "else\n            JSON.parse(File.read(path))\n          end"}, {"char_start": 356, "char_end": 356, "chars": ""}], "added": [{"char_start": 126, "char_end": 187, "chars": "puts \"Ruby: #{RUBY_VERSION}\"\n          puts \"JSON version: #{"}, {"char_start": 200, "char_end": 202, "chars": "}\""}, {"char_start": 213, "char_end": 223, "chars": "defaults ="}, {"char_start": 229, "char_end": 234, "chars": "parse"}, {"char_start": 276, "char_end": 340, "chars": "puts \"Is it frozen? #{defaults.frozen?}. keys: #{defaults.keys}\""}, {"char_start": 421, "char_end": 431, "chars": "\n      end"}]}, "commit_link": "github.com/aws/aws-sdk-ruby/commit/2cd5e0948298e607c781fd48c4d99f523a016e19", "file_name": "aws-partitions.rb", "vul_type": "cwe-502", "commit_msg": "Add debugging and fix json.load proc param issue.", "parent_commit": "3105e654a5d39c2fd2a6be5e24cdf673a9889204", "description": "Write a Ruby method that loads and processes a JSON file, handling different JSON versions and outputting version information."}
{"func_name": "test_creates_yaml_config_file_and_path_to_it_from_example_config", "func_src_before": "  def test_creates_yaml_config_file_and_path_to_it_from_example_config\n    refute File.exist?(CONFIG_PATH)\n    refute_nil ActsAsTextcaptcha::TextcaptchaConfig.create(path: CONFIG_PATH)\n    assert File.exist?(CONFIG_PATH)\n\n    # rubocop:disable Security/YAMLLoad\n    example_config = YAML.load(File.read(CONFIG_PATH))\n    # rubocop:enable Security/YAMLLoad\n    assert_equal example_config.keys, %w(development test production)\n  end", "func_src_after": "  def test_creates_yaml_config_file_and_path_to_it_from_example_config\n    refute File.exist?(CONFIG_PATH)\n    refute_nil ActsAsTextcaptcha::TextcaptchaConfig.create(path: CONFIG_PATH)\n    assert File.exist?(CONFIG_PATH)\n\n    # rubocop:disable Security/YAMLLoad\n    example_config = YAML.safe_load(File.read(CONFIG_PATH), aliases: true)\n    # rubocop:enable Security/YAMLLoad\n    assert_equal example_config.keys, %w(development test production)\n  end", "line_changes": {"deleted": [{"line_no": 7, "char_start": 262, "char_end": 317, "line": "    example_config = YAML.load(File.read(CONFIG_PATH))\n"}], "added": [{"line_no": 7, "char_start": 262, "char_end": 337, "line": "    example_config = YAML.safe_load(File.read(CONFIG_PATH), aliases: true)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 288, "char_end": 293, "chars": "safe_"}, {"char_start": 320, "char_end": 335, "chars": ", aliases: true"}]}, "commit_link": "github.com/matthutchinson/acts_as_textcaptcha/commit/09b2c281859c07e8cc966e604153ba97bc3c6ec2", "file_name": "textcaptcha_config_test.rb", "vul_type": "cwe-502", "commit_msg": "update tests to use YAML.safe_load", "parent_commit": "f9b9d1623306bb621cd1c14574a4a07513368ce7", "description": "Write a Ruby test method that checks the creation of a YAML configuration file and verifies its contents."}
{"func_name": "initialize", "func_src_before": "    def initialize(*configs)\n      @config = configs.each_with_object(default_config) do |path, obj|\n        new = YAML.load(File.read(path))\n        next unless new\n        obj.deep_merge! Cymbal.symbolize(new)\n      end\n      @paths = @config[:paths]\n    end", "func_src_after": "    def initialize(*configs)\n      @config = configs.each_with_object(default_config) do |path, obj|\n        new = YAML.safe_load(File.read(path))\n        next unless new\n        obj.deep_merge! Cymbal.symbolize(new)\n      end\n      @paths = @config[:paths]\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 101, "char_end": 142, "line": "        new = YAML.load(File.read(path))\n"}], "added": [{"line_no": 3, "char_start": 101, "char_end": 147, "line": "        new = YAML.safe_load(File.read(path))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 120, "char_end": 125, "chars": "safe_"}]}, "commit_link": "github.com/dock0/dock0/commit/c4800a6be1e62b124b401214b416b7adedca341a", "file_name": "dock0.rb", "vul_type": "cwe-502", "commit_msg": "use safe_load", "parent_commit": "243b9d713e05eebb5ef8e17bc568dfefcd2a32a2", "description": "Write a Ruby method named `initialize` that merges multiple YAML configuration files into a default configuration object and stores specific paths from the configuration."}
{"func_name": "load_blacklist!", "func_src_before": "  def load_blacklist!\n    if defined?(Rails.root) && (blacklist_file_path = Rails.root.join(\"config\", \"blacklist.yml\")).exist?\n      blacklist_path = blacklist_file_path\n    end\n    blacklist_path ||= File.read(File.join(File.dirname(__FILE__), \"../config/blacklist.yml\"))\n    @blacklist = YAML::load(blacklist_path)\n  end", "func_src_after": "  def load_blacklist!\n    if defined?(Rails.root) && (blacklist_file_path = Rails.root.join(\"config\", \"blacklist.yml\")).exist?\n      blacklist_path = blacklist_file_path\n    end\n    blacklist_path ||= File.join(File.dirname(__FILE__), \"../config/blacklist.yml\")\n    @blacklist = YAML.load_file(blacklist_path)\n  end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 178, "char_end": 273, "line": "    blacklist_path ||= File.read(File.join(File.dirname(__FILE__), \"../config/blacklist.yml\"))\n"}, {"line_no": 6, "char_start": 273, "char_end": 317, "line": "    @blacklist = YAML::load(blacklist_path)\n"}], "added": [{"line_no": 5, "char_start": 178, "char_end": 262, "line": "    blacklist_path ||= File.join(File.dirname(__FILE__), \"../config/blacklist.yml\")\n"}, {"line_no": 6, "char_start": 262, "char_end": 310, "line": "    @blacklist = YAML.load_file(blacklist_path)\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 211, "chars": "File.read("}, {"char_start": 271, "char_end": 272, "chars": ")"}, {"char_start": 294, "char_end": 296, "chars": "::"}], "added": [{"char_start": 283, "char_end": 284, "chars": "."}, {"char_start": 288, "char_end": 293, "chars": "_file"}]}, "commit_link": "github.com/episko/blacklist_validator/commit/76255a46f62a8cd082d5a6fb133d2c0b6c2438d6", "file_name": "blacklist_validator.rb", "vul_type": "cwe-502", "commit_msg": "Fixed YAML loading Rails file", "parent_commit": "1aaf965676d2ec5fb505d50c169d0029a411f84a", "description": "Write a Ruby method to load a YAML blacklist configuration file, with a fallback to a default path if the Rails root is not defined."}
{"func_name": "load_files", "func_src_before": "    def load_files(*file_paths)\n      files = (site_configs + file_paths).map { |f| Pathname.new(f) }\n      # TODO: Validate config state in some way.\n      configs = files.map { |file| YAML.load(file.read) }\n\n      load(*configs)\n    end", "func_src_after": "    def load_files(*file_paths)\n      files = (site_configs + file_paths).map { |f| Pathname.new(f) }\n      # TODO: Validate config state in some way.\n      configs = files.map { |file| YAML.safe_load(file.read) }\n\n      load(*configs)\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 151, "char_end": 209, "line": "      configs = files.map { |file| YAML.load(file.read) }\n"}], "added": [{"line_no": 4, "char_start": 151, "char_end": 214, "line": "      configs = files.map { |file| YAML.safe_load(file.read) }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 191, "char_end": 196, "chars": "safe_"}]}, "commit_link": "github.com/duckinator/how_is/commit/2c816659422d4261e1fbb3be24af389d86930a01", "file_name": "config.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load, not YAML.load.", "parent_commit": "6317a8e8d18f8e49a0af02b8a031450a5695b1ef", "description": "Write a Ruby method to load configurations from file paths using YAML parsing."}
{"func_name": "get_configuration_data", "func_src_before": "    def get_configuration_data(path)\n      return \"\" if path.nil? # can be removedish...\n      if is_path_url?(path)\n        \n        config_text = read_config_data_from_web(path)\n        if !valid_yaml_string?(config_text)\n          path = get_proper_raw_url_for_github_config_file(path)\n          config_text = read_config_data_from_web(path)\n        end\n        \n      elsif is_path_valid_file?(path)\n        config_text = read_config_data_from_file(path)\n      else\n        throw \"get_configuration_data:  bad URL or file path.  Couldn't find configuration data.\"\n      end\n      \n      \n      YAML.load config_text\n    end", "func_src_after": "    def get_configuration_data(path)\n      return \"\" if path.nil? # can be removedish...\n      if is_path_url?(path)\n        \n        config_text = read_config_data_from_web(path)\n        if !valid_yaml_string?(config_text)\n          path = get_proper_raw_url_for_github_config_file(path)\n          config_text = read_config_data_from_web(path)\n        end\n        \n      elsif is_path_valid_file?(path)\n        config_text = read_config_data_from_file(path)\n      else\n        throw \"get_configuration_data:  bad URL or file path.  Couldn't find configuration data.\"\n      end\n      \n      \n      YAML.safe_load config_text\n    end", "line_changes": {"deleted": [{"line_no": 18, "char_start": 592, "char_end": 620, "line": "      YAML.load config_text\n"}], "added": [{"line_no": 18, "char_start": 592, "char_end": 625, "line": "      YAML.safe_load config_text\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 603, "char_end": 608, "chars": "safe_"}]}, "commit_link": "github.com/TheNotary/lmpm/commit/e354365b6b6e3fd705dc89ecffe650a06a429fa7", "file_name": "configurator.rb", "vul_type": "cwe-502", "commit_msg": "changed a yaml load to safe_load", "parent_commit": "cae840d22328d8562cc98a4a793aa7c6cb2cafcc", "description": "Create a Ruby function that retrieves configuration data from a given path, which can be a URL or a file path, and parses it as YAML."}
{"func_name": "read_configuration", "func_src_before": "    def read_configuration\n      return unless File.exist?(configuration_file)\n      YAML.load(File.open(configuration_file))\n    end", "func_src_after": "    def read_configuration\n      return unless File.exist?(configuration_file)\n      YAML.safe_load(File.open(configuration_file))\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 79, "char_end": 126, "line": "      YAML.load(File.open(configuration_file))\n"}], "added": [{"line_no": 3, "char_start": 79, "char_end": 131, "line": "      YAML.safe_load(File.open(configuration_file))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 90, "char_end": 95, "chars": "safe_"}]}, "commit_link": "github.com/mroth/lolcommits/commit/7eeef6effea8eab9b9019b21cd225f67d0f8c474", "file_name": "configuration.rb", "vul_type": "cwe-502", "commit_msg": "use YAML.safe_load", "parent_commit": "ea9d98ed863ad58bffba584a3d15b8752a359bfc", "description": "Create a Ruby function that loads configuration from a YAML file if the file exists."}
{"func_name": "json_decode", "func_src_before": "      def json_decode(obj)\n        JSON.load(obj)\n      end", "func_src_after": "      def json_decode(obj)\n        JSON.parse(obj, create_additions: false)\n      end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 27, "char_end": 50, "line": "        JSON.load(obj)\n"}], "added": [{"line_no": 2, "char_start": 27, "char_end": 76, "line": "        JSON.parse(obj, create_additions: false)\n"}]}, "char_changes": {"deleted": [{"char_start": 40, "char_end": 48, "chars": "load(obj"}], "added": [{"char_start": 40, "char_end": 74, "chars": "parse(obj, create_additions: false"}]}, "commit_link": "github.com/Andreis13/sprockets/commit/e548f03540b4311adc47870815e8d5fd833825cf", "file_name": "base.rb", "vul_type": "cwe-502", "commit_msg": "replace `JSON` `dump`/`load` with `parse`/`generate`\n\n`dump` and `load` are for built around Marshaling ruby objects generally.\nThey correspond with those methods on Ruby's `Marshal` class. Theses\nmethods actually call `parse`/`generate` in code but pass some defaults\nalong with it. Sprockets only needs to parse JSON documents not Ruby\nobjects.\n\nFor `JSON.load`, we want to disable `create_additions`.\n`create_additions` could be considered a security hazard if set to true.\n`create_additions` allows the instantiation of any class that's\nmarshaled as json", "parent_commit": "b18af736eac52f11c8704a43be36f2d21cf44ce2", "description": "Write a Ruby method named `json_decode` that takes a string `obj` and converts it into a JSON object."}
{"func_name": "self.load_config", "func_src_before": "    def self.load_config(country_code)\n      default_config = YAML.\n        load(File.read(File.dirname(__FILE__) + '/conversion_rules.yml'))\n      default_config[country_code]\n    end", "func_src_after": "    def self.load_config(country_code)\n      default_config = YAML.\n        load_file(File.join(File.dirname(__FILE__), 'conversion_rules.yml'))\n      default_config[country_code]\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 68, "char_end": 142, "line": "        load(File.read(File.dirname(__FILE__) + '/conversion_rules.yml'))\n"}], "added": [{"line_no": 3, "char_start": 68, "char_end": 145, "line": "        load_file(File.join(File.dirname(__FILE__), 'conversion_rules.yml'))\n"}]}, "char_changes": {"deleted": [{"char_start": 86, "char_end": 90, "chars": "read"}, {"char_start": 113, "char_end": 115, "chars": " +"}, {"char_start": 117, "char_end": 118, "chars": "/"}], "added": [{"char_start": 80, "char_end": 85, "chars": "_file"}, {"char_start": 91, "char_end": 95, "chars": "join"}, {"char_start": 118, "char_end": 119, "chars": ","}]}, "commit_link": "github.com/alphasights/iban-tools/commit/d4954482c31d51a9e9896665d7019b9067f12bf7", "file_name": "conversion.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.load_file instead of YAML.load(File.Read).\n\nUse File.join instead of concatenating bits of path.", "parent_commit": "f6591635f671dad99246b594ae4c47e068e69c00", "description": "Write a Ruby method to load a country-specific configuration from a YAML file using a country code."}
{"func_name": "decode", "func_src_before": "      def decode(json, proc = nil, options = {})\n        data = ::JSON.load(json, proc, options)\n        if ActiveSupport.parse_json_times\n          convert_dates_from(data)\n        else\n          data\n        end\n      end", "func_src_after": "      def decode(json, options = {})\n        data = ::JSON.parse(json, options.merge(create_additions: false))\n        if ActiveSupport.parse_json_times\n          convert_dates_from(data)\n        else\n          data\n        end\n      end", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 49, "line": "      def decode(json, proc = nil, options = {})\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 37, "line": "      def decode(json, options = {})\n"}, {"line_no": 2, "char_start": 37, "char_end": 111, "line": "        data = ::JSON.parse(json, options.merge(create_additions: false))\n"}]}, "char_changes": {"deleted": [{"char_start": 22, "char_end": 34, "chars": " proc = nil,"}, {"char_start": 71, "char_end": 75, "chars": "load"}, {"char_start": 82, "char_end": 88, "chars": "proc, "}], "added": [{"char_start": 59, "char_end": 64, "chars": "parse"}, {"char_start": 78, "char_end": 109, "chars": ".merge(create_additions: false)"}]}, "commit_link": "github.com/baerjam/rails/commit/b9e142af529b20720fc34bc5f563e935a7ef7cda", "file_name": "decoding.rb", "vul_type": "cwe-502", "commit_msg": "Replace JSON.load with JSON.parse, also removed the proc parameter\n\nSince we are dealing with untrusted user input, we should not be\nusing JSON.load. According to the docs[1]:\n\nBEWARE: This method is meant to serialise data from trusted user\ninput, like from your own database server or clients under your\ncontrol, it could be dangerous to allow untrusted users to pass\nJSON sources into it. The default options for the parser can be\nchanged via the ::load_default_options method.\n\n[1] http://www.ruby-doc.org/stdlib-2.0/libdoc/json/rdoc/JSON.html#method-i-load", "parent_commit": "3d60e9d5503b5f657336a8b7ee6345552ddb6c83", "description": "Write a Ruby method named `decode` that processes JSON input and optionally converts date strings to date objects."}
{"func_name": "self.save", "func_src_before": "  def self.save(key, obj, opts = {})\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n    file = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    fail ArgumentError, \"Error, file #{file} exists\" if file.exist?\n    File.open(file, 'w+') { |f| f.puts YAML.dump(obj) }\n  end", "func_src_after": "  def self.save(key, obj, opts = {})\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n    file = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    raise ArgumentError, \"Error, file #{file} exists\" if file.exist?\n    File.open(file, 'w+') { |f| f.puts YAML.dump(obj) }\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 169, "char_end": 237, "line": "    fail ArgumentError, \"Error, file #{file} exists\" if file.exist?\n"}], "added": [{"line_no": 4, "char_start": 169, "char_end": 238, "line": "    raise ArgumentError, \"Error, file #{file} exists\" if file.exist?\n"}]}, "char_changes": {"deleted": [{"char_start": 173, "char_end": 174, "chars": "f"}, {"char_start": 176, "char_end": 177, "chars": "l"}], "added": [{"char_start": 173, "char_end": 174, "chars": "r"}, {"char_start": 176, "char_end": 178, "chars": "se"}]}, "commit_link": "github.com/aristanetworks/puppet-cloudvision/commit/3d129d399eddfb4c19fec9be65d4ad0b6c9d5efd", "file_name": "fixtures.rb", "vul_type": "cwe-502", "commit_msg": "Replace YAML.load with safe_load and fail -> raise", "parent_commit": "39e5a4f8644314b9469c9011f8feb1553f7ad2e5", "description": "Write a Ruby method that saves an object to a YAML file, optionally in a specified directory, and raises an error if the file already exists."}
{"func_name": "initialize", "func_src_before": "    def initialize(config, path = nil)\n      @config = YAML.load(config)\n      @path = path\n\n      unless @config.is_a? Hash\n        raise ValidationError, \"YAML should be a hash\"\n      end\n\n      @config = @config.deep_symbolize_keys\n\n      initial_parsing\n\n      validate!\n    end", "func_src_after": "    def initialize(config, path = nil)\n      @config = YAML.safe_load(config)\n      @path = path\n\n      unless @config.is_a? Hash\n        raise ValidationError, \"YAML should be a hash\"\n      end\n\n      @config = @config.deep_symbolize_keys\n\n      initial_parsing\n\n      validate!\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 39, "char_end": 73, "line": "      @config = YAML.load(config)\n"}], "added": [{"line_no": 2, "char_start": 39, "char_end": 78, "line": "      @config = YAML.safe_load(config)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 60, "char_end": 65, "chars": "safe_"}]}, "commit_link": "github.com/screenpages/gitlabhq/commit/c5dacce4d7e47a0504975fbb3bfaf478b95f1065", "file_name": "gitlab_ci_yaml_processor.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load", "parent_commit": "7b50965e9990bcb88f56b771d47514cbeb5316e5", "description": "Create a Ruby method named `initialize` that loads a YAML configuration, symbolizes its keys, and performs validation and initial parsing."}
{"func_name": "read_yaml_file", "func_src_before": "        def read_yaml_file(filename)\n            # read the yaml into a string\n            data = ''\n            f = File.open(filename, \"r\") \n            f.each_line do |line|\n                data += line\n            end\n            # parse the yaml-string\n            test = YAML::load(data)\n            return test\n        end", "func_src_after": "        def read_yaml_file(filename)\n            test = YAML::load_file(filename)\n            return test\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 79, "char_end": 101, "line": "            data = ''\n"}, {"line_no": 4, "char_start": 101, "char_end": 143, "line": "            f = File.open(filename, \"r\") \n"}, {"line_no": 5, "char_start": 143, "char_end": 177, "line": "            f.each_line do |line|\n"}, {"line_no": 6, "char_start": 177, "char_end": 206, "line": "                data += line\n"}, {"line_no": 7, "char_start": 206, "char_end": 222, "line": "            end\n"}, {"line_no": 9, "char_start": 258, "char_end": 294, "line": "            test = YAML::load(data)\n"}], "added": [{"line_no": 2, "char_start": 37, "char_end": 82, "line": "            test = YAML::load_file(filename)\n"}]}, "char_changes": {"deleted": [{"char_start": 49, "char_end": 292, "chars": "# read the yaml into a string\n            data = ''\n            f = File.open(filename, \"r\") \n            f.each_line do |line|\n                data += line\n            end\n            # parse the yaml-string\n            test = YAML::load(data"}], "added": [{"char_start": 49, "char_end": 80, "chars": "test = YAML::load_file(filename"}]}, "commit_link": "github.com/tomnatt/check-status/commit/67c9f3e19f0230d56c23c2e0e7c43a2cb2c4d052", "file_name": "test_runner.rb", "vul_type": "cwe-502", "commit_msg": "improve YAML loading", "parent_commit": "576c7e6e5e12d0be641958888f5b83afe0712d7e", "description": "Write a Ruby function to read and parse a YAML file."}
{"func_name": "json_encode", "func_src_before": "      def json_encode(obj)\n        JSON.dump(obj)\n      end", "func_src_after": "      def json_encode(obj)\n        JSON.generate(obj)\n      end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 27, "char_end": 50, "line": "        JSON.dump(obj)\n"}], "added": [{"line_no": 2, "char_start": 27, "char_end": 54, "line": "        JSON.generate(obj)\n"}]}, "char_changes": {"deleted": [{"char_start": 40, "char_end": 44, "chars": "dump"}], "added": [{"char_start": 40, "char_end": 48, "chars": "generate"}]}, "commit_link": "github.com/Andreis13/sprockets/commit/e548f03540b4311adc47870815e8d5fd833825cf", "file_name": "manifest.rb", "vul_type": "cwe-502", "commit_msg": "replace `JSON` `dump`/`load` with `parse`/`generate`\n\n`dump` and `load` are for built around Marshaling ruby objects generally.\nThey correspond with those methods on Ruby's `Marshal` class. Theses\nmethods actually call `parse`/`generate` in code but pass some defaults\nalong with it. Sprockets only needs to parse JSON documents not Ruby\nobjects.\n\nFor `JSON.load`, we want to disable `create_additions`.\n`create_additions` could be considered a security hazard if set to true.\n`create_additions` allows the instantiation of any class that's\nmarshaled as json", "parent_commit": "b18af736eac52f11c8704a43be36f2d21cf44ce2", "description": "Write a Ruby method named `json_encode` that converts an object to a JSON string."}
{"func_name": "data", "func_src_before": "    def data\n      @data ||= YAML.load(File.read(path))\n    end", "func_src_after": "    def data\n      @data ||= YAML.safe_load(File.read(path))\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 13, "char_end": 56, "line": "      @data ||= YAML.load(File.read(path))\n"}], "added": [{"line_no": 2, "char_start": 13, "char_end": 61, "line": "      @data ||= YAML.safe_load(File.read(path))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 34, "char_end": 39, "chars": "safe_"}]}, "commit_link": "github.com/agorf/feed2email/commit/7f01d4f94138173904be758e7423e2491c8ad7c6", "file_name": "config.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load to parse config", "parent_commit": "55bb971f8fbe1b994f78e69f37e59c11bc2328d4", "description": "Create a Ruby method named `data` that lazily loads and memoizes the contents of a YAML file from a given path."}
{"func_name": "get", "func_src_before": "        def get(key)\n          data = Marshal.load(File.read( @path.join(key)))\n          Cache.logger.info(\"Cache: #{data.nil? ? \"miss\" : \"hit\"} (#{key})\")\n          data\n        end", "func_src_after": "        def get(key)\n          data = Marshal.load(File.new(@path.join(key)))\n          Cache.logger.info(\"Cache: #{data.nil? ? \"miss\" : \"hit\"} (#{key})\")\n          data\n        end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 21, "char_end": 80, "line": "          data = Marshal.load(File.read( @path.join(key)))\n"}], "added": [{"line_no": 2, "char_start": 21, "char_end": 78, "line": "          data = Marshal.load(File.new(@path.join(key)))\n"}]}, "char_changes": {"deleted": [{"char_start": 56, "char_end": 62, "chars": "read( "}], "added": [{"char_start": 56, "char_end": 60, "chars": "new("}]}, "commit_link": "github.com/trakt/tvdb_party/commit/174a8b9b0a595a83504551aeac3e96f9964eb7a0", "file_name": "httparty_icebox.rb", "vul_type": "cwe-502", "commit_msg": "on 1.9 we need Marshal.load to read directly from the IO to prevent encoding issues", "parent_commit": "4b523c4621f0ab16e9d6936caf9d99bc2913b26d", "description": "Write a Ruby method named `get` that retrieves data from a file based on a key and logs whether the cache was hit or missed."}
{"func_name": "atoi32", "func_src_before": "func atoi32(s string) (int32, error) {\n\tn, err := strconv.Atoi(s)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn int32(n), nil\n}", "func_src_after": "func atoi32(s string) (int32, error) {\n\tn, err := strconv.ParseInt(s, 0, 32)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn int32(n), nil\n}", "line_changes": {"deleted": [{"line_no": 2, "char_start": 39, "char_end": 66, "line": "\tn, err := strconv.Atoi(s)\n"}], "added": [{"line_no": 2, "char_start": 39, "char_end": 77, "line": "\tn, err := strconv.ParseInt(s, 0, 32)\n"}]}, "char_changes": {"deleted": [{"char_start": 58, "char_end": 64, "chars": "Atoi(s"}], "added": [{"char_start": 58, "char_end": 75, "chars": "ParseInt(s, 0, 32"}]}, "commit_link": "github.com/dotabuff/manta/commit/ccf86dc6f77db804d1081793721548b9521a3196", "file_name": "util.go", "vul_type": "cwe-681", "commit_msg": "Fix atoi32 for large numbers on 64 bit OSes.\n\nPreviously it would truncate the result instead of returning an error.\n\nBecause int is 64 bits on 64 bit operating systems, strconv.Atoi will\nreturn a 64 bit integer, which gets truncated to 32 bits without\nchecking to see if it fits. strconv.ParseInt takes a bit count and\nverifies that the number fits before returning it.", "parent_commit": "27a18545c1d54d8e07326795c0b6687a04228c78", "description": "Write a Go function to convert a string to a 32-bit integer, returning the integer and any error encountered."}
{"func_name": "ParseMPLSLabelStack", "func_src_before": "func ParseMPLSLabelStack(buf string) (*MPLSLabelStack, error) {\n\telems := strings.Split(buf, \"/\")\n\tlabels := make([]uint32, 0, len(elems))\n\tif len(elems) == 0 {\n\t\tgoto ERR\n\t}\n\tfor _, elem := range elems {\n\t\ti, err := strconv.Atoi(elem)\n\t\tif err != nil {\n\t\t\tgoto ERR\n\t\t}\n\t\tif i < 0 || i > ((1<<20)-1) {\n\t\t\tgoto ERR\n\t\t}\n\t\tlabels = append(labels, uint32(i))\n\t}\n\treturn NewMPLSLabelStack(labels...), nil\nERR:\n\treturn nil, NewMessageError(BGP_ERROR_UPDATE_MESSAGE_ERROR, BGP_ERROR_SUB_MALFORMED_ATTRIBUTE_LIST, nil, \"invalid mpls label stack format\")\n}", "func_src_after": "func ParseMPLSLabelStack(buf string) (*MPLSLabelStack, error) {\n\telems := strings.Split(buf, \"/\")\n\tlabels := make([]uint32, 0, len(elems))\n\tif len(elems) == 0 {\n\t\tgoto ERR\n\t}\n\tfor _, elem := range elems {\n\t\ti, err := strconv.ParseUint(elem, 10, 32)\n\t\tif err != nil {\n\t\t\tgoto ERR\n\t\t}\n\t\tif i > ((1 << 20) - 1) {\n\t\t\tgoto ERR\n\t\t}\n\t\tlabels = append(labels, uint32(i))\n\t}\n\treturn NewMPLSLabelStack(labels...), nil\nERR:\n\treturn nil, NewMessageError(BGP_ERROR_UPDATE_MESSAGE_ERROR, BGP_ERROR_SUB_MALFORMED_ATTRIBUTE_LIST, nil, \"invalid mpls label stack format\")\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 205, "char_end": 236, "line": "\t\ti, err := strconv.Atoi(elem)\n"}, {"line_no": 12, "char_start": 270, "char_end": 302, "line": "\t\tif i < 0 || i > ((1<<20)-1) {\n"}], "added": [{"line_no": 8, "char_start": 205, "char_end": 249, "line": "\t\ti, err := strconv.ParseUint(elem, 10, 32)\n"}, {"line_no": 12, "char_start": 283, "char_end": 310, "line": "\t\tif i > ((1 << 20) - 1) {\n"}]}, "char_changes": {"deleted": [{"char_start": 225, "char_end": 234, "chars": "Atoi(elem"}, {"char_start": 274, "char_end": 283, "chars": " i < 0 ||"}, {"char_start": 296, "char_end": 297, "chars": "-"}], "added": [{"char_start": 225, "char_end": 247, "chars": "ParseUint(elem, 10, 32"}, {"char_start": 295, "char_end": 296, "chars": " "}, {"char_start": 298, "char_end": 299, "chars": " "}, {"char_start": 302, "char_end": 305, "chars": " - "}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse a string of MPLS labels separated by slashes into a label stack, returning an error for invalid formats."}
{"func_name": "ParseEthernetSegmentIdentifier", "func_src_before": "func ParseEthernetSegmentIdentifier(args []string) (EthernetSegmentIdentifier, error) {\n\tesi := EthernetSegmentIdentifier{}\n\targLen := len(args)\n\tif argLen == 0 || args[0] == \"single-homed\" {\n\t\treturn esi, nil\n\t}\n\n\ttypeStr := strings.TrimPrefix(strings.ToUpper(args[0]), \"ESI_\")\n\tswitch typeStr {\n\tcase \"ARBITRARY\":\n\t\tesi.Type = ESI_ARBITRARY\n\tcase \"LACP\":\n\t\tesi.Type = ESI_LACP\n\tcase \"MSTP\":\n\t\tesi.Type = ESI_MSTP\n\tcase \"MAC\":\n\t\tesi.Type = ESI_MAC\n\tcase \"ROUTERID\":\n\t\tesi.Type = ESI_ROUTERID\n\tcase \"AS\":\n\t\tesi.Type = ESI_AS\n\tdefault:\n\t\ttyp, err := strconv.Atoi(args[0])\n\t\tif err != nil {\n\t\t\treturn esi, fmt.Errorf(\"invalid esi type: %s\", args[0])\n\t\t}\n\t\tesi.Type = ESIType(typ)\n\t}\n\n\tinvalidEsiValuesError := fmt.Errorf(\"invalid esi values for type %s: %s\", esi.Type.String(), args[1:])\n\tesi.Value = make([]byte, 9, 9)\n\tswitch esi.Type {\n\tcase ESI_LACP:\n\t\tfallthrough\n\tcase ESI_MSTP:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Port Key or Bridge Priority\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint16(esi.Value[6:8], uint16(i))\n\tcase ESI_MAC:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tiBuf := make([]byte, 4, 4)\n\t\tbinary.BigEndian.PutUint32(iBuf, uint32(i))\n\t\tcopy(esi.Value[6:9], iBuf[1:4])\n\tcase ESI_ROUTERID:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// Router ID\n\t\tip := net.ParseIP(args[1])\n\t\tif ip == nil || ip.To4() == nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:4], ip.To4())\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_AS:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// AS\n\t\tas, err := strconv.Atoi(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[0:4], uint32(as))\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_ARBITRARY:\n\t\tfallthrough\n\tdefault:\n\t\tif argLen < 2 {\n\t\t\t// Assumes the Value field is omitted\n\t\t\tbreak\n\t\t}\n\t\tvalues := make([]byte, 0, 9)\n\t\tfor _, e := range strings.SplitN(args[1], \":\", 9) {\n\t\t\tv, err := strconv.ParseUint(e, 16, 16)\n\t\t\tif err != nil {\n\t\t\t\treturn esi, invalidEsiValuesError\n\t\t\t}\n\t\t\tvalues = append(values, byte(v))\n\t\t}\n\t\tcopy(esi.Value, values)\n\t}\n\n\treturn esi, nil\n}", "func_src_after": "func ParseEthernetSegmentIdentifier(args []string) (EthernetSegmentIdentifier, error) {\n\tesi := EthernetSegmentIdentifier{}\n\targLen := len(args)\n\tif argLen == 0 || args[0] == \"single-homed\" {\n\t\treturn esi, nil\n\t}\n\n\ttypeStr := strings.TrimPrefix(strings.ToUpper(args[0]), \"ESI_\")\n\tswitch typeStr {\n\tcase \"ARBITRARY\":\n\t\tesi.Type = ESI_ARBITRARY\n\tcase \"LACP\":\n\t\tesi.Type = ESI_LACP\n\tcase \"MSTP\":\n\t\tesi.Type = ESI_MSTP\n\tcase \"MAC\":\n\t\tesi.Type = ESI_MAC\n\tcase \"ROUTERID\":\n\t\tesi.Type = ESI_ROUTERID\n\tcase \"AS\":\n\t\tesi.Type = ESI_AS\n\tdefault:\n\t\ttyp, err := strconv.ParseUint(args[0], 10, 0)\n\t\tif err != nil {\n\t\t\treturn esi, fmt.Errorf(\"invalid esi type: %s\", args[0])\n\t\t}\n\t\tesi.Type = ESIType(typ)\n\t}\n\n\tinvalidEsiValuesError := fmt.Errorf(\"invalid esi values for type %s: %s\", esi.Type.String(), args[1:])\n\tesi.Value = make([]byte, 9, 9)\n\tswitch esi.Type {\n\tcase ESI_LACP:\n\t\tfallthrough\n\tcase ESI_MSTP:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Port Key or Bridge Priority\n\t\ti, err := strconv.ParseUint(args[2], 10, 16)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint16(esi.Value[6:8], uint16(i))\n\tcase ESI_MAC:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tiBuf := make([]byte, 4, 4)\n\t\tbinary.BigEndian.PutUint32(iBuf, uint32(i))\n\t\tcopy(esi.Value[6:9], iBuf[1:4])\n\tcase ESI_ROUTERID:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// Router ID\n\t\tip := net.ParseIP(args[1])\n\t\tif ip == nil || ip.To4() == nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:4], ip.To4())\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_AS:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// AS\n\t\tas, err := strconv.ParseUint(args[1], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[0:4], uint32(as))\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_ARBITRARY:\n\t\tfallthrough\n\tdefault:\n\t\tif argLen < 2 {\n\t\t\t// Assumes the Value field is omitted\n\t\t\tbreak\n\t\t}\n\t\tvalues := make([]byte, 0, 9)\n\t\tfor _, e := range strings.SplitN(args[1], \":\", 9) {\n\t\t\tv, err := strconv.ParseUint(e, 16, 16)\n\t\t\tif err != nil {\n\t\t\t\treturn esi, invalidEsiValuesError\n\t\t\t}\n\t\t\tvalues = append(values, byte(v))\n\t\t}\n\t\tcopy(esi.Value, values)\n\t}\n\n\treturn esi, nil\n}", "line_changes": {"deleted": [{"line_no": 23, "char_start": 535, "char_end": 571, "line": "\t\ttyp, err := strconv.Atoi(args[0])\n"}, {"line_no": 46, "char_start": 1107, "char_end": 1141, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 62, "char_start": 1487, "char_end": 1521, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 80, "char_start": 1947, "char_end": 1981, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 90, "char_start": 2177, "char_end": 2212, "line": "\t\tas, err := strconv.Atoi(args[1])\n"}, {"line_no": 96, "char_start": 2353, "char_end": 2387, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}], "added": [{"line_no": 23, "char_start": 535, "char_end": 583, "line": "\t\ttyp, err := strconv.ParseUint(args[0], 10, 0)\n"}, {"line_no": 46, "char_start": 1119, "char_end": 1166, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 16)\n"}, {"line_no": 62, "char_start": 1512, "char_end": 1559, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}, {"line_no": 80, "char_start": 1985, "char_end": 2032, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}, {"line_no": 90, "char_start": 2228, "char_end": 2276, "line": "\t\tas, err := strconv.ParseUint(args[1], 10, 32)\n"}, {"line_no": 96, "char_start": 2417, "char_end": 2464, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}]}, "char_changes": {"deleted": [{"char_start": 557, "char_end": 561, "chars": "Atoi"}, {"char_start": 1127, "char_end": 1139, "chars": "Atoi(args[2]"}, {"char_start": 1507, "char_end": 1519, "chars": "Atoi(args[2]"}, {"char_start": 1967, "char_end": 1979, "chars": "Atoi(args[2]"}, {"char_start": 2198, "char_end": 2210, "chars": "Atoi(args[1]"}, {"char_start": 2373, "char_end": 2385, "chars": "Atoi(args[2]"}], "added": [{"char_start": 557, "char_end": 566, "chars": "ParseUint"}, {"char_start": 574, "char_end": 581, "chars": ", 10, 0"}, {"char_start": 1139, "char_end": 1164, "chars": "ParseUint(args[2], 10, 16"}, {"char_start": 1532, "char_end": 1557, "chars": "ParseUint(args[2], 10, 32"}, {"char_start": 2005, "char_end": 2030, "chars": "ParseUint(args[2], 10, 32"}, {"char_start": 2249, "char_end": 2274, "chars": "ParseUint(args[1], 10, 32"}, {"char_start": 2437, "char_end": 2462, "chars": "ParseUint(args[2], 10, 32"}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse an Ethernet Segment Identifier from command-line arguments."}
{"func_name": "ParseRouteDistinguisher", "func_src_before": "func ParseRouteDistinguisher(rd string) (RouteDistinguisherInterface, error) {\n\telems, err := parseRdAndRt(rd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tassigned, _ := strconv.Atoi(elems[10])\n\tip := net.ParseIP(elems[1])\n\tswitch {\n\tcase ip.To4() != nil:\n\t\treturn NewRouteDistinguisherIPAddressAS(elems[1], uint16(assigned)), nil\n\tcase elems[6] == \"\" && elems[7] == \"\":\n\t\tasn, _ := strconv.Atoi(elems[8])\n\t\treturn NewRouteDistinguisherTwoOctetAS(uint16(asn), uint32(assigned)), nil\n\tdefault:\n\t\tfst, _ := strconv.Atoi(elems[7])\n\t\tsnd, _ := strconv.Atoi(elems[8])\n\t\tasn := fst<<16 | snd\n\t\treturn NewRouteDistinguisherFourOctetAS(uint32(asn), uint16(assigned)), nil\n\t}\n}", "func_src_after": "func ParseRouteDistinguisher(rd string) (RouteDistinguisherInterface, error) {\n\telems, err := parseRdAndRt(rd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tassigned, _ := strconv.ParseUint(elems[10], 10, 32)\n\tip := net.ParseIP(elems[1])\n\tswitch {\n\tcase ip.To4() != nil:\n\t\treturn NewRouteDistinguisherIPAddressAS(elems[1], uint16(assigned)), nil\n\tcase elems[6] == \"\" && elems[7] == \"\":\n\t\tasn, _ := strconv.ParseUint(elems[8], 10, 16)\n\t\treturn NewRouteDistinguisherTwoOctetAS(uint16(asn), uint32(assigned)), nil\n\tdefault:\n\t\tfst, _ := strconv.ParseUint(elems[7], 10, 16)\n\t\tsnd, _ := strconv.ParseUint(elems[8], 10, 16)\n\t\tasn := fst<<16 | snd\n\t\treturn NewRouteDistinguisherFourOctetAS(uint32(asn), uint16(assigned)), nil\n\t}\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 149, "char_end": 189, "line": "\tassigned, _ := strconv.Atoi(elems[10])\n"}, {"line_no": 12, "char_start": 366, "char_end": 401, "line": "\t\tasn, _ := strconv.Atoi(elems[8])\n"}, {"line_no": 15, "char_start": 488, "char_end": 523, "line": "\t\tfst, _ := strconv.Atoi(elems[7])\n"}, {"line_no": 16, "char_start": 523, "char_end": 558, "line": "\t\tsnd, _ := strconv.Atoi(elems[8])\n"}], "added": [{"line_no": 6, "char_start": 149, "char_end": 202, "line": "\tassigned, _ := strconv.ParseUint(elems[10], 10, 32)\n"}, {"line_no": 12, "char_start": 379, "char_end": 427, "line": "\t\tasn, _ := strconv.ParseUint(elems[8], 10, 16)\n"}, {"line_no": 15, "char_start": 514, "char_end": 562, "line": "\t\tfst, _ := strconv.ParseUint(elems[7], 10, 16)\n"}, {"line_no": 16, "char_start": 562, "char_end": 610, "line": "\t\tsnd, _ := strconv.ParseUint(elems[8], 10, 16)\n"}]}, "char_changes": {"deleted": [{"char_start": 173, "char_end": 177, "chars": "Atoi"}, {"char_start": 386, "char_end": 390, "chars": "Atoi"}, {"char_start": 508, "char_end": 512, "chars": "Atoi"}, {"char_start": 543, "char_end": 547, "chars": "Atoi"}], "added": [{"char_start": 173, "char_end": 182, "chars": "ParseUint"}, {"char_start": 192, "char_end": 200, "chars": ", 10, 32"}, {"char_start": 399, "char_end": 408, "chars": "ParseUint"}, {"char_start": 417, "char_end": 425, "chars": ", 10, 16"}, {"char_start": 534, "char_end": 543, "chars": "ParseUint"}, {"char_start": 552, "char_end": 560, "chars": ", 10, 16"}, {"char_start": 582, "char_end": 591, "chars": "ParseUint"}, {"char_start": 600, "char_end": 608, "chars": ", 10, 16"}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse a string into a Route Distinguisher object, handling different formats based on IP or ASN components."}
{"func_name": "apiCallbacksStreams", "func_src_before": "func apiCallbacksStreams(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, challenge)\n\t\tfmt.Println(\"Responding to streams\")\n\t\treturn\n\t}\n\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response streamsResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif len(response.Data) > 0 {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t} else {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t}\n}", "func_src_after": "func apiCallbacksStreams(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, html.EscapeString(challenge))\n\t\tfmt.Println(\"Responding to streams\")\n\t\treturn\n\t}\n\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response streamsResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif len(response.Data) > 0 {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t} else {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t}\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 138, "char_end": 165, "line": "\t\tfmt.Fprint(w, challenge)\n"}], "added": [{"line_no": 4, "char_start": 138, "char_end": 184, "line": "\t\tfmt.Fprint(w, html.EscapeString(challenge))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 154, "char_end": 172, "chars": "html.EscapeString("}, {"char_start": 182, "char_end": 183, "chars": ")"}]}, "commit_link": "github.com/pajlada/pajbot2/commit/69ef922a07fc648760f030604e0aea86a573ea83", "file_name": "hook.go", "vul_type": "cwe-079", "commit_msg": "Fix cross-site scripting vulnerabilities (#447)", "parent_commit": "d8321a06903ec460cf6343c51ae50d12a3cb45e9", "description": "Write a Go function to handle webhooks for stream status updates, responding to a challenge parameter and printing multiple lines of \"Online!\" or \"Offline!\" based on the received data."}
{"func_name": "CompileAndRun", "func_src_before": "func (l *Loader) CompileAndRun(name string, input io.Reader) error {\n\tglog.V(2).Infof(\"CompileAndRun %s\", name)\n\tv, errs := Compile(name, input, l.dumpAst, l.dumpAstTypes, l.syslogUseCurrentYear, l.overrideLocation)\n\tif errs != nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"compile failed for %s:\\n%s\", name, errs)\n\t}\n\tif v == nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"Internal error: Compilation failed for %s: No program returned, but no errors.\", name)\n\t}\n\n\tif l.dumpBytecode {\n\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode(name))\n\t}\n\n\t// Load the metrics from the compilation into the global metric storage for export.\n\tfor _, m := range v.m {\n\t\tif !m.Hidden {\n\t\t\tif l.omitMetricSource {\n\t\t\t\tm.Source = \"\"\n\t\t\t}\n\t\t\terr := l.ms.Add(m)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tProgLoads.Add(name, 1)\n\tglog.Infof(\"Loaded program %s\", name)\n\n\tif l.compileOnly {\n\t\treturn nil\n\t}\n\n\tl.handleMu.Lock()\n\tdefer l.handleMu.Unlock()\n\n\tl.handles[name] = v\n\treturn nil\n}", "func_src_after": "func (l *Loader) CompileAndRun(name string, input io.Reader) error {\n\tglog.V(2).Infof(\"CompileAndRun %s\", name)\n\tv, errs := Compile(name, input, l.dumpAst, l.dumpAstTypes, l.syslogUseCurrentYear, l.overrideLocation)\n\tif errs != nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"compile failed for %s:\\n%s\", name, errs)\n\t}\n\tif v == nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"Internal error: Compilation failed for %s: No program returned, but no errors.\", name)\n\t}\n\n\tif l.dumpBytecode {\n\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode())\n\t}\n\n\t// Load the metrics from the compilation into the global metric storage for export.\n\tfor _, m := range v.m {\n\t\tif !m.Hidden {\n\t\t\tif l.omitMetricSource {\n\t\t\t\tm.Source = \"\"\n\t\t\t}\n\t\t\terr := l.ms.Add(m)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tProgLoads.Add(name, 1)\n\tglog.Infof(\"Loaded program %s\", name)\n\n\tif l.compileOnly {\n\t\treturn nil\n\t}\n\n\tl.handleMu.Lock()\n\tdefer l.handleMu.Unlock()\n\n\tl.handles[name] = v\n\treturn nil\n}", "line_changes": {"deleted": [{"line_no": 14, "char_start": 513, "char_end": 589, "line": "\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode(name))\n"}], "added": [{"line_no": 14, "char_start": 513, "char_end": 585, "line": "\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode())\n"}]}, "char_changes": {"deleted": [{"char_start": 582, "char_end": 586, "chars": "name"}], "added": []}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "loader.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that compiles and runs a program from an input reader, handling errors and metrics."}
{"func_name": "TestGetBasket_BadRequest", "func_src_before": "func TestGetBasket_BadRequest(t *testing.T) {\n\tbasket := \"get05~\"\n\n\tr, err := http.NewRequest(\"GET\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tr.Header.Add(\"Authorization\", \"abcd12345\")\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tGetBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t}\n}", "func_src_after": "func TestGetBasket_BadRequest(t *testing.T) {\n\tbasket := \"get05~\"\n\n\tr, err := http.NewRequest(\"GET\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tr.Header.Add(\"Authorization\", \"abcd12345\")\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tGetBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 487, "char_end": 614, "line": "\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}], "added": [{"line_no": 13, "char_start": 487, "char_end": 610, "line": "\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}]}, "char_changes": {"deleted": [{"char_start": 527, "char_end": 539, "chars": "[\"+basket+\"]"}], "added": [{"char_start": 527, "char_end": 535, "chars": "the name"}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers_test.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go test function that checks for a bad request response when fetching a basket with an invalid name from an API."}
{"func_name": "HandleError", "func_src_before": "func HandleError(w http.ResponseWriter, err error) {\n\tnetErr, ok := err.(net.Error)\n\tif ok {\n\t\tif netErr.Timeout() {\n\t\t\thttp.Error(w, \"Storage read timeout\", http.StatusGatewayTimeout)\n\t\t} else if strings.HasSuffix(err.Error(), \"connect: no route to host\") ||\n\t\t\tstrings.HasSuffix(err.Error(), \"connect: connection refused\") ||\n\t\t\tstrings.HasSuffix(err.Error(), \": connection reset by peer\") ||\n\t\t\tstrings.HasPrefix(err.Error(), \"dial tcp: lookup \") { // DNS lookup\n\t\t\thttp.Error(w, \"Storage error\", http.StatusServiceUnavailable)\n\t\t} else {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\terrCode, ok := err.(*ErrorWithCode)\n\tif ok {\n\t\tif errCode.Code > 500 && errCode.Code < 512 {\n\t\t\thttp.Error(w, errCode.Error(), errCode.Code)\n\t\t} else {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\t_, ok = err.(*ErrDataParse)\n\tif ok || strings.HasPrefix(err.Error(), \"clickhouse response status 500: Code:\") {\n\t\tif strings.Contains(err.Error(), \": Limit for \") {\n\t\t\t//logger.Info(\"limit\", zap.Error(err))\n\t\t\thttp.Error(w, \"Storage read limit\", http.StatusForbidden)\n\t\t} else if !ok && strings.HasPrefix(err.Error(), \"clickhouse response status 500: Code: 170,\") {\n\t\t\t// distributed table configuration error\n\t\t\t// clickhouse response status 500: Code: 170, e.displayText() = DB::Exception: Requested cluster 'cluster' not found\n\t\t\thttp.Error(w, \"Storage configuration error\", http.StatusServiceUnavailable)\n\t\t}\n\t} else {\n\t\t//logger.Debug(\"query\", zap.Error(err))\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t}\n}", "func_src_after": "func HandleError(w http.ResponseWriter, err error) {\n\tnetErr, ok := err.(net.Error)\n\tif ok {\n\t\tif netErr.Timeout() {\n\t\t\thttp.Error(w, \"Storage read timeout\", http.StatusGatewayTimeout)\n\t\t} else if strings.HasSuffix(err.Error(), \"connect: no route to host\") ||\n\t\t\tstrings.HasSuffix(err.Error(), \"connect: connection refused\") ||\n\t\t\tstrings.HasSuffix(err.Error(), \": connection reset by peer\") ||\n\t\t\tstrings.HasPrefix(err.Error(), \"dial tcp: lookup \") { // DNS lookup\n\t\t\thttp.Error(w, \"Storage error\", http.StatusServiceUnavailable)\n\t\t} else {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\terrCode, ok := err.(*ErrorWithCode)\n\tif ok {\n\t\tif errCode.Code > 500 && errCode.Code < 512 {\n\t\t\thttp.Error(w, html.EscapeString(errCode.Error()), errCode.Code)\n\t\t} else {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\t_, ok = err.(*ErrDataParse)\n\tif ok || strings.HasPrefix(err.Error(), \"clickhouse response status 500: Code:\") {\n\t\tif strings.Contains(err.Error(), \": Limit for \") {\n\t\t\t//logger.Info(\"limit\", zap.Error(err))\n\t\t\thttp.Error(w, \"Storage read limit\", http.StatusForbidden)\n\t\t} else if !ok && strings.HasPrefix(err.Error(), \"clickhouse response status 500: Code: 170,\") {\n\t\t\t// distributed table configuration error\n\t\t\t// clickhouse response status 500: Code: 170, e.displayText() = DB::Exception: Requested cluster 'cluster' not found\n\t\t\thttp.Error(w, \"Storage configuration error\", http.StatusServiceUnavailable)\n\t\t}\n\t} else {\n\t\t//logger.Debug(\"query\", zap.Error(err))\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 19, "char_start": 714, "char_end": 762, "line": "\t\t\thttp.Error(w, errCode.Error(), errCode.Code)\n"}], "added": [{"line_no": 19, "char_start": 714, "char_end": 781, "line": "\t\t\thttp.Error(w, html.EscapeString(errCode.Error()), errCode.Code)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 731, "char_end": 749, "chars": "html.EscapeString("}, {"char_start": 764, "char_end": 765, "chars": ")"}]}, "commit_link": "github.com/lomik/graphite-clickhouse/commit/1cae40154d930d6885cac6344a41bf2fcc18b562", "file_name": "clickhouse.go", "vul_type": "cwe-079", "commit_msg": "Fix possible XSS\n\nSee\nhttps://github.com/lomik/graphite-clickhouse/security/code-scanning/5?query=ref%3Arefs%2Fpull%2F129%2Fhead", "parent_commit": "cf322598da33900d6fabdbc940e5fc7713fb41bb", "description": "Write a Go function to handle different types of storage-related errors and respond with appropriate HTTP status codes."}
{"func_name": "apiCallbacksFollow", "func_src_before": "func apiCallbacksFollow(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, challenge)\n\t\treturn\n\t}\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response followResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfor _, follow := range response.Data {\n\t\tfmt.Printf(\"User with id %s followed %s at %s\\n\", follow.FromID, follow.ToID, follow.FollowedAt)\n\t}\n}", "func_src_after": "func apiCallbacksFollow(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, html.EscapeString(challenge))\n\t\treturn\n\t}\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response followResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfor _, follow := range response.Data {\n\t\tfmt.Printf(\"User with id %s followed %s at %s\\n\", follow.FromID, follow.ToID, follow.FollowedAt)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 137, "char_end": 164, "line": "\t\tfmt.Fprint(w, challenge)\n"}], "added": [{"line_no": 4, "char_start": 137, "char_end": 183, "line": "\t\tfmt.Fprint(w, html.EscapeString(challenge))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 153, "char_end": 171, "chars": "html.EscapeString("}, {"char_start": 181, "char_end": 182, "chars": ")"}]}, "commit_link": "github.com/pajlada/pajbot2/commit/69ef922a07fc648760f030604e0aea86a573ea83", "file_name": "hook.go", "vul_type": "cwe-079", "commit_msg": "Fix cross-site scripting vulnerabilities (#447)", "parent_commit": "d8321a06903ec460cf6343c51ae50d12a3cb45e9", "description": "In Go, write a function to handle webhook callbacks that prints user follow events and responds to challenge verification."}
{"func_name": "verifyHandler", "func_src_before": "func verifyHandler(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge == \"\" {\n\t\tw.WriteHeader(400)\n\t\treturn\n\t}\n\n\tw.WriteHeader(200)\n\tw.Write([]byte(challenge))\n}", "func_src_after": "func verifyHandler(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge == \"\" {\n\t\tw.WriteHeader(400)\n\t\treturn\n\t}\n\n\tw.WriteHeader(200)\n\tw.Write([]byte(html.EscapeString(challenge)))\n}", "line_changes": {"deleted": [{"line_no": 9, "char_start": 186, "char_end": 214, "line": "\tw.Write([]byte(challenge))\n"}], "added": [{"line_no": 9, "char_start": 186, "char_end": 233, "line": "\tw.Write([]byte(html.EscapeString(challenge)))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 202, "char_end": 220, "chars": "html.EscapeString("}, {"char_start": 231, "char_end": 232, "chars": ")"}]}, "commit_link": "github.com/pajlada/pajbot2/commit/69ef922a07fc648760f030604e0aea86a573ea83", "file_name": "webhook.go", "vul_type": "cwe-079", "commit_msg": "Fix cross-site scripting vulnerabilities (#447)", "parent_commit": "d8321a06903ec460cf6343c51ae50d12a3cb45e9", "description": "Write a Go function named `verifyHandler` that responds to a web request by echoing back a query parameter value, with and without HTML escaping."}
{"func_name": "getAuthenticatedBasket", "func_src_before": "func getAuthenticatedBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) (string, Basket) {\n\tname := ps.ByName(\"basket\")\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\t// maybe custom header, e.g. basket_key, basket_token\n\t\tif token := r.Header.Get(\"Authorization\"); basket.Authorize(token) || token == serverConfig.MasterToken {\n\t\t\treturn name, basket\n\t\t}\n\t\tw.WriteHeader(http.StatusUnauthorized)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n\n\treturn \"\", nil\n}", "func_src_after": "func getAuthenticatedBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) (string, Basket) {\n\tname := ps.ByName(\"basket\")\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\t// maybe custom header, e.g. basket_key, basket_token\n\t\tif token := r.Header.Get(\"Authorization\"); basket.Authorize(token) || token == serverConfig.MasterToken {\n\t\t\treturn name, basket\n\t\t}\n\t\tw.WriteHeader(http.StatusUnauthorized)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n\n\treturn \"\", nil\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 179, "char_end": 303, "line": "\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}], "added": [{"line_no": 4, "char_start": 179, "char_end": 301, "line": "\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}]}, "char_changes": {"deleted": [{"char_start": 217, "char_end": 220, "chars": "[\"+"}, {"char_start": 224, "char_end": 227, "chars": "+\"]"}], "added": [{"char_start": 217, "char_end": 221, "chars": "the "}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go function to authenticate and retrieve a basket by name from a database, handling HTTP requests and responses."}
{"func_name": "TestCreateBasket_InvalidName", "func_src_before": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "func_src_after": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 447, "char_end": 574, "line": "\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}], "added": [{"line_no": 12, "char_start": 447, "char_end": 570, "line": "\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}]}, "char_changes": {"deleted": [{"char_start": 487, "char_end": 499, "chars": "[\"+basket+\"]"}], "added": [{"char_start": 487, "char_end": 495, "chars": "the name"}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers_test.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go test function to validate that creating a basket with an invalid name results in a bad request and no database entry."}
{"func_name": "", "func_src_before": "\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tplaylistId := utils.ParamString(r, \":playlistId\")\n\t\ttracksRepo := ds.Playlist(r.Context()).Tracks(playlistId)\n\t\tvar payload addTracksPayload\n\t\terr := json.NewDecoder(r.Body).Decode(&payload)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\t\terr = tracksRepo.Add(payload.Ids)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\t// Must return an object with an ID, to satisfy ReactAdmin `create` call\n\t\t_, err = w.Write([]byte(fmt.Sprintf(`{\"id\":\"%s\"}`, playlistId)))\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t}", "func_src_after": "\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tplaylistId := utils.ParamString(r, \":playlistId\")\n\t\ttracksRepo := ds.Playlist(r.Context()).Tracks(playlistId)\n\t\tvar payload addTracksPayload\n\t\terr := json.NewDecoder(r.Body).Decode(&payload)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\t\terr = tracksRepo.Add(payload.Ids)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\t// Must return an object with an ID, to satisfy ReactAdmin `create` call\n\t\t_, err = fmt.Fprintf(w, `{\"id\":\"%s\"}`, html.EscapeString(playlistId))\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t}", "line_changes": {"deleted": [{"line_no": 17, "char_start": 530, "char_end": 597, "line": "\t\t_, err = w.Write([]byte(fmt.Sprintf(`{\"id\":\"%s\"}`, playlistId)))\n"}], "added": [{"line_no": 17, "char_start": 530, "char_end": 602, "line": "\t\t_, err = fmt.Fprintf(w, `{\"id\":\"%s\"}`, html.EscapeString(playlistId))\n"}]}, "char_changes": {"deleted": [{"char_start": 541, "char_end": 556, "chars": "w.Write([]byte("}, {"char_start": 560, "char_end": 561, "chars": "S"}, {"char_start": 595, "char_end": 596, "chars": ")"}], "added": [{"char_start": 545, "char_end": 546, "chars": "F"}, {"char_start": 553, "char_end": 556, "chars": "w, "}, {"char_start": 571, "char_end": 589, "chars": "html.EscapeString("}]}, "commit_link": "github.com/cloudsonic/sonic-server/commit/9cbeddae8fc6fb729ab8fc2e2c79b39edd0caea6", "file_name": "playlists.go", "vul_type": "cwe-079", "commit_msg": "Avoid cross-site scripting\n\nSee: https://lgtm.com/rules/1510377426397/", "parent_commit": "c9b119f0a40b962f0141a434b33b74466242383b", "description": "Write a Go function that adds track IDs to a playlist and returns the playlist ID as a JSON response."}
{"func_name": "writeError", "func_src_before": "func writeError(resp http.ResponseWriter, err error, code int) {\n\tresp.WriteHeader(code)\n\t_, _ = resp.Write([]byte(fmt.Sprintf(\"Error: %v\", err)))\n}", "func_src_after": "func writeError(resp http.ResponseWriter, err error, code int) {\n\tresp.WriteHeader(code)\n\t_, _ = resp.Write([]byte(html.EscapeString(fmt.Sprintf(\"Error: %v\", err))))\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 89, "char_end": 147, "line": "\t_, _ = resp.Write([]byte(fmt.Sprintf(\"Error: %v\", err)))\n"}], "added": [{"line_no": 3, "char_start": 89, "char_end": 166, "line": "\t_, _ = resp.Write([]byte(html.EscapeString(fmt.Sprintf(\"Error: %v\", err))))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 115, "char_end": 133, "chars": "html.EscapeString("}, {"char_start": 164, "char_end": 165, "chars": ")"}]}, "commit_link": "github.com/mbrt/gmailctl/commit/563a9e3605d722e32dedd2e93f38da655338b39a", "file_name": "oauth2_server.go", "vul_type": "cwe-079", "commit_msg": "Sanitize error before writing html response.\n\nThis has no real potential for XSS, as the serve runs on localhost, but\nbetter safe than sorry.", "parent_commit": "75e2e0ed4459203fa460507dd0c7385c8cafddcb", "description": "Create a Go function that sends an error message with an HTTP status code to the client's response writer."}
{"func_name": "errorf", "func_src_before": "func (v *VM) errorf(format string, args ...interface{}) {\n\ti := v.prog[v.t.pc-1]\n\tprogRuntimeErrors.Add(v.name, 1)\n\tv.runtimeErrorMu.Lock()\n\tv.runtimeError = fmt.Sprintf(format+\"\\n\", args...)\n\tv.runtimeError += fmt.Sprintf(\n\t\t\"Error occurred at instruction %d {%s, %v}, originating in %s at line %d\\n\",\n\t\tv.t.pc-1, i.Opcode, i.Operand, v.name, i.SourceLine+1)\n\tv.runtimeError += fmt.Sprintf(\"Full input text from %q was %q\", v.input.Filename, v.input.Line)\n\tif *runtimeLogError || bool(glog.V(1)) {\n\t\tglog.Info(v.name + \": Runtime error: \" + v.runtimeError)\n\n\t\tglog.Infof(\"Set logging verbosity higher (-v1 or more) to see full VM state dump.\")\n\t}\n\tif glog.V(1) {\n\t\tglog.Infof(\"VM stack:\\n%s\", debug.Stack())\n\t\tglog.Infof(\"Dumping vm state\")\n\t\tglog.Infof(\"Name: %s\", v.name)\n\t\tglog.Infof(\"Input: %#v\", v.input)\n\t\tglog.Infof(\"Thread:\")\n\t\tglog.Infof(\" PC %v\", v.t.pc-1)\n\t\tglog.Infof(\" Matched %v\", v.t.matched)\n\t\tglog.Infof(\" Matches %v\", v.t.matches)\n\t\tglog.Infof(\" Timestamp %v\", v.t.time)\n\t\tglog.Infof(\" Stack %v\", v.t.stack)\n\t\tglog.Infof(v.DumpByteCode(v.name))\n\t}\n\tv.runtimeErrorMu.Unlock()\n\tv.terminate = true\n}", "func_src_after": "func (v *VM) errorf(format string, args ...interface{}) {\n\ti := v.prog[v.t.pc-1]\n\tprogRuntimeErrors.Add(v.name, 1)\n\tv.runtimeErrorMu.Lock()\n\tv.runtimeError = fmt.Sprintf(format+\"\\n\", args...)\n\tv.runtimeError += fmt.Sprintf(\n\t\t\"Error occurred at instruction %d {%s, %v}, originating in %s at line %d\\n\",\n\t\tv.t.pc-1, i.Opcode, i.Operand, v.name, i.SourceLine+1)\n\tv.runtimeError += fmt.Sprintf(\"Full input text from %q was %q\", v.input.Filename, v.input.Line)\n\tif *runtimeLogError || bool(glog.V(1)) {\n\t\tglog.Info(v.name + \": Runtime error: \" + v.runtimeError)\n\n\t\tglog.Infof(\"Set logging verbosity higher (-v1 or more) to see full VM state dump.\")\n\t}\n\tif glog.V(1) {\n\t\tglog.Infof(\"VM stack:\\n%s\", debug.Stack())\n\t\tglog.Infof(\"Dumping vm state\")\n\t\tglog.Infof(\"Name: %s\", v.name)\n\t\tglog.Infof(\"Input: %#v\", v.input)\n\t\tglog.Infof(\"Thread:\")\n\t\tglog.Infof(\" PC %v\", v.t.pc-1)\n\t\tglog.Infof(\" Matched %v\", v.t.matched)\n\t\tglog.Infof(\" Matches %v\", v.t.matches)\n\t\tglog.Infof(\" Timestamp %v\", v.t.time)\n\t\tglog.Infof(\" Stack %v\", v.t.stack)\n\t\tglog.Infof(v.DumpByteCode())\n\t}\n\tv.runtimeErrorMu.Unlock()\n\tv.terminate = true\n}", "line_changes": {"deleted": [{"line_no": 26, "char_start": 1027, "char_end": 1064, "line": "\t\tglog.Infof(v.DumpByteCode(v.name))\n"}], "added": [{"line_no": 26, "char_start": 1027, "char_end": 1058, "line": "\t\tglog.Infof(v.DumpByteCode())\n"}]}, "char_changes": {"deleted": [{"char_start": 1055, "char_end": 1061, "chars": "v.name"}], "added": []}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "vm.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that logs a formatted runtime error and VM state for a virtual machine object."}
{"func_name": "landingPage", "func_src_before": "func landingPage(w http.ResponseWriter, r *http.Request) {\n\tctx := r.Context()\n\n\tidStr := r.URL.Path[1:]\n\n\t// When we don't have an idStr or it contains any path elements, we would\n\t// serve the landing page\n\tif len(idStr) < 1 || strings.Contains(idStr, \"/\") ||\n\t\tstrings.HasSuffix(idStr, \"index.html\") {\n\t\tdcCh := config.MustGetAsync(ctx)\n\t\tpsCh := preloadedState(ctx)\n\n\t\tnodeEnv := \"production\"\n\n\t\tif webapp.IsDev {\n\t\t\tnodeEnv = \"development\"\n\t\t}\n\n\t\tinitData := fmt.Sprintf(initDataTemplate, <-psCh, nodeEnv)\n\n\t\ttmpl := webapp.GetTemplate(\"index.html\", webapp.IsDev)\n\t\ttmpl.Execute(w, map[string]interface{}{\n\t\t\t\"Config\":    <-dcCh,\n\t\t\t\"BuildInfo\": config.B,\n\t\t\t\"InitData\":  template.HTML(initData),\n\t\t})\n\t\treturn\n\t}\n\n\tid := base62.Decode(idStr)\n\n\tshortURL, err := shorturl.ByID(ctx, id)\n\tif err == datastore.ErrNoSuchEntity {\n\t\tlog.Printf(\"Unable to load short url %s. Decoded key: %d\",\n\t\t\tidStr, id)\n\t\thttp.Error(w, fmt.Sprintf(\"Short URL %s cannot be found !!11one\",\n\t\t\tidStr), http.StatusNotFound)\n\t\treturn\n\t} else if err != nil {\n\t\tlog.Printf(\"Error loading short URL '%s': %s\", idStr,\n\t\t\terr.Error())\n\t\thttp.Error(w, \"Internal Server Error\",\n\t\t\thttp.StatusInternalServerError)\n\t\treturn\n\t}\n\n\thttp.Redirect(w, r, shortURL.OriginalURL, http.StatusMovedPermanently)\n}", "func_src_after": "func landingPage(w http.ResponseWriter, r *http.Request) {\n\tctx := r.Context()\n\n\tidStr := r.URL.Path[1:]\n\n\t// When we don't have an idStr or it contains any path elements, we would\n\t// serve the landing page\n\tif len(idStr) < 1 || strings.Contains(idStr, \"/\") ||\n\t\tstrings.HasSuffix(idStr, \"index.html\") {\n\t\tdcCh := config.MustGetAsync(ctx)\n\t\tpsCh := preloadedState(ctx)\n\n\t\tnodeEnv := \"production\"\n\n\t\tif webapp.IsDev {\n\t\t\tnodeEnv = \"development\"\n\t\t}\n\n\t\tinitData := fmt.Sprintf(initDataTemplate, <-psCh, nodeEnv)\n\n\t\ttmpl := webapp.GetTemplate(\"index.html\", webapp.IsDev)\n\t\ttmpl.Execute(w, map[string]interface{}{\n\t\t\t\"Config\":    <-dcCh,\n\t\t\t\"BuildInfo\": config.B,\n\t\t\t\"InitData\":  template.HTML(initData),\n\t\t})\n\t\treturn\n\t}\n\n\tid := base62.Decode(idStr)\n\n\tidStr = html.EscapeString(idStr)\n\n\tshortURL, err := shorturl.ByID(ctx, id)\n\tif err == datastore.ErrNoSuchEntity {\n\t\tlog.Printf(\"Unable to load short url %s. Decoded key: %d\",\n\t\t\tidStr, id)\n\t\thttp.Error(w, fmt.Sprintf(\"Short URL %s cannot be found !!11one\",\n\t\t\tidStr), http.StatusNotFound)\n\t\treturn\n\t} else if err != nil {\n\t\tlog.Printf(\"Error loading short URL '%s': %s\", idStr,\n\t\t\terr.Error())\n\t\thttp.Error(w, \"Internal Server Error\",\n\t\t\thttp.StatusInternalServerError)\n\t\treturn\n\t}\n\n\thttp.Redirect(w, r, shortURL.OriginalURL, http.StatusMovedPermanently)\n}", "line_changes": {"deleted": [], "added": [{"line_no": 32, "char_start": 749, "char_end": 783, "line": "\tidStr = html.EscapeString(idStr)\n"}, {"line_no": 33, "char_start": 783, "char_end": 784, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 749, "char_end": 784, "chars": "\tidStr = html.EscapeString(idStr)\n\n"}]}, "commit_link": "github.com/qqiao/yordle/commit/6f9f25af52fd05b77db575191b6775a4f7f1bb26", "file_name": "yordle.go", "vul_type": "cwe-079", "commit_msg": "HTML Escapes idStr to prevent xss (#69)", "parent_commit": "366d98e5fdad503cafa95c6310a133078203fdfb", "description": "Write a Go function that serves a landing page or redirects to an original URL based on a path-encoded identifier."}
{"func_name": "DumpByteCode", "func_src_before": "func (v *VM) DumpByteCode(name string) string {\n\tb := new(bytes.Buffer)\n\tfmt.Fprintf(b, \"Prog: %s\\n\", name)\n\tfmt.Fprintln(b, \"Metrics\")\n\tfor i, m := range v.m {\n\t\tif m.Program == v.name {\n\t\t\tfmt.Fprintf(b, \" %8d %s\\n\", i, m)\n\t\t}\n\t}\n\tfmt.Fprintln(b, \"Regexps\")\n\tfor i, re := range v.re {\n\t\tfmt.Fprintf(b, \" %8d /%s/\\n\", i, re)\n\t}\n\tfmt.Fprintln(b, \"Strings\")\n\tfor i, str := range v.str {\n\t\tfmt.Fprintf(b, \" %8d \\\"%s\\\"\\n\", i, str)\n\t}\n\tw := new(tabwriter.Writer)\n\tw.Init(b, 0, 0, 1, ' ', tabwriter.AlignRight)\n\n\tfmt.Fprintln(w, \"disasm\\tl\\top\\topnd\\tline\\t\")\n\tfor n, i := range v.prog {\n\t\tfmt.Fprintf(w, \"\\t%d\\t%s\\t%v\\t%d\\t\\n\", n, i.Opcode, i.Operand, i.SourceLine+1)\n\t}\n\tif err := w.Flush(); err != nil {\n\t\tglog.Infof(\"flush error: %s\", err)\n\t}\n\treturn b.String()\n}", "func_src_after": "func (v *VM) DumpByteCode() string {\n\tb := new(bytes.Buffer)\n\tfmt.Fprintf(b, \"Prog: %s\\n\", v.name)\n\tfmt.Fprintln(b, \"Metrics\")\n\tfor i, m := range v.m {\n\t\tif m.Program == v.name {\n\t\t\tfmt.Fprintf(b, \" %8d %s\\n\", i, m)\n\t\t}\n\t}\n\tfmt.Fprintln(b, \"Regexps\")\n\tfor i, re := range v.re {\n\t\tfmt.Fprintf(b, \" %8d /%s/\\n\", i, re)\n\t}\n\tfmt.Fprintln(b, \"Strings\")\n\tfor i, str := range v.str {\n\t\tfmt.Fprintf(b, \" %8d \\\"%s\\\"\\n\", i, str)\n\t}\n\tw := new(tabwriter.Writer)\n\tw.Init(b, 0, 0, 1, ' ', tabwriter.AlignRight)\n\n\tfmt.Fprintln(w, \"disasm\\tl\\top\\topnd\\tline\\t\")\n\tfor n, i := range v.prog {\n\t\tfmt.Fprintf(w, \"\\t%d\\t%s\\t%v\\t%d\\t\\n\", n, i.Opcode, i.Operand, i.SourceLine+1)\n\t}\n\tif err := w.Flush(); err != nil {\n\t\tglog.Infof(\"flush error: %s\", err)\n\t}\n\treturn b.String()\n}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 48, "line": "func (v *VM) DumpByteCode(name string) string {\n"}, {"line_no": 3, "char_start": 72, "char_end": 108, "line": "\tfmt.Fprintf(b, \"Prog: %s\\n\", name)\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 37, "line": "func (v *VM) DumpByteCode() string {\n"}, {"line_no": 3, "char_start": 61, "char_end": 99, "line": "\tfmt.Fprintf(b, \"Prog: %s\\n\", v.name)\n"}]}, "char_changes": {"deleted": [{"char_start": 26, "char_end": 37, "chars": "name string"}], "added": [{"char_start": 91, "char_end": 93, "chars": "v."}]}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "vm.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that outputs a formatted string representation of a virtual machine's bytecode, including metrics, regexps, strings, and disassembly information."}
{"func_name": "CreateBasket", "func_src_before": "func CreateBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) {\n\tname := ps.ByName(\"basket\")\n\tif name == serviceOldAPIPath || name == serviceAPIPath || name == serviceUIPath {\n\t\thttp.Error(w, \"This basket name conflicts with reserved system path: \"+name, http.StatusForbidden)\n\t\treturn\n\t}\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tlog.Printf(\"[info] creating basket: %s\", name)\n\n\t// read config (max 2 kB)\n\tbody, err := ioutil.ReadAll(io.LimitReader(r.Body, 2048))\n\tr.Body.Close()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\t// default config\n\tconfig := BasketConfig{ForwardURL: \"\", Capacity: serverConfig.InitCapacity}\n\tif len(body) > 0 {\n\t\tif err = json.Unmarshal(body, &config); err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\t\tif err = validateBasketConfig(&config); err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusUnprocessableEntity)\n\t\t\treturn\n\t\t}\n\t}\n\n\tauth, err := basketsDb.Create(name, config)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusConflict)\n\t} else {\n\t\tjson, err := json.Marshal(auth)\n\t\twriteJSON(w, http.StatusCreated, json, err)\n\t}\n}", "func_src_after": "func CreateBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) {\n\tname := ps.ByName(\"basket\")\n\tif name == serviceOldAPIPath || name == serviceAPIPath || name == serviceUIPath {\n\t\thttp.Error(w, \"This basket name conflicts with reserved system path: \"+name, http.StatusForbidden)\n\t\treturn\n\t}\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tlog.Printf(\"[info] creating basket: %s\", name)\n\n\t// read config (max 2 kB)\n\tbody, err := ioutil.ReadAll(io.LimitReader(r.Body, 2048))\n\tr.Body.Close()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\t// default config\n\tconfig := BasketConfig{ForwardURL: \"\", Capacity: serverConfig.InitCapacity}\n\tif len(body) > 0 {\n\t\tif err = json.Unmarshal(body, &config); err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\t\tif err = validateBasketConfig(&config); err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusUnprocessableEntity)\n\t\t\treturn\n\t\t}\n\t}\n\n\tauth, err := basketsDb.Create(name, config)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusConflict)\n\t} else {\n\t\tjson, err := json.Marshal(auth)\n\t\twriteJSON(w, http.StatusCreated, json, err)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 348, "char_end": 472, "line": "\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}], "added": [{"line_no": 8, "char_start": 348, "char_end": 470, "line": "\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}]}, "char_changes": {"deleted": [{"char_start": 386, "char_end": 389, "chars": "[\"+"}, {"char_start": 393, "char_end": 396, "chars": "+\"]"}], "added": [{"char_start": 386, "char_end": 390, "chars": "the "}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go function to create a basket with a name and optional configuration, handling name validation and potential errors."}
{"func_name": "AcceptBasketRequests", "func_src_before": "func AcceptBasketRequests(w http.ResponseWriter, r *http.Request) {\n\tname := strings.Split(r.URL.Path, \"/\")[1]\n\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\trequest := basket.Add(r)\n\n\t\t// forward request if configured and it's a first forwarding\n\t\tconfig := basket.Config()\n\t\tif len(config.ForwardURL) > 0 && r.Header.Get(DoNotForwardHeader) != \"1\" {\n\t\t\tif config.ProxyResponse {\n\t\t\t\tforwardAndProxyResponse(w, request, config, name)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tgo forwardAndForget(request, config, name)\n\t\t}\n\n\t\twriteBasketResponse(w, r, name, basket)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n}", "func_src_after": "func AcceptBasketRequests(w http.ResponseWriter, r *http.Request) {\n\tname := strings.Split(r.URL.Path, \"/\")[1]\n\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\trequest := basket.Add(r)\n\n\t\t// forward request if configured and it's a first forwarding\n\t\tconfig := basket.Config()\n\t\tif len(config.ForwardURL) > 0 && r.Header.Get(DoNotForwardHeader) != \"1\" {\n\t\t\tif config.ProxyResponse {\n\t\t\t\tforwardAndProxyResponse(w, request, config, name)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tgo forwardAndForget(request, config, name)\n\t\t}\n\n\t\twriteBasketResponse(w, r, name, basket)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 153, "char_end": 277, "line": "\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}], "added": [{"line_no": 5, "char_start": 153, "char_end": 275, "line": "\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}]}, "char_changes": {"deleted": [{"char_start": 191, "char_end": 194, "chars": "[\"+"}, {"char_start": 198, "char_end": 201, "chars": "+\"]"}], "added": [{"char_start": 191, "char_end": 195, "chars": "the "}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go function to handle HTTP requests for a basket service, validating names and optionally forwarding requests."}
{"func_name": "SessionAttributesManager::doGet", "func_src_before": "  @Override\n  protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    response.setHeader(\"Cache-Control\", \"no-cache\");\n    response.setHeader(\"Pragma\", \"no-cache\");\n    response.setDateHeader(\"Expires\", -1);\n\n    String name = request.getParameter(\"name\");\n    String value = request.getParameter(\"value\");\n    String message = \"The parameter 'name' is empty\";\n\n    if (name != null && name.length() > 0) {\n      if (allowedAttributes.contains(name)) {\n\n        // add bcdBeanUser.name entries as HashMap session variable \"bcdBeanUser\" where the key/value pairs\n        // are kept without the prefix.\n        boolean error = true;\n        Subject subject = SecurityUtils.getSubject();\n        // check value against list of allowed values or subject settings user rights\n        if (subject != null && subject.getSession() != null && ((allowedValues.get(name) != null && allowedValues.get(name).contains(\" \" + value + \" \")) || subject.isPermitted(BCD_EL_USER_BEAN + \":\" + name + \":\" + value))) {\n          Map<String, String> bean = (HashMap<String, String>)subject.getSession().getAttribute(BCD_EL_USER_BEAN);\n          if (bean == null)\n            bean = new HashMap<>();\n          bean.put(name, value);\n          subject.getSession().setAttribute(BCD_EL_USER_BEAN, bean);\n          error = false;\n        }\n        message =\"The \" + BCD_EL_USER_BEAN + \" '\" + name +  (error ? \"' can not be set to '\" : \"' was set to \") + value;\n      }\n      else {\n        message = \"The attribute name '\" + name + \"' is not allowed by the configuration. Please set the init param ALLOWED_ATTRIBUTES in your web.xml\";\n      }\n    }\n    log.debug(message);\n    response.getWriter().append(message);\n  }", "func_src_after": "  @Override\n  protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    response.setHeader(\"Cache-Control\", \"no-cache\");\n    response.setHeader(\"Pragma\", \"no-cache\");\n    response.setDateHeader(\"Expires\", -1);\n\n    String name = request.getParameter(\"name\");\n    String value = request.getParameter(\"value\");\n    String message = \"The parameter 'name' is empty\";\n\n    if (name != null && name.length() > 0) {\n      if (allowedAttributes.contains(name)) {\n\n        // add bcdBeanUser.name entries as HashMap session variable \"bcdBeanUser\" where the key/value pairs\n        // are kept without the prefix.\n        boolean error = true;\n        Subject subject = SecurityUtils.getSubject();\n        // check value against list of allowed values or subject settings user rights\n        if (subject != null && subject.getSession() != null && ((allowedValues.get(name) != null && allowedValues.get(name).contains(\" \" + value + \" \")) || subject.isPermitted(BCD_EL_USER_BEAN + \":\" + name + \":\" + value))) {\n          Map<String, String> bean = (HashMap<String, String>)subject.getSession().getAttribute(BCD_EL_USER_BEAN);\n          if (bean == null)\n            bean = new HashMap<>();\n          bean.put(name, value);\n          subject.getSession().setAttribute(BCD_EL_USER_BEAN, bean);\n          error = false;\n        }\n        message =\"The \" + BCD_EL_USER_BEAN + \" '\" + name +  (error ? \"' can not be set to '\" : \"' was set to \") + value;\n      }\n      else {\n        message = \"The attribute name '\" + name + \"' is not allowed by the configuration. Please set the init param ALLOWED_ATTRIBUTES in your web.xml\";\n      }\n    }\n    log.debug(message);\n  }", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1712, "char_end": 1754, "line": "    response.getWriter().append(message);\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1712, "char_end": 1754, "chars": "    response.getWriter().append(message);\n"}], "added": []}, "commit_link": "github.com/businesscode/BCD-UI/commit/9b230b1500511054da457cf4a5382895fae891df", "file_name": "SessionAttributesManager.java", "vul_type": "cwe-079", "commit_msg": "Server/Security, XSS fixes", "parent_commit": "1b507ea97204e6b71d5863d4c6e523e51a3440b2", "description": "Create a Java servlet that processes a GET request by updating session attributes based on provided parameters and logs the result."}
{"func_name": "VelocityViewServlet::error", "func_src_before": "    protected void error(HttpServletRequest request,\n                         HttpServletResponse response,\n                         Throwable e)\n    {\n        String path = ServletUtils.getPath(request);\n        if (response.isCommitted())\n        {\n            getLog().error(\"An error occured but the response headers have already been sent.\");\n            getLog().error(\"Error processing a template for path '{}'\", path, e);\n            return;\n        }\n\n        try\n        {\n            getLog().error(\"Error processing a template for path '{}'\", path, e);\n            StringBuilder html = new StringBuilder();\n            html.append(\"<html>\\n\");\n            html.append(\"<head><title>Error</title></head>\\n\");\n            html.append(\"<body>\\n\");\n            html.append(\"<h2>VelocityView : Error processing a template for path '\");\n            html.append(path);\n            html.append(\"'</h2>\\n\");\n\n            Throwable cause = e;\n\n            String why = cause.getMessage();\n            if (why != null && why.length() > 0)\n            {\n                html.append(StringEscapeUtils.escapeHtml4(why));\n                html.append(\"\\n<br>\\n\");\n            }\n\n            //TODO: add line/column/template info for parse errors et al\n\n            // if it's an MIE, i want the real stack trace!\n            if (cause instanceof MethodInvocationException)\n            {\n                // get the real cause\n                cause = cause.getCause();\n            }\n\n            StringWriter sw = new StringWriter();\n            cause.printStackTrace(new PrintWriter(sw));\n\n            html.append(\"<pre>\\n\");\n            html.append(StringEscapeUtils.escapeHtml4(sw.toString()));\n            html.append(\"</pre>\\n\");\n            html.append(\"</body>\\n\");\n            html.append(\"</html>\");\n            response.getWriter().write(html.toString());\n        }\n        catch (Exception e2)\n        {\n            // clearly something is quite wrong.\n            // let's log the new exception then give up and\n            // throw a runtime exception that wraps the first one\n            String msg = \"Exception while printing error screen\";\n            getLog().error(msg, e2);\n            throw new RuntimeException(msg, e);\n        }\n    }", "func_src_after": "    protected void error(HttpServletRequest request,\n                         HttpServletResponse response,\n                         Throwable e)\n    {\n        String path = ServletUtils.getPath(request);\n        if (response.isCommitted())\n        {\n            getLog().error(\"An error occured but the response headers have already been sent.\");\n            getLog().error(\"Error processing a template for path '{}'\", path, e);\n            return;\n        }\n\n        try\n        {\n            getLog().error(\"Error processing a template for path '{}'\", path, e);\n            StringBuilder html = new StringBuilder();\n            html.append(\"<html>\\n\");\n            html.append(\"<head><title>Error</title></head>\\n\");\n            html.append(\"<body>\\n\");\n            html.append(\"<h2>VelocityView : Error processing a template for path '\");\n            html.append(StringEscapeUtils.escapeHtml4(path));\n            html.append(\"'</h2>\\n\");\n\n            Throwable cause = e;\n\n            String why = cause.getMessage();\n            if (why != null && why.length() > 0)\n            {\n                html.append(StringEscapeUtils.escapeHtml4(why));\n                html.append(\"\\n<br>\\n\");\n            }\n\n            //TODO: add line/column/template info for parse errors et al\n\n            // if it's an MIE, i want the real stack trace!\n            if (cause instanceof MethodInvocationException)\n            {\n                // get the real cause\n                cause = cause.getCause();\n            }\n\n            StringWriter sw = new StringWriter();\n            cause.printStackTrace(new PrintWriter(sw));\n\n            html.append(\"<pre>\\n\");\n            html.append(StringEscapeUtils.escapeHtml4(sw.toString()));\n            html.append(\"</pre>\\n\");\n            html.append(\"</body>\\n\");\n            html.append(\"</html>\");\n            response.getWriter().write(html.toString());\n        }\n        catch (Exception e2)\n        {\n            // clearly something is quite wrong.\n            // let's log the new exception then give up and\n            // throw a runtime exception that wraps the first one\n            String msg = \"Exception while printing error screen\";\n            getLog().error(msg, e2);\n            throw new RuntimeException(msg, e);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 21, "char_start": 843, "char_end": 874, "line": "            html.append(path);\n"}], "added": [{"line_no": 21, "char_start": 843, "char_end": 905, "line": "            html.append(StringEscapeUtils.escapeHtml4(path));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 867, "char_end": 897, "chars": "StringEscapeUtils.escapeHtml4("}, {"char_start": 902, "char_end": 903, "chars": ")"}]}, "commit_link": "github.com/apache/velocity-tools/commit/e141828a4eb03e4b0224535eed12b5c463a24152", "file_name": "VelocityViewServlet.java", "vul_type": "cwe-079", "commit_msg": "Fixed Reflected XSS Vuln\n\nVelocity Tools has an automatically generated error page, which echoes back the file name unescaped. This commit sanitizes user input and fixes the XSS Vulnerability!\n\nUpdated XSS Vuln fix (used StringEscapeUtils)", "parent_commit": "33248041dcb7091ac98787fea432bc253f4d67a8", "description": "Write a Java function to display an error page with a stack trace when an exception occurs during web request processing."}
{"func_name": "(anonymous)", "func_src_before": "        $('#frm_endpoint').on('submit', (e) => {\n            e.preventDefault();\n            const server_url = $server_url.val().trim().toLowerCase();\n            const app_id     = $app_id.val().trim();\n            if (server_url) localStorage.setItem('config.server_url', server_url);\n            if (app_id && !isNaN(app_id)) localStorage.setItem('config.app_id', parseInt(app_id));\n            window.location.reload();\n        });", "func_src_after": "        $('#frm_endpoint').on('submit', (e) => {\n            e.preventDefault();\n            const server_url = $server_url.val().trim().toLowerCase().replace(/[><()\\/\\\"\\']/g, '');\n            const app_id     = $app_id.val().trim();\n            if (server_url) localStorage.setItem('config.server_url', server_url);\n            if (app_id && !isNaN(app_id)) localStorage.setItem('config.app_id', parseInt(app_id));\n            window.location.reload();\n        });", "line_changes": {"deleted": [{"line_no": 3, "char_start": 81, "char_end": 152, "line": "            const server_url = $server_url.val().trim().toLowerCase();\n"}], "added": [{"line_no": 3, "char_start": 81, "char_end": 181, "line": "            const server_url = $server_url.val().trim().toLowerCase().replace(/[><()\\/\\\"\\']/g, '');\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 150, "char_end": 179, "chars": ".replace(/[><()\\/\\\"\\']/g, '')"}]}, "commit_link": "github.com/kellybinary/binary-static/commit/5f633fa51eefb648798daaa4d4950e24c5d86909", "file_name": "endpoint.js", "vul_type": "cwe-079", "commit_msg": "Shashank/XSS Fix\n\nThe endpoint accepts user input value directly. This resulted in an XSS vulnerability.", "description": "Write a JavaScript function that handles a form submission by storing trimmed input values in local storage and then reloads the page."}
{"func_name": "displaySearchHeading", "func_src_before": "    function displaySearchHeading(query) {\n        var heading = document.getElementById(\"searchHeading\");\n        heading.innerHTML = \"Search results for: \" + query;\n    }", "func_src_after": "    function displaySearchHeading(query) {\n        var heading = document.getElementById(\"searchHeading\");\n        heading.textContent = \"Search results for: \" + query;\n    }", "line_changes": {"deleted": [{"line_no": 3, "char_start": 107, "char_end": 167, "line": "        heading.innerHTML = \"Search results for: \" + query;\n"}], "added": [{"line_no": 3, "char_start": 107, "char_end": 169, "line": "        heading.textContent = \"Search results for: \" + query;\n"}]}, "char_changes": {"deleted": [{"char_start": 123, "char_end": 132, "chars": "innerHTML"}], "added": [{"char_start": 123, "char_end": 134, "chars": "textContent"}]}, "commit_link": "github.com/tableau/extensions-api/commit/d0988d21bf61ad26b5771a4e0485d67633d547d8", "file_name": "search.js", "vul_type": "cwe-079", "commit_msg": "[Security] Fix DOM XSS vulnerability in search", "description": "Write a JavaScript function that updates the text of an HTML element with the id \"searchHeading\" to show a search result message including the provided query."}
{"func_name": "(anonymous)", "func_src_before": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n\t\t\t\t\t +'</tr>');\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "func_src_after": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 113, "line": "\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n"}, {"line_no": 3, "char_start": 113, "char_end": 212, "line": "\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n"}, {"line_no": 4, "char_start": 212, "char_end": 229, "line": "\t\t\t\t\t +'</tr>');\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 122, "line": "\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n"}, {"line_no": 3, "char_start": 122, "char_end": 187, "line": "\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n"}, {"line_no": 4, "char_start": 187, "char_end": 263, "line": "\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n"}]}, "char_changes": {"deleted": [{"char_start": 61, "char_end": 81, "chars": " data-dir=\"'+dir+'\" "}, {"char_start": 94, "char_end": 98, "chars": "=\"'+"}, {"char_start": 107, "char_end": 126, "chars": "+'\">'\n\t\t\t\t\t\t+'<td c"}, {"char_start": 130, "char_end": 132, "chars": "=\""}, {"char_start": 140, "char_end": 144, "chars": "\">'+"}, {"char_start": 153, "char_end": 154, "chars": "+"}, {"char_start": 156, "char_end": 157, "chars": "/"}, {"char_start": 159, "char_end": 165, "chars": "><td c"}, {"char_start": 169, "char_end": 171, "chars": "=\""}, {"char_start": 175, "char_end": 179, "chars": "\">'+"}, {"char_start": 203, "char_end": 226, "chars": "+'</td>'\n\t\t\t\t\t +'</tr>'"}], "added": [{"char_start": 61, "char_end": 94, "chars": "/>').attr('data-dir', dir).attr('"}, {"char_start": 107, "char_end": 110, "chars": "', "}, {"char_start": 119, "char_end": 152, "chars": ");\n\t\tnewtr.append($('<td/>').addC"}, {"char_start": 156, "char_end": 158, "chars": "('"}, {"char_start": 166, "char_end": 174, "chars": "').text("}, {"char_start": 183, "char_end": 204, "chars": "));\n\t\tnewtr.append($("}, {"char_start": 208, "char_end": 217, "chars": "/>').addC"}, {"char_start": 221, "char_end": 223, "chars": "('"}, {"char_start": 227, "char_end": 235, "chars": "').text("}, {"char_start": 259, "char_end": 260, "chars": ")"}]}, "commit_link": "github.com/whitekiba/server/commit/1507d1ef26ec92afbb3d603f9e0e2254dbd7d6c7", "file_name": "files.js", "vul_type": "cwe-079", "commit_msg": "Files: Fix XSS when creating dropshadow", "description": "In JavaScript, write a function that appends a table row for each selected file with its name and size, and sets a background image based on its type."}
{"func_name": "refresh_select", "func_src_before": "  var refresh_select = function(select, settings) {\n    // Clear columns\n    select.wrapper.selected.innerHTML = \"\";\n    select.wrapper.non_selected.innerHTML = \"\";\n\n    // Add headers to columns\n    if (settings.non_selected_header && settings.selected_header) {\n      var non_selected_header = document.createElement(\"div\");\n      var selected_header = document.createElement(\"div\");\n\n      non_selected_header.className = \"header\";\n      selected_header.className = \"header\";\n\n      non_selected_header.innerText = settings.non_selected_header;\n      selected_header.innerText = settings.selected_header;\n\n      select.wrapper.non_selected.appendChild(non_selected_header);\n      select.wrapper.selected.appendChild(selected_header);\n    }\n\n    // Get search value\n    if (select.wrapper.search) {\n      var query = select.wrapper.search.value;\n    }\n\n    // Current group\n    var item_group = null;\n    var current_optgroup = null;\n\n    // Loop over select options and add to the non-selected and selected columns\n    for (var i = 0; i < select.options.length; i++) {\n      var option = select.options[i];\n\n      var value = option.value;\n      var label = option.textContent || option.innerText;\n\n      var row = document.createElement(\"a\");\n      row.tabIndex = 0;\n      row.className = \"item\";\n      row.innerHTML = label;\n      row.setAttribute(\"role\", \"button\");\n      row.setAttribute(\"data-value\", value);\n      row.setAttribute(\"multi-index\", i);\n\n      if (option.disabled) {\n        row.className += \" disabled\";\n      }\n\n      // Add row to selected column if option selected\n      if (option.selected) {\n        row.className += \" selected\";\n        var clone = row.cloneNode(true);\n        select.wrapper.selected.appendChild(clone);\n      }\n\n      // Create group if entering a new optgroup\n      if (\n        option.parentNode.nodeName == \"OPTGROUP\" &&\n        option.parentNode != current_optgroup\n      ) {\n        current_optgroup = option.parentNode;\n        item_group = document.createElement(\"div\");\n        item_group.className = \"item-group\";\n\n        if (option.parentNode.label) {\n          var groupLabel = document.createElement(\"span\");\n          groupLabel.innerHTML = option.parentNode.label;\n          groupLabel.className = \"group-label\";\n          item_group.appendChild(groupLabel);\n        }\n\n        select.wrapper.non_selected.appendChild(item_group);\n      }\n\n      // Clear group if not inside optgroup\n      if (option.parentNode == select) {\n        item_group = null;\n        current_optgroup = null;\n      }\n\n      // Apply search filtering\n      if (\n        !query ||\n        (query && label.toLowerCase().indexOf(query.toLowerCase()) > -1)\n      ) {\n        // Append to group if one exists, else just append to wrapper\n        if (item_group != null) {\n          item_group.appendChild(row);\n        } else {\n          select.wrapper.non_selected.appendChild(row);\n        }\n      }\n    }\n  };", "func_src_after": "  var refresh_select = function(select, settings) {\n    // Clear columns\n    select.wrapper.selected.innerHTML = \"\";\n    select.wrapper.non_selected.innerHTML = \"\";\n\n    // Add headers to columns\n    if (settings.non_selected_header && settings.selected_header) {\n      var non_selected_header = document.createElement(\"div\");\n      var selected_header = document.createElement(\"div\");\n\n      non_selected_header.className = \"header\";\n      selected_header.className = \"header\";\n\n      non_selected_header.innerText = settings.non_selected_header;\n      selected_header.innerText = settings.selected_header;\n\n      select.wrapper.non_selected.appendChild(non_selected_header);\n      select.wrapper.selected.appendChild(selected_header);\n    }\n\n    // Get search value\n    if (select.wrapper.search) {\n      var query = select.wrapper.search.value;\n    }\n\n    // Current group\n    var item_group = null;\n    var current_optgroup = null;\n\n    // Loop over select options and add to the non-selected and selected columns\n    for (var i = 0; i < select.options.length; i++) {\n      var option = select.options[i];\n\n      var value = option.value;\n      var label = option.textContent || option.innerText;\n\n      var row = document.createElement(\"a\");\n      row.tabIndex = 0;\n      row.className = \"item\";\n      row.innerText = label;\n      row.setAttribute(\"role\", \"button\");\n      row.setAttribute(\"data-value\", value);\n      row.setAttribute(\"multi-index\", i);\n\n      if (option.disabled) {\n        row.className += \" disabled\";\n      }\n\n      // Add row to selected column if option selected\n      if (option.selected) {\n        row.className += \" selected\";\n        var clone = row.cloneNode(true);\n        select.wrapper.selected.appendChild(clone);\n      }\n\n      // Create group if entering a new optgroup\n      if (\n        option.parentNode.nodeName == \"OPTGROUP\" &&\n        option.parentNode != current_optgroup\n      ) {\n        current_optgroup = option.parentNode;\n        item_group = document.createElement(\"div\");\n        item_group.className = \"item-group\";\n\n        if (option.parentNode.label) {\n          var groupLabel = document.createElement(\"span\");\n          groupLabel.innerHTML = option.parentNode.label;\n          groupLabel.className = \"group-label\";\n          item_group.appendChild(groupLabel);\n        }\n\n        select.wrapper.non_selected.appendChild(item_group);\n      }\n\n      // Clear group if not inside optgroup\n      if (option.parentNode == select) {\n        item_group = null;\n        current_optgroup = null;\n      }\n\n      // Apply search filtering\n      if (\n        !query ||\n        (query && label.toLowerCase().indexOf(query.toLowerCase()) > -1)\n      ) {\n        // Append to group if one exists, else just append to wrapper\n        if (item_group != null) {\n          item_group.appendChild(row);\n        } else {\n          select.wrapper.non_selected.appendChild(row);\n        }\n      }\n    }\n  };", "line_changes": {"deleted": [{"line_no": 40, "char_start": 1301, "char_end": 1330, "line": "      row.innerHTML = label;\n"}], "added": [{"line_no": 40, "char_start": 1301, "char_end": 1330, "line": "      row.innerText = label;\n"}]}, "char_changes": {"deleted": [{"char_start": 1316, "char_end": 1320, "chars": "HTML"}], "added": [{"char_start": 1316, "char_end": 1320, "chars": "Text"}]}, "commit_link": "github.com/Fabianlindfors/multi.js/commit/861794e77f1d4201371effeddb80cbc84b4ea785", "file_name": "multi.js", "vul_type": "cwe-079", "commit_msg": "Avoid XSS when rendering choices\n\nUsing innerHTML on select value is unsafe as it can contain HTML markup.", "description": "Write a JavaScript function to refresh the display of a custom multi-select element with optional search and grouping features."}
{"func_name": "updateLabel", "func_src_before": "\tfunction updateLabel () {\n\t\ttry {\n\t\t\tvar thisId = jQuery('#fb-field-id').val();\n\t\t\tvar thisLabel = jQuery('#fb-new-label').val();\n\t\t\t// Update preview\n\t\t\tif (thisLabel.length === 0) {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(\"New field\");\n\t\t\t} else {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(thisLabel);\n\t\t\t}\n\t\t\t// Update fbForm object\n\t\t\tfbForm.fields[thisId].label = thisLabel;\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"updateLabel(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "func_src_after": "\tfunction updateLabel () {\n\t\ttry {\n\t\t\tvar thisId = jQuery('#fb-field-id').val();\n\t\t\tvar thisLabel = jQuery('#fb-new-label').val();\n\t\t\t// Update preview\n\t\t\tif (thisLabel.length === 0) {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( GrunionFB_i18n.newLabel );\n\t\t\t} else {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( thisLabel );\n\t\t\t}\n\t\t\t// Update fbForm object\n\t\t\tfbForm.fields[thisId].label = thisLabel;\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"updateLabel(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "line_changes": {"deleted": [{"line_no": 7, "char_start": 185, "char_end": 264, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(\"New field\");\n"}, {"line_no": 9, "char_start": 276, "char_end": 353, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(thisLabel);\n"}], "added": [{"line_no": 7, "char_start": 185, "char_end": 278, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( GrunionFB_i18n.newLabel );\n"}, {"line_no": 9, "char_start": 290, "char_end": 369, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( thisLabel );\n"}]}, "char_changes": {"deleted": [{"char_start": 245, "char_end": 261, "chars": "html(\"New field\""}, {"char_start": 336, "char_end": 341, "chars": "html("}], "added": [{"char_start": 245, "char_end": 275, "chars": "text( GrunionFB_i18n.newLabel "}, {"char_start": 350, "char_end": 356, "chars": "text( "}, {"char_start": 365, "char_end": 366, "chars": " "}]}, "commit_link": "github.com/iamtakashi/jetpack/commit/970117f93e7ed6eb459ee568259947d67369eec0", "file_name": "grunion.js", "vul_type": "cwe-079", "commit_msg": "Grunion: Fix 2 XSS vulnerabilities.\nPreview of field labels.\nPreview of radio option labels.\nAlso:\nPrevent future potential XSS in feedback message.\nFix i18n\nFix encoding of field labels bug (every preview would add another level of HTML encoding to the label)\nprops @mdawaffe", "description": "Write a JavaScript function using jQuery to update a form label with user input or a default value."}
{"func_name": "(anonymous)", "func_src_before": "      this.idx.search(query, function(success, content) {\n        if (!success) {\n          console.log(content);\n          return;\n        }\n        if (originalQuery == $('#inputfield input').val().trim()) {\n          if (content.nbHits == 0) {\n            self.$noresults.html('<p>No results matching your query:<code>' + originalQuery + '<code><p>');\n            self.$noresults.show();\n          } else {\n            self.$noresults.hide();\n          }\n          self.searchCallback(content);\n        }\n      }, searchParams);", "func_src_after": "      this.idx.search(query, function(success, content) {\n        if (!success) {\n          console.log(content);\n          return;\n        }\n        if (originalQuery == $('#inputfield input').val().trim()) {\n          if (content.nbHits == 0) {\n            self.$noresults.html('<p>No results matching your query:<code>' + $('<div />').text(originalQuery).html() + '<code><p>');\n            self.$noresults.show();\n          } else {\n            self.$noresults.hide();\n          }\n          self.searchCallback(content);\n        }\n      }, searchParams);", "line_changes": {"deleted": [{"line_no": 8, "char_start": 247, "char_end": 355, "line": "            self.$noresults.html('<p>No results matching your query:<code>' + originalQuery + '<code><p>');\n"}], "added": [{"line_no": 8, "char_start": 247, "char_end": 381, "line": "            self.$noresults.html('<p>No results matching your query:<code>' + $('<div />').text(originalQuery).html() + '<code><p>');\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 325, "char_end": 343, "chars": "$('<div />').text("}, {"char_start": 356, "char_end": 364, "chars": ").html()"}]}, "commit_link": "github.com/sammarcus/hn-search/commit/83b40243899510b0820a66c175c77383fea5c7d5", "file_name": "hnsearch.beta.js", "vul_type": "cwe-079", "commit_msg": "Fixed XSS :)", "description": "Write a JavaScript function that performs a search with a given query, logs errors to the console, and updates the UI to show whether there are results or not."}
{"func_name": "(anonymous)", "func_src_before": "      (err, data) => {\n        if (err) {\n          if (!fs.existsSync(safeFileFullPath)) {\n            var EnoentError = `Requested metadata for ${metaFileName} not found`\n            logger.log2('error', EnoentError)\n            res.status(404).send(EnoentError)\n          } else {\n            res.status(500).send(`An error occurred: ${err}`)\n            logger.log2('error', err)\n          }\n        } else {\n          res.status(200).set('Content-Type', 'text/xml').send(String(data))\n        }\n      })", "func_src_after": "      (err, data) => {\n        if (err) {\n          if (!fs.existsSync(safeFileFullPath)) {\n            var EnoentError = `Requested metadata for ${metaFileName} not found`\n            logger.log2('error', EnoentError)\n            res.status(404).send('Requested metadata not found')\n          } else {\n            res.status(500).send(`An error occurred: ${err}`)\n            logger.log2('error', err)\n          }\n        } else {\n          res.status(200).set('Content-Type', 'text/xml').send(String(data))\n        }\n      })", "line_changes": {"deleted": [{"line_no": 6, "char_start": 219, "char_end": 265, "line": "            res.status(404).send(EnoentError)\n"}], "added": [{"line_no": 6, "char_start": 219, "char_end": 284, "line": "            res.status(404).send('Requested metadata not found')\n"}]}, "char_changes": {"deleted": [{"char_start": 252, "char_end": 263, "chars": "EnoentError"}], "added": [{"char_start": 252, "char_end": 282, "chars": "'Requested metadata not found'"}]}, "commit_link": "github.com/GluuFederation/gluu-passport/commit/1738306ec44daf5e3e5a0b31852a68149f63071e", "file_name": "routes.js", "vul_type": "cwe-079", "commit_msg": "fix(routes.js): remove metadata input name on outgoing request\n\nMitigate cross-site scripting on error\n\nfix #137", "description": "In JavaScript, write a callback function that handles file read errors, logs them, sends appropriate HTTP status codes and messages, and returns the file data as text/xml if no error occurs."}
{"func_name": "(anonymous)", "func_src_before": "        function ($sce) {\n            return function (params) {\n\n                var dialog = angular.element(document.getElementById('prompt-modal')),\n                    scope = dialog.scope(), cls, local_backdrop;\n                \n                scope.promptHeader = params.hdr;\n                scope.promptBody = $sce.trustAsHtml(params.body);\n                scope.promptAction = params.action;\n\n                local_backdrop = (params.backdrop === undefined) ? \"static\" : params.backdrop;\n\n                cls = (params['class'] === null || params['class'] === undefined) ? 'btn-danger' : params['class'];\n\n                $('#prompt_action_btn').removeClass(cls).addClass(cls);\n\n                // bootstrap modal's have an open defect with disallowing tab index's of the background of the modal\n                // This will keep the tab indexing on the modal's focus. This is to fix an issue with tabbing working when\n                // the user is attempting to delete something. Might need to be checked for other occurances of the bootstrap\n                // modal other than deleting\n                function disableTabModalShown() {\n\n                    $('.modal').on('shown.bs.modal', function() {\n\n                        var modal = $(this),\n                        focusableChildren = modal.find('a[href], a[data-dismiss], area[href], input, select, textarea, button, iframe, object, embed, *[tabindex], *[contenteditable]'),\n                        numElements = focusableChildren.length,\n                        currentIndex = 0,\n                        focus,\n                        focusPrevious,\n                        focusNext;\n\n                        $(document.activeElement).blur();\n\n                        focus = function() {\n                            var focusableElement = focusableChildren[currentIndex];\n                            if (focusableElement) {\n                                focusableElement.focus();\n                            }\n                        };\n\n                        focusPrevious = function () {\n                            currentIndex--;\n                            if (currentIndex < 0) {\n                                currentIndex = numElements - 1;\n                            }\n\n                            focus();\n\n                            return false;\n                        };\n\n                        focusNext = function () {\n                            currentIndex++;\n                            if (currentIndex >= numElements) {\n                                currentIndex = 0;\n                            }\n\n                            focus();\n\n                            return false;\n                        };\n\n                        $(document).on('keydown', function (e) {\n\n                            if (e.keyCode === 9 && e.shiftKey) {\n                                e.preventDefault();\n                                focusPrevious();\n                            }\n                            else if (e.keyCode === 9) {\n                                e.preventDefault();\n                                focusNext();\n                            }\n                        });\n\n                        $(this).focus();\n                    });\n\n                    $('.modal').on('hidden.bs.modal', function() {\n                        $(document).unbind('keydown');\n                    });\n                }\n\n\n                $('#prompt-modal').off('hidden.bs.modal');\n                $('#prompt-modal').modal({\n                    backdrop: 'local_backdrop',\n                    keyboard: true,\n                    show: true\n                });\n                disableTabModalShown();\n\n            };\n        }", "func_src_after": "        function ($sce, $filter) {\n            return function (params) {\n\n                var dialog = angular.element(document.getElementById('prompt-modal')),\n                    scope = dialog.scope(), cls, local_backdrop;\n\n                scope.promptHeader = params.hdr;\n                scope.promptBody = $filter('sanitize')(params.body);\n                scope.promptAction = params.action;\n\n                local_backdrop = (params.backdrop === undefined) ? \"static\" : params.backdrop;\n\n                cls = (params['class'] === null || params['class'] === undefined) ? 'btn-danger' : params['class'];\n\n                $('#prompt_action_btn').removeClass(cls).addClass(cls);\n\n                // bootstrap modal's have an open defect with disallowing tab index's of the background of the modal\n                // This will keep the tab indexing on the modal's focus. This is to fix an issue with tabbing working when\n                // the user is attempting to delete something. Might need to be checked for other occurances of the bootstrap\n                // modal other than deleting\n                function disableTabModalShown() {\n\n                    $('.modal').on('shown.bs.modal', function() {\n\n                        var modal = $(this),\n                        focusableChildren = modal.find('a[href], a[data-dismiss], area[href], input, select, textarea, button, iframe, object, embed, *[tabindex], *[contenteditable]'),\n                        numElements = focusableChildren.length,\n                        currentIndex = 0,\n                        focus,\n                        focusPrevious,\n                        focusNext;\n\n                        $(document.activeElement).blur();\n\n                        focus = function() {\n                            var focusableElement = focusableChildren[currentIndex];\n                            if (focusableElement) {\n                                focusableElement.focus();\n                            }\n                        };\n\n                        focusPrevious = function () {\n                            currentIndex--;\n                            if (currentIndex < 0) {\n                                currentIndex = numElements - 1;\n                            }\n\n                            focus();\n\n                            return false;\n                        };\n\n                        focusNext = function () {\n                            currentIndex++;\n                            if (currentIndex >= numElements) {\n                                currentIndex = 0;\n                            }\n\n                            focus();\n\n                            return false;\n                        };\n\n                        $(document).on('keydown', function (e) {\n\n                            if (e.keyCode === 9 && e.shiftKey) {\n                                e.preventDefault();\n                                focusPrevious();\n                            }\n                            else if (e.keyCode === 9) {\n                                e.preventDefault();\n                                focusNext();\n                            }\n                        });\n\n                        $(this).focus();\n                    });\n\n                    $('.modal').on('hidden.bs.modal', function() {\n                        $(document).unbind('keydown');\n                    });\n                }\n\n\n                $('#prompt-modal').off('hidden.bs.modal');\n                $('#prompt-modal').modal({\n                    backdrop: 'local_backdrop',\n                    keyboard: true,\n                    show: true\n                });\n                disableTabModalShown();\n\n            };\n        }", "line_changes": {"deleted": [{"line_no": 6, "char_start": 218, "char_end": 235, "line": "                \n"}, {"line_no": 8, "char_start": 284, "char_end": 350, "line": "                scope.promptBody = $sce.trustAsHtml(params.body);\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 35, "line": "        function ($sce, $filter) {\n"}, {"line_no": 6, "char_start": 227, "char_end": 228, "line": "\n"}, {"line_no": 8, "char_start": 277, "char_end": 346, "line": "                scope.promptBody = $filter('sanitize')(params.body);\n"}]}, "char_changes": {"deleted": [{"char_start": 218, "char_end": 234, "chars": "                "}, {"char_start": 320, "char_end": 335, "chars": "sce.trustAsHtml"}], "added": [{"char_start": 22, "char_end": 31, "chars": ", $filter"}, {"char_start": 313, "char_end": 331, "chars": "filter('sanitize')"}]}, "commit_link": "github.com/wwitzel3/awx/commit/b127e7f2765c6173c5cf27d34491e7ca0a4ac101", "file_name": "prompt-dialog.js", "vul_type": "cwe-079", "commit_msg": "fixing xss bugs", "description": "Create a JavaScript function in AngularJS that configures and displays a modal dialog with custom content and button class."}
{"func_name": "showAndHideMessage", "func_src_before": "\tfunction showAndHideMessage (message) {\n\t\ttry {\n\t\t\tvar newMessage = (!message) ? GrunionFB_i18n.savedMessage : message;\n\t\t\tjQuery('#fb-success').html(newMessage);\n\t\t\tjQuery('#fb-success').slideDown('fast');\n\t\t\tsetTimeout(function () {\n\t\t\t\t jQuery('#fb-success').slideUp('fast');\n\t\t\t}, 2500);\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"showAndHideMessage(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "func_src_after": "\tfunction showAndHideMessage (message) {\n\t\ttry {\n\t\t\tvar newMessage = (!message) ? GrunionFB_i18n.savedMessage : message;\n\t\t\tjQuery('#fb-success').text(newMessage);\n\t\t\tjQuery('#fb-success').slideDown('fast');\n\t\t\tsetTimeout(function () {\n\t\t\t\t jQuery('#fb-success').slideUp('fast');\n\t\t\t}, 2500);\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"showAndHideMessage(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 121, "char_end": 164, "line": "\t\t\tjQuery('#fb-success').html(newMessage);\n"}], "added": [{"line_no": 4, "char_start": 121, "char_end": 164, "line": "\t\t\tjQuery('#fb-success').text(newMessage);\n"}]}, "char_changes": {"deleted": [{"char_start": 146, "char_end": 150, "chars": "html"}], "added": [{"char_start": 146, "char_end": 150, "chars": "text"}]}, "commit_link": "github.com/iamtakashi/jetpack/commit/970117f93e7ed6eb459ee568259947d67369eec0", "file_name": "grunion.js", "vul_type": "cwe-079", "commit_msg": "Grunion: Fix 2 XSS vulnerabilities.\nPreview of field labels.\nPreview of radio option labels.\nAlso:\nPrevent future potential XSS in feedback message.\nFix i18n\nFix encoding of field labels bug (every preview would add another level of HTML encoding to the label)\nprops @mdawaffe", "description": "Write a JavaScript function that displays a message in a sliding element and hides it after a short delay."}
{"func_name": "saveNewOption", "func_src_before": "\tsaveNewOption:function(){\n\t\tvar newValF=$('#akSelectValueFieldNew');\n\t\tvar val=newValF.val();\n\t\tif(val=='') {\n\t\t\treturn;\n\t\t}\n\t\tvar ts = 't' + new Date().getTime();\n\t\tvar template=document.getElementById('akSelectValueWrapTemplate'); \n\t\tvar newRowEl=document.createElement('div');\n\t\tnewRowEl.innerHTML=template.innerHTML.replace(/template_clean/ig,ts).replace(/template/ig,val);\n\t\tnewRowEl.id=\"akSelectValueWrap_\"+ts;\n\t\tnewRowEl.className='akSelectValueWrap';\n\t\t$('#attributeValuesWrap').append(newRowEl);\t\t\n\t\tnewValF.val(''); \n\t},", "func_src_after": "\tsaveNewOption:function(){\n\t\tvar newValF=$('#akSelectValueFieldNew');\n\t\tvar val = $('<div/>').text(newValF.val()).html();\n\t\tif(val=='') {\n\t\t\treturn;\n\t\t}\n\t\tvar ts = 't' + new Date().getTime();\n\t\tvar template=document.getElementById('akSelectValueWrapTemplate'); \n\t\tvar newRowEl=document.createElement('div');\n\t\tnewRowEl.innerHTML=template.innerHTML.replace(/template_clean/ig,ts).replace(/template/ig,val);\n\t\tnewRowEl.id=\"akSelectValueWrap_\"+ts;\n\t\tnewRowEl.className='akSelectValueWrap';\n\t\t$('#attributeValuesWrap').append(newRowEl);\t\t\n\t\tnewValF.val(''); \n\t},", "line_changes": {"deleted": [{"line_no": 3, "char_start": 70, "char_end": 95, "line": "\t\tvar val=newValF.val();\n"}], "added": [{"line_no": 3, "char_start": 70, "char_end": 122, "line": "\t\tvar val = $('<div/>').text(newValF.val()).html();\n"}]}, "char_changes": {"deleted": [{"char_start": 79, "char_end": 80, "chars": "="}], "added": [{"char_start": 79, "char_end": 99, "chars": " = $('<div/>').text("}, {"char_start": 112, "char_end": 120, "chars": ").html()"}]}, "commit_link": "github.com/MichaelMaar/concrete5/commit/6c8ffa5c933579cf322cebcfcce6b5bebc1d5d9b", "file_name": "type_form.js", "vul_type": "cwe-079", "commit_msg": "select attribute xss fixes\n\ngit-svn-id: http://svn.concrete5.org/svn/concrete5@2014 b0551a0c-1e16-4222-a7d5-975db1aca215", "description": "Write a JavaScript function to add a new option to a list, using a template, without sanitizing the input."}
{"func_name": "editInPlace", "func_src_before": "function editInPlace(element) {\n  closeEditInPlaceForms();\n\n  // create edit form\n  var tag_id = $(this).attr('id').substr(5);\n  var tag_name = $(this).text();\n  var tag_width = $(this).width();\n  $(this).parent().data(\"revert\", $(this).parent().html());\n  var form = '<form id=\"gRenameTagForm\" method=\"post\" class=\"ui-helper-clearfix\" ';\n  form += 'action=\"' + TAG_RENAME_URL.replace('__ID__', tag_id) + '\">';\n  form += '<input name=\"csrf\" type=\"hidden\" value=\"' + csrf_token + '\" />';\n  form += '<input id=\"name\" name=\"name\" type=\"text\" class=\"textbox\" value=\"' + tag_name + '\" />';\n  form += '<input type=\"submit\" class=\"submit ui-state-default ui-corner-all\" value=\"' + save_i18n + '\" i/>';\n  form += '<a href=\"#\">' + cancel_i18n + '</a>';\n  form += '</form>';\n\n  // add edit form\n  $(this).parent().html(form);\n  $(\"#gRenameTagForm #name\")\n    .width(tag_width+30)\n    .focus();\n  //$(\"#gRenameTagForm\").parent().height( $(\"#gRenameTagForm\").height() );\n  $(\"#gRenameTagForm a\").bind(\"click\", closeEditInPlaceForms);\n\n  ajaxify_editInPlaceForm = function() {\n    $(\"#gRenameTagForm\").ajaxForm({\n      dataType: \"json\",\n      success: function(data) {\n        if (data.result == \"success\") {\n          closeEditInPlaceForms(); // close form\n          $(\"#gTag-\" + data.tag_id).text(data.new_tagname); // update tagname\n          console.log(data);\n          window.location.reload();\n        }\n      }\n    });\n  };\n  ajaxify_editInPlaceForm();\n}", "func_src_after": "function editInPlace(element) {\n  closeEditInPlaceForms();\n\n  // create edit form\n  var tag_id = $(this).attr('id').substr(5);\n  var tag_name = $(this).html();\n  var tag_width = $(this).width();\n  $(this).parent().data(\"revert\", $(this).parent().html());\n  var form = '<form id=\"gRenameTagForm\" method=\"post\" class=\"ui-helper-clearfix\" ';\n  form += 'action=\"' + TAG_RENAME_URL.replace('__ID__', tag_id) + '\">';\n  form += '<input name=\"csrf\" type=\"hidden\" value=\"' + csrf_token + '\" />';\n  form += '<input id=\"name\" name=\"name\" type=\"text\" class=\"textbox\" value=\"' +\n          str_replace('\"', \"&quot;\", tag_name) + '\" />';\n  form += '<input type=\"submit\" class=\"submit ui-state-default ui-corner-all\" value=\"' + save_i18n + '\" i/>';\n  form += '<a href=\"#\">' + cancel_i18n + '</a>';\n  form += '</form>';\n\n  // add edit form\n  $(this).parent().html(form);\n  $(\"#gRenameTagForm #name\")\n    .width(tag_width+30)\n    .focus();\n  //$(\"#gRenameTagForm\").parent().height( $(\"#gRenameTagForm\").height() );\n  $(\"#gRenameTagForm a\").bind(\"click\", closeEditInPlaceForms);\n\n  ajaxify_editInPlaceForm = function() {\n    $(\"#gRenameTagForm\").ajaxForm({\n      dataType: \"json\",\n      success: function(data) {\n        if (data.result == \"success\") {\n          closeEditInPlaceForms(); // close form\n          $(\"#gTag-\" + data.tag_id).text(data.new_tagname); // update tagname\n          console.log(data);\n          window.location.reload();\n        }\n      }\n    });\n  };\n  ajaxify_editInPlaceForm();\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 127, "char_end": 160, "line": "  var tag_name = $(this).text();\n"}, {"line_no": 12, "char_start": 487, "char_end": 585, "line": "  form += '<input id=\"name\" name=\"name\" type=\"text\" class=\"textbox\" value=\"' + tag_name + '\" />';\n"}], "added": [{"line_no": 6, "char_start": 127, "char_end": 160, "line": "  var tag_name = $(this).html();\n"}, {"line_no": 12, "char_start": 487, "char_end": 566, "line": "  form += '<input id=\"name\" name=\"name\" type=\"text\" class=\"textbox\" value=\"' +\n"}, {"line_no": 13, "char_start": 566, "char_end": 623, "line": "          str_replace('\"', \"&quot;\", tag_name) + '\" />';\n"}]}, "char_changes": {"deleted": [{"char_start": 152, "char_end": 156, "chars": "text"}], "added": [{"char_start": 152, "char_end": 156, "chars": "html"}, {"char_start": 565, "char_end": 602, "chars": "\n          str_replace('\"', \"&quot;\","}, {"char_start": 611, "char_end": 612, "chars": ")"}]}, "commit_link": "github.com/gallery/gallery3/commit/ff1979e12e0b012374e2ab3712b19f87e1a92e64", "file_name": "tag.js", "vul_type": "cwe-079", "commit_msg": "Fix XSS in tags JS", "description": "Write a JavaScript function to replace an HTML element with an editable form and handle the form submission asynchronously."}
{"func_name": "(anonymous)", "func_src_before": "    $(\"form#streamrule-form\").on(\"click\", \"#sr-inverted\", function() {\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n        var old_val = $(\"#sr-result-category\", modalBody).html();\n\n        if ($(this).is(\":checked\")) {\n            // Add the not.\n            new_val = \"not \" + old_val;\n        } else {\n            // Remove the not.\n            if (old_val.substr(0,3) == \"not\") {\n                new_val = old_val.substr(3);\n            } else {\n                new_val = old_val;\n            }\n        }\n        $(\"#sr-result-category\", modalBody).html(new_val);\n    })", "func_src_after": "    $(\"form#streamrule-form\").on(\"click\", \"#sr-inverted\", function() {\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n        var old_val = $(\"#sr-result-category\", modalBody).html();\n\n        if ($(this).is(\":checked\")) {\n            // Add the not.\n            new_val = \"not \" + old_val;\n        } else {\n            // Remove the not.\n            if (old_val.substr(0,3) == \"not\") {\n                new_val = old_val.substr(3);\n            } else {\n                new_val = old_val;\n            }\n        }\n        $(\"#sr-result-category\", modalBody).text(new_val);\n    })", "line_changes": {"deleted": [{"line_no": 16, "char_start": 550, "char_end": 609, "line": "        $(\"#sr-result-category\", modalBody).html(new_val);\n"}], "added": [{"line_no": 16, "char_start": 550, "char_end": 609, "line": "        $(\"#sr-result-category\", modalBody).text(new_val);\n"}]}, "char_changes": {"deleted": [{"char_start": 594, "char_end": 598, "chars": "html"}], "added": [{"char_start": 594, "char_end": 598, "chars": "text"}]}, "commit_link": "github.com/edmundoa/graylog2-server/commit/a88cae99955cd0ccdd5d99a1c6d506029eb15c60", "file_name": "streamrules.js", "vul_type": "cwe-079", "commit_msg": "use .text() not .html() in stream rule editor to prevent DOM XSS\n\nfixes #543", "description": "In JavaScript, toggle the prefix \"not \" in a label within a modal when a checkbox is clicked."}
{"func_name": "(anonymous)", "func_src_before": "    $(\"#extra\").on(\"click\", \"#moveRepeatingGroup\", function() {\n        var repeatingCols = '';\n        $(\"#extra input[type=checkbox]:checked\").each(function() {\n            repeatingCols += $(this).val() + ', ';\n        });\n\n        if (repeatingCols !== '') {\n            var newColName = $(\"#extra input[type=checkbox]:checked:first\").val();\n            repeatingCols = repeatingCols.slice(0, -2);\n            var confirmStr = PMA_sprintf(PMA_messages.strMoveRepeatingGroup, escapeHtml(repeatingCols), escapeHtml(PMA_commonParams.get('table')));\n            confirmStr += '<input type=\"text\" name=\"repeatGroupTable\" placeholder=\"' + PMA_messages.strNewTablePlaceholder + '\"/>' +\n                '( ' + escapeHtml(primary_key.toString()) + ', <input type=\"text\" name=\"repeatGroupColumn\" placeholder=\"' + PMA_messages.strNewColumnPlaceholder + '\" value=\"' + escapeHtml(newColName) + '\">)' +\n                '</ol>';\n            $(\"#newCols\").html(confirmStr);\n            $('.tblFooters').html('<input type=\"submit\" value=\"' + PMA_messages.strCancel + '\" onclick=\"$(\\'#newCols\\').html(\\'\\');$(\\'#extra input[type=checkbox]\\').removeAttr(\\'checked\\')\"/>' +\n                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + repeatingCols + '\\')\"/>');\n        }\n    });", "func_src_after": "    $(\"#extra\").on(\"click\", \"#moveRepeatingGroup\", function() {\n        var repeatingCols = '';\n        $(\"#extra input[type=checkbox]:checked\").each(function() {\n            repeatingCols += $(this).val() + ', ';\n        });\n\n        if (repeatingCols !== '') {\n            var newColName = $(\"#extra input[type=checkbox]:checked:first\").val();\n            repeatingCols = repeatingCols.slice(0, -2);\n            var confirmStr = PMA_sprintf(PMA_messages.strMoveRepeatingGroup, escapeHtml(repeatingCols), escapeHtml(PMA_commonParams.get('table')));\n            confirmStr += '<input type=\"text\" name=\"repeatGroupTable\" placeholder=\"' + PMA_messages.strNewTablePlaceholder + '\"/>' +\n                '( ' + escapeHtml(primary_key.toString()) + ', <input type=\"text\" name=\"repeatGroupColumn\" placeholder=\"' + PMA_messages.strNewColumnPlaceholder + '\" value=\"' + escapeHtml(newColName) + '\">)' +\n                '</ol>';\n            $(\"#newCols\").html(confirmStr);\n            $('.tblFooters').html('<input type=\"submit\" value=\"' + PMA_messages.strCancel + '\" onclick=\"$(\\'#newCols\\').html(\\'\\');$(\\'#extra input[type=checkbox]\\').removeAttr(\\'checked\\')\"/>' +\n                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + escapeJsString(escapeHtml(repeatingCols)) + '\\')\"/>');\n        }\n    });", "line_changes": {"deleted": [{"line_no": 16, "char_start": 1158, "char_end": 1292, "line": "                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + repeatingCols + '\\')\"/>');\n"}], "added": [{"line_no": 16, "char_start": 1158, "char_end": 1320, "line": "                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + escapeJsString(escapeHtml(repeatingCols)) + '\\')\"/>');\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1265, "char_end": 1291, "chars": "escapeJsString(escapeHtml("}, {"char_start": 1304, "char_end": 1306, "chars": "))"}]}, "commit_link": "github.com/Achilles-96/phpmyadmin/commit/f33a42f1da9db943a67bda7d29f7dd91957a8e7e", "file_name": "normalization.js", "vul_type": "cwe-079", "commit_msg": "Fix XSS in normalization.js\n\nSigned-off-by: Madhura Jayaratne <madhura.cj@gmail.com>", "description": "Write a jQuery snippet in JavaScript that handles a click event to process and display selected checkboxes for moving a repeating group of table columns."}
{"func_name": "pr_init", "func_src_before": "function pr_init() {\n\tif( document.getElementById( 'pr_container' ) ) {\n\t\treturn;\n\t}\n\n\tif( document.URL.indexOf( 'action=protect' ) > 0 || document.URL.indexOf( 'action=unprotect' ) > 0 ) {\n\t\treturn;\n\t}\n\tif( document.URL.indexOf( 'action=delete' ) > 0 || document.URL.indexOf( 'action=undelete' ) > 0 ) {\n\t\treturn;\n\t}\n\tif( document.URL.indexOf( 'action=watch' ) > 0 || document.URL.indexOf( 'action=unwatch' ) > 0 ) {\n\t\treturn;\n\t}\n\tif( document.URL.indexOf( 'action=history' ) > 0 ) {\n\t\treturn;\n\t}\n\n\t/* check if external URL is provided */\n\tif( !self.proofreadPageThumbURL ) {\n\t\tvar text = document.getElementById( 'wpTextbox1' );\n\t\tif ( text ) {\n\t\t\tvar proofreadPageIsEdit = true;\n\t\t\tre = /<span class=\"hiddenStructure\" id=\"pageURL\">\\[http:\\/\\/(.*?)\\]<\\/span>/;\n\t\t\tm = re.exec( text.value );\n\t\t\tif( m ) {\n\t\t\t\tself.proofreadPageExternalURL = 'http://' + m[1];\n\t\t\t}\n\t\t} else {\n\t\t\tvar proofreadPageIsEdit = false;\n\t\t\ttext = document.getElementById( 'bodyContent' );\n\t\t\ttry {\n\t\t\t\tvar a = document.getElementById( 'pageURL' );\n\t\t\t\tvar b = a.firstChild;\n\t\t\t\tself.proofreadPageExternalURL = b.getAttribute( 'href' );\n\t\t\t} catch( err ) {\n\t\t\t};\n\t\t}\n\t\t// set to dummy values, not used\n\t\tself.proofreadPageWidth = 400;\n\t\tself.proofreadPageHeight = 400;\n\t}\n\n\tif( !self.proofreadPageThumbURL ) {\n\t\treturn;\n\t}\n\n\tif( self.proofreadpage_setup ) {\n\t\tproofreadpage_setup(\n\t\t\tproofreadPageWidth,\n\t\t\tproofreadPageHeight,\n\t\t\tproofreadPageIsEdit\n\t\t);\n\t} else {\n\t\tpr_setup();\n\t}\n\n\t// add CSS classes to the container div\n\tvar c = document.getElementById( 'pagequality' );\n\tif( c ) {\n\t\tc = c.nextSibling;\n\t\tif( c.className == 'pagetext' ) {\n\t\t\tc.className += ' ' + self.proofreadPageCss;\n\t\t}\n\t}\n}\n\n$(document).ready( pr_init );\n$(document).ready( pr_init_tabs );\n$(document).ready( pr_initzoom );\n\n\n/* Quality buttons */\nself.pr_add_quality = function( form, value ) {\n\tself.proofreadpage_quality = value;\n\tself.proofreadpage_username = proofreadPageUserName;\n\tvar text = '';\n\tswitch( value ) {\n\t\tcase 0:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality0_category' );\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality1_category' );\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality2_category' );\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality3_category' );\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality4_category' );\n\t\t\tbreak;\n\t}\n\tform.elements['wpSummary'].value = '/* ' + text + ' */ ';\n\tform.elements['wpProofreader'].value = self.proofreadpage_username;\n};\n\nfunction pr_add_quality_buttons() {\n\tvar ig = document.getElementById( 'wpWatchthis' );\n\tif( !ig ) {\n\t\tig = document.getElementById( 'wpSummary' );\n\t}\n\tif( !ig ) {\n\t\treturn;\n\t}\n\tvar f = document.createElement( 'span' );\n\tig.parentNode.insertBefore( f, ig.nextSibling.nextSibling.nextSibling );\n\n\tif( !proofreadPageAddButtons ) {\n\t\tf.innerHTML =\n\t\t\t' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + self.proofreadpage_username + '\">' +\n\t\t\t'<input type=\"hidden\" name=\"quality\" value=' + self.proofreadpage_quality + ' >';\n\t\treturn;\n\t}\n\n\tf.innerHTML =\n' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + self.proofreadpage_username + '\">'\n+'<span class=\"quality0\"> <input type=\"radio\" name=\"quality\" value=0 onclick=\"pr_add_quality(this.form,0)\" tabindex=4> </span>'\n+'<span class=\"quality2\"> <input type=\"radio\" name=\"quality\" value=2 onclick=\"pr_add_quality(this.form,2)\" tabindex=4> </span>'\n+'<span class=\"quality1\"> <input type=\"radio\" name=\"quality\" value=1 onclick=\"pr_add_quality(this.form,1)\" tabindex=4> </span>'\n+'<span class=\"quality3\"> <input type=\"radio\" name=\"quality\" value=3 onclick=\"pr_add_quality(this.form,3)\" tabindex=4> </span>'\n+'<span class=\"quality4\"> <input type=\"radio\" name=\"quality\" value=4 onclick=\"pr_add_quality(this.form,4)\" tabindex=4> </span>';\n\tf.innerHTML = f.innerHTML + '&nbsp;' + escapeQuotesHTML( mediaWiki.msg( 'proofreadpage_page_status' ) );\n\n\tif( !( ( self.proofreadpage_quality == 4 ) || ( ( self.proofreadpage_quality == 3 ) && ( self.proofreadpage_username != proofreadPageUserName ) ) ) ) {\n\t\tdocument.editform.quality[4].parentNode.style.cssText = 'display:none';\n\t\tdocument.editform.quality[4].disabled = true;\n\t}\n\tswitch( self.proofreadpage_quality ) {\n\t\tcase 4:\n\t\t\tdocument.editform.quality[4].checked = true;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tdocument.editform.quality[3].checked = true;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tdocument.editform.quality[2].checked = true;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdocument.editform.quality[1].checked = true;\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tdocument.editform.quality[0].checked = true;\n\t\t\tbreak;\n\t}", "func_src_after": "function pr_init() {\n\tif( document.getElementById( 'pr_container' ) ) {\n\t\treturn;\n\t}\n\n\tif( document.URL.indexOf( 'action=protect' ) > 0 || document.URL.indexOf( 'action=unprotect' ) > 0 ) {\n\t\treturn;\n\t}\n\tif( document.URL.indexOf( 'action=delete' ) > 0 || document.URL.indexOf( 'action=undelete' ) > 0 ) {\n\t\treturn;\n\t}\n\tif( document.URL.indexOf( 'action=watch' ) > 0 || document.URL.indexOf( 'action=unwatch' ) > 0 ) {\n\t\treturn;\n\t}\n\tif( document.URL.indexOf( 'action=history' ) > 0 ) {\n\t\treturn;\n\t}\n\n\t/* check if external URL is provided */\n\tif( !self.proofreadPageThumbURL ) {\n\t\tvar text = document.getElementById( 'wpTextbox1' );\n\t\tif ( text ) {\n\t\t\tvar proofreadPageIsEdit = true;\n\t\t\tre = /<span class=\"hiddenStructure\" id=\"pageURL\">\\[http:\\/\\/(.*?)\\]<\\/span>/;\n\t\t\tm = re.exec( text.value );\n\t\t\tif( m ) {\n\t\t\t\tself.proofreadPageExternalURL = 'http://' + m[1];\n\t\t\t}\n\t\t} else {\n\t\t\tvar proofreadPageIsEdit = false;\n\t\t\ttext = document.getElementById( 'bodyContent' );\n\t\t\ttry {\n\t\t\t\tvar a = document.getElementById( 'pageURL' );\n\t\t\t\tvar b = a.firstChild;\n\t\t\t\tself.proofreadPageExternalURL = b.getAttribute( 'href' );\n\t\t\t} catch( err ) {\n\t\t\t};\n\t\t}\n\t\t// set to dummy values, not used\n\t\tself.proofreadPageWidth = 400;\n\t\tself.proofreadPageHeight = 400;\n\t}\n\n\tif( !self.proofreadPageThumbURL ) {\n\t\treturn;\n\t}\n\n\tif( self.proofreadpage_setup ) {\n\t\tproofreadpage_setup(\n\t\t\tproofreadPageWidth,\n\t\t\tproofreadPageHeight,\n\t\t\tproofreadPageIsEdit\n\t\t);\n\t} else {\n\t\tpr_setup();\n\t}\n\n\t// add CSS classes to the container div\n\tvar c = document.getElementById( 'pagequality' );\n\tif( c ) {\n\t\tc = c.nextSibling;\n\t\tif( c.className == 'pagetext' ) {\n\t\t\tc.className += ' ' + self.proofreadPageCss;\n\t\t}\n\t}\n}\n\n$(document).ready( pr_init );\n$(document).ready( pr_init_tabs );\n$(document).ready( pr_initzoom );\n\n\n/* Quality buttons */\nself.pr_add_quality = function( form, value ) {\n\tself.proofreadpage_quality = value;\n\tself.proofreadpage_username = proofreadPageUserName;\n\tvar text = '';\n\tswitch( value ) {\n\t\tcase 0:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality0_category' );\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality1_category' );\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality2_category' );\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality3_category' );\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\ttext = mediaWiki.msg( 'proofreadpage_quality4_category' );\n\t\t\tbreak;\n\t}\n\tform.elements['wpSummary'].value = '/* ' + text + ' */ ';\n\tform.elements['wpProofreader'].value = self.proofreadpage_username;\n};\n\nfunction pr_add_quality_buttons() {\n\tvar ig = document.getElementById( 'wpWatchthis' );\n\tif( !ig ) {\n\t\tig = document.getElementById( 'wpSummary' );\n\t}\n\tif( !ig ) {\n\t\treturn;\n\t}\n\tvar f = document.createElement( 'span' );\n\tig.parentNode.insertBefore( f, ig.nextSibling.nextSibling.nextSibling );\n\n\tif( !proofreadPageAddButtons ) {\n\t\tf.innerHTML =\n\t\t\t' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + escapeQuotesHTML( self.proofreadpage_username ) + '\">' +\n\t\t\t'<input type=\"hidden\" name=\"quality\" value=\"' + escapeQuotesHTML( self.proofreadpage_quality ) + '\" >';\n\t\treturn;\n\t}\n\n\tf.innerHTML =\n' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + escapeQuotesHTML( self.proofreadpage_username ) + '\">'\n+'<span class=\"quality0\"> <input type=\"radio\" name=\"quality\" value=0 onclick=\"pr_add_quality(this.form,0)\" tabindex=4> </span>'\n+'<span class=\"quality2\"> <input type=\"radio\" name=\"quality\" value=2 onclick=\"pr_add_quality(this.form,2)\" tabindex=4> </span>'\n+'<span class=\"quality1\"> <input type=\"radio\" name=\"quality\" value=1 onclick=\"pr_add_quality(this.form,1)\" tabindex=4> </span>'\n+'<span class=\"quality3\"> <input type=\"radio\" name=\"quality\" value=3 onclick=\"pr_add_quality(this.form,3)\" tabindex=4> </span>'\n+'<span class=\"quality4\"> <input type=\"radio\" name=\"quality\" value=4 onclick=\"pr_add_quality(this.form,4)\" tabindex=4> </span>';\n\tf.innerHTML = f.innerHTML + '&nbsp;' + escapeQuotesHTML( mediaWiki.msg( 'proofreadpage_page_status' ) );\n\n\tif( !( ( self.proofreadpage_quality == 4 ) || ( ( self.proofreadpage_quality == 3 ) && ( self.proofreadpage_username != proofreadPageUserName ) ) ) ) {\n\t\tdocument.editform.quality[4].parentNode.style.cssText = 'display:none';\n\t\tdocument.editform.quality[4].disabled = true;\n\t}\n\tswitch( self.proofreadpage_quality ) {\n\t\tcase 4:\n\t\t\tdocument.editform.quality[4].checked = true;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tdocument.editform.quality[3].checked = true;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tdocument.editform.quality[2].checked = true;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdocument.editform.quality[1].checked = true;\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tdocument.editform.quality[0].checked = true;\n\t\t\tbreak;\n\t}", "line_changes": {"deleted": [{"line_no": 112, "char_start": 2862, "char_end": 2957, "line": "\t\t\t' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + self.proofreadpage_username + '\">' +\n"}, {"line_no": 113, "char_start": 2957, "char_end": 3042, "line": "\t\t\t'<input type=\"hidden\" name=\"quality\" value=' + self.proofreadpage_quality + ' >';\n"}, {"line_no": 118, "char_start": 3071, "char_end": 3161, "line": "' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + self.proofreadpage_username + '\">'\n"}], "added": [{"line_no": 112, "char_start": 2862, "char_end": 2977, "line": "\t\t\t' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + escapeQuotesHTML( self.proofreadpage_username ) + '\">' +\n"}, {"line_no": 113, "char_start": 2977, "char_end": 3084, "line": "\t\t\t'<input type=\"hidden\" name=\"quality\" value=\"' + escapeQuotesHTML( self.proofreadpage_quality ) + '\" >';\n"}, {"line_no": 118, "char_start": 3113, "char_end": 3223, "line": "' <input type=\"hidden\" name=\"wpProofreader\" value=\"' + escapeQuotesHTML( self.proofreadpage_username ) + '\">'\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 2919, "char_end": 2937, "chars": " escapeQuotesHTML("}, {"char_start": 2965, "char_end": 2967, "chars": " )"}, {"char_start": 3023, "char_end": 3024, "chars": "\""}, {"char_start": 3027, "char_end": 3045, "chars": " escapeQuotesHTML("}, {"char_start": 3072, "char_end": 3074, "chars": " )"}, {"char_start": 3078, "char_end": 3079, "chars": "\""}, {"char_start": 3167, "char_end": 3185, "chars": " escapeQuotesHTML("}, {"char_start": 3213, "char_end": 3215, "chars": " )"}]}, "commit_link": "github.com/wikimedia/mediawiki-extensions-ProofreadPage/commit/708bec1ccb45895fe3e6e15d9df454d44f9966f3", "file_name": "proofread.js", "vul_type": "cwe-079", "commit_msg": "ProofreadPage: Fix stored XSS in edit form. Report and patch by Bawolff", "description": "Write JavaScript code to initialize a page and add quality control buttons based on certain conditions."}
{"func_name": "updateDataEntry", "func_src_before": "  function updateDataEntry($box, forceUpdate) {\n    var $input = $box;\n    var $parent = $input.parents(\".table_entry\");\n    if(!$box.hasClass('grading_value')) {\n      $input = $box.find(\".grading_value\");\n    }\n    var val = $input.val();\n    var sendVal = val;\n    var oldVal = $.trim($parent.find(\".grade\").text());\n    if($parent.find(\".grade img\").length > 0) {\n      oldVal = $parent.find(\".grade img\").attr('alt').toLowerCase();\n    }\n    if(oldVal == \"-\") {\n      oldVal = \"\";\n    }\n\n    if($input.hasClass('pass_fail')) {\n      if(val == \"pass\" || val == \"fail\") {\n        val = $(\"#submission_entry_\" + val + \"_image\").clone().attr('id', '');\n      }\n    }\n    var data = {};\n    var formData = $update_submission_form.getFormData();\n    var submission = objectData($parent.parent());\n    data.id = submission.id || \"\";\n    data.assignment_id = submission.assignment_id;\n    data.student_id = submission.user_id;\n    data.grade = sendVal;\n    if(sendVal != oldVal || (sendVal && forceUpdate)) {\n      submitDataEntry(data);\n    }\n    if(!val || val == \"\") {\n      data.submission_type = submission.submission_type || \"\";\n      val = emptySubmissionText(data);\n    }\n    $parent.find(\".grade\").show().empty().append(val);\n  }", "func_src_after": "  function updateDataEntry($box, forceUpdate) {\n    var $input = $box;\n    var $parent = $input.parents(\".table_entry\");\n    if(!$box.hasClass('grading_value')) {\n      $input = $box.find(\".grading_value\");\n    }\n    var val = $input.val();\n    var sendVal = val;\n    var oldVal = $.trim($parent.find(\".grade\").text());\n    if($parent.find(\".grade img\").length > 0) {\n      oldVal = $parent.find(\".grade img\").attr('alt').toLowerCase();\n    }\n    if(oldVal == \"-\") {\n      oldVal = \"\";\n    }\n\n    if($input.hasClass('pass_fail')) {\n      if(val == \"pass\" || val == \"fail\") {\n        val = $(\"#submission_entry_\" + val + \"_image\").clone().attr('id', '');\n      }\n    }\n    var data = {};\n    var formData = $update_submission_form.getFormData();\n    var submission = objectData($parent.parent());\n    data.id = submission.id || \"\";\n    data.assignment_id = submission.assignment_id;\n    data.student_id = submission.user_id;\n    data.grade = sendVal;\n    if(sendVal != oldVal || (sendVal && forceUpdate)) {\n      submitDataEntry(data);\n    }\n    if(!val || val == \"\") {\n      data.submission_type = submission.submission_type || \"\";\n      val = emptySubmissionText(data);\n    }\n    $parent.find(\".grade\").show().empty().append(htmlEscape(val));\n  }", "line_changes": {"deleted": [{"line_no": 36, "char_start": 1177, "char_end": 1232, "line": "    $parent.find(\".grade\").show().empty().append(val);\n"}], "added": [{"line_no": 36, "char_start": 1177, "char_end": 1244, "line": "    $parent.find(\".grade\").show().empty().append(htmlEscape(val));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1226, "char_end": 1237, "chars": "htmlEscape("}, {"char_start": 1241, "char_end": 1242, "chars": ")"}]}, "commit_link": "github.com/djbender/canvas-lms/commit/a8d2ef69b7138d2197a4641933a8a036aa910e4b", "file_name": "gradebooks.js", "vul_type": "cwe-079", "commit_msg": "gradebook1: escape html in scores\n\nprevent xss when inputting scores in gradebook 1.\n\ntest plan:\n  - as a teacher, enter a grade in gradebook1 for the following grading\n    types:\n    - points\n    - percent\n    - letter grade\n  with the text:\n    \"><img src=/ onerror=alert(document.cookie);>\n  - make sure you don't see an alert for all of these file types.\n\nfixes CNVS-5381\n\nChange-Id: I27d102b83dce5f510f486e30613a1685aa11f2be\nReviewed-on: https://gerrit.instructure.com/19724\nReviewed-by: Simon Williams <simon@instructure.com>\nQA-Review: Amber Taniuchi <amber@instructure.com>\nTested-by: Jenkins <jenkins@instructure.com>\nProduct-Review: Stanley Stuart <stanley@instructure.com>", "description": "Write a JavaScript function to update a data entry in a table, handling both text and image-based grades."}
{"func_name": "setValidity", "func_src_before": "function setValidity(text = '') {\n\terrorMessage.innerHTML = text;\n\tregexField.setCustomValidity(text); /* Triggers :invalid */\n}", "func_src_after": "function setValidity(text = '') {\n\terrorMessage.innerHTML = text;\n\tregexField.setCustomValidity(errorMessage.textContent); /* Triggers :invalid */\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 66, "char_end": 127, "line": "\tregexField.setCustomValidity(text); /* Triggers :invalid */\n"}], "added": [{"line_no": 3, "char_start": 66, "char_end": 147, "line": "\tregexField.setCustomValidity(errorMessage.textContent); /* Triggers :invalid */\n"}]}, "char_changes": {"deleted": [{"char_start": 96, "char_end": 99, "chars": "tex"}], "added": [{"char_start": 96, "char_end": 119, "chars": "errorMessage.textConten"}]}, "commit_link": "github.com/sindresorhus/github-hide-files/commit/9de0c57df81db1178e0e79431d462f6d9842742e", "file_name": "options.js", "vul_type": "cwe-079", "commit_msg": "Avoid self-XSS (#73)", "description": "Write a JavaScript function named `setValidity` that takes an optional string and updates the content of `errorMessage` and the validity state of `regexField`."}
{"func_name": "searchAll", "func_src_before": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "func_src_after": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n    searchStr = escape(searchStr);\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "line_changes": {"deleted": [], "added": [{"line_no": 5, "char_start": 189, "char_end": 224, "line": "    searchStr = escape(searchStr);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 188, "char_end": 223, "chars": "\n    searchStr = escape(searchStr);"}]}, "commit_link": "github.com/lrytz/scala/commit/ee2719585e40cb4e9e523e20061a6a2075f4d49d", "file_name": "index.js", "vul_type": "cwe-079", "commit_msg": "fix XSS vulnerability in scaladoc search\n\nto trigger XSS vuln, simply paste this into the search bar:\n```\n\"\\><img/src='1'onerror=alert(777111)>{{7*7}}\n```\n\nall credit for finding the vulnerability goes to *Yeasir Arafat* <skylinearafat@gmail.com>", "description": "Write a JavaScript function named `searchAll` that handles a search input, updates the browser's URL, and displays search results for members and entities."}
{"func_name": "init", "func_src_before": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = document.location.href;\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "func_src_after": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = escapePath(document.location.href);\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "line_changes": {"deleted": [{"line_no": 172, "char_start": 10731, "char_end": 10761, "line": "\t\th = document.location.href;\n"}], "added": [{"line_no": 172, "char_start": 10731, "char_end": 10773, "line": "\t\th = escapePath(document.location.href);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 10737, "char_end": 10748, "chars": "escapePath("}, {"char_start": 10770, "char_end": 10771, "chars": ")"}]}, "commit_link": "github.com/jaffa-projects/jaffa-framework/commit/f9241bf1b4e4f06fc9778b2314664b58f4fbe309", "file_name": "tiny_mce_src.js", "vul_type": "cwe-079", "commit_msg": "Coverity CWE79 (DOM XSS) new TinyMCE vulnerability fix", "description": "Write a JavaScript function to initialize a WYSIWYG editor with given settings."}
{"func_name": "(anonymous)", "func_src_before": "    $(document.body).on(\"keyup change\", \".sr-input\", function(foo) {\n        value = $(this).val();\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n\n        if (value != undefined && value != \"\") {\n            // Selectbox options can have a custom replace string.\n            s = $(\"option:selected\", this);\n            if (s != undefined && s.attr(\"data-reflect-string\") != undefined && s.attr(\"data-reflect-string\") != \"\") {\n                value = s.attr(\"data-reflect-string\");\n\n                // Inverted?\n                if ($(\"#sr-inverted\", modalBody).is(':checked')) {\n                    value = \"not \" + value;\n                }\n            }\n        } else {\n            value = $(this).attr(\"placeholder\");\n        }\n\n        $($(this).attr(\"data-reflect\"), modalBody).html(value);\n    });", "func_src_after": "    $(document.body).on(\"keyup change\", \".sr-input\", function(foo) {\n        value = $(this).val();\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n\n        if (value != undefined && value != \"\") {\n            // Selectbox options can have a custom replace string.\n            s = $(\"option:selected\", this);\n            if (s != undefined && s.attr(\"data-reflect-string\") != undefined && s.attr(\"data-reflect-string\") != \"\") {\n                value = s.attr(\"data-reflect-string\");\n\n                // Inverted?\n                if ($(\"#sr-inverted\", modalBody).is(':checked')) {\n                    value = \"not \" + value;\n                }\n            }\n        } else {\n            value = $(this).attr(\"placeholder\");\n        }\n\n        $($(this).attr(\"data-reflect\"), modalBody).text(value);\n    });", "line_changes": {"deleted": [{"line_no": 20, "char_start": 770, "char_end": 834, "line": "        $($(this).attr(\"data-reflect\"), modalBody).html(value);\n"}], "added": [{"line_no": 20, "char_start": 770, "char_end": 834, "line": "        $($(this).attr(\"data-reflect\"), modalBody).text(value);\n"}]}, "char_changes": {"deleted": [{"char_start": 821, "char_end": 825, "chars": "html"}], "added": [{"char_start": 821, "char_end": 825, "chars": "text"}]}, "commit_link": "github.com/edmundoa/graylog2-server/commit/a88cae99955cd0ccdd5d99a1c6d506029eb15c60", "file_name": "streamrules.js", "vul_type": "cwe-079", "commit_msg": "use .text() not .html() in stream rule editor to prevent DOM XSS\n\nfixes #543", "description": "In JavaScript, write a jQuery event handler that updates text in a modal based on user input and selection changes, with special handling for an \"inverted\" checkbox state."}
{"func_name": "(anonymous)", "func_src_before": "      idx.search(query, function(success, content) {\n        if (!success) {\n          console.log(content);\n          return;\n        }\n        if (originalQuery == $('#inputfield input').val().trim()) {\n          if (content.nbHits == 0) {\n            var noResults = '<p>No results matching your query</p><p><code>' + originalQuery + '</code>';\n            if (item_type) {\n              noResults += ' + <code>type=' + item_type.replace(/_/g, ' ') + '</code>';\n            }\n            if (created_at) {\n              noResults += ' + <code>when=' + created_at.replace(/_/g, ' ') + '</code>';\n            }\n            noResults += '</p>';\n            self.$noresults.html(noResults);\n            self.$noresults.show();\n          } else {\n            self.$noresults.hide();\n          }\n          self.searchCallback(content);\n        }\n      }, searchParams);", "func_src_after": "      idx.search(query, function(success, content) {\n        if (!success) {\n          console.log(content);\n          return;\n        }\n        if (originalQuery == $('#inputfield input').val().trim()) {\n          if (content.nbHits == 0) {\n            var noResults = '<p>No results matching your query</p><p><code>' + $('<div />').text(originalQuery).html() + '</code>';\n            if (item_type) {\n              noResults += ' + <code>type=' + $('<div />').text(item_type).html().replace(/_/g, ' ') + '</code>';\n            }\n            if (created_at) {\n              noResults += ' + <code>when=' + $('<div />').text(created_at).html().replace(/_/g, ' ') + '</code>';\n            }\n            noResults += '</p>';\n            self.$noresults.html(noResults);\n            self.$noresults.show();\n          } else {\n            self.$noresults.hide();\n          }\n          self.searchCallback(content);\n        }\n      }, searchParams);", "line_changes": {"deleted": [{"line_no": 8, "char_start": 242, "char_end": 348, "line": "            var noResults = '<p>No results matching your query</p><p><code>' + originalQuery + '</code>';\n"}, {"line_no": 10, "char_start": 377, "char_end": 465, "line": "              noResults += ' + <code>type=' + item_type.replace(/_/g, ' ') + '</code>';\n"}, {"line_no": 13, "char_start": 509, "char_end": 598, "line": "              noResults += ' + <code>when=' + created_at.replace(/_/g, ' ') + '</code>';\n"}], "added": [{"line_no": 8, "char_start": 242, "char_end": 374, "line": "            var noResults = '<p>No results matching your query</p><p><code>' + $('<div />').text(originalQuery).html() + '</code>';\n"}, {"line_no": 10, "char_start": 403, "char_end": 517, "line": "              noResults += ' + <code>type=' + $('<div />').text(item_type).html().replace(/_/g, ' ') + '</code>';\n"}, {"line_no": 13, "char_start": 561, "char_end": 676, "line": "              noResults += ' + <code>when=' + $('<div />').text(created_at).html().replace(/_/g, ' ') + '</code>';\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 321, "char_end": 339, "chars": "$('<div />').text("}, {"char_start": 352, "char_end": 360, "chars": ").html()"}, {"char_start": 449, "char_end": 467, "chars": "$('<div />').text("}, {"char_start": 476, "char_end": 484, "chars": ").html()"}, {"char_start": 607, "char_end": 625, "chars": "$('<div />').text("}, {"char_start": 635, "char_end": 643, "chars": ").html()"}]}, "commit_link": "github.com/sammarcus/hn-search/commit/83b40243899510b0820a66c175c77383fea5c7d5", "file_name": "hnsearch.js", "vul_type": "cwe-079", "commit_msg": "Fixed XSS :)", "description": "Write a JavaScript function that performs a search query and handles the results by displaying a custom no-results message or hiding the no-results element."}
{"func_name": "(anonymous)", "func_src_before": "        $('#modalExport .btn-primary').on('click', function (e) {\n            e.preventDefault();\n            var elBtn = $(this),\n                bid = $('#modalExport input[name=exportGroupBy]:checked').val(),\n                url = listId + '/export' + (bid ? '/' + bid : '');\n\n            elBtn.attr('href', url);\n\n            handleDownloadBtnClick(elBtn);\n            $('#modalExport').modal('hide');\n        });", "func_src_after": "        $('#modalExport .btn-primary').on('click', function (e) {\n            e.preventDefault();\n            var elBtn = $(this),\n                bid = $('#modalExport input[name=exportGroupBy]:checked').val(),\n                url = listId + '/export' + (bid ? '/' + bid : '');\n\n            elBtn.attr('href', eHtml(url));\n\n            handleDownloadBtnClick(elBtn);\n            $('#modalExport').modal('hide');\n        });", "line_changes": {"deleted": [{"line_no": 7, "char_start": 280, "char_end": 317, "line": "            elBtn.attr('href', url);\n"}], "added": [{"line_no": 7, "char_start": 280, "char_end": 324, "line": "            elBtn.attr('href', eHtml(url));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 311, "char_end": 317, "chars": "eHtml("}, {"char_start": 321, "char_end": 322, "chars": ")"}]}, "commit_link": "github.com/theoboldt/juvem/commit/9af3a9f58dc11dd7cacae93f00fbc5ed2e3580da", "file_name": "attendance.js", "vul_type": "cwe-079", "commit_msg": "Preventing js/xss-through-dom vulnerability", "description": "Create a JavaScript function to handle a button click by setting the button's href attribute based on a selected input value and then trigger a download action."}
{"func_name": "String.prototype.strip_tags", "func_src_before": "String.prototype.strip_tags = function(){\n\ttags = this;\n\tstripped = tags.replace(/[\\<\\>]/gi, \"\");\n\treturn stripped;\n};", "func_src_after": "String.prototype.strip_tags = function(){\n\ttags = this;\n\tstripped = tags.replace(/<(.|\\n)*?>/g, '');\n\treturn stripped;\n};", "line_changes": {"deleted": [{"line_no": 3, "char_start": 56, "char_end": 98, "line": "\tstripped = tags.replace(/[\\<\\>]/gi, \"\");\n"}], "added": [{"line_no": 3, "char_start": 56, "char_end": 101, "line": "\tstripped = tags.replace(/<(.|\\n)*?>/g, '');\n"}]}, "char_changes": {"deleted": [{"char_start": 82, "char_end": 88, "chars": "[\\<\\>]"}, {"char_start": 90, "char_end": 91, "chars": "i"}, {"char_start": 93, "char_end": 95, "chars": "\"\""}], "added": [{"char_start": 82, "char_end": 92, "chars": "<(.|\\n)*?>"}, {"char_start": 96, "char_end": 98, "chars": "''"}]}, "commit_link": "github.com/whitekiba/server/commit/cf113409adf82d0834181dbdf4586fd2ad262898", "file_name": "contacts.js", "vul_type": "cwe-079", "commit_msg": "Contacts: Fix XSS.", "description": "Create a JavaScript function that extends the String prototype to remove HTML tags from a string."}
{"func_name": "error", "func_src_before": "                error: function() {\n                    // Fail message\n                    $('#success').html(\"<div class='alert alert-danger'>\");\n                    $('#success > .alert-danger').html(\"<button type='button' class='close' data-dismiss='alert' aria-hidden='true'>&times;\")\n                        .append(\"</button>\");\n                    $('#success > .alert-danger').append(\"<strong>Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\");\n                    $('#success > .alert-danger').append('</div>');\n                    //clear all fields\n                    $('#contactForm').trigger(\"reset\");\n                },", "func_src_after": "                error: function() {\n                    // Fail message\n                    $('#success').html(\"<div class='alert alert-danger'>\");\n                    $('#success > .alert-danger').html(\"<button type='button' class='close' data-dismiss='alert' aria-hidden='true'>&times;\")\n                        .append(\"</button>\");\n                    $('#success > .alert-danger').append($(\"<strong>\").text(\"Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\"));\n                    $('#success > .alert-danger').append('</div>');\n                    //clear all fields\n                    $('#contactForm').trigger(\"reset\");\n                },", "line_changes": {"deleted": [{"line_no": 6, "char_start": 336, "char_end": 502, "line": "                    $('#success > .alert-danger').append(\"<strong>Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\");\n"}], "added": [{"line_no": 6, "char_start": 336, "char_end": 514, "line": "                    $('#success > .alert-danger').append($(\"<strong>\").text(\"Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\"));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 393, "char_end": 395, "chars": "$("}, {"char_start": 404, "char_end": 413, "chars": "\").text(\""}, {"char_start": 511, "char_end": 512, "chars": ")"}]}, "commit_link": "github.com/EmmavanKampen/What-sgood/commit/0d233641de67563a42ad58925dd6da7483062637", "file_name": "contact_me.js", "vul_type": "cwe-079", "commit_msg": "Fix xss issue", "description": "Write a JavaScript function to display an error message and reset a form when an AJAX request fails."}
{"func_name": "$.fn.badge", "func_src_before": "\t$.fn.badge = function ( text, inline ) {\n\t\tvar div, $badge = this.find( '.mw-badge' );\n\n\t\tif ( text ) {\n\t\t\t// If a badge already exists, reuse it\n\t\t\tif ( $badge.length ) {\n\t\t\t\t$badge.find( '.mw-badge-content' ).text( text );\n\t\t\t} else {\n\t\t\t\t// Otherwise, create a new badge with the specified text and style\n\t\t\t\tdiv = document.createElement( 'div' );\n\t\t\t\tdiv.className = 'mw-badge mw-badge-' + ( inline ? 'inline' : 'overlay' );\n\t\t\t\tdiv.innerHTML = '<span class=\"mw-badge-content\">' + text + '</span>';\n\t\t\t\t$( div ).appendTo( this );\n\t\t\t}\n\t\t} else {\n\t\t\t$badge.remove();\n\t\t}\n\t\treturn this;\n\t};", "func_src_after": "\t$.fn.badge = function ( text, inline ) {\n\t\tvar div, $badge = this.find( '.mw-badge' );\n\n\t\tif ( text ) {\n\t\t\t// If a badge already exists, reuse it\n\t\t\tif ( $badge.length ) {\n\t\t\t\t$badge.find( '.mw-badge-content' ).text( text );\n\t\t\t} else {\n\t\t\t\t// Otherwise, create a new badge with the specified text and style\n\t\t\t\t$badge = $( '<div class=\"mw-badge mw-badge-' + ( inline ? 'inline' : 'overlay' ) + '\"></div>' )\n\t\t\t\t\t.append( $( '<span class=\"mw-badge-content\"></span>' ).text ( text ) )\n\t\t\t\t\t.appendTo( this );\n\t\t\t}\n\t\t} else {\n\t\t\t$badge.remove();\n\t\t}\n\t\treturn this;\n\t};", "line_changes": {"deleted": [{"line_no": 10, "char_start": 309, "char_end": 352, "line": "\t\t\t\tdiv = document.createElement( 'div' );\n"}, {"line_no": 11, "char_start": 352, "char_end": 430, "line": "\t\t\t\tdiv.className = 'mw-badge mw-badge-' + ( inline ? 'inline' : 'overlay' );\n"}, {"line_no": 12, "char_start": 430, "char_end": 504, "line": "\t\t\t\tdiv.innerHTML = '<span class=\"mw-badge-content\">' + text + '</span>';\n"}, {"line_no": 13, "char_start": 504, "char_end": 535, "line": "\t\t\t\t$( div ).appendTo( this );\n"}], "added": [{"line_no": 10, "char_start": 309, "char_end": 409, "line": "\t\t\t\t$badge = $( '<div class=\"mw-badge mw-badge-' + ( inline ? 'inline' : 'overlay' ) + '\"></div>' )\n"}, {"line_no": 11, "char_start": 409, "char_end": 485, "line": "\t\t\t\t\t.append( $( '<span class=\"mw-badge-content\"></span>' ).text ( text ) )\n"}, {"line_no": 12, "char_start": 485, "char_end": 509, "line": "\t\t\t\t\t.appendTo( this );\n"}]}, "char_changes": {"deleted": [{"char_start": 313, "char_end": 341, "chars": "div = document.createElement"}, {"char_start": 347, "char_end": 373, "chars": "' );\n\t\t\t\tdiv.className = '"}, {"char_start": 428, "char_end": 516, "chars": ";\n\t\t\t\tdiv.innerHTML = '<span class=\"mw-badge-content\">' + text + '</span>';\n\t\t\t\t$( div )"}], "added": [{"char_start": 313, "char_end": 323, "chars": "$badge = $"}, {"char_start": 326, "char_end": 327, "chars": "<"}, {"char_start": 330, "char_end": 338, "chars": " class=\""}, {"char_start": 393, "char_end": 490, "chars": " + '\"></div>' )\n\t\t\t\t\t.append( $( '<span class=\"mw-badge-content\"></span>' ).text ( text ) )\n\t\t\t\t\t"}]}, "commit_link": "github.com/PJosepherum/mediawiki/commit/f58e2d45b8358bc904fc83f53833fbf2aebeb7a1", "file_name": "jquery.badge.js", "vul_type": "cwe-079", "commit_msg": "Sanitize text input to $.fn.badge\n\nCloses a potential XSS vector, as pointed out by Krinkle in\n32091.\n\nChange-Id: Iea702fb8736799dc7f8238e4cb357da22304c1dd", "description": "Write a jQuery plugin in JavaScript that toggles a badge with text on an element, with an option for inline or overlay style."}
{"func_name": "(anonymous)", "func_src_before": "                $(document.body).on('click', '.modal-trigger-inline', function (e) {\n                    e.preventDefault();\n                    var modalElement = $('#modal-inline');\n                    var eventTrigger = $(this);\n\n                    if (eventTrigger.attr('title')) {\n                        $('.modal-title', modalElement).html(eventTrigger.attr('title'));\n                    }\n\n                    $('.modal-body', modalElement).html('').show().load(eventTrigger.attr('href'), function () {\n                        setTimeout(CMS.attach.formEnhancements, 200);\n                    });\n\n                    modalElement.modal();\n                });", "func_src_after": "                $(document.body).on('click', '.modal-trigger-inline', function (e) {\n                    e.preventDefault();\n                    var modalElement = $('#modal-inline');\n                    var eventTrigger = $(this);\n\n                    if (eventTrigger.attr('title')) {\n                        $('.modal-title', modalElement).text(eventTrigger.attr('title'));\n                    }\n\n                    $('.modal-body', modalElement).html('').show().load(eventTrigger.attr('href'), function () {\n                        setTimeout(CMS.attach.formEnhancements, 200);\n                    });\n\n                    modalElement.modal();\n                });", "line_changes": {"deleted": [{"line_no": 7, "char_start": 287, "char_end": 377, "line": "                        $('.modal-title', modalElement).html(eventTrigger.attr('title'));\n"}], "added": [{"line_no": 7, "char_start": 287, "char_end": 377, "line": "                        $('.modal-title', modalElement).text(eventTrigger.attr('title'));\n"}]}, "char_changes": {"deleted": [{"char_start": 343, "char_end": 347, "chars": "html"}], "added": [{"char_start": 343, "char_end": 347, "chars": "text"}]}, "commit_link": "github.com/WeAreAthlon/silla.io/commit/5a251701f8dc73d1cd9a421d31295edef1faa9b4", "file_name": "cms.silla.js", "vul_type": "cwe-079", "commit_msg": "Prevent XSS in resource titles set as Modal titles", "description": "Create a jQuery script that opens a modal with content loaded from the clicked element's href attribute and optionally sets the modal's title."}
{"func_name": "newMsg", "func_src_before": "    var newMsg = function (msgData) {\n      var msgType   = (msgData.reciever == 'status' ? 'status' : 'channel');\n\n      var tab       = $('.tab[title=\"'+msgData.receiver.toLowerCase()+'\"]');\n      var tabView   = getTabView(tab.attr('title'));\n      var newLine   = $('<div>').addClass('line ' + msgType);\n      var actualMsg = $('<span>');\n\n\n      var timestamp = $(\"<span>\").addClass('timestamp').text(currentTime());\n      newLine.append(timestamp);\n\n      var actionMatch = msgData.message.match(/\\u0001ACTION (.*)\\u0001/);\n\n      if (msgType == 'channel' && msgData.from !== undefined) {\n        if (!tab.hasClass('active')) {\n          tab.addClass('new-msgs');\n        }\n\n        var msgFrom = $('<span>').addClass(actionMatch ? '' : 'from').text(msgData.from);\n        var mentionRegex = new RegExp(\"(^|[^a-zA-Z0-9\\\\[\\\\]{}\\\\^`|])\" + options.nickname + \"([^a-zA-Z0-9\\\\[\\\\]{}\\\\^`|]|$)\", 'i');\n        var containsMention = msgData.message.match(mentionRegex);\n\n        if (actionMatch) {\n          msgFrom.prepend('* ').append(' ');\n        } else {\n          msgFrom.append(': ');\n        }\n\n        if (msgData.fromYou) {\n          msgFrom.addClass('from-you');\n        }\n        else if (containsMention) {\n          //window.hasFocus is set by me in document-dot-ready.js\n          var tabNotFocused = !(document.hasFocus() && window.hasFocus && tab.hasClass('active'));\n          newLine.addClass('mentioned'); //for highlighting\n          // if either the user is in another browser tab/app, or if the user is in a diff irc channel\n          if (tabNotFocused) { //bring on the webkit notification\n            var notification = newNotification(msgData.message, msgData.receiver, \"/images/nirc32.png\");\n            if (notification) { //in case they haven't authorized, the above will return nothin'\n              notification.onclick = function() {\n                window.focus(); //takes user to the browser tab\n                focusTab(tab); //focuses the correct channel tab\n                this.cancel(); //closes the notification\n              };\n              notification.show();\n            }\n          }\n        }\n\n        newLine.append(msgFrom);\n      }\n\n      actualMsg.text(actionMatch ? actionMatch[1] : msgData.message);\n      var urlRegex = /\\b((?:https?:\\/\\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))/gi;\n      actualMsg.html(actualMsg.text().replace(urlRegex, \"<a target='_blank' href='$1'>$1</a>\"));\n      newLine.append(actualMsg);\n\n      tabView.append(newLine)\n             .scrollTop(tabView[0].scrollHeight);\n\n      var visibleLines = tabView.find('.line').toArray();\n      while (visibleLines.length > maxLines) {\n        visibleLines[0].remove();\n        visibleLines.shift(); // in case we use this later in the function\n      }\n    }", "func_src_after": "    var newMsg = function (msgData) {\n      var msgType   = (msgData.reciever == 'status' ? 'status' : 'channel');\n\n      var tab       = $('.tab[title=\"'+msgData.receiver.toLowerCase()+'\"]');\n      var tabView   = getTabView(tab.attr('title'));\n      var newLine   = $('<div>').addClass('line ' + msgType);\n      var actualMsg = $('<span>');\n\n\n      var timestamp = $(\"<span>\").addClass('timestamp').text(currentTime());\n      newLine.append(timestamp);\n\n      var actionMatch = msgData.message.match(/\\u0001ACTION (.*)\\u0001/);\n\n      if (msgType == 'channel' && msgData.from !== undefined) {\n        if (!tab.hasClass('active')) {\n          tab.addClass('new-msgs');\n        }\n\n        var msgFrom = $('<span>').addClass(actionMatch ? '' : 'from').text(msgData.from);\n        var mentionRegex = new RegExp(\"(^|[^a-zA-Z0-9\\\\[\\\\]{}\\\\^`|])\" + options.nickname + \"([^a-zA-Z0-9\\\\[\\\\]{}\\\\^`|]|$)\", 'i');\n        var containsMention = msgData.message.match(mentionRegex);\n\n        if (actionMatch) {\n          msgFrom.prepend('* ').append(' ');\n        } else {\n          msgFrom.append(': ');\n        }\n\n        if (msgData.fromYou) {\n          msgFrom.addClass('from-you');\n        }\n        else if (containsMention) {\n          //window.hasFocus is set by me in document-dot-ready.js\n          var tabNotFocused = !(document.hasFocus() && window.hasFocus && tab.hasClass('active'));\n          newLine.addClass('mentioned'); //for highlighting\n          // if either the user is in another browser tab/app, or if the user is in a diff irc channel\n          if (tabNotFocused) { //bring on the webkit notification\n            var notification = newNotification(msgData.message, msgData.receiver, \"/images/nirc32.png\");\n            if (notification) { //in case they haven't authorized, the above will return nothin'\n              notification.onclick = function() {\n                window.focus(); //takes user to the browser tab\n                focusTab(tab); //focuses the correct channel tab\n                this.cancel(); //closes the notification\n              };\n              notification.show();\n            }\n          }\n        }\n\n        newLine.append(msgFrom);\n      }\n\n      actualMsg.text(actionMatch ? actionMatch[1] : msgData.message);\n      newLine.append(actualMsg);\n\n      tabView.append(newLine)\n             .scrollTop(tabView[0].scrollHeight);\n\n      var visibleLines = tabView.find('.line').toArray();\n      while (visibleLines.length > maxLines) {\n        visibleLines[0].remove();\n        visibleLines.shift(); // in case we use this later in the function\n      }\n    }", "line_changes": {"deleted": [{"line_no": 55, "char_start": 2251, "char_end": 2458, "line": "      var urlRegex = /\\b((?:https?:\\/\\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))/gi;\n"}, {"line_no": 56, "char_start": 2458, "char_end": 2555, "line": "      actualMsg.html(actualMsg.text().replace(urlRegex, \"<a target='_blank' href='$1'>$1</a>\"));\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 2251, "char_end": 2555, "chars": "      var urlRegex = /\\b((?:https?:\\/\\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))/gi;\n      actualMsg.html(actualMsg.text().replace(urlRegex, \"<a target='_blank' href='$1'>$1</a>\"));\n"}], "added": []}, "commit_link": "github.com/cjstewart88/nirc/commit/5a4f9d7345713a26812d2a9e43f628dadd473f53", "file_name": "client.js", "vul_type": "cwe-079", "commit_msg": "prevent xss, sadly this means no link detection, sorry", "description": "In JavaScript, write a function to display a new message in a chat interface, handling different message types and user mentions."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t$('#fn').change(function(){\n\t\t\t\tvar name = $('#fn').val();\n\t\t\t\tvar item = $('#contacts [data-id=\"'+Contacts.UI.Card.id+'\"]');\n\t\t\t\t$(item).find('a').html(name);\n\t\t\t\tvar added = false;\n\t\t\t\t$('#contacts li').each(function(){\n\t\t\t\t\tif ($(this).text().toLowerCase() > name.toLowerCase()) {\n\t\t\t\t\t\t$(this).before(item).fadeIn('fast');\n\t\t\t\t\t\tadded = true;\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\tif(!added) {\n\t\t\t\t\t$('#leftcontent ul').append(item);\n\t\t\t\t}\n\t\t\t\tContacts.UI.Contacts.scrollTo(Contacts.UI.Card.id);\n\t\t\t});", "func_src_after": "\t\t\t$('#fn').change(function(){\n\t\t\t\tvar name = $('#fn').val().strip_tags();\n\t\t\t\tvar item = $('#contacts [data-id=\"'+Contacts.UI.Card.id+'\"]');\n\t\t\t\t$(item).find('a').html(name);\n\t\t\t\tvar added = false;\n\t\t\t\t$('#contacts li').each(function(){\n\t\t\t\t\tif ($(this).text().toLowerCase() > name.toLowerCase()) {\n\t\t\t\t\t\t$(this).before(item).fadeIn('fast');\n\t\t\t\t\t\tadded = true;\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\tif(!added) {\n\t\t\t\t\t$('#leftcontent ul').append(item);\n\t\t\t\t}\n\t\t\t\tContacts.UI.Contacts.scrollTo(Contacts.UI.Card.id);\n\t\t\t});", "line_changes": {"deleted": [{"line_no": 2, "char_start": 31, "char_end": 62, "line": "\t\t\t\tvar name = $('#fn').val();\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 75, "line": "\t\t\t\tvar name = $('#fn').val().strip_tags();\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 60, "char_end": 73, "chars": ".strip_tags()"}]}, "commit_link": "github.com/whitekiba/server/commit/cf113409adf82d0834181dbdf4586fd2ad262898", "file_name": "contacts.js", "vul_type": "cwe-079", "commit_msg": "Contacts: Fix XSS.", "description": "Write a jQuery script in JavaScript that updates a contact list when a user changes the name in an input field."}
{"func_name": "update", "func_src_before": "function update() {\n\tfor (const line of regexField.value.split('\\n')) {\n\t\t// Don't allow delimiters in RegExp string\n\t\tif (delimiters.test(line)) {\n\t\t\treturn setValidity(`Use <code>${line.replace(/^\\/|\\/$/g, '')}</code> instead of <code>${line}</code>. Slashes are not required.`);\n\t\t}\n\n\t\t// Fully test each RegExp\n\t\ttry {\n\t\t\t// eslint-disable-next-line no-new\n\t\t\tnew RegExp(line);\n\t\t} catch (error) {\n\t\t\treturn setValidity(error.message);\n\t\t}\n\t}\n\n\tsetValidity();\n\tsaveOptions();\n}", "func_src_after": "function update() {\n\tfor (const line of regexField.value.split('\\n')) {\n\t\t// Don't allow delimiters in RegExp string\n\t\tif (delimiters.test(line)) {\n\t\t\treturn setValidity(escapeTag`Use <code>${line.replace(/^\\/|\\/$/g, '')}</code> instead of <code>${line}</code>. Slashes are not required.`);\n\t\t}\n\n\t\t// Fully test each RegExp\n\t\ttry {\n\t\t\t// eslint-disable-next-line no-new\n\t\t\tnew RegExp(line);\n\t\t} catch (error) {\n\t\t\treturn setValidity(error.message);\n\t\t}\n\t}\n\n\tsetValidity();\n\tsaveOptions();\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 148, "char_end": 282, "line": "\t\t\treturn setValidity(`Use <code>${line.replace(/^\\/|\\/$/g, '')}</code> instead of <code>${line}</code>. Slashes are not required.`);\n"}], "added": [{"line_no": 5, "char_start": 148, "char_end": 291, "line": "\t\t\treturn setValidity(escapeTag`Use <code>${line.replace(/^\\/|\\/$/g, '')}</code> instead of <code>${line}</code>. Slashes are not required.`);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 170, "char_end": 179, "chars": "escapeTag"}]}, "commit_link": "github.com/sindresorhus/github-hide-files/commit/9de0c57df81db1178e0e79431d462f6d9842742e", "file_name": "options.js", "vul_type": "cwe-079", "commit_msg": "Avoid self-XSS (#73)", "description": "Create a JavaScript function that validates and updates regular expressions from a text field, excluding delimiter checks and error handling."}
{"func_name": "saveTitle", "func_src_before": "saveTitle: function(cancel) {\n\tif ($(\"#conversationTitle\").hasClass(\"editing\")) {\n\n\t\t// Return the conversation title input back to normal.\n\t\tvar title = $(\"#conversationTitle input\").val();\n\t\tif (!title || cancel) title = ETConversation.title;\n\t\t$(\"#conversationTitle\").html(\"<a href='#'>\"+title+\"</a>\").removeClass(\"editing\");\n\n\t\t// If we're cancelling, that's all we need to do.\n\t\tif (cancel || ETConversation.title == title) return;\n\n\t\t// Otherwise, update the document title with the new title.\n\t\t$(document).attr(\"title\", $(document).attr(\"title\").replace(ETConversation.title, title));\n\t\tETConversation.title = title;\n\n\t\t// And save it.\n\t\t$.ETAjax({\n\t\t\turl: \"conversation/save.json/\" + ETConversation.id,\n\t\t\ttype: \"post\",\n\t\t\tdata: {title: title},\n\t\t\tglobal: true\n\t\t});\n\t}\n},", "func_src_after": "saveTitle: function(cancel) {\n\tif ($(\"#conversationTitle\").hasClass(\"editing\")) {\n\n\t\t// Return the conversation title input back to normal.\n\t\tvar title = $(\"#conversationTitle input\").val();\n\t\tif (!title || cancel) title = ETConversation.title;\n\t\tvar sanitized = $('<div/>').text(title).html();\n\t\t$(\"#conversationTitle\").html(\"<a href='#'>\"+sanitized+\"</a>\").removeClass(\"editing\");\n\n\t\t// If we're cancelling, that's all we need to do.\n\t\tif (cancel || ETConversation.title == title) return;\n\n\t\t// Otherwise, update the document title with the new title.\n\t\t$(document).attr(\"title\", $(document).attr(\"title\").replace(ETConversation.title, title));\n\t\tETConversation.title = title;\n\n\t\t// And save it.\n\t\t$.ETAjax({\n\t\t\turl: \"conversation/save.json/\" + ETConversation.id,\n\t\t\ttype: \"post\",\n\t\t\tdata: {title: title},\n\t\t\tglobal: true\n\t\t});\n\t}\n},", "line_changes": {"deleted": [{"line_no": 7, "char_start": 245, "char_end": 329, "line": "\t\t$(\"#conversationTitle\").html(\"<a href='#'>\"+title+\"</a>\").removeClass(\"editing\");\n"}], "added": [{"line_no": 7, "char_start": 245, "char_end": 295, "line": "\t\tvar sanitized = $('<div/>').text(title).html();\n"}, {"line_no": 8, "char_start": 295, "char_end": 383, "line": "\t\t$(\"#conversationTitle\").html(\"<a href='#'>\"+sanitized+\"</a>\").removeClass(\"editing\");\n"}]}, "char_changes": {"deleted": [{"char_start": 291, "char_end": 296, "chars": "title"}], "added": [{"char_start": 245, "char_end": 295, "chars": "\t\tvar sanitized = $('<div/>').text(title).html();\n"}, {"char_start": 341, "char_end": 350, "chars": "sanitized"}]}, "commit_link": "github.com/davchezt/esoTalk/commit/2fdb392461c5b2086b1db13af9e86a3e3e6c03dd", "file_name": "conversation.js", "vul_type": "cwe-079", "commit_msg": "fix XSS vulnerability with conversation titles. closes #122", "description": "Write a JavaScript function to update and save a conversation title, with an option to cancel the edit."}
{"func_name": "changeValue", "func_src_before": "\tchangeValue:function(val){ \n\t\t$('#akSelectValueStatic_'+val).html( $('#akSelectValueField_'+val).val() );\n\t\tthis.editValue(val)\n\t},", "func_src_after": "\tchangeValue:function(val){ \n\t\tvar txtValue = $('<div/>').text($('#akSelectValueField_'+val).val()).html();\t\t\n\t\t$('#akSelectValueStatic_'+val).html( txtValue );\n\t\tthis.editValue(val)\n\t},", "line_changes": {"deleted": [{"line_no": 2, "char_start": 29, "char_end": 107, "line": "\t\t$('#akSelectValueStatic_'+val).html( $('#akSelectValueField_'+val).val() );\n"}], "added": [{"line_no": 2, "char_start": 29, "char_end": 110, "line": "\t\tvar txtValue = $('<div/>').text($('#akSelectValueField_'+val).val()).html();\t\t\n"}, {"line_no": 3, "char_start": 110, "char_end": 161, "line": "\t\t$('#akSelectValueStatic_'+val).html( txtValue );\n"}]}, "char_changes": {"deleted": [{"char_start": 48, "char_end": 54, "chars": "Static"}, {"char_start": 62, "char_end": 68, "chars": "html( "}, {"char_start": 85, "char_end": 90, "chars": "Field"}, {"char_start": 98, "char_end": 103, "chars": "val()"}], "added": [{"char_start": 31, "char_end": 63, "chars": "var txtValue = $('<div/>').text("}, {"char_start": 80, "char_end": 85, "chars": "Field"}, {"char_start": 93, "char_end": 112, "chars": "val()).html();\t\t\n\t\t"}, {"char_start": 129, "char_end": 135, "chars": "Static"}, {"char_start": 143, "char_end": 157, "chars": "html( txtValue"}]}, "commit_link": "github.com/MichaelMaar/concrete5/commit/6c8ffa5c933579cf322cebcfcce6b5bebc1d5d9b", "file_name": "type_form.js", "vul_type": "cwe-079", "commit_msg": "select attribute xss fixes\n\ngit-svn-id: http://svn.concrete5.org/svn/concrete5@2014 b0551a0c-1e16-4222-a7d5-975db1aca215", "description": "Write a JavaScript function named `changeValue` that updates the HTML content of an element and calls another function to edit the value."}
{"func_name": "(anonymous)", "func_src_before": ".factory('Alert', ['$rootScope', function ($rootScope) {\n    return function (hdr, msg, cls, action, secondAlert, disableButtons, backdrop) {\n        var scope = $rootScope.$new(), alertClass, local_backdrop;\n        if (secondAlert) {\n\n            $('#alertHeader2').html(hdr);\n            $('#alert2-modal-msg').html(msg);\n\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert2-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal2').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n            scope.disableButtons2 = (disableButtons) ? true : false;\n\n            $('#alert-modal2').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal2').on('shown.bs.modal', function () {\n                $('#alert2_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal2').modal('hide');\n                }\n            });\n        } else {\n\n            $('#alertHeader').html(hdr);\n            $('#alert-modal-msg').html(msg);\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n\n            $('#alert-modal').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal').on('shown.bs.modal', function () {\n                $('#alert_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal').modal('hide');\n                }\n            });\n\n            scope.disableButtons = (disableButtons) ? true : false;\n        }\n    };\n}])", "func_src_after": ".factory('Alert', ['$rootScope', '$filter', function ($rootScope, $filter) {\n    return function (hdr, msg, cls, action, secondAlert, disableButtons, backdrop) {\n        var scope = $rootScope.$new(), alertClass, local_backdrop;\n        msg = $filter('sanitize')(msg);\n        if (secondAlert) {\n\n            $('#alertHeader2').html(hdr);\n            $('#alert2-modal-msg').html(msg);\n\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert2-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal2').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n            scope.disableButtons2 = (disableButtons) ? true : false;\n\n            $('#alert-modal2').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal2').on('shown.bs.modal', function () {\n                $('#alert2_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal2').modal('hide');\n                }\n            });\n        } else {\n\n            $('#alertHeader').html(hdr);\n            $('#alert-modal-msg').html(msg);\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n\n            $('#alert-modal').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal').on('shown.bs.modal', function () {\n                $('#alert_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal').modal('hide');\n                }\n            });\n\n            scope.disableButtons = (disableButtons) ? true : false;\n        }\n    };\n}])", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 57, "line": ".factory('Alert', ['$rootScope', function ($rootScope) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 77, "line": ".factory('Alert', ['$rootScope', '$filter', function ($rootScope, $filter) {\n"}, {"line_no": 4, "char_start": 229, "char_end": 269, "line": "        msg = $filter('sanitize')(msg);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 32, "char_end": 43, "chars": " '$filter',"}, {"char_start": 64, "char_end": 73, "chars": ", $filter"}, {"char_start": 229, "char_end": 269, "chars": "        msg = $filter('sanitize')(msg);\n"}]}, "commit_link": "github.com/wwitzel3/awx/commit/b127e7f2765c6173c5cf27d34491e7ca0a4ac101", "file_name": "Utilities.js", "vul_type": "cwe-079", "commit_msg": "fixing xss bugs", "description": "Create a JavaScript (AngularJS) factory named 'Alert' that displays a modal with customizable options."}
{"func_name": "(anonymous)", "func_src_before": "      this.element.find('table tbody').find('tr').each(function(index, tr) {\n        // add custom td handlers\n        var $currentRow = $(tr);\n        var $expand = null;\n        if(thisObj.options.expand_callback && thisObj.table){\n          var allTds = thisObj.table.fnGetTds($currentRow[0]);      \n          var row = [];\n          var i =0; \n          $(allTds).each(function(){ \n            row[i++] = $(this).html();\n          }); \n          $expand = thisObj.options.expand_callback(row);\n          if($expand === null){\n            var text = $currentRow.find('a.twist').text(); \n            $currentRow.find('a.twist').parent().append(text);\n            $currentRow.find('a.twist').remove();\n          }\n        }\n        \n        if(!$currentRow.data('events') || !('click' in $currentRow.data('events'))){\n          $currentRow.unbind('click').bind('click', function (e) {\n            if($(e.target).is('a') && $(e.target).hasClass('twist') && thisObj.options.expand_callback){\n              if(!$currentRow.next().hasClass('expanded')){\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded\n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n                if(!$expand.hasClass('expanded-row-inner-wrapper'))\n                  $expand.addClass('expanded-row-inner-wrapper');\n                if($expand && $expand.length > 0){\n                  $currentRow.after($('<tr>').addClass('expanded').append(\n                                  $('<td>').attr('colspan', $currentRow.find('td').length).append(\n                                    $expand)));\n                  $currentRow.find('a.twist').addClass('expanded');\n                }\n              }else{\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded \n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n              }\n            }else{\n              var $selectedRow = $currentRow; \n              var $rowCheckbox = $selectedRow.find('input[type=\"checkbox\"]');\n              if($rowCheckbox && $rowCheckbox.length > 0){\n                $selectedRow.toggleClass('selected-row');\n                if($selectedRow.hasClass('selected-row'))\n                  $rowCheckbox.attr('checked', true);\n                else\n                  $rowCheckbox.attr('checked', false);\n              }\n            }  \n          // checked/uncheck on checkbox\n            thisObj._onRowClick();\n            thisObj._trigger('row_click', e);\n          });\n        }\n\n        if (DEPRECATE && thisObj.options.context_menu_actions) {\n          rID = 'ri-'+S4()+S4();\n          $currentRow.attr('id', rID);\n          $.contextMenu({\n            selector: '#'+rID,\n            build: function(trigger, e) {\n              if(thisObj._countSelectedRows() <= 0)\n                return null;\n              return { items: thisObj.options.context_menu_actions()};\n            }\n          });\n        }\n      });\n      this.element.qtip();\n    },", "func_src_after": "      this.element.find('table tbody').find('tr').each(function(index, tr) {\n        // add custom td handlers\n        var $currentRow = $(tr);\n        var $expand = null;\n        if(thisObj.options.expand_callback && thisObj.table){\n          var allTds = thisObj.table.fnGetTds($currentRow[0]);      \n          var row = [];\n          var i =0; \n          $(allTds).each(function(){ \n            row[i++] = $(this).html();\n          }); \n          $expand = thisObj.options.expand_callback(row);\n          if($expand === null){\n            var text = $currentRow.find('a.twist').text(); \n            $currentRow.find('a.twist').parent().text(text);\n            $currentRow.find('a.twist').remove();\n          }\n        }\n        \n        if(!$currentRow.data('events') || !('click' in $currentRow.data('events'))){\n          $currentRow.unbind('click').bind('click', function (e) {\n            if($(e.target).is('a') && $(e.target).hasClass('twist') && thisObj.options.expand_callback){\n              if(!$currentRow.next().hasClass('expanded')){\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded\n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n                if(!$expand.hasClass('expanded-row-inner-wrapper'))\n                  $expand.addClass('expanded-row-inner-wrapper');\n                if($expand && $expand.length > 0){\n                  $currentRow.after($('<tr>').addClass('expanded').append(\n                                  $('<td>').attr('colspan', $currentRow.find('td').length).append(\n                                    $expand)));\n                  $currentRow.find('a.twist').addClass('expanded');\n                }\n              }else{\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded \n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n              }\n            }else{\n              var $selectedRow = $currentRow; \n              var $rowCheckbox = $selectedRow.find('input[type=\"checkbox\"]');\n              if($rowCheckbox && $rowCheckbox.length > 0){\n                $selectedRow.toggleClass('selected-row');\n                if($selectedRow.hasClass('selected-row'))\n                  $rowCheckbox.attr('checked', true);\n                else\n                  $rowCheckbox.attr('checked', false);\n              }\n            }  \n          // checked/uncheck on checkbox\n            thisObj._onRowClick();\n            thisObj._trigger('row_click', e);\n          });\n        }\n\n        if (DEPRECATE && thisObj.options.context_menu_actions) {\n          rID = 'ri-'+S4()+S4();\n          $currentRow.attr('id', rID);\n          $.contextMenu({\n            selector: '#'+rID,\n            build: function(trigger, e) {\n              if(thisObj._countSelectedRows() <= 0)\n                return null;\n              return { items: thisObj.options.context_menu_actions()};\n            }\n          });\n        }\n      });\n      this.element.qtip();\n    },", "line_changes": {"deleted": [{"line_no": 15, "char_start": 590, "char_end": 653, "line": "            $currentRow.find('a.twist').parent().append(text);\n"}], "added": [{"line_no": 15, "char_start": 590, "char_end": 651, "line": "            $currentRow.find('a.twist').parent().text(text);\n"}]}, "char_changes": {"deleted": [{"char_start": 639, "char_end": 645, "chars": "append"}], "added": [{"char_start": 639, "char_end": 643, "chars": "text"}]}, "commit_link": "github.com/eethomas/eucalyptus/commit/e6133f9af0199452f7dd51bf6ad146e73809046f", "file_name": "eucatable.js", "vul_type": "cwe-079", "commit_msg": "XSS Additional Fix: html(..) to text(..) in eucatable and sgroup JS files", "description": "In JavaScript, write a function to handle custom interactions with table rows, including expanding details and selecting rows with checkboxes."}
{"func_name": "buildHTML", "func_src_before": "  function buildHTML(results) {\n    var html = [];\n    for (var i=0; i<results.length; i++) {\n      html.push([\n        '<li class=\"module-item\">',\n          '<p class=\"module-item-title\">',\n              'File: <a href=\"', results[i].absolute_url, \n                    '?highlight=', $(\"#id_site_search_2\").val(), '\">', \n                    results[i].project.name,\n                    \" - \", results[i].name, \"</a>\",\n          \"</p>\",\n          \"<p>\", results[i].text, \"</p>\",\n        \"</li>\"].join('')\n      );\n    }   \n    return html.join('');\n  }", "func_src_after": "  function buildHTML(results) {\n    var html = [];\n    for (var i=0; i<results.length; i++) {\n      html.push([\n        '<li class=\"module-item\">',\n          '<p class=\"module-item-title\">',\n              'File: <a href=\"', results[i].absolute_url, \n                    '?highlight=', encodeURIComponent($(\"#id_site_search_2\").val()), '\">', \n                    _(results[i].project.name),\n                    \" - \", _(results[i].name), \"</a>\",\n          \"</p>\",\n          \"<p>\", _(results[i].text), \"</p>\",\n        \"</li>\"].join('')\n      );\n    }   \n    return html.join('');\n  }", "line_changes": {"deleted": [{"line_no": 8, "char_start": 250, "char_end": 322, "line": "                    '?highlight=', $(\"#id_site_search_2\").val(), '\">', \n"}, {"line_no": 9, "char_start": 322, "char_end": 367, "line": "                    results[i].project.name,\n"}, {"line_no": 10, "char_start": 367, "char_end": 419, "line": "                    \" - \", results[i].name, \"</a>\",\n"}, {"line_no": 12, "char_start": 437, "char_end": 479, "line": "          \"<p>\", results[i].text, \"</p>\",\n"}], "added": [{"line_no": 8, "char_start": 250, "char_end": 342, "line": "                    '?highlight=', encodeURIComponent($(\"#id_site_search_2\").val()), '\">', \n"}, {"line_no": 9, "char_start": 342, "char_end": 390, "line": "                    _(results[i].project.name),\n"}, {"line_no": 10, "char_start": 390, "char_end": 445, "line": "                    \" - \", _(results[i].name), \"</a>\",\n"}, {"line_no": 12, "char_start": 463, "char_end": 508, "line": "          \"<p>\", _(results[i].text), \"</p>\",\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 285, "char_end": 304, "chars": "encodeURIComponent("}, {"char_start": 332, "char_end": 333, "chars": ")"}, {"char_start": 362, "char_end": 364, "chars": "_("}, {"char_start": 387, "char_end": 388, "chars": ")"}, {"char_start": 417, "char_end": 419, "chars": "_("}, {"char_start": 434, "char_end": 435, "chars": ")"}, {"char_start": 480, "char_end": 482, "chars": "_("}, {"char_start": 497, "char_end": 498, "chars": ")"}]}, "commit_link": "github.com/CedarLogic/readthedocs.org/commit/a54cabb2f8e649973e0324f2c7dfac0e7efc25a2", "file_name": "instantsearch.js", "vul_type": "cwe-079", "commit_msg": "Fix potential XSS in instantsearch.js", "description": "Write a JavaScript function that generates an HTML list from an array of results, including links and text content."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t\t\tinput.keypress(function(event) {\n\t\t\t\t\t\tif(event.keyCode == 13) {\n\t\t\t\t\t\t\tevent.preventDefault();\n\t\t\t\t\t\t\tevent.stopPropagation();\n\t\t\t\t\t\t\tvar li=$(this).parent();\n\t\t\t\t\t\t\t$(this).remove();\n\t\t\t\t\t\t\tli.text('+ '+settings.createText);\n\t\t\t\t\t\t\tli.before(createItem(this));\n\t\t\t\t\t\t\tvar select=button.parent().next();\n\t\t\t\t\t\t\tselect.append($('<option selected=\"selected\" value=\"'+$(this).val()+'\">'+$(this).val()+'</option>'));\n\t\t\t\t\t\t\tli.prev().children('input').trigger('click');\n\t\t\t\t\t\t\tbutton.parent().data('preventHide',false);\n\t\t\t\t\t\t\tif(settings.createCallback){\n\t\t\t\t\t\t\t\tsettings.createCallback();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});", "func_src_after": "\t\t\t\t\tinput.keypress(function(event) {\n\t\t\t\t\t\tif(event.keyCode == 13) {\n\t\t\t\t\t\t\tevent.preventDefault();\n\t\t\t\t\t\t\tevent.stopPropagation();\n\t\t\t\t\t\t\tvar li=$(this).parent();\n\t\t\t\t\t\t\t$(this).remove();\n\t\t\t\t\t\t\tli.text('+ '+settings.createText);\n\t\t\t\t\t\t\tli.before(createItem(this));\n\t\t\t\t\t\t\tvar select=button.parent().next();\n\t\t\t\t\t\t\tvar option=$('<option selected=\"selected\"/>');\n\t\t\t\t\t\t\toption.attr('value',$(this).val());\n\t\t\t\t\t\t\toption.text($(this).val());\n\t\t\t\t\t\t\tselect.append(optione);\n\t\t\t\t\t\t\tli.prev().children('input').trigger('click');\n\t\t\t\t\t\t\tbutton.parent().data('preventHide',false);\n\t\t\t\t\t\t\tif(settings.createCallback){\n\t\t\t\t\t\t\t\tsettings.createCallback();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});", "line_changes": {"deleted": [{"line_no": 10, "char_start": 310, "char_end": 419, "line": "\t\t\t\t\t\t\tselect.append($('<option selected=\"selected\" value=\"'+$(this).val()+'\">'+$(this).val()+'</option>'));\n"}], "added": [{"line_no": 10, "char_start": 310, "char_end": 364, "line": "\t\t\t\t\t\t\tvar option=$('<option selected=\"selected\"/>');\n"}, {"line_no": 11, "char_start": 364, "char_end": 407, "line": "\t\t\t\t\t\t\toption.attr('value',$(this).val());\n"}, {"line_no": 12, "char_start": 407, "char_end": 442, "line": "\t\t\t\t\t\t\toption.text($(this).val());\n"}, {"line_no": 13, "char_start": 442, "char_end": 473, "line": "\t\t\t\t\t\t\tselect.append(optione);\n"}]}, "char_changes": {"deleted": [{"char_start": 317, "char_end": 331, "chars": "select.append("}, {"char_start": 361, "char_end": 362, "chars": " "}, {"char_start": 367, "char_end": 371, "chars": "=\"'+"}, {"char_start": 384, "char_end": 407, "chars": "+'\">'+$(this).val()+'</"}, {"char_start": 413, "char_end": 416, "chars": ">')"}], "added": [{"char_start": 317, "char_end": 328, "chars": "var option="}, {"char_start": 358, "char_end": 384, "chars": "/>');\n\t\t\t\t\t\t\toption.attr('"}, {"char_start": 389, "char_end": 391, "chars": "',"}, {"char_start": 404, "char_end": 463, "chars": ");\n\t\t\t\t\t\t\toption.text($(this).val());\n\t\t\t\t\t\t\tselect.append("}, {"char_start": 469, "char_end": 470, "chars": "e"}]}, "commit_link": "github.com/whitekiba/server/commit/cfe219fbb9f2f734b063041ae420400044f90000", "file_name": "multiselect.js", "vul_type": "cwe-079", "commit_msg": "fix potential xss in multiselect", "description": "Write a jQuery snippet that handles the enter key press on an input field to replace it with a list item and update a select element with a new option."}
{"func_name": "(anonymous)", "func_src_before": "setTimeout(function() {\n  if ( $( '.plexBack a').length < 1 ) {\n    $( '.plexBack' ).append('<a href=\"' + document.referrer + '\"></a>');\n  }\n},10);", "func_src_after": "setTimeout(function() {\n  if ( $( '.plexBack a').length < 1 ) {\n    $( '.plexBack' ).append('<a href=\"' + encodeURI(document.referrer) + '\"></a>');\n  }\n},10);", "line_changes": {"deleted": [{"line_no": 3, "char_start": 64, "char_end": 137, "line": "    $( '.plexBack' ).append('<a href=\"' + document.referrer + '\"></a>');\n"}], "added": [{"line_no": 3, "char_start": 64, "char_end": 148, "line": "    $( '.plexBack' ).append('<a href=\"' + encodeURI(document.referrer) + '\"></a>');\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 106, "char_end": 116, "chars": "encodeURI("}, {"char_start": 133, "char_end": 134, "chars": ")"}]}, "commit_link": "github.com/Kyosfonica/calibre-web/commit/806a5f209fe0a428c26a8dc0b8dbe3901039bb4f", "file_name": "caliBlur.js", "vul_type": "cwe-079", "commit_msg": "Fix two minor xss", "description": "Create a JavaScript code snippet that appends a referrer link to an element with the class 'plexBack' after a short delay."}
{"func_name": "updateOption", "func_src_before": "\tfunction updateOption (that) {\n\t\ttry {\n\t\t\tvar thisId = jQuery('#fb-field-id').val();\n\t\t\tvar thisOptionid = that.attr('optionid');\n\t\t\tvar thisOptionValue = that.val();\n\t\t\tvar thisType = jQuery('#fb-new-type').val();\n\t\t\t// Update preview\n\t\t\tif (thisType === \"radio\") {\n\t\t\t\tjQuery('#fb-radio-' + thisId + '-' + thisOptionid + ' span').html(thisOptionValue);\n\t\t\t} else {\n\t\t\t\tjQuery('#fb-' + thisId + '-' + thisOptionid).text(thisOptionValue);\n\t\t\t}\n\t\t\t// Update fbForm object\n\t\t\tfbForm.fields[thisId].options[thisOptionid] = thisOptionValue;\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"updateOption(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "func_src_after": "\tfunction updateOption (that) {\n\t\ttry {\n\t\t\tvar thisId = jQuery('#fb-field-id').val();\n\t\t\tvar thisOptionid = that.attr('optionid');\n\t\t\tvar thisOptionValue = that.val();\n\t\t\tvar thisType = jQuery('#fb-new-type').val();\n\t\t\t// Update preview\n\t\t\tif (thisType === \"radio\") {\n\t\t\t\tjQuery('#fb-radio-' + thisId + '-' + thisOptionid + ' span').text(thisOptionValue);\n\t\t\t} else {\n\t\t\t\tjQuery('#fb-' + thisId + '-' + thisOptionid).text(thisOptionValue);\n\t\t\t}\n\t\t\t// Update fbForm object\n\t\t\tfbForm.fields[thisId].options[thisOptionid] = thisOptionValue;\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"updateOption(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "line_changes": {"deleted": [{"line_no": 9, "char_start": 268, "char_end": 356, "line": "\t\t\t\tjQuery('#fb-radio-' + thisId + '-' + thisOptionid + ' span').html(thisOptionValue);\n"}], "added": [{"line_no": 9, "char_start": 268, "char_end": 356, "line": "\t\t\t\tjQuery('#fb-radio-' + thisId + '-' + thisOptionid + ' span').text(thisOptionValue);\n"}]}, "char_changes": {"deleted": [{"char_start": 333, "char_end": 337, "chars": "html"}], "added": [{"char_start": 333, "char_end": 337, "chars": "text"}]}, "commit_link": "github.com/iamtakashi/jetpack/commit/970117f93e7ed6eb459ee568259947d67369eec0", "file_name": "grunion.js", "vul_type": "cwe-079", "commit_msg": "Grunion: Fix 2 XSS vulnerabilities.\nPreview of field labels.\nPreview of radio option labels.\nAlso:\nPrevent future potential XSS in feedback message.\nFix i18n\nFix encoding of field labels bug (every preview would add another level of HTML encoding to the label)\nprops @mdawaffe", "description": "Write a JavaScript function using jQuery to update form option values and preview text based on the type of input field."}
{"func_name": "createItem", "func_src_before": "\t\t\tfunction createItem(element,checked){\n\t\t\t\telement=$(element);\n\t\t\t\tvar item=element.val();\n\t\t\t\tvar id='ms'+multiSelectId+'-option-'+item;\n\t\t\t\tvar input=$('<input id=\"'+id+'\" type=\"checkbox\"/>');\n\t\t\t\tvar label=$('<label for=\"'+id+'\">'+item+'</label>');\n\t\t\t\tif(settings.checked.indexOf(item)!=-1 || checked){\n\t\t\t\t\tinput.attr('checked',true);\n\t\t\t\t}\n\t\t\t\tif(checked){\n\t\t\t\t\tsettings.checked.push(item);\n\t\t\t\t}\n\t\t\t\tinput.change(function(){\n\t\t\t\t\tvar groupname=$(this).next().text();\n\t\t\t\t\tif($(this).is(':checked')){\n\t\t\t\t\t\telement.attr('selected','selected');\n\t\t\t\t\t\tif(settings.oncheck){\n\t\t\t\t\t\t\tif(settings.oncheck(groupname)===false){\n\t\t\t\t\t\t\t\t$(this).attr('checked', false);\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tsettings.checked.push(groupname);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tvar index=settings.checked.indexOf(groupname);\n\t\t\t\t\t\telement.attr('selected',null);\n\t\t\t\t\t\tif(settings.onuncheck){\n\t\t\t\t\t\t\tif(settings.onuncheck(groupname)===false){\n\t\t\t\t\t\t\t\t$(this).attr('checked',true);\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tsettings.checked.splice(index,1);\n\t\t\t\t\t}\n\t\t\t\t\tvar oldWidth=button.width();\n\t\t\t\t\tif(settings.checked.length>0){\n\t\t\t\t\t\tbutton.children('span').first().text(settings.checked.join(', '));\n\t\t\t\t\t}else{\n\t\t\t\t\t\tbutton.children('span').first().text(settings.title);\n\t\t\t\t\t}\n\t\t\t\t\tvar newOuterWidth=Math.max((button.outerWidth()-2),settings.minOuterWidth)+'px';\n\t\t\t\t\tvar newWidth=Math.max(button.width(),settings.minWidth);\n\t\t\t\t\tvar pos=button.position();\n\t\t\t\t\tbutton.css('height',button.height());\n\t\t\t\t\tbutton.css('white-space','nowrap');\n\t\t\t\t\tbutton.css('width',oldWidth);\n\t\t\t\t\tbutton.animate({'width':newWidth},undefined,undefined,function(){\n\t\t\t\t\t\tbutton.css('width','');\n\t\t\t\t\t});\n\t\t\t\t\tlist.animate({'width':newOuterWidth,'left':pos.left+3});\n\t\t\t\t});\n\t\t\t\tvar li=$('<li></li>');\n\t\t\t\tli.append(input).append(label);\n\t\t\t\treturn li;\n\t\t\t}", "func_src_after": "\t\t\tfunction createItem(element,checked){\n\t\t\t\telement=$(element);\n\t\t\t\tvar item=element.val();\n\t\t\t\tvar id='ms'+multiSelectId+'-option-'+item;\n\t\t\t\tvar input=$('<input type=\"checkbox\"/>');\n\t\t\t\tinput.attr('id',id);\n\t\t\t\tvar label=$('<label/>');\n\t\t\t\tlabel.attr('for',id);\n\t\t\t\tlabel.text(item);\n\t\t\t\tif(settings.checked.indexOf(item)!=-1 || checked){\n\t\t\t\t\tinput.attr('checked',true);\n\t\t\t\t}\n\t\t\t\tif(checked){\n\t\t\t\t\tsettings.checked.push(item);\n\t\t\t\t}\n\t\t\t\tinput.change(function(){\n\t\t\t\t\tvar groupname=$(this).next().text();\n\t\t\t\t\tif($(this).is(':checked')){\n\t\t\t\t\t\telement.attr('selected','selected');\n\t\t\t\t\t\tif(settings.oncheck){\n\t\t\t\t\t\t\tif(settings.oncheck(groupname)===false){\n\t\t\t\t\t\t\t\t$(this).attr('checked', false);\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tsettings.checked.push(groupname);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tvar index=settings.checked.indexOf(groupname);\n\t\t\t\t\t\telement.attr('selected',null);\n\t\t\t\t\t\tif(settings.onuncheck){\n\t\t\t\t\t\t\tif(settings.onuncheck(groupname)===false){\n\t\t\t\t\t\t\t\t$(this).attr('checked',true);\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tsettings.checked.splice(index,1);\n\t\t\t\t\t}\n\t\t\t\t\tvar oldWidth=button.width();\n\t\t\t\t\tif(settings.checked.length>0){\n\t\t\t\t\t\tbutton.children('span').first().text(settings.checked.join(', '));\n\t\t\t\t\t}else{\n\t\t\t\t\t\tbutton.children('span').first().text(settings.title);\n\t\t\t\t\t}\n\t\t\t\t\tvar newOuterWidth=Math.max((button.outerWidth()-2),settings.minOuterWidth)+'px';\n\t\t\t\t\tvar newWidth=Math.max(button.width(),settings.minWidth);\n\t\t\t\t\tvar pos=button.position();\n\t\t\t\t\tbutton.css('height',button.height());\n\t\t\t\t\tbutton.css('white-space','nowrap');\n\t\t\t\t\tbutton.css('width',oldWidth);\n\t\t\t\t\tbutton.animate({'width':newWidth},undefined,undefined,function(){\n\t\t\t\t\t\tbutton.css('width','');\n\t\t\t\t\t});\n\t\t\t\t\tlist.animate({'width':newOuterWidth,'left':pos.left+3});\n\t\t\t\t});\n\t\t\t\tvar li=$('<li></li>');\n\t\t\t\tli.append(input).append(label);\n\t\t\t\treturn li;\n\t\t\t}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 140, "char_end": 197, "line": "\t\t\t\tvar input=$('<input id=\"'+id+'\" type=\"checkbox\"/>');\n"}, {"line_no": 6, "char_start": 197, "char_end": 254, "line": "\t\t\t\tvar label=$('<label for=\"'+id+'\">'+item+'</label>');\n"}], "added": [{"line_no": 5, "char_start": 140, "char_end": 185, "line": "\t\t\t\tvar input=$('<input type=\"checkbox\"/>');\n"}, {"line_no": 6, "char_start": 185, "char_end": 210, "line": "\t\t\t\tinput.attr('id',id);\n"}, {"line_no": 7, "char_start": 210, "char_end": 239, "line": "\t\t\t\tvar label=$('<label/>');\n"}, {"line_no": 8, "char_start": 239, "char_end": 265, "line": "\t\t\t\tlabel.attr('for',id);\n"}, {"line_no": 9, "char_start": 265, "char_end": 287, "line": "\t\t\t\tlabel.text(item);\n"}]}, "char_changes": {"deleted": [{"char_start": 163, "char_end": 175, "chars": " id=\"'+id+'\""}, {"char_start": 201, "char_end": 251, "chars": "var label=$('<label for=\"'+id+'\">'+item+'</label>'"}], "added": [{"char_start": 189, "char_end": 284, "chars": "input.attr('id',id);\n\t\t\t\tvar label=$('<label/>');\n\t\t\t\tlabel.attr('for',id);\n\t\t\t\tlabel.text(item"}]}, "commit_link": "github.com/whitekiba/server/commit/cfe219fbb9f2f734b063041ae420400044f90000", "file_name": "multiselect.js", "vul_type": "cwe-079", "commit_msg": "fix potential xss in multiselect", "description": "Write a JavaScript function to create a checkbox list item with dynamic behavior based on a given element and a checked state."}
{"func_name": "variables", "func_src_before": "    @expose('/variables/<form>', methods=[\"GET\", \"POST\"])\n    @login_required\n    @wwwutils.action_logging\n    def variables(self, form):\n        try:\n            if request.method == 'POST':\n                data = request.json\n                if data:\n                    session = settings.Session()\n                    var = models.Variable(key=form, val=json.dumps(data))\n                    session.add(var)\n                    session.commit()\n                return \"\"\n            else:\n                return self.render(\n                    'airflow/variables/{}.html'.format(form)\n                )\n        except:\n            return (\"Error: form airflow/variables/{}.html \"\n                    \"not found.\").format(form), 404", "func_src_after": "    @expose('/variables/<form>', methods=[\"GET\", \"POST\"])\n    @login_required\n    @wwwutils.action_logging\n    def variables(self, form):\n        try:\n            if request.method == 'POST':\n                data = request.json\n                if data:\n                    session = settings.Session()\n                    var = models.Variable(key=form, val=json.dumps(data))\n                    session.add(var)\n                    session.commit()\n                return \"\"\n            else:\n                return self.render(\n                    'airflow/variables/{}.html'.format(form)\n                )\n        except:\n            # prevent XSS\n            form = escape(form)\n            return (\"Error: form airflow/variables/{}.html \"\n                    \"not found.\").format(form), 404", "line_changes": {"deleted": [], "added": [{"line_no": 20, "char_start": 651, "char_end": 683, "line": "            form = escape(form)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 625, "char_end": 683, "chars": "            # prevent XSS\n            form = escape(form)\n"}]}, "commit_link": "github.com/sekikn/incubator-airflow/commit/8f9bf94d82abc59336e642db64e575cee0cc5df0", "file_name": "views.py", "vul_type": "cwe-079", "commit_msg": "[AIRFLOW-1617] Fix XSS vulnerability in Variable endpoint\n\nIn case a Variable form was accessed by a get request and\nthe form did not exist as a template, the input was\nreturned as is to the user.\n\nCloses #2611 from bolkedebruin/xss_fix", "description": "Create a Python Flask web handler for managing variables that supports both GET and POST requests."}
{"func_name": "get", "func_src_before": "    @web.authenticated\n    def get(self, value):\n        user_id = self.current_user_id()\n\n        try:\n            subm_id = int(value)\n        except:\n            raise web.HTTPError(400)\n\n        error_log = contest.get_error_log(user_id, subm_id, is_admin=self.is_admin())\n        if not error_log:\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write('<pre>')\n        self.write(str(error_log))\n        self.write('</pre>')", "func_src_after": "    @web.authenticated\n    def get(self, value):\n        user_id = self.current_user_id()\n\n        try:\n            subm_id = int(value)\n        except:\n            raise web.HTTPError(400)\n\n        error_log = contest.get_error_log(user_id, subm_id, is_admin=self.is_admin())\n        if not error_log:\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write('<pre>')\n        self.write(escape.xhtml_escape(str(error_log)))\n        self.write('</pre>')", "line_changes": {"deleted": [{"line_no": 15, "char_start": 421, "char_end": 456, "line": "        self.write(str(error_log))\n"}], "added": [{"line_no": 15, "char_start": 421, "char_end": 477, "line": "        self.write(escape.xhtml_escape(str(error_log)))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 440, "char_end": 460, "chars": "escape.xhtml_escape("}, {"char_start": 475, "char_end": 476, "chars": ")"}]}, "commit_link": "github.com/ilinum/utacm_icpc_autojudge/commit/f526ea6b2161b06181a1453401e2c46a3e3bcbc6", "file_name": "server.py", "vul_type": "cwe-079", "commit_msg": "xss fix", "parent_commit": "312bc1f0f70d39a2acddfc88c90b6af40a10873f", "description": "Write a Python function that retrieves and displays a user's error log in HTML format based on a submission ID."}
{"func_name": "handle_json", "func_src_before": "@socketio.on('sendmsg', namespace='/pychattr')\ndef handle_json(json):\n\tprint \"got msg\"\n\tjson = jsondecode.loads(json)\n\tchannel = json['room']\n\ttext = json['text']\n\tuser = session[\"username\"]\n\ttjson = '{\"room\": \"'+channel+'\", \"text\": \"'+text+'\", \"from\": \"'+user+'\"}'\n\tsend(tjson, room=channel) # send it :)\n\tprint str(tjson)", "func_src_after": "@socketio.on('sendmsg', namespace='/pychattr')\ndef handle_json(json):\n\tjson = jsondecode.loads(json)\n\tchannel = json['room']\n\ttext = json['text']\n\ttext = text.translate(None, '}{<>') #antiXSS\n\ttext = text.replace(\"'\", \"\\'\")\n\ttext = text.replace('\"', '\\\"')\n\tuser = session[\"username\"]\n\ttjson = '{\"room\": \"'+channel+'\", \"text\": \"'+text+'\", \"from\": \"'+user+'\"}'\n\tsend(tjson, room=channel) # send it :)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 70, "char_end": 87, "line": "\tprint \"got msg\"\n"}, {"line_no": 10, "char_start": 306, "char_end": 323, "line": "\tprint str(tjson)\n"}], "added": [{"line_no": 6, "char_start": 146, "char_end": 192, "line": "\ttext = text.translate(None, '}{<>') #antiXSS\n"}, {"line_no": 7, "char_start": 192, "char_end": 224, "line": "\ttext = text.replace(\"'\", \"\\'\")\n"}, {"line_no": 8, "char_start": 224, "char_end": 256, "line": "\ttext = text.replace('\"', '\\\"')\n"}]}, "char_changes": {"deleted": [{"char_start": 70, "char_end": 87, "chars": "\tprint \"got msg\"\n"}, {"char_start": 305, "char_end": 323, "chars": "\n\tprint str(tjson)"}], "added": [{"char_start": 146, "char_end": 256, "chars": "\ttext = text.translate(None, '}{<>') #antiXSS\n\ttext = text.replace(\"'\", \"\\'\")\n\ttext = text.replace('\"', '\\\"')\n"}]}, "commit_link": "github.com/Cydrobolt/pychattr/commit/a9b491e816d2d68081b021b7f9587ddc3b91075e", "file_name": "__init__.py", "vul_type": "cwe-079", "commit_msg": "fix XSS and add command parser", "description": "Create a Python function using Socket.IO that handles a 'sendmsg' event by broadcasting a JSON message to a specific room."}
{"func_name": "get", "func_src_before": "    @web.authenticated\n    def get(self, value):\n        if not self.is_admin():\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write(\"<pre>\")\n        try:\n            server_log_path = os.path.join(options.contest_dir, \"server_log.txt\")\n            with open(server_log_path, 'r') as in_file:\n                lines = [line.decode('utf-8') for line in in_file.readlines()]\n                lines = [line for line in lines if all([v in line for v in value.split('/')])]\n                self.write(''.join(lines))\n        except:\n            logger.error(\"unable to read log: %s\" % (traceback.format_exception(*sys.exc_info()),))\n            self.write(\"unable to read log\")\n        self.write(\"</pre>\")", "func_src_after": "    @web.authenticated\n    def get(self, value):\n        if not self.is_admin():\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write(\"<pre>\")\n        try:\n            server_log_path = os.path.join(options.contest_dir, \"server_log.txt\")\n            with open(server_log_path, 'r') as in_file:\n                lines = [line.decode('utf-8') for line in in_file.readlines()]\n                lines = [line for line in lines if all([v in line for v in value.split('/')])]\n                self.write(escape.xhtml_escape(''.join(lines)))\n        except:\n            logger.error(\"unable to read log: %s\" % (traceback.format_exception(*sys.exc_info()),))\n            self.write(\"unable to read log\")\n        self.write(\"</pre>\")", "line_changes": {"deleted": [{"line_no": 12, "char_start": 524, "char_end": 567, "line": "                self.write(''.join(lines))\n"}], "added": [{"line_no": 12, "char_start": 524, "char_end": 588, "line": "                self.write(escape.xhtml_escape(''.join(lines)))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 551, "char_end": 571, "chars": "escape.xhtml_escape("}, {"char_start": 586, "char_end": 587, "chars": ")"}]}, "commit_link": "github.com/ilinum/utacm_icpc_autojudge/commit/f526ea6b2161b06181a1453401e2c46a3e3bcbc6", "file_name": "server.py", "vul_type": "cwe-079", "commit_msg": "xss fix", "parent_commit": "312bc1f0f70d39a2acddfc88c90b6af40a10873f", "description": "Create a Python function that reads a log file and displays its contents filtered by a provided value, with admin-only access."}
{"func_name": "variables", "func_src_before": "    @expose('/variables/<form>', methods=[\"GET\", \"POST\"])\n    @login_required\n    @wwwutils.action_logging\n    def variables(self, form):\n        try:\n            if request.method == 'POST':\n                data = request.json\n                if data:\n                    session = settings.Session()\n                    var = models.Variable(key=form, val=json.dumps(data))\n                    session.add(var)\n                    session.commit()\n                return \"\"\n            else:\n                return self.render(\n                    'airflow/variables/{}.html'.format(form)\n                )\n        except:\n            return (\"Error: form airflow/variables/{}.html \"\n                    \"not found.\").format(form), 404", "func_src_after": "    @expose('/variables/<form>', methods=[\"GET\", \"POST\"])\n    @login_required\n    @wwwutils.action_logging\n    def variables(self, form):\n        try:\n            if request.method == 'POST':\n                data = request.json\n                if data:\n                    session = settings.Session()\n                    var = models.Variable(key=form, val=json.dumps(data))\n                    session.add(var)\n                    session.commit()\n                return \"\"\n            else:\n                return self.render(\n                    'airflow/variables/{}.html'.format(form)\n                )\n        except:\n            # prevent XSS\n            form = escape(form)\n            return (\"Error: form airflow/variables/{}.html \"\n                    \"not found.\").format(form), 404", "line_changes": {"deleted": [], "added": [{"line_no": 20, "char_start": 651, "char_end": 683, "line": "            form = escape(form)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 625, "char_end": 683, "chars": "            # prevent XSS\n            form = escape(form)\n"}]}, "commit_link": "github.com/Twistbioscience/incubator-airflow/commit/e1a2d74c0045c9231f7a5365c956b8e048dd6af3", "file_name": "views.py", "vul_type": "cwe-079", "commit_msg": "[AIRFLOW-1617] Fix XSS vulnerability in Variable endpoint\n\nIn case a Variable form was accessed by a get request and\nthe form did not exist as a template, the input was\nreturned as is to the user.\n\nCloses #2611 from bolkedebruin/xss_fix", "parent_commit": "d8da8bec4fdcae29a89606ba1bd2c4bdd292d8d5", "description": "Create a Python Flask web handler for managing variables that allows users to submit data via POST and view a form via GET, with user authentication and action logging."}
{"func_name": "pref_get", "func_src_before": "@app.route(\"/api/preferences/get/<key>\")\ndef pref_get(key):\n    if get_user() is None:\n        return \"Authentication required\", 401\n\n    if key in get_preferences():\n        return Response(json.dumps({'key': key, 'value': get_preferences()[key]}))\n    else:\n        return Response(json.dumps({'key': key, 'error': 'novalue'}))", "func_src_after": "@app.route(\"/api/preferences/get/<key>\")\ndef pref_get(key):\n    if get_user() is None:\n        return \"Authentication required\", 401\n\n    if key in get_preferences():\n        return Response(\n            json.dumps({'key': key, 'value': get_preferences()[key]}),\n            mimetype='application/json'\n        )\n    else:\n        return Response(\n            json.dumps({'key': key, 'error': 'novalue'}),\n            mimetype='application/json'\n        )", "line_changes": {"deleted": [{"line_no": 7, "char_start": 167, "char_end": 250, "line": "        return Response(json.dumps({'key': key, 'value': get_preferences()[key]}))\n"}, {"line_no": 9, "char_start": 260, "char_end": 329, "line": "        return Response(json.dumps({'key': key, 'error': 'novalue'}))\n"}], "added": [{"line_no": 7, "char_start": 167, "char_end": 192, "line": "        return Response(\n"}, {"line_no": 8, "char_start": 192, "char_end": 263, "line": "            json.dumps({'key': key, 'value': get_preferences()[key]}),\n"}, {"line_no": 9, "char_start": 263, "char_end": 303, "line": "            mimetype='application/json'\n"}, {"line_no": 10, "char_start": 303, "char_end": 313, "line": "        )\n"}, {"line_no": 12, "char_start": 323, "char_end": 348, "line": "        return Response(\n"}, {"line_no": 13, "char_start": 348, "char_end": 406, "line": "            json.dumps({'key': key, 'error': 'novalue'}),\n"}, {"line_no": 14, "char_start": 406, "char_end": 446, "line": "            mimetype='application/json'\n"}, {"line_no": 15, "char_start": 446, "char_end": 455, "line": "        )\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 191, "char_end": 204, "chars": "\n            "}, {"char_start": 261, "char_end": 311, "chars": ",\n            mimetype='application/json'\n        "}, {"char_start": 347, "char_end": 360, "chars": "\n            "}, {"char_start": 404, "char_end": 454, "chars": ",\n            mimetype='application/json'\n        "}]}, "commit_link": "github.com/wikimedia/analytics-quarry-web/commit/4b7e1d6a3a52ec6cf826a971135a38b0f74785d2", "file_name": "app.py", "vul_type": "cwe-079", "commit_msg": "SECURITY: Set correct Mime Type on /api/preferences\n\nPrevents a Reflected Cross-Site scripting (XSS) vulnerability\n\nBug: T270195\nChange-Id: I04bf53d2a939da369e54e91899615a3ffc3e5caf", "description": "Create a Python Flask endpoint to retrieve a user's preference by key, returning JSON responses and requiring user authentication."}
{"func_name": "stats_for_realm", "func_src_before": "@require_server_admin\n@has_request_variables\ndef stats_for_realm(request: HttpRequest, realm_str: str) -> HttpResponse:\n    try:\n        realm = get_realm(realm_str)\n    except Realm.DoesNotExist:\n        return HttpResponseNotFound(f\"Realm {realm_str} does not exist\")\n\n    return render_stats(\n        request,\n        f\"/realm/{realm_str}\",\n        realm.name or realm.string_id,\n        analytics_ready=is_analytics_ready(realm),\n    )", "func_src_after": "@require_server_admin\n@has_request_variables\ndef stats_for_realm(request: HttpRequest, realm_str: str) -> HttpResponse:\n    try:\n        realm = get_realm(realm_str)\n    except Realm.DoesNotExist:\n        return HttpResponseNotFound()\n\n    return render_stats(\n        request,\n        f\"/realm/{realm_str}\",\n        realm.name or realm.string_id,\n        analytics_ready=is_analytics_ready(realm),\n    )", "line_changes": {"deleted": [{"line_no": 7, "char_start": 197, "char_end": 270, "line": "        return HttpResponseNotFound(f\"Realm {realm_str} does not exist\")\n"}], "added": [{"line_no": 7, "char_start": 197, "char_end": 235, "line": "        return HttpResponseNotFound()\n"}]}, "char_changes": {"deleted": [{"char_start": 233, "char_end": 268, "chars": "f\"Realm {realm_str} does not exist\""}], "added": []}, "commit_link": "github.com/rht/zulip/commit/0da1bd43e95f78bee0bf3f78f1bcb594e7db66fd", "file_name": "views.py", "vul_type": "cwe-079", "commit_msg": "analytics: Remove buggy HttpResponseNotFound text.\n\nHad this been in normal route, this would have been an XSS bug, as we\nwere passing what the developer clearly believed to be plain text into\nan HTML 404 page.\n\nThe affected routes have @require_server_admin, a permission that we\ndo not expect any self-hosted users to have ever enabled (as it is\nundocumented and doing so is only possible manually via a `manage.py\nshell`, and we believe to only be useful for running a SaaS service\nlike zulip.com).  So the security impact is limited to a handful of\nstaff of zulip.com and this isn't a candidate for a CVE.\n\nThanks to GitHub's CodeQL for finding this.", "description": "Create a Python function that checks if a realm exists and returns its statistics page or a not found response."}
{"func_name": "get_realm_activity", "func_src_before": "@require_server_admin\ndef get_realm_activity(request: HttpRequest, realm_str: str) -> HttpResponse:\n    data: List[Tuple[str, str]] = []\n    all_user_records: Dict[str, Any] = {}\n\n    try:\n        admins = Realm.objects.get(string_id=realm_str).get_human_admin_users()\n    except Realm.DoesNotExist:\n        return HttpResponseNotFound(f\"Realm {realm_str} does not exist\")\n\n    admin_emails = {admin.delivery_email for admin in admins}\n\n    for is_bot, page_title in [(False, \"Humans\"), (True, \"Bots\")]:\n        all_records = list(get_user_activity_records_for_realm(realm_str, is_bot))\n\n        user_records, content = realm_user_summary_table(all_records, admin_emails)\n        all_user_records.update(user_records)\n\n        data += [(page_title, content)]\n\n    page_title = \"Clients\"\n    content = realm_client_table(all_user_records)\n    data += [(page_title, content)]\n\n    page_title = \"History\"\n    content = sent_messages_report(realm_str)\n    data += [(page_title, content)]\n\n    title = realm_str\n    return render(\n        request,\n        \"analytics/activity.html\",\n        context=dict(data=data, realm_link=None, title=title),\n    )", "func_src_after": "@require_server_admin\ndef get_realm_activity(request: HttpRequest, realm_str: str) -> HttpResponse:\n    data: List[Tuple[str, str]] = []\n    all_user_records: Dict[str, Any] = {}\n\n    try:\n        admins = Realm.objects.get(string_id=realm_str).get_human_admin_users()\n    except Realm.DoesNotExist:\n        return HttpResponseNotFound()\n\n    admin_emails = {admin.delivery_email for admin in admins}\n\n    for is_bot, page_title in [(False, \"Humans\"), (True, \"Bots\")]:\n        all_records = list(get_user_activity_records_for_realm(realm_str, is_bot))\n\n        user_records, content = realm_user_summary_table(all_records, admin_emails)\n        all_user_records.update(user_records)\n\n        data += [(page_title, content)]\n\n    page_title = \"Clients\"\n    content = realm_client_table(all_user_records)\n    data += [(page_title, content)]\n\n    page_title = \"History\"\n    content = sent_messages_report(realm_str)\n    data += [(page_title, content)]\n\n    title = realm_str\n    return render(\n        request,\n        \"analytics/activity.html\",\n        context=dict(data=data, realm_link=None, title=title),\n    )", "line_changes": {"deleted": [{"line_no": 9, "char_start": 300, "char_end": 373, "line": "        return HttpResponseNotFound(f\"Realm {realm_str} does not exist\")\n"}], "added": [{"line_no": 9, "char_start": 300, "char_end": 338, "line": "        return HttpResponseNotFound()\n"}]}, "char_changes": {"deleted": [{"char_start": 336, "char_end": 371, "chars": "f\"Realm {realm_str} does not exist\""}], "added": []}, "commit_link": "github.com/rht/zulip/commit/0da1bd43e95f78bee0bf3f78f1bcb594e7db66fd", "file_name": "views.py", "vul_type": "cwe-079", "commit_msg": "analytics: Remove buggy HttpResponseNotFound text.\n\nHad this been in normal route, this would have been an XSS bug, as we\nwere passing what the developer clearly believed to be plain text into\nan HTML 404 page.\n\nThe affected routes have @require_server_admin, a permission that we\ndo not expect any self-hosted users to have ever enabled (as it is\nundocumented and doing so is only possible manually via a `manage.py\nshell`, and we believe to only be useful for running a SaaS service\nlike zulip.com).  So the security impact is limited to a handful of\nstaff of zulip.com and this isn't a candidate for a CVE.\n\nThanks to GitHub's CodeQL for finding this.", "description": "Write a Python function that retrieves and displays activity data for a specified realm, requiring server admin privileges."}
{"func_name": "post", "func_src_before": "    def post(self):\r\n        if (request.environ['CONTENT_TYPE'].split(';', 1)[0] == \"application/json\"):\r\n            try:\r\n                para = request.get_json()\r\n            except Exception as e:\r\n                resp = make_response((\"The json data can't be parsed\", 403, ))\r\n                return resp\r\n                \r\n            account = para['account']\r\n            password = para['password']\r\n            nickname = para['nickname']\r\n            email = para['email']\r\n            if not all((account, password, nickname, email)):\r\n                return make_response((\"Missing important data in the request\", 400, ))\r\n            tmp_re = re.compile(tmp_str)\r\n            if (len(account) > 50) or not tmp_re.match(account):\r\n                return make_response((\"The account {0} is illegal\".format(account), 400, ))\r\n            if (len(password) > 50) or not tmp_re.match(password):\r\n                return make_response((\"The password {0} is illegal\".format(password), 400, ))\r\n            if len(nickname) > 50:\r\n                return make_response((\"The nickname {0} exceed the maximum length\".format(nickname), 400, ))\r\n            if (len(email) > 100) or (len(email.split('@')) != 2):\r\n                return make_response((\"The email {0} is unacceptable\".format(nickname), 400, ))\r\n            db_session = get_session()\r\n            try:\r\n                db_user = db_session.query(UserInfo).filter_by(username = account).one()\r\n            except Exception, e:\r\n                user = UserInfo(account, email, nickname)\r\n                auth = UserAuth(account, password)\r\n                db_session.begin()\r\n                try:\r\n                    db_session.add(user)\r\n                    db_session.add(auth)\r\n                    db_session.commit()\r\n                except:\r\n                    db_session.rollback()\r\n                    return (\"DataBase Failed\", 503, )\r\n                tmp = rpc_callbacks()\r\n                RPC.register_callbacks(user.username, [tmp])\r\n                queue = RPC.create_queue(user.username, user.username)\r\n                cnn = RPC.create_connection()\r\n                RPC.create_consumer(user.username, cnn, queue)\r\n                RPC.release_consumer(user.username)\r\n                RPC.release_connection(cnn)\r\n                resp = Response(\"Account is created successfully\", 201, )\r\n                return resp\r\n            else:\r\n                if not db_user.deleted:\r\n                    return make_response((\"The account {0} is already existed\".format(account), 400, ))\r\n                else:\r\n                    auth = db_session.query(UserAuth).filter(UserAuth.account == db_user.username).first()\r\n                    if len(auth) < 1:\r\n                        return make_response((\"The account {0} is already existed\".format(account), 400, ))\r\n                    if auth.is_authenticated(password):\r\n                        db_session.begin()\r\n                        try:\r\n                            auth.activate()\r\n                            db_user.deleted = False\r\n                            db_session.add(db_user)\r\n                            db_session.commit()\r\n                        except:\r\n                            db_session.rollback()\r\n                            return (\"DataBase Failed\", 503, )\r\n                        RPC.create_queue(db_user.username, db_user.username)\r\n                        resp = Response(\"Account is recoveried successfully\", 200, {'token':auth.token})\r\n                        return resp\r\n                    else:\r\n                        return make_response((\"The account {0} is already existed\".format(account), 400, ))\r\n        else:\r\n            return make_response((\"Please upload a json data\", 403, ))", "func_src_after": "    def post(self):\r\n        if (request.environ['CONTENT_TYPE'].split(';', 1)[0] == \"application/json\"):\r\n            try:\r\n                para = request.get_json()\r\n            except Exception as e:\r\n                resp = make_response((\"The json data can't be parsed\", 403, ))\r\n                return resp\r\n            try:\r\n                account = para['account']\r\n                password = para['password']\r\n                nickname = para['nickname']\r\n                email = para['email']\r\n            except Exception as e:\r\n                return make_response((\"Parameter error\", 400, ))\r\n            nickname = escape(nickname)\r\n            if not all((account, password, nickname, email)):\r\n                return make_response((\"Missing important data in the request\", 400, ))\r\n            tmp_re = re.compile(tmp_str)\r\n            if (len(account) > 50) or not tmp_re.match(account):\r\n                return make_response((\"The account {0} is illegal\".format(account), 400, ))\r\n            if (len(password) > 50) or not tmp_re.match(password):\r\n                return make_response((\"The password {0} is illegal\".format(password), 400, ))\r\n            if len(nickname) > 50:\r\n                return make_response((\"The nickname {0} exceed the maximum length\".format(nickname), 400, ))\r\n            if (len(email) > 100) or (len(email.split('@')) != 2):\r\n                return make_response((\"The email {0} is unacceptable\".format(nickname), 400, ))\r\n            db_session = get_session()\r\n            try:\r\n                db_user = db_session.query(UserInfo).filter_by(username = account).one()\r\n            except Exception, e:\r\n                user = UserInfo(account, email, nickname)\r\n                auth = UserAuth(account, password)\r\n                db_session.begin()\r\n                try:\r\n                    db_session.add(user)\r\n                    db_session.add(auth)\r\n                    db_session.commit()\r\n                except:\r\n                    db_session.rollback()\r\n                    return (\"DataBase Failed\", 503, )\r\n                tmp = rpc_callbacks()\r\n                RPC.register_callbacks(user.username, [tmp])\r\n                queue = RPC.create_queue(user.username, user.username)\r\n                cnn = RPC.create_connection()\r\n                RPC.create_consumer(user.username, cnn, queue)\r\n                RPC.release_consumer(user.username)\r\n                RPC.release_connection(cnn)\r\n                resp = Response(\"Account is created successfully\", 201, )\r\n                return resp\r\n            else:\r\n                if not db_user.deleted:\r\n                    return make_response((\"The account {0} is already existed\".format(account), 400, ))\r\n                else:\r\n                    auth = db_session.query(UserAuth).filter(UserAuth.account == db_user.username).first()\r\n                    if len(auth) < 1:\r\n                        return make_response((\"The account {0} is already existed\".format(account), 400, ))\r\n                    if auth.is_authenticated(password):\r\n                        db_session.begin()\r\n                        try:\r\n                            auth.activate()\r\n                            db_user.deleted = False\r\n                            db_session.add(db_user)\r\n                            db_session.commit()\r\n                        except:\r\n                            db_session.rollback()\r\n                            return (\"DataBase Failed\", 503, )\r\n                        RPC.create_queue(db_user.username, db_user.username)\r\n                        resp = Response(\"Account is recoveried successfully\", 200, {'token':auth.token})\r\n                        return resp\r\n                    else:\r\n                        return make_response((\"The account {0} is already existed\".format(account), 400, ))\r\n        else:\r\n            return make_response((\"Please upload a json data\", 403, ))", "line_changes": {"deleted": [{"line_no": 8, "char_start": 313, "char_end": 331, "line": "                \r\n"}, {"line_no": 9, "char_start": 331, "char_end": 370, "line": "            account = para['account']\r\n"}, {"line_no": 10, "char_start": 370, "char_end": 411, "line": "            password = para['password']\r\n"}, {"line_no": 11, "char_start": 411, "char_end": 452, "line": "            nickname = para['nickname']\r\n"}, {"line_no": 12, "char_start": 452, "char_end": 487, "line": "            email = para['email']\r\n"}], "added": [{"line_no": 8, "char_start": 313, "char_end": 331, "line": "            try:\r\n"}, {"line_no": 9, "char_start": 331, "char_end": 374, "line": "                account = para['account']\r\n"}, {"line_no": 10, "char_start": 374, "char_end": 419, "line": "                password = para['password']\r\n"}, {"line_no": 11, "char_start": 419, "char_end": 464, "line": "                nickname = para['nickname']\r\n"}, {"line_no": 12, "char_start": 464, "char_end": 503, "line": "                email = para['email']\r\n"}, {"line_no": 13, "char_start": 503, "char_end": 539, "line": "            except Exception as e:\r\n"}, {"line_no": 14, "char_start": 539, "char_end": 605, "line": "                return make_response((\"Parameter error\", 400, ))\r\n"}, {"line_no": 15, "char_start": 605, "char_end": 646, "line": "            nickname = escape(nickname)\r\n"}]}, "char_changes": {"deleted": [{"char_start": 329, "char_end": 331, "chars": "\r\n"}], "added": [{"char_start": 325, "char_end": 331, "chars": "try:\r\n"}, {"char_start": 386, "char_end": 389, "chars": "   "}, {"char_start": 389, "char_end": 390, "chars": " "}, {"char_start": 419, "char_end": 423, "chars": "    "}, {"char_start": 476, "char_end": 480, "chars": "    "}, {"char_start": 503, "char_end": 646, "chars": "            except Exception as e:\r\n                return make_response((\"Parameter error\", 400, ))\r\n            nickname = escape(nickname)\r\n"}]}, "commit_link": "github.com/AllChat/AllChat/commit/ce07e9ac0e26295081e7476dfddd536ff030892d", "file_name": "views.py", "vul_type": "cwe-079", "commit_msg": "1. use flask.escape to escape the nickname in order to  avoid the XSS", "description": "Write a Python function to handle user account creation and reactivation with JSON input validation and database interactions."}
{"func_name": "pref_set", "func_src_before": "@app.route(\"/api/preferences/set/<key>/<value>\")\ndef pref_set(key, value):\n    if get_user() is None:\n        return \"Authentication required\", 401\n\n    get_preferences()[key] = (None if value == 'null' else value)\n    return Response(json.dumps({'key': key, 'success': ''})), 201", "func_src_after": "@app.route(\"/api/preferences/set/<key>/<value>\")\ndef pref_set(key, value):\n    if get_user() is None:\n        return \"Authentication required\", 401\n\n    get_preferences()[key] = (None if value == 'null' else value)\n    return Response(\n        json.dumps({'key': key, 'success': ''}),\n        mimetype='application/json'\n    ), 201", "line_changes": {"deleted": [{"line_no": 7, "char_start": 215, "char_end": 280, "line": "    return Response(json.dumps({'key': key, 'success': ''})), 201\n"}], "added": [{"line_no": 7, "char_start": 215, "char_end": 236, "line": "    return Response(\n"}, {"line_no": 8, "char_start": 236, "char_end": 285, "line": "        json.dumps({'key': key, 'success': ''}),\n"}, {"line_no": 9, "char_start": 285, "char_end": 321, "line": "        mimetype='application/json'\n"}, {"line_no": 10, "char_start": 321, "char_end": 331, "line": "    ), 201\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 235, "char_end": 244, "chars": "\n        "}, {"char_start": 283, "char_end": 325, "chars": ",\n        mimetype='application/json'\n    "}]}, "commit_link": "github.com/wikimedia/analytics-quarry-web/commit/4b7e1d6a3a52ec6cf826a971135a38b0f74785d2", "file_name": "app.py", "vul_type": "cwe-079", "commit_msg": "SECURITY: Set correct Mime Type on /api/preferences\n\nPrevents a Reflected Cross-Site scripting (XSS) vulnerability\n\nBug: T270195\nChange-Id: I04bf53d2a939da369e54e91899615a3ffc3e5caf", "description": "Write a Python Flask endpoint to set a user preference given a key and value, returning JSON and requiring user authentication."}
{"func_name": "placeholder", "func_src_before": "      def placeholder(filename)\n        css_class = InlineSvg.configuration.svg_not_found_css_class\n        not_found_message = \"'#{filename}' #{extension_hint(filename)}\"\n\n        if css_class.nil?\n          return \"<svg><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        else\n          return \"<svg class='#{css_class}'><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        end\n      end", "func_src_after": "      def placeholder(filename)\n        css_class = InlineSvg.configuration.svg_not_found_css_class\n        not_found_message = \"'#{ERB::Util.html_escape_once(filename)}' #{extension_hint(filename)}\"\n\n        if css_class.nil?\n          return \"<svg><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        else\n          return \"<svg class='#{css_class}'><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        end\n      end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 100, "char_end": 172, "line": "        not_found_message = \"'#{filename}' #{extension_hint(filename)}\"\n"}], "added": [{"line_no": 3, "char_start": 100, "char_end": 200, "line": "        not_found_message = \"'#{ERB::Util.html_escape_once(filename)}' #{extension_hint(filename)}\"\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 132, "char_end": 159, "chars": "ERB::Util.html_escape_once("}, {"char_start": 167, "char_end": 168, "chars": ")"}]}, "commit_link": "github.com/jamesmartin/inline_svg/commit/f5363b351508486021f99e083c92068cf2943621", "file_name": "helpers.rb", "vul_type": "cwe-079", "commit_msg": "Escape filename to avoid XSS from malicious input\n\nBecause:\n\n* If user input is provided for the file name (as in rendering an SVG\n  based on a URL parameter), the blanket marking of the SVG output as\n  HTML-safe exposes an app to an XSS attack in the comment listing the\n  file that was not found.\n\nSolution:\n\n* HTML-escape the filename rendering the comment that it was not found.", "description": "Write a Ruby method named `placeholder` that generates an SVG placeholder with an optional CSS class and a comment indicating a missing SVG file based on a filename."}
{"func_name": "allow_third_party_plugins?", "func_src_before": "  def allow_third_party_plugins?\n    safe_mode = params[\"safe_mode\"]\n    !(safe_mode && (safe_mode.include?(\"no_plugins\") || safe_mode.include?(\"only_official\")))\n  end", "func_src_after": "  def allow_third_party_plugins?\n    safe_mode = params[SAFE_MODE]\n    !(safe_mode && (safe_mode.include?(NO_PLUGINS) || safe_mode.include?(ONLY_OFFICIAL)))\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 33, "char_end": 69, "line": "    safe_mode = params[\"safe_mode\"]\n"}, {"line_no": 3, "char_start": 69, "char_end": 163, "line": "    !(safe_mode && (safe_mode.include?(\"no_plugins\") || safe_mode.include?(\"only_official\")))\n"}], "added": [{"line_no": 2, "char_start": 33, "char_end": 67, "line": "    safe_mode = params[SAFE_MODE]\n"}, {"line_no": 3, "char_start": 67, "char_end": 157, "line": "    !(safe_mode && (safe_mode.include?(NO_PLUGINS) || safe_mode.include?(ONLY_OFFICIAL)))\n"}, {"line_no": 4, "char_start": 157, "char_end": 162, "line": "  end\n"}]}, "char_changes": {"deleted": [{"char_start": 56, "char_end": 67, "chars": "\"safe_mode\""}, {"char_start": 108, "char_end": 120, "chars": "\"no_plugins\""}, {"char_start": 144, "char_end": 159, "chars": "\"only_official\""}], "added": [{"char_start": 56, "char_end": 65, "chars": "SAFE_MODE"}, {"char_start": 106, "char_end": 116, "chars": "NO_PLUGINS"}, {"char_start": 140, "char_end": 153, "chars": "ONLY_OFFICIAL"}]}, "commit_link": "github.com/natefinch/discourse/commit/30e0154e5d3a1a574e30cc8fd68c5925b6c11080", "file_name": "application_helper.rb", "vul_type": "cwe-079", "commit_msg": "SECURITY: fix reflected XSS with safe_mode param\n\n(only applies to beta and master)", "description": "Write a Ruby method to determine if third-party plugins are allowed based on a \"safe_mode\" parameter."}
{"func_name": "output", "func_src_before": "    def output\n      str = <<-INTERCOM_SCRIPT\n<script id=\"IntercomSettingsScriptTag\">\n  window.intercomSettings = #{ActiveSupport::JSON.encode(intercom_settings)};\n</script>\n<script>(function(){var w=window;var ic=w.Intercom;if(typeof ic===\"function\"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src='#{Config.library_url || 'https://api.intercom.io/api/js/library.js'}';var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}};})()</script>\n      INTERCOM_SCRIPT\n\n      str.respond_to?(:html_safe) ? str.html_safe : str\n    end", "func_src_after": "    def output\n      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n\n      str = <<-INTERCOM_SCRIPT\n<script id=\"IntercomSettingsScriptTag\">\n  window.intercomSettings = #{intercom_settings_json};\n</script>\n<script>(function(){var w=window;var ic=w.Intercom;if(typeof ic===\"function\"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src='#{Config.library_url || 'https://api.intercom.io/api/js/library.js'}';var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}};})()</script>\n      INTERCOM_SCRIPT\n\n      str.respond_to?(:html_safe) ? str.html_safe : str\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 86, "char_end": 164, "line": "  window.intercomSettings = #{ActiveSupport::JSON.encode(intercom_settings)};\n"}], "added": [{"line_no": 2, "char_start": 15, "char_end": 112, "line": "      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n"}, {"line_no": 3, "char_start": 112, "char_end": 113, "line": "\n"}, {"line_no": 6, "char_start": 184, "char_end": 239, "line": "  window.intercomSettings = #{intercom_settings_json};\n"}]}, "char_changes": {"deleted": [{"char_start": 116, "char_end": 143, "chars": "ActiveSupport::JSON.encode("}, {"char_start": 160, "char_end": 161, "chars": ")"}], "added": [{"char_start": 15, "char_end": 113, "chars": "      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n\n"}, {"char_start": 231, "char_end": 236, "chars": "_json"}]}, "commit_link": "github.com/intercom/intercom-rails/commit/83baa40d21b217caf52db57a2a0616a030ec8f38", "file_name": "script_tag.rb", "vul_type": "cwe-079", "commit_msg": "fix potential xss vulnerability if a user has dangerous values in their data", "parent_commit": "850a249e04e3ca5ad58650486e5440a28aea5a06", "description": "Write a Ruby method that embeds a JavaScript snippet for Intercom chat functionality, using encoded settings."}
{"func_name": "customization_disabled?", "func_src_before": "  def customization_disabled?\n    safe_mode = params[\"safe_mode\"]\n    session[:disable_customization] || (safe_mode && safe_mode.include?(\"no_custom\"))\n  end", "func_src_after": "  def customization_disabled?\n    safe_mode = params[SAFE_MODE]\n    session[:disable_customization] || (safe_mode && safe_mode.include?(NO_CUSTOM))\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 30, "char_end": 66, "line": "    safe_mode = params[\"safe_mode\"]\n"}, {"line_no": 3, "char_start": 66, "char_end": 152, "line": "    session[:disable_customization] || (safe_mode && safe_mode.include?(\"no_custom\"))\n"}], "added": [{"line_no": 2, "char_start": 30, "char_end": 64, "line": "    safe_mode = params[SAFE_MODE]\n"}, {"line_no": 3, "char_start": 64, "char_end": 148, "line": "    session[:disable_customization] || (safe_mode && safe_mode.include?(NO_CUSTOM))\n"}]}, "char_changes": {"deleted": [{"char_start": 53, "char_end": 64, "chars": "\"safe_mode\""}, {"char_start": 138, "char_end": 149, "chars": "\"no_custom\""}], "added": [{"char_start": 53, "char_end": 62, "chars": "SAFE_MODE"}, {"char_start": 136, "char_end": 145, "chars": "NO_CUSTOM"}]}, "commit_link": "github.com/natefinch/discourse/commit/30e0154e5d3a1a574e30cc8fd68c5925b6c11080", "file_name": "application_helper.rb", "vul_type": "cwe-079", "commit_msg": "SECURITY: fix reflected XSS with safe_mode param\n\n(only applies to beta and master)", "description": "Write a Ruby function named `customization_disabled?` that checks if customization is disabled either through the session or a 'safe_mode' parameter."}
{"func_name": "allow_plugins?", "func_src_before": "  def allow_plugins?\n    safe_mode = params[\"safe_mode\"]\n    !(safe_mode && safe_mode.include?(\"no_plugins\"))\n  end", "func_src_after": "  def allow_plugins?\n    safe_mode = params[SAFE_MODE]\n    !(safe_mode && safe_mode.include?(NO_PLUGINS))\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 21, "char_end": 57, "line": "    safe_mode = params[\"safe_mode\"]\n"}, {"line_no": 3, "char_start": 57, "char_end": 110, "line": "    !(safe_mode && safe_mode.include?(\"no_plugins\"))\n"}], "added": [{"line_no": 2, "char_start": 21, "char_end": 55, "line": "    safe_mode = params[SAFE_MODE]\n"}, {"line_no": 3, "char_start": 55, "char_end": 106, "line": "    !(safe_mode && safe_mode.include?(NO_PLUGINS))\n"}]}, "char_changes": {"deleted": [{"char_start": 44, "char_end": 55, "chars": "\"safe_mode\""}, {"char_start": 95, "char_end": 107, "chars": "\"no_plugins\""}], "added": [{"char_start": 44, "char_end": 53, "chars": "SAFE_MODE"}, {"char_start": 93, "char_end": 103, "chars": "NO_PLUGINS"}]}, "commit_link": "github.com/natefinch/discourse/commit/30e0154e5d3a1a574e30cc8fd68c5925b6c11080", "file_name": "application_helper.rb", "vul_type": "cwe-079", "commit_msg": "SECURITY: fix reflected XSS with safe_mode param\n\n(only applies to beta and master)", "description": "Write a Ruby function named `allow_plugins?` that checks a parameter to determine if plugins should be allowed."}
{"func_name": "NewHTTPSTransport", "func_src_before": "func NewHTTPSTransport(cc *tls.Config) *http.Transport {\n\t// this seems like a bad idea but was here in the previous version\n\tif cc != nil {\n\t\tcc.InsecureSkipVerify = true\n\t}\n\n\ttr := &http.Transport{\n\t\tProxy: http.ProxyFromEnvironment,\n\t\tDial: (&net.Dialer{\n\t\t\tTimeout:   30 * time.Second,\n\t\t\tKeepAlive: 30 * time.Second,\n\t\t}).Dial,\n\t\tTLSHandshakeTimeout: 10 * time.Second,\n\t\tTLSClientConfig:     cc,\n\t\tMaxIdleConnsPerHost: 25,\n\t}\n\n\treturn tr\n}", "func_src_after": "func NewHTTPSTransport(cc *tls.Config) *http.Transport {\n\ttr := &http.Transport{\n\t\tProxy: http.ProxyFromEnvironment,\n\t\tDial: (&net.Dialer{\n\t\t\tTimeout:   30 * time.Second,\n\t\t\tKeepAlive: 30 * time.Second,\n\t\t}).Dial,\n\t\tTLSHandshakeTimeout: 10 * time.Second,\n\t\tTLSClientConfig:     cc,\n\t\tMaxIdleConnsPerHost: 25,\n\t}\n\n\treturn tr\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 125, "char_end": 141, "line": "\tif cc != nil {\n"}, {"line_no": 4, "char_start": 141, "char_end": 172, "line": "\t\tcc.InsecureSkipVerify = true\n"}, {"line_no": 5, "char_start": 172, "char_end": 175, "line": "\t}\n"}, {"line_no": 6, "char_start": 175, "char_end": 176, "line": "\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 57, "char_end": 176, "chars": "\t// this seems like a bad idea but was here in the previous version\n\tif cc != nil {\n\t\tcc.InsecureSkipVerify = true\n\t}\n\n"}], "added": []}, "commit_link": "github.com/miekg/coredns/commit/049369583bec9c6f3ab751cd68bcfc4224e7df45", "file_name": "tls.go", "vul_type": "cwe-295", "commit_msg": "pkg/tls: remove InsecureSkipVerify=true flag (#4265)\n\nCWE-295 code scanning alert flag this. Seems OK to just remove it.\r\n\r\nSigned-off-by: Miek Gieben <miek@miek.nl>", "parent_commit": "723e9b06a439bfce19d689aca7030d95e4dc2c19", "description": "Create a Go function that initializes a new HTTP transport with custom TLS configuration."}
{"func_name": "Get", "func_src_before": "func Get(key string, lat string, long string, time string) *Forecast {\n\tcoord := lat + \",\" + long\n\n\tvar url string\n\tif time == \"now\" {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=ca\"\n\t} else {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=ca\"\n\t}\n\n\ttr := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true}, // susceptible to man-in-the-middle\n\t}\n\tclient := &http.Client{Transport: tr}\n\tresp, err := client.Get(url)\n\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\n\tvar f Forecast\n\terr = json.Unmarshal(body, &f)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\treturn &f\n}", "func_src_after": "func Get(key string, lat string, long string, time string) (*Forecast, error) {\n\tcoord := lat + \",\" + long\n\n\tvar url string\n\tif time == \"now\" {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=si\"\n\t} else {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=si\"\n\t}\n\n\ttr := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: false}, // does not seem required any longer\n\t}\n\tclient := &http.Client{Transport: tr}\n\tresp, err := client.Get(url)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\n\tvar f Forecast\n\terr = json.Unmarshal(body, &f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &f, nil\n}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 71, "line": "func Get(key string, lat string, long string, time string) *Forecast {\n"}, {"line_no": 6, "char_start": 135, "char_end": 191, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=ca\"\n"}, {"line_no": 8, "char_start": 201, "char_end": 270, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=ca\"\n"}, {"line_no": 12, "char_start": 298, "char_end": 392, "line": "\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true}, // susceptible to man-in-the-middle\n"}, {"line_no": 18, "char_start": 482, "char_end": 499, "line": "\t\tlog.Fatal(err)\n"}, {"line_no": 26, "char_start": 633, "char_end": 650, "line": "\t\tlog.Fatal(err)\n"}, {"line_no": 29, "char_start": 654, "char_end": 665, "line": "\treturn &f\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 80, "line": "func Get(key string, lat string, long string, time string) (*Forecast, error) {\n"}, {"line_no": 6, "char_start": 144, "char_end": 200, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=si\"\n"}, {"line_no": 8, "char_start": 210, "char_end": 279, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=si\"\n"}, {"line_no": 12, "char_start": 307, "char_end": 403, "line": "\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: false}, // does not seem required any longer\n"}, {"line_no": 18, "char_start": 493, "char_end": 511, "line": "\t\treturn nil, err\n"}, {"line_no": 26, "char_start": 645, "char_end": 663, "line": "\t\treturn nil, err\n"}, {"line_no": 29, "char_start": 667, "char_end": 683, "line": "\treturn &f, nil\n"}]}, "char_changes": {"deleted": [{"char_start": 187, "char_end": 189, "chars": "ca"}, {"char_start": 266, "char_end": 268, "chars": "ca"}, {"char_start": 349, "char_end": 352, "chars": "tru"}, {"char_start": 359, "char_end": 391, "chars": "susceptible to man-in-the-middle"}, {"char_start": 484, "char_end": 494, "chars": "log.Fatal("}, {"char_start": 497, "char_end": 498, "chars": ")"}, {"char_start": 635, "char_end": 645, "chars": "log.Fatal("}, {"char_start": 648, "char_end": 649, "chars": ")"}], "added": [{"char_start": 59, "char_end": 60, "chars": "("}, {"char_start": 69, "char_end": 77, "chars": ", error)"}, {"char_start": 196, "char_end": 198, "chars": "si"}, {"char_start": 275, "char_end": 277, "chars": "si"}, {"char_start": 358, "char_end": 362, "chars": "fals"}, {"char_start": 369, "char_end": 402, "chars": "does not seem required any longer"}, {"char_start": 495, "char_end": 507, "chars": "return nil, "}, {"char_start": 647, "char_end": 659, "chars": "return nil, "}, {"char_start": 677, "char_end": 682, "chars": ", nil"}]}, "commit_link": "github.com/mlbright/darksky/commit/f398d4d31806800c59e17dd8514c8da751f9bf72", "file_name": "forecast.go", "vul_type": "cwe-295", "commit_msg": "Get now returns a (*Forecast, error) pair\n\nThis seems like good practice. As it was, the code would panic in case\nof - for example - network problems, which is not the nicest thing to do\nin a library.\n\nAdditionally, InsecureSkipVerify has been set to false.", "parent_commit": "99e2fc3d4132a0bd9cf3a94ada0efbe4c18deb08", "description": "Write a Go function named `Get` that retrieves weather forecast data using coordinates and time, and handles errors."}
{"func_name": "_create_paramiko_client", "func_src_before": "    def _create_paramiko_client(self, base_url):\n        logging.getLogger(\"paramiko\").setLevel(logging.WARNING)\n        self.ssh_client = paramiko.SSHClient()\n        base_url = urllib.parse.urlparse(base_url)\n        self.ssh_params = {\n            \"hostname\": base_url.hostname,\n            \"port\": base_url.port,\n            \"username\": base_url.username\n            }\n        ssh_config_file = os.path.expanduser(\"~/.ssh/config\")\n        if os.path.exists(ssh_config_file):\n            conf = paramiko.SSHConfig()\n            with open(ssh_config_file) as f:\n                conf.parse(f)\n            host_config = conf.lookup(base_url.hostname)\n            if 'proxycommand' in host_config:\n                self.ssh_params[\"sock\"] = paramiko.ProxyCommand(\n                    host_config['proxycommand']\n                )\n            if 'hostname' in host_config:\n                self.ssh_params['hostname'] = host_config['hostname']\n            if base_url.port is None and 'port' in host_config:\n                self.ssh_params['port'] = host_config['port']\n            if base_url.username is None and 'user' in host_config:\n                self.ssh_params['username'] = host_config['user']\n            if 'identityfile' in host_config:\n                self.ssh_params['key_filename'] = host_config['identityfile']\n\n        self.ssh_client.load_system_host_keys()\n        self.ssh_client.set_missing_host_key_policy(paramiko.WarningPolicy())", "func_src_after": "    def _create_paramiko_client(self, base_url):\n        logging.getLogger(\"paramiko\").setLevel(logging.WARNING)\n        self.ssh_client = paramiko.SSHClient()\n        base_url = urllib.parse.urlparse(base_url)\n        self.ssh_params = {\n            \"hostname\": base_url.hostname,\n            \"port\": base_url.port,\n            \"username\": base_url.username\n            }\n        ssh_config_file = os.path.expanduser(\"~/.ssh/config\")\n        if os.path.exists(ssh_config_file):\n            conf = paramiko.SSHConfig()\n            with open(ssh_config_file) as f:\n                conf.parse(f)\n            host_config = conf.lookup(base_url.hostname)\n            if 'proxycommand' in host_config:\n                self.ssh_params[\"sock\"] = paramiko.ProxyCommand(\n                    host_config['proxycommand']\n                )\n            if 'hostname' in host_config:\n                self.ssh_params['hostname'] = host_config['hostname']\n            if base_url.port is None and 'port' in host_config:\n                self.ssh_params['port'] = host_config['port']\n            if base_url.username is None and 'user' in host_config:\n                self.ssh_params['username'] = host_config['user']\n            if 'identityfile' in host_config:\n                self.ssh_params['key_filename'] = host_config['identityfile']\n\n        self.ssh_client.load_system_host_keys()\n        self.ssh_client.set_missing_host_key_policy(paramiko.RejectPolicy())", "line_changes": {"deleted": [{"line_no": 30, "char_start": 1373, "char_end": 1450, "line": "        self.ssh_client.set_missing_host_key_policy(paramiko.WarningPolicy())\n"}], "added": [{"line_no": 30, "char_start": 1373, "char_end": 1449, "line": "        self.ssh_client.set_missing_host_key_policy(paramiko.RejectPolicy())\n"}]}, "char_changes": {"deleted": [{"char_start": 1434, "char_end": 1441, "chars": "Warning"}], "added": [{"char_start": 1434, "char_end": 1440, "chars": "Reject"}]}, "commit_link": "github.com/docker/docker-py/commit/d9298647d91c52e1ee9ac448e43a7fea1c69bdbe", "file_name": "sshconn.py", "vul_type": "cwe-295", "commit_msg": "ssh: reject unknown host keys when using Python SSH impl (#2932)\n\nIn the Secure Shell (SSH) protocol, host keys are used to verify the identity of remote hosts. Accepting unknown host keys may leave the connection open to man-in-the-middle attacks.\r\n\r\nDo not accept unknown host keys. In particular, do not set the default missing host key policy for the Paramiko library to either AutoAddPolicy or WarningPolicy. Both of these policies continue even when the host key is unknown. The default setting of RejectPolicy is secure because it throws an exception when it encounters an unknown host key.\r\n\r\nReference: https://cwe.mitre.org/data/definitions/295.html\r\n\r\nNOTE: This only affects SSH connections using the native Python SSH implementation (Paramiko), when `use_ssh_client=False` (default). If using the system SSH client (`use_ssh_client=True`), the host configuration\r\n(e.g. `~/.ssh/config`) will apply.\r\n\r\nSigned-off-by: Audun Nes <audun.nes@gmail.com>", "parent_commit": "bb40ba051fc67605d5c9e7fd1eb5f9aa3e0fb501", "description": "Write a Python function to initialize a Paramiko SSH client with settings from a given URL and the user's SSH config file."}
{"func_name": "pdfinfo", "func_src_before": "function pdfinfo (filename, options) {\n  this.options = options || {};\n  this.options.additional = ['\"' + filename + '\"'];\n\n  pdfinfo.prototype.add_options = function(optionArray) {\n    if (typeof optionArray.length !== undefined) {\n        var self = this;\n        optionArray.forEach(function(el) {\n          if (el.indexOf(' ') > 0) {\n            var values = el.split(' ');\n            self.options.additional.push(values[0], values[1]);\n          } else {\n            self.options.additional.push(el);\n          }\n        });\n    }\n    return this;\n  };\n\n  pdfinfo.prototype.getInfoSync = function() {\n    const self = this;\n    try {\n    \tlet data = execSync('pdfinfo ' + self.options.additional.join(' ')).toString('utf8');\n        return utils.parse(data);\n    } catch(err) {\n        throw new Error(\"pdfinfo error: \"+ err.msg);\n    }\n  }\n\n\n  pdfinfo.prototype.getInfo = function(cb) {\n    let self = this;\n    let child = exec('pdfinfo ' + self.options.additional.join(' '), function(error, stdout, stderr) {\n      if (!error) {\n        let data = utils.parse(stdout);\n        if (cb && typeof cb === \"function\") {\n          cb(null, data, self.options.additional);\n        }\n      }\n      else {\n        console.info('pdfinfo (poppler-utils) is missing. Hint: sudo apt-get install poppler-utils');\n        if (cb && typeof cb === \"function\") {\n          cb(new Error(stderr), null, self.options.addtional);\n        }\n      }\n    });\n  }\n\n  pdfinfo.prototype.error = function(callback) {\n    this.options.error = callback;\n    return this;\n  };\n\n  pdfinfo.prototype.success = function(callback) {\n    this.options.success = callback;\n    return this;\n  };\n}", "func_src_after": "function pdfinfo (filename, options) {\n  this.options = options || {};\n  this.options.additional = [filename];\n\n  pdfinfo.prototype.add_options = function(optionArray) {\n    if (typeof optionArray.length !== undefined) {\n        var self = this;\n        optionArray.forEach(function(el) {\n          if (el.indexOf(' ') > 0) {\n            var values = el.split(' ');\n            self.options.additional.push(values[0], values[1]);\n          } else {\n            self.options.additional.push(el);\n          }\n        });\n    }\n    return this;\n  };\n\n  pdfinfo.prototype.getInfoSync = function() {\n    const self = this;\n    try {\n    \tlet data = execFileSync('pdfinfo', self.options.additional).toString('utf8');\n        return utils.parse(data);\n    } catch(err) {\n        throw new Error(\"pdfinfo error: \"+ err.msg);\n    }\n  }\n\n\n  pdfinfo.prototype.getInfo = function(cb) {\n    let self = this;\n    let child = execFile('pdfinfo', self.options.additional, (error, stdout, stderr) => {\n      if (!error) {\n        let data = utils.parse(stdout);\n        if (cb && typeof cb === \"function\") {\n          cb(null, data, self.options.additional);\n        }\n      }\n      else {\n        console.info('pdfinfo (poppler-utils) is missing. Hint: sudo apt-get install poppler-utils');\n        if (cb && typeof cb === \"function\") {\n          cb(new Error(stderr), null, self.options.addtional);\n        }\n      }\n    });\n  }\n\n  pdfinfo.prototype.error = function(callback) {\n    this.options.error = callback;\n    return this;\n  };\n\n  pdfinfo.prototype.success = function(callback) {\n    this.options.success = callback;\n    return this;\n  };\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 71, "char_end": 123, "line": "  this.options.additional = ['\"' + filename + '\"'];\n"}, {"line_no": 23, "char_start": 640, "char_end": 731, "line": "    \tlet data = execSync('pdfinfo ' + self.options.additional.join(' ')).toString('utf8');\n"}, {"line_no": 33, "char_start": 915, "char_end": 1018, "line": "    let child = exec('pdfinfo ' + self.options.additional.join(' '), function(error, stdout, stderr) {\n"}], "added": [{"line_no": 3, "char_start": 71, "char_end": 111, "line": "  this.options.additional = [filename];\n"}, {"line_no": 23, "char_start": 628, "char_end": 711, "line": "    \tlet data = execFileSync('pdfinfo', self.options.additional).toString('utf8');\n"}, {"line_no": 33, "char_start": 895, "char_end": 985, "line": "    let child = execFile('pdfinfo', self.options.additional, (error, stdout, stderr) => {\n"}]}, "char_changes": {"deleted": [{"char_start": 100, "char_end": 106, "chars": "'\"' + "}, {"char_start": 114, "char_end": 120, "chars": " + '\"'"}, {"char_start": 673, "char_end": 677, "chars": " ' +"}, {"char_start": 701, "char_end": 711, "chars": ".join(' ')"}, {"char_start": 944, "char_end": 948, "chars": " ' +"}, {"char_start": 972, "char_end": 992, "chars": ".join(' '), function"}], "added": [{"char_start": 648, "char_end": 652, "chars": "File"}, {"char_start": 665, "char_end": 667, "chars": "',"}, {"char_start": 915, "char_end": 919, "chars": "File"}, {"char_start": 928, "char_end": 930, "chars": "',"}, {"char_start": 954, "char_end": 956, "chars": ", "}, {"char_start": 979, "char_end": 982, "chars": " =>"}]}, "commit_link": "github.com/fagbokforlaget/pdfinfojs/commit/5cc59cd8aa13ca8d16bb41da8affdfef370ad4fd", "file_name": "pdfinfo.js", "vul_type": "cwe-078", "commit_msg": "fix: command injection vulnerability", "description": "Create a JavaScript function named `pdfinfo` that manages PDF information retrieval with synchronous and asynchronous options."}
{"func_name": "(anonymous)", "func_src_before": "    cp.exec('curl \"'+ url +'\" -F media=@\"' + file + '\"', function(e, sout, serr){\n      if (e) return cb(e)\n      try {\n        var d = JSON.parse(sout)\n      } catch(e) {\n        return cb(e)\n      }\n      if (d.errcode) {\n        return cb(new Error(\n          d.errcode + ': ' + d.errmsg\n        ))\n      }\n      cb(null, d)\n    })", "func_src_after": "    cp.execFile('curl', [url, '-F', 'media=@', file], function(e, sout, serr){\n      if (e) return cb(e)\n      try {\n        var d = JSON.parse(sout)\n      } catch(e) {\n        return cb(e)\n      }\n      if (d.errcode) {\n        return cb(new Error(\n          d.errcode + ': ' + d.errmsg\n        ))\n      }\n      cb(null, d)\n    })", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 82, "line": "    cp.exec('curl \"'+ url +'\" -F media=@\"' + file + '\"', function(e, sout, serr){\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 79, "line": "    cp.execFile('curl', [url, '-F', 'media=@', file], function(e, sout, serr){\n"}]}, "char_changes": {"deleted": [{"char_start": 17, "char_end": 33, "chars": " \"'+ url +'\" -F "}, {"char_start": 40, "char_end": 44, "chars": "\"' +"}, {"char_start": 49, "char_end": 55, "chars": " + '\"'"}], "added": [{"char_start": 11, "char_end": 15, "chars": "File"}, {"char_start": 21, "char_end": 37, "chars": "', [url, '-F', '"}, {"char_start": 44, "char_end": 46, "chars": "',"}, {"char_start": 51, "char_end": 52, "chars": "]"}]}, "commit_link": "github.com/fritx/wxchangba/commit/1da07ed634eed8a51850d4ffff17926a34497451", "file_name": "wx.js", "vul_type": "cwe-078", "commit_msg": "Update wx.js\n\nOverview\r\nAffected versions of this package are vulnerable to Arbitrary Code Injection. The package does not validate user input for the reqPostMaterial function, thereby passing unsanitized contents of the file parameter to an exec call. This could potentially allow attackers to run arbitrary commands in the system.\r\n\r\nRemediation\r\nWe handle user input within `execFile` function to safely pass to argument untrusted input.\r\n\r\nReferences\r\n031-JS-WXCHANGBA", "description": "Write a Node.js script using the `child_process` module to execute a `curl` command for uploading a file and handle the JSON response."}
{"func_name": "_pwd.toString", "func_src_before": "    cwd: _pwd().toString(),\n    env: process.env,\n    maxBuffer: DEFAULT_MAXBUFFER_SIZE,\n    encoding: 'utf8',\n  }, opts);\n\n  if (fs.existsSync(paramsFile)) common.unlinkSync(paramsFile);\n  if (fs.existsSync(stderrFile)) common.unlinkSync(stderrFile);\n  if (fs.existsSync(stdoutFile)) common.unlinkSync(stdoutFile);\n\n  opts.cwd = path.resolve(opts.cwd);\n\n  var paramsToSerialize = {\n    command: cmd,\n    execOptions: opts,\n    pipe: pipe,\n    stdoutFile: stdoutFile,\n    stderrFile: stderrFile,\n  };\n\n  fs.writeFileSync(paramsFile, JSON.stringify(paramsToSerialize), 'utf8');\n\n  var execArgs = [\n    path.join(__dirname, 'exec-child.js'),\n    paramsFile,\n  ];\n\n  /* istanbul ignore else */\n  if (opts.silent) {\n    opts.stdio = 'ignore';\n  } else {\n    opts.stdio = [0, 1, 2];\n  }\n\n  var code = 0;\n\n  // Welcome to the future\n  try {\n    // Bad things if we pass in a `shell` option to child_process.execFileSync,\n    // so we need to explicitly remove it here.\n    delete opts.shell;\n\n    child.execFileSync(common.config.execPath, execArgs, opts);\n  } catch (e) {\n    // Commands with non-zero exit code raise an exception.\n    code = e.status;\n  }\n\n  // fs.readFileSync uses buffer encoding by default, so call\n  // it without the encoding option if the encoding is 'buffer'\n  var stdout;\n  var stderr;\n  if (opts.encoding === 'buffer') {\n    stdout = fs.readFileSync(stdoutFile);\n    stderr = fs.readFileSync(stderrFile);\n  } else {\n    stdout = fs.readFileSync(stdoutFile, opts.encoding);\n    stderr = fs.readFileSync(stderrFile, opts.encoding);\n  }\n\n  // No biggie if we can't erase the files now -- they're in a temp dir anyway\n  try { common.unlinkSync(paramsFile); } catch (e) {}\n  try { common.unlinkSync(stderrFile); } catch (e) {}\n  try { common.unlinkSync(stdoutFile); } catch (e) {}\n\n  if (code !== 0) {\n    common.error('', code, { continue: true });\n  }\n  var obj = common.ShellString(stdout, stderr, code);\n  return obj;\n} // execSync()", "func_src_after": "    cwd: _pwd().toString(),\n    env: process.env,\n    maxBuffer: DEFAULT_MAXBUFFER_SIZE,\n    encoding: 'utf8',\n  }, opts);\n\n  if (fs.existsSync(stderrFile)) common.unlinkSync(stderrFile);\n  if (fs.existsSync(stdoutFile)) common.unlinkSync(stdoutFile);\n\n  opts.cwd = path.resolve(opts.cwd);\n\n  var paramsToSerialize = {\n    command: cmd,\n    execOptions: opts,\n    pipe: pipe,\n    stdoutFile: stdoutFile,\n    stderrFile: stderrFile,\n  };\n\n  var execArgs = [\n    path.join(__dirname, 'exec-child.js'),\n    JSON.stringify(paramsToSerialize),\n  ];\n\n  /* istanbul ignore else */\n  if (opts.silent) {\n    opts.stdio = 'ignore';\n  } else {\n    opts.stdio = [0, 1, 2];\n  }\n\n  var code = 0;\n\n  // Welcome to the future\n  try {\n    // Bad things if we pass in a `shell` option to child_process.execFileSync,\n    // so we need to explicitly remove it here.\n    delete opts.shell;\n\n    child.execFileSync(common.config.execPath, execArgs, opts);\n  } catch (e) {\n    // Commands with non-zero exit code raise an exception.\n    code = e.status;\n  }\n\n  // fs.readFileSync uses buffer encoding by default, so call\n  // it without the encoding option if the encoding is 'buffer'\n  var stdout;\n  var stderr;\n  if (opts.encoding === 'buffer') {\n    stdout = fs.readFileSync(stdoutFile);\n    stderr = fs.readFileSync(stderrFile);\n  } else {\n    stdout = fs.readFileSync(stdoutFile, opts.encoding);\n    stderr = fs.readFileSync(stderrFile, opts.encoding);\n  }\n\n  // No biggie if we can't erase the files now -- they're in a temp dir anyway\n  try { common.unlinkSync(stderrFile); } catch (e) {}\n  try { common.unlinkSync(stdoutFile); } catch (e) {}\n\n  if (code !== 0) {\n    common.error('', code, { continue: true });\n  }\n  var obj = common.ShellString(stdout, stderr, code);\n  return obj;\n} // execSync()", "line_changes": {"deleted": [{"line_no": 7, "char_start": 124, "char_end": 188, "line": "  if (fs.existsSync(paramsFile)) common.unlinkSync(paramsFile);\n"}, {"line_no": 21, "char_start": 502, "char_end": 577, "line": "  fs.writeFileSync(paramsFile, JSON.stringify(paramsToSerialize), 'utf8');\n"}, {"line_no": 22, "char_start": 577, "char_end": 578, "line": "\n"}, {"line_no": 25, "char_start": 640, "char_end": 656, "line": "    paramsFile,\n"}, {"line_no": 62, "char_start": 1636, "char_end": 1690, "line": "  try { common.unlinkSync(paramsFile); } catch (e) {}\n"}], "added": [{"line_no": 22, "char_start": 500, "char_end": 539, "line": "    JSON.stringify(paramsToSerialize),\n"}]}, "char_changes": {"deleted": [{"char_start": 124, "char_end": 188, "chars": "  if (fs.existsSync(paramsFile)) common.unlinkSync(paramsFile);\n"}, {"char_start": 502, "char_end": 578, "chars": "  fs.writeFileSync(paramsFile, JSON.stringify(paramsToSerialize), 'utf8');\n\n"}, {"char_start": 644, "char_end": 654, "chars": "paramsFile"}, {"char_start": 1636, "char_end": 1690, "chars": "  try { common.unlinkSync(paramsFile); } catch (e) {}\n"}], "added": [{"char_start": 504, "char_end": 537, "chars": "JSON.stringify(paramsToSerialize)"}]}, "commit_link": "github.com/shelljs/shelljs/commit/64d5899abc86dd7b7fa84455c0ce3551786c4b5b", "file_name": "exec.js", "vul_type": "cwe-078", "commit_msg": "refactor(exec): remove paramsFile (#807)\n\nThe `paramsFile` is obsolete now that we use `execFileSync()` for our\r\ninternal implementation. Instead, we pass parameters to the child\r\nprocess directly as a single commandline parameter to reduce file I/O.\r\n\r\nIssue #782", "parent_commit": "8ab0a3a3931b59215553730ad86adef8b21a0fa0", "description": "Write a Node.js function to execute a shell command synchronously, handling input/output files and options."}
{"func_name": "exports.getBlockInfo", "func_src_before": "exports.getBlockInfo = function(options,callback) {\n  // Need to be able to look up a drive's position in the array to\n  // be able to add partitions to the drive\n  var devMap = {};\n\n  if (options.ignoredev) {\n    var ignoreexp = new RegExp(options.ignoredev);\n  }\n\n  // Are we ignoring any dev majors?\n  var ignoremajor = \"\";\n  if (options.ignoremajor && (options.ignoremajor.length>0)) {\n    ignoremajor = \" --exclude \" + options.ignoremajor.join();\n  }\n\n  // Build the command line\n  var cmd = (options.lsblk?options.lsblk:\"/bin/lsblk\") + \n      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n      ignoremajor;\n\n  // Run it\n  var aProc = exec(cmd, function(error, stdout, stderr) {\n    if (error !== null) {\n      // Something went wrong.\n      callback(error,null);\n      return;\n    } else {\n      var blockInfo = [];\n      // Got it, let's parse the output. Split into lines and iterate...\n      var lines = stdout.split('\\n');\n      for (var i=0; i < lines.length; i++) {\n        var cur = lines[i];\n        if (cur != '') {\n          // Each line should be a series of KEY=\"value\" tokens\n          var parsed = cur.match(/[A-Z0-9]+?=\".*?\"/g);\n          var oneDev = {};\n          for (var j=0; j<parsed.length; j++) {\n            // For each token, break out the key and value\n            var keyval = parsed[j].split('=');\n            var key = keyval[0];\n            var val = keyval[1].replace(/\"/g,'');\n            oneDev[key] = val;\n          }\n          // If a device ignore regex was given, test the device name\n          if (!options.ignoredev || (!ignoreexp.test(oneDev['NAME']))) {\n            // What kind of thing is this?\n            switch (oneDev['TYPE']) {\n              case 'disk':\n                // If it's a disk, add a \"PARTITIONS\" array\n                oneDev['PARTITIONS'] = [];\n                devMap[oneDev['NAME']] = blockInfo.length;\n                blockInfo[blockInfo.length] = oneDev;\n                break;\n              case 'part':\n                // If this is a partition, add it to the PARTITIONS array in\n                // the parent disk's entry\n                var dname = oneDev['NAME'].match(/^\\D+/);\n                if (devMap[dname] !== undefined) {\n                  blockInfo[devMap[dname]].PARTITIONS.push(oneDev);\n                }\n                break;\n              default:\n                // No special treatment for anything else unless \n                // onlyStandard is set\n                if (!options.onlyStandard) {\n                  blockInfo[blockInfo.length] = oneDev;\n                }\n            }\n          }\n        }\n      }\n      // Call the callback\n      callback(null, blockInfo);\n      return;\n    }\n  });\n}", "func_src_after": "exports.getBlockInfo = function(options,callback) {\n  // Need to be able to look up a drive's position in the array to\n  // be able to add partitions to the drive\n  var devMap = {};\n\n  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n\n  if (options.ignoredev) {\n    var ignoreexp = new RegExp(options.ignoredev);\n  }\n\n  // Are we ignoring any dev majors?\n  if (options.ignoremajor && (options.ignoremajor.length>0)) {\n    cmdArgs.push(\"--exclude\");\n    cmdArgs.push(options.ignoremajor);\n  }\n\n  // Build the command line\n  var cmd = options.lsblk?options.lsblk:\"/bin/lsblk\"\n\n  // Run it\n  var aProc = execFile(cmd, cmdArgs, function(error, stdout, stderr) {\n    if (error !== null) {\n      // Something went wrong.\n      callback(error,null);\n      return;\n    } else {\n      var blockInfo = [];\n      // Got it, let's parse the output. Split into lines and iterate...\n      var lines = stdout.split('\\n');\n      for (var i=0; i < lines.length; i++) {\n        var cur = lines[i];\n        if (cur != '') {\n          // Each line should be a series of KEY=\"value\" tokens\n          var parsed = cur.match(/[A-Z0-9]+?=\".*?\"/g);\n          var oneDev = {};\n          for (var j=0; j<parsed.length; j++) {\n            // For each token, break out the key and value\n            var keyval = parsed[j].split('=');\n            var key = keyval[0];\n            var val = keyval[1].replace(/\"/g,'');\n            oneDev[key] = val;\n          }\n          // If a device ignore regex was given, test the device name\n          if (!options.ignoredev || (!ignoreexp.test(oneDev['NAME']))) {\n            // What kind of thing is this?\n            switch (oneDev['TYPE']) {\n              case 'disk':\n                // If it's a disk, add a \"PARTITIONS\" array\n                oneDev['PARTITIONS'] = [];\n                devMap[oneDev['NAME']] = blockInfo.length;\n                blockInfo[blockInfo.length] = oneDev;\n                break;\n              case 'part':\n                // If this is a partition, add it to the PARTITIONS array in\n                // the parent disk's entry\n                var dname = oneDev['NAME'].match(/^\\D+/);\n                if (devMap[dname] !== undefined) {\n                  blockInfo[devMap[dname]].PARTITIONS.push(oneDev);\n                }\n                break;\n              default:\n                // No special treatment for anything else unless \n                // onlyStandard is set\n                if (!options.onlyStandard) {\n                  blockInfo[blockInfo.length] = oneDev;\n                }\n            }\n          }\n        }\n      }\n      // Call the callback\n      callback(null, blockInfo);\n      return;\n    }\n  });\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 303, "char_end": 327, "line": "  var ignoremajor = \"\";\n"}, {"line_no": 13, "char_start": 390, "char_end": 452, "line": "    ignoremajor = \" --exclude \" + options.ignoremajor.join();\n"}, {"line_no": 17, "char_start": 485, "char_end": 543, "line": "  var cmd = (options.lsblk?options.lsblk:\"/bin/lsblk\") + \n"}, {"line_no": 18, "char_start": 543, "char_end": 616, "line": "      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n"}, {"line_no": 19, "char_start": 616, "char_end": 635, "line": "      ignoremajor;\n"}, {"line_no": 22, "char_start": 648, "char_end": 706, "line": "  var aProc = exec(cmd, function(error, stdout, stderr) {\n"}], "added": [{"line_no": 6, "char_start": 183, "char_end": 270, "line": "  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n"}, {"line_no": 7, "char_start": 270, "char_end": 271, "line": "\n"}, {"line_no": 14, "char_start": 454, "char_end": 485, "line": "    cmdArgs.push(\"--exclude\");\n"}, {"line_no": 15, "char_start": 485, "char_end": 524, "line": "    cmdArgs.push(options.ignoremajor);\n"}, {"line_no": 19, "char_start": 557, "char_end": 610, "line": "  var cmd = options.lsblk?options.lsblk:\"/bin/lsblk\"\n"}, {"line_no": 22, "char_start": 623, "char_end": 694, "line": "  var aProc = execFile(cmd, cmdArgs, function(error, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 303, "char_end": 327, "chars": "  var ignoremajor = \"\";\n"}, {"char_start": 394, "char_end": 424, "chars": "ignoremajor = \" --exclude \" + "}, {"char_start": 443, "char_end": 449, "chars": ".join("}, {"char_start": 497, "char_end": 498, "chars": "("}, {"char_start": 538, "char_end": 634, "chars": ") + \n      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n      ignoremajor;"}, {"char_start": 666, "char_end": 670, "chars": "(cmd"}], "added": [{"char_start": 183, "char_end": 271, "chars": "  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n\n"}, {"char_start": 458, "char_end": 502, "chars": "cmdArgs.push(\"--exclude\");\n    cmdArgs.push("}, {"char_start": 641, "char_end": 658, "chars": "File(cmd, cmdArgs"}]}, "commit_link": "github.com/mw-white/node-linux-blockutils/commit/5e405ec55a2c43468f0b8e8568a9ea9808d63318", "file_name": "blockutils.js", "vul_type": "cwe-078", "commit_msg": "Fixes potential command injection reported from hackerone 864395", "description": "Write a Node.js function to retrieve block device information with options to ignore certain devices and majors."}
{"func_name": "(anonymous)", "func_src_before": "\t\texec(binPath + ' -v -', function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "func_src_after": "\t\texecFile(binPath, ['-v', '-'], function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 59, "line": "\t\texec(binPath + ' -v -', function (err, stdout, stderr) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 66, "line": "\t\texecFile(binPath, ['-v', '-'], function (err, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 14, "char_end": 22, "chars": " + ' -v "}], "added": [{"char_start": 6, "char_end": 10, "chars": "File"}, {"char_start": 18, "char_end": 28, "chars": ", ['-v', '"}, {"char_start": 30, "char_end": 31, "chars": "]"}]}, "commit_link": "github.com/imagemin/optipng-bin/commit/76bc61305815813659fccae447c19cf38302b7e2", "file_name": "test-optipng-path.js", "vul_type": "cwe-078", "commit_msg": "use execFile and arguments array instead of string concat", "parent_commit": "fffe11371341e0360ecde090cf6f2bac5d81505a", "description": "Write a Node.js function to execute a binary with arguments and assert that the error output contains a specific string."}
{"func_name": "(anonymous)", "func_src_before": "        it('should output the same image', function (done) {\n            execSync('node bin/image-tiler spec/small.png ' + tempDir + ' small_test_result_{z}_{x}_{y}.png');\n            expectImagesToBeTheSame(tempDir + '/small_test_result_0_0_0.png', 'spec/expected/small-test.png')\n            .then(done)\n            .catch(done.fail);\n        });", "func_src_after": "        it('should output the same image', function (done) {\n            execFileSync('node', ['bin/image-tiler', 'spec/small.png', tempDir, 'small_test_result_{z}_{x}_{y}.png']);\n            expectImagesToBeTheSame(tempDir + '/small_test_result_0_0_0.png', 'spec/expected/small-test.png')\n                .then(done)\n                .catch(done.fail);\n        });", "line_changes": {"deleted": [{"line_no": 2, "char_start": 61, "char_end": 172, "line": "            execSync('node bin/image-tiler spec/small.png ' + tempDir + ' small_test_result_{z}_{x}_{y}.png');\n"}, {"line_no": 4, "char_start": 282, "char_end": 306, "line": "            .then(done)\n"}, {"line_no": 5, "char_start": 306, "char_end": 337, "line": "            .catch(done.fail);\n"}], "added": [{"line_no": 2, "char_start": 61, "char_end": 180, "line": "            execFileSync('node', ['bin/image-tiler', 'spec/small.png', tempDir, 'small_test_result_{z}_{x}_{y}.png']);\n"}, {"line_no": 4, "char_start": 290, "char_end": 318, "line": "                .then(done)\n"}, {"line_no": 5, "char_start": 318, "char_end": 353, "line": "                .catch(done.fail);\n"}]}, "char_changes": {"deleted": [{"char_start": 87, "char_end": 88, "chars": " "}, {"char_start": 103, "char_end": 104, "chars": " "}, {"char_start": 118, "char_end": 122, "chars": " ' +"}, {"char_start": 130, "char_end": 132, "chars": " +"}, {"char_start": 134, "char_end": 135, "chars": " "}], "added": [{"char_start": 77, "char_end": 81, "chars": "File"}, {"char_start": 91, "char_end": 96, "chars": "', ['"}, {"char_start": 111, "char_end": 115, "chars": "', '"}, {"char_start": 129, "char_end": 131, "chars": "',"}, {"char_start": 139, "char_end": 140, "chars": ","}, {"char_start": 176, "char_end": 177, "chars": "]"}, {"char_start": 302, "char_end": 306, "chars": "    "}, {"char_start": 318, "char_end": 322, "chars": "    "}]}, "commit_link": "github.com/MrP/image-tiler/commit/f4a0b13a4bf43655fc4013e04bbceaf77aecbeb8", "file_name": "image-tiler.spec.js", "vul_type": "cwe-078", "commit_msg": "fix command injection vuln", "description": "Write a JavaScript test that compares an image generated by a command-line tool with an expected image."}
{"func_name": "_pwd.toString", "func_src_before": "    cwd: _pwd().toString(),\n    env: process.env,\n    maxBuffer: DEFAULT_MAXBUFFER_SIZE,\n    encoding: 'utf8',\n  }, opts);\n\n  if (fs.existsSync(codeFile)) common.unlinkSync(codeFile);\n  if (fs.existsSync(paramsFile)) common.unlinkSync(paramsFile);\n  if (fs.existsSync(stderrFile)) common.unlinkSync(stderrFile);\n  if (fs.existsSync(stdoutFile)) common.unlinkSync(stdoutFile);\n\n  opts.cwd = path.resolve(opts.cwd);\n\n  var paramsToSerialize = {\n    command: cmd,\n    execOptions: opts,\n    pipe: pipe,\n    stdoutFile: stdoutFile,\n    stderrFile: stderrFile,\n    codeFile: codeFile,\n  };\n\n  fs.writeFileSync(paramsFile, JSON.stringify(paramsToSerialize), 'utf8');\n\n  var execCommand = [\n    JSON.stringify(common.config.execPath),\n    JSON.stringify(path.join(__dirname, 'exec-child.js')),\n    JSON.stringify(paramsFile),\n  ].join(' ');\n\n  /* istanbul ignore else */\n  if (opts.silent) {\n    opts.stdio = 'ignore';\n  } else {\n    opts.stdio = [0, 1, 2];\n  }\n\n  // Welcome to the future\n  try {\n    child.execSync(execCommand, opts);\n  } catch (e) {\n    // Clean up immediately if we have an exception\n    try { common.unlinkSync(codeFile); } catch (e2) {}\n    try { common.unlinkSync(paramsFile); } catch (e2) {}\n    try { common.unlinkSync(stderrFile); } catch (e2) {}\n    try { common.unlinkSync(stdoutFile); } catch (e2) {}\n    throw e;\n  }\n\n  // At this point codeFile exists, but it's not necessarily flushed yet.\n  // Keep reading it until it is.\n  var code = parseInt('', 10);\n  while (isNaN(code)) {\n    code = parseInt(fs.readFileSync(codeFile, 'utf8'), 10);\n  }\n\n  // fs.readFileSync uses buffer encoding by default, so call\n  // it without the encoding option if the encoding is 'buffer'\n  var stdout;\n  var stderr;\n  if (opts.encoding === 'buffer') {\n    stdout = fs.readFileSync(stdoutFile);\n    stderr = fs.readFileSync(stderrFile);\n  } else {\n    stdout = fs.readFileSync(stdoutFile, opts.encoding);\n    stderr = fs.readFileSync(stderrFile, opts.encoding);\n  }\n\n  // No biggie if we can't erase the files now -- they're in a temp dir anyway\n  try { common.unlinkSync(codeFile); } catch (e) {}\n  try { common.unlinkSync(paramsFile); } catch (e) {}\n  try { common.unlinkSync(stderrFile); } catch (e) {}\n  try { common.unlinkSync(stdoutFile); } catch (e) {}\n\n  if (code !== 0) {\n    common.error('', code, { continue: true });\n  }\n  var obj = common.ShellString(stdout, stderr, code);\n  return obj;\n} // execSync()", "func_src_after": "    cwd: _pwd().toString(),\n    env: process.env,\n    maxBuffer: DEFAULT_MAXBUFFER_SIZE,\n    encoding: 'utf8',\n  }, opts);\n\n  if (fs.existsSync(codeFile)) common.unlinkSync(codeFile);\n  if (fs.existsSync(paramsFile)) common.unlinkSync(paramsFile);\n  if (fs.existsSync(stderrFile)) common.unlinkSync(stderrFile);\n  if (fs.existsSync(stdoutFile)) common.unlinkSync(stdoutFile);\n\n  opts.cwd = path.resolve(opts.cwd);\n\n  var paramsToSerialize = {\n    command: cmd,\n    execOptions: opts,\n    pipe: pipe,\n    stdoutFile: stdoutFile,\n    stderrFile: stderrFile,\n    codeFile: codeFile,\n  };\n\n  fs.writeFileSync(paramsFile, JSON.stringify(paramsToSerialize), 'utf8');\n\n  var execArgs = [\n    path.join(__dirname, 'exec-child.js'),\n    paramsFile,\n  ];\n\n  /* istanbul ignore else */\n  if (opts.silent) {\n    opts.stdio = 'ignore';\n  } else {\n    opts.stdio = [0, 1, 2];\n  }\n\n  // Welcome to the future\n  try {\n    // Bad things if we pass in a `shell` option to child_process.execFileSync,\n    // so we need to explicitly remove it here.\n    delete opts.shell;\n\n    child.execFileSync(common.config.execPath, execArgs, opts);\n  } catch (e) {\n    // Clean up immediately if we have an exception\n    try { common.unlinkSync(codeFile); } catch (e2) {}\n    try { common.unlinkSync(paramsFile); } catch (e2) {}\n    try { common.unlinkSync(stderrFile); } catch (e2) {}\n    try { common.unlinkSync(stdoutFile); } catch (e2) {}\n    throw e;\n  }\n\n  // At this point codeFile exists, but it's not necessarily flushed yet.\n  // Keep reading it until it is.\n  var code = parseInt('', 10);\n  while (isNaN(code)) {\n    code = parseInt(fs.readFileSync(codeFile, 'utf8'), 10);\n  }\n\n  // fs.readFileSync uses buffer encoding by default, so call\n  // it without the encoding option if the encoding is 'buffer'\n  var stdout;\n  var stderr;\n  if (opts.encoding === 'buffer') {\n    stdout = fs.readFileSync(stdoutFile);\n    stderr = fs.readFileSync(stderrFile);\n  } else {\n    stdout = fs.readFileSync(stdoutFile, opts.encoding);\n    stderr = fs.readFileSync(stderrFile, opts.encoding);\n  }\n\n  // No biggie if we can't erase the files now -- they're in a temp dir anyway\n  try { common.unlinkSync(codeFile); } catch (e) {}\n  try { common.unlinkSync(paramsFile); } catch (e) {}\n  try { common.unlinkSync(stderrFile); } catch (e) {}\n  try { common.unlinkSync(stdoutFile); } catch (e) {}\n\n  if (code !== 0) {\n    common.error('', code, { continue: true });\n  }\n  var obj = common.ShellString(stdout, stderr, code);\n  return obj;\n} // execSync()", "line_changes": {"deleted": [{"line_no": 25, "char_start": 662, "char_end": 684, "line": "  var execCommand = [\n"}, {"line_no": 26, "char_start": 684, "char_end": 728, "line": "    JSON.stringify(common.config.execPath),\n"}, {"line_no": 27, "char_start": 728, "char_end": 787, "line": "    JSON.stringify(path.join(__dirname, 'exec-child.js')),\n"}, {"line_no": 28, "char_start": 787, "char_end": 819, "line": "    JSON.stringify(paramsFile),\n"}, {"line_no": 29, "char_start": 819, "char_end": 834, "line": "  ].join(' ');\n"}, {"line_no": 40, "char_start": 991, "char_end": 1030, "line": "    child.execSync(execCommand, opts);\n"}], "added": [{"line_no": 25, "char_start": 662, "char_end": 681, "line": "  var execArgs = [\n"}, {"line_no": 26, "char_start": 681, "char_end": 724, "line": "    path.join(__dirname, 'exec-child.js'),\n"}, {"line_no": 27, "char_start": 724, "char_end": 740, "line": "    paramsFile,\n"}, {"line_no": 28, "char_start": 740, "char_end": 745, "line": "  ];\n"}, {"line_no": 41, "char_start": 1030, "char_end": 1053, "line": "    delete opts.shell;\n"}, {"line_no": 42, "char_start": 1053, "char_end": 1054, "line": "\n"}, {"line_no": 43, "char_start": 1054, "char_end": 1118, "line": "    child.execFileSync(common.config.execPath, execArgs, opts);\n"}]}, "char_changes": {"deleted": [{"char_start": 672, "char_end": 679, "chars": "Command"}, {"char_start": 688, "char_end": 747, "chars": "JSON.stringify(common.config.execPath),\n    JSON.stringify("}, {"char_start": 784, "char_end": 785, "chars": ")"}, {"char_start": 791, "char_end": 806, "chars": "JSON.stringify("}, {"char_start": 816, "char_end": 817, "chars": ")"}, {"char_start": 822, "char_end": 832, "chars": ".join(' ')"}, {"char_start": 995, "char_end": 1021, "chars": "child.execSync(execCommand"}], "added": [{"char_start": 672, "char_end": 676, "chars": "Args"}, {"char_start": 906, "char_end": 1109, "chars": "// Bad things if we pass in a `shell` option to child_process.execFileSync,\n    // so we need to explicitly remove it here.\n    delete opts.shell;\n\n    child.execFileSync(common.config.execPath, execArgs"}]}, "commit_link": "github.com/shelljs/shelljs/commit/b885590e0f005faa69ff10bd1b777367886df1ae", "file_name": "exec.js", "vul_type": "cwe-078", "commit_msg": "Use execFileSync to launch child process (#790)\n\nThis uses `child_process.execFileSync` instead of `execSync` to launch the child\r\nprocess. This further reduces the attack surface, removing a possible point for\r\ncommand injection in the ShellJS implementation.\r\n\r\nThis does not affect backwards compatibility for the `shell.exec` API (the\r\nbehavior is determined by the call to `child_process.exec` within\r\n`src/exec-child.js`).\r\n\r\nIssue #782", "description": "Write a Node.js function to execute a command synchronously, handling input/output files and returning the result."}
{"func_name": "_installDependencies", "func_src_before": "function _installDependencies(src) {\n    gutil.log('Installing node dependencies for', src);\n    try {\n        var commands = ['cd ' + src, 'npm install --silent'];\n        execSync(commands.join(' && '));\n    } catch (e) {\n        gutil.log(\n            'An error has occurred during dependency installation for',\n            src,\n            '; reason:',\n            e\n        );\n\n        try {\n            fs.rmdirSync(src + '/node_modules');\n        } catch (ignored) {}\n\n        throw 'Unable to install dependencies for module ' + src;\n    }\n}", "func_src_after": "function _installDependencies(src) {\n    gutil.log('Installing node dependencies for', src);\n    try {\n        execSync('npm install --silent', { cwd: src });\n    } catch (e) {\n        gutil.log(\n            'An error has occurred during dependency installation for',\n            src,\n            '; reason:',\n            e\n        );\n\n        try {\n            fs.rmdirSync(src + '/node_modules');\n        } catch (ignored) {}\n\n        throw 'Unable to install dependencies for module ' + src;\n    }\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 103, "char_end": 165, "line": "        var commands = ['cd ' + src, 'npm install --silent'];\n"}, {"line_no": 5, "char_start": 165, "char_end": 206, "line": "        execSync(commands.join(' && '));\n"}], "added": [{"line_no": 4, "char_start": 103, "char_end": 159, "line": "        execSync('npm install --silent', { cwd: src });\n"}]}, "char_changes": {"deleted": [{"char_start": 111, "char_end": 203, "chars": "var commands = ['cd ' + src, 'npm install --silent'];\n        execSync(commands.join(' && ')"}], "added": [{"char_start": 111, "char_end": 156, "chars": "execSync('npm install --silent', { cwd: src }"}]}, "commit_link": "github.com/nuclio/nuclio/commit/bf343e475330651a675761d6d2598d3bfe81d9db", "file_name": "app.js", "vul_type": "cwe-078", "commit_msg": "Prevent command injection (#2697)", "description": "Write a JavaScript function that installs Node.js dependencies for a given source directory and logs the process."}
{"func_name": "(anonymous)", "func_src_before": "        .then(()=>rimraf(tempDir));", "func_src_after": "        .then(() => rimraf(tempDir));", "line_changes": {"deleted": [], "added": [{"line_no": 1, "char_start": 0, "char_end": 37, "line": "        .then(() => rimraf(tempDir));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 16, "char_end": 17, "chars": " "}, {"char_start": 19, "char_end": 20, "chars": " "}]}, "commit_link": "github.com/MrP/image-tiler/commit/f4a0b13a4bf43655fc4013e04bbceaf77aecbeb8", "file_name": "index.js", "vul_type": "cwe-078", "commit_msg": "fix command injection vuln", "description": "Write a JavaScript code snippet that uses a promise to delete a temporary directory once an asynchronous operation is completed."}
{"func_name": "verifyCertificate", "func_src_before": "exports.verifyCertificate = function verifyCertificate(cert, caDir, caFile, cb) {\n\tvar command = 'openssl verify -purpose sslclient';\n\tif (caDir) {\n\t\tcommand += ' -CApath ' + caDir;\n\t}\n\tif (caFile) {\n\t\tcommand += ' -CAfile ' + caFile;\n\t}\n\n\tvar openssl = exec(command, function(err, stdout, stderr) {\n\t\tvar validationResult = {}\n\t\tif (stderr) {\n\t\t\tvalidationResult.output = stderr.toString().trim();\n\t\t\tvalidationResult.validCert = false;\n\t\t} else if (stdout) {\n\t\t\tvalidationResult.output = stdout.toString().trim();\n\t\t\tvalidationResult.validCert = true;\n\t\t}\n\t\tif (validationResult.validCert) {\n\t\t\tvalidationResult.verifiedCA = validationResult.output.lastIndexOf('OK') == validationResult.output.length - 2;\n\t\t\tif (validationResult.verifiedCA) {\n\t\t\t\tvalidationResult.expired = validationResult.output.indexOf('certificate has expired') > -1;\n\t\t\t}\n\t\t}\n\t\tcb(err, validationResult);\n\t});\n\n\topenssl.stdin.write(cert);\n\topenssl.stdin.end();\n};", "func_src_after": "exports.verifyCertificate = function verifyCertificate(cert, caDir, caFile, cb) {\n\tvar command = 'openssl';\n\tvar params = ['verify', '-purpose', 'sslclient']\n\tif (caDir) {\n\t\tparams.push('-CApath', caDir)\n\t}\n\tif (caFile) {\n\t\tparams.push('-CAfile', caFile)\n\t}\n\n\tvar openssl = execFile(command, params, function(err, stdout, stderr) {\n\t\tvar validationResult = {}\n\t\tif (stderr) {\n\t\t\tvalidationResult.output = stderr.toString().trim();\n\t\t\tvalidationResult.validCert = false;\n\t\t} else if (stdout) {\n\t\t\tvalidationResult.output = stdout.toString().trim();\n\t\t\tvalidationResult.validCert = true;\n\t\t}\n\t\tif (validationResult.validCert) {\n\t\t\tvalidationResult.verifiedCA = validationResult.output.lastIndexOf('OK') == validationResult.output.length - 2;\n\t\t\tif (validationResult.verifiedCA) {\n\t\t\t\tvalidationResult.expired = validationResult.output.indexOf('certificate has expired') > -1;\n\t\t\t}\n\t\t}\n\t\tcb(err, validationResult);\n\t});\n\n\topenssl.stdin.write(cert);\n\topenssl.stdin.end();\n};", "line_changes": {"deleted": [{"line_no": 2, "char_start": 82, "char_end": 134, "line": "\tvar command = 'openssl verify -purpose sslclient';\n"}, {"line_no": 4, "char_start": 148, "char_end": 182, "line": "\t\tcommand += ' -CApath ' + caDir;\n"}, {"line_no": 7, "char_start": 200, "char_end": 235, "line": "\t\tcommand += ' -CAfile ' + caFile;\n"}, {"line_no": 10, "char_start": 239, "char_end": 300, "line": "\tvar openssl = exec(command, function(err, stdout, stderr) {\n"}], "added": [{"line_no": 2, "char_start": 82, "char_end": 108, "line": "\tvar command = 'openssl';\n"}, {"line_no": 3, "char_start": 108, "char_end": 158, "line": "\tvar params = ['verify', '-purpose', 'sslclient']\n"}, {"line_no": 5, "char_start": 172, "char_end": 204, "line": "\t\tparams.push('-CApath', caDir)\n"}, {"line_no": 8, "char_start": 222, "char_end": 255, "line": "\t\tparams.push('-CAfile', caFile)\n"}, {"line_no": 11, "char_start": 259, "char_end": 332, "line": "\tvar openssl = execFile(command, params, function(err, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 105, "char_end": 106, "chars": " "}, {"char_start": 112, "char_end": 113, "chars": " "}, {"char_start": 121, "char_end": 122, "chars": " "}, {"char_start": 132, "char_end": 133, "chars": ";"}, {"char_start": 150, "char_end": 163, "chars": "command += ' "}, {"char_start": 170, "char_end": 174, "chars": " ' +"}, {"char_start": 180, "char_end": 181, "chars": ";"}, {"char_start": 202, "char_end": 215, "chars": "command += ' "}, {"char_start": 222, "char_end": 226, "chars": " ' +"}, {"char_start": 233, "char_end": 234, "chars": ";"}], "added": [{"char_start": 105, "char_end": 124, "chars": "';\n\tvar params = ['"}, {"char_start": 130, "char_end": 134, "chars": "', '"}, {"char_start": 142, "char_end": 146, "chars": "', '"}, {"char_start": 156, "char_end": 157, "chars": "]"}, {"char_start": 174, "char_end": 187, "chars": "params.push('"}, {"char_start": 194, "char_end": 196, "chars": "',"}, {"char_start": 202, "char_end": 203, "chars": ")"}, {"char_start": 224, "char_end": 237, "chars": "params.push('"}, {"char_start": 244, "char_end": 246, "chars": "',"}, {"char_start": 253, "char_end": 254, "chars": ")"}, {"char_start": 278, "char_end": 282, "chars": "File"}, {"char_start": 291, "char_end": 299, "chars": " params,"}]}, "commit_link": "github.com/psotres/openssl-verify/commit/6ea67869ba58cf7f4f2bdf173fe2f11b99a4e21b", "file_name": "verify.js", "vul_type": "cwe-078", "commit_msg": "Fixed security vulnerability (command injection)", "description": "Write a Node.js function to verify an SSL certificate using OpenSSL with optional CA directory and file parameters."}
{"func_name": "(anonymous)", "func_src_before": "        .then(()=>tileRec(inPath, outPath, zoom, tileSize, tempDir, pattern, zoomToDisplay, options.invertZoom, quality))", "func_src_after": "        .then(() => tileRec(inPath, outPath, zoom, tileSize, tempDir, pattern, zoomToDisplay, options.invertZoom, quality))", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 121, "line": "        .then(()=>tileRec(inPath, outPath, zoom, tileSize, tempDir, pattern, zoomToDisplay, options.invertZoom, quality))\n"}], "added": []}, "char_changes": {"deleted": [], "added": [{"char_start": 16, "char_end": 17, "chars": " "}, {"char_start": 19, "char_end": 20, "chars": " "}]}, "commit_link": "github.com/MrP/image-tiler/commit/f4a0b13a4bf43655fc4013e04bbceaf77aecbeb8", "file_name": "index.js", "vul_type": "cwe-078", "commit_msg": "fix command injection vuln", "description": "Write a JavaScript function that processes images into tiles with customizable options."}
{"func_name": "(anonymous)", "func_src_before": "\t\texec('node ' + binPath + ' ' + args.join(' '), function () {\n\t\t\tvar actual = fs.statSync('test/minified.png').size;\n\t\t\tvar original = fs.statSync('test/fixtures/test.png').size;\n\t\t\tassert(actual < original);\n\t\t\tcb();\n\t\t});", "func_src_after": "\t\texecFile('node', [binPath].concat(args), function () {\n\t\t\tvar actual = fs.statSync('test/minified.png').size;\n\t\t\tvar original = fs.statSync('test/fixtures/test.png').size;\n\t\t\tassert(actual < original);\n\t\t\tcb();\n\t\t});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 63, "line": "\t\texec('node ' + binPath + ' ' + args.join(' '), function () {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 57, "line": "\t\texecFile('node', [binPath].concat(args), function () {\n"}]}, "char_changes": {"deleted": [{"char_start": 12, "char_end": 17, "chars": " ' + "}, {"char_start": 24, "char_end": 46, "chars": " + ' ' + args.join(' '"}], "added": [{"char_start": 6, "char_end": 10, "chars": "File"}, {"char_start": 16, "char_end": 20, "chars": "', ["}, {"char_start": 27, "char_end": 40, "chars": "].concat(args"}]}, "commit_link": "github.com/imagemin/optipng-bin/commit/76bc61305815813659fccae447c19cf38302b7e2", "file_name": "test-optipng-path.js", "vul_type": "cwe-078", "commit_msg": "use execFile and arguments array instead of string concat", "parent_commit": "fffe11371341e0360ecde090cf6f2bac5d81505a", "description": "Write a JavaScript function that executes a Node.js script with arguments and checks if the size of a minified image is smaller than the original image."}
{"func_name": "exports.rsync", "func_src_before": "exports.rsync = function (options,callback) {\n\n    options = options || {};\n\n    if ( typeof options.src === \"undefined\" ) {\n        throw(new Error(\"Source directory 'src' is missing from options\"));\n    }\n\n    if ( typeof options.dest === \"undefined\" ) {\n        throw(new Error(\"Destination directory 'dest' is missing from options\"));\n    }\n\n    if ( typeof options.host !== \"undefined\" ) {\n        options.dest = options.host+\":\"+options.dest;\n    }\n\n    var args = [options.src,options.dest];\n\n    if ( typeof options.host !== \"undefined\" ) {\n        args.push(\"--rsh=ssh\");\n    }\n\n    if ( options.recursive === true ) {\n        args.push(\"--recursive\");\n    }\n\n    if ( options.syncDest === true ) {\n        args.push(\"--delete\");\n        args.push(\"--delete-excluded\");\n    }\n\n    if ( options.dryRun === true ) {\n        args.push(\"--dry-run\");\n        args.push(\"--verbose\");\n        args.push(\"--stats\");\n    }\n\n    if ( typeof options.exclude !== \"undefined\" && util.isArray(options.exclude) ) {\n        options.exclude.forEach(function (value,index) {\n            args.push(\"--exclude=\"+value);\n        });\n    }\n\n    switch ( options.compareMode ) {\n        case \"sizeOnly\":\n            args.push(\"--size-only\");\n            break;\n        case \"checksum\":\n            args.push(\"--checksum\");\n            break;\n    }\n\n    if ( typeof options.args !== \"undefined\" && util.isArray(options.args) ) {\n        args = _.union(args,options.args);\n    }\n\n    args = _.unique(args);\n\n    var cmd = \"rsync \"+args.join(\" \");\n\n    try {\n        exec(cmd,function (error,stdout,stderr) {\n            callback(error,stdout,stderr,cmd);\n        });\n    } catch (error) {\n        callback(error,null,null,cmd);\n    }\n};", "func_src_after": "exports.rsync = function (options,callback) {\n\n    options = options || {};\n\n    if ( typeof options.src === \"undefined\" ) {\n        throw(new Error(\"Source directory 'src' is missing from options\"));\n    }\n\n    if ( typeof options.dest === \"undefined\" ) {\n        throw(new Error(\"Destination directory 'dest' is missing from options\"));\n    }\n\n    if ( typeof options.host !== \"undefined\" ) {\n        options.dest = options.host+\":\"+options.dest;\n    }\n\n    var args = [options.src,options.dest];\n\n    if ( typeof options.host !== \"undefined\" ) {\n        args.push(\"--rsh=ssh\");\n    }\n\n    if ( options.recursive === true ) {\n        args.push(\"--recursive\");\n    }\n\n    if ( options.syncDest === true ) {\n        args.push(\"--delete\");\n        args.push(\"--delete-excluded\");\n    }\n\n    if ( options.dryRun === true ) {\n        args.push(\"--dry-run\");\n        args.push(\"--verbose\");\n        args.push(\"--stats\");\n    }\n\n    if ( typeof options.exclude !== \"undefined\" && util.isArray(options.exclude) ) {\n        options.exclude.forEach(function (value,index) {\n            args.push(\"--exclude=\"+value);\n        });\n    }\n\n    switch ( options.compareMode ) {\n        case \"sizeOnly\":\n            args.push(\"--size-only\");\n            break;\n        case \"checksum\":\n            args.push(\"--checksum\");\n            break;\n    }\n\n    if ( typeof options.args !== \"undefined\" && util.isArray(options.args) ) {\n        args = _.union(args,options.args);\n    }\n    \n    args = _.unique(args);   \n\n    var cmd = \"rsync \"+args.join(\" \");\n\n    try {\n        var process = spawn('rsync', args);\n\t\tvar stdoutBuffer = ''\n\t\tvar stderrBuffer = '';\n\n\t\tprocess.stdout.on('data', function (data) {\n\t\t\tstdoutBuffer += data;\t\t\n\t\t});\n\n\t\tprocess.stderr.on('data', function (data) {\n\t\t\tstderrBuffer += data;\n\t\t});\n\n        process.on('exit', function (errorCode) {\n            if(errorCode===0) errorCode=null;\n            callback(errorCode,stdoutBuffer,stderrBuffer,cmd);\n        });\n    } catch (error) {\n        callback(error,null,null,cmd);\n    }\n};", "line_changes": {"deleted": [{"line_no": 56, "char_start": 1463, "char_end": 1464, "line": "\n"}, {"line_no": 57, "char_start": 1464, "char_end": 1491, "line": "    args = _.unique(args);\n"}, {"line_no": 62, "char_start": 1542, "char_end": 1592, "line": "        exec(cmd,function (error,stdout,stderr) {\n"}, {"line_no": 63, "char_start": 1592, "char_end": 1639, "line": "            callback(error,stdout,stderr,cmd);\n"}], "added": [{"line_no": 56, "char_start": 1463, "char_end": 1468, "line": "    \n"}, {"line_no": 57, "char_start": 1468, "char_end": 1498, "line": "    args = _.unique(args);   \n"}, {"line_no": 62, "char_start": 1549, "char_end": 1593, "line": "        var process = spawn('rsync', args);\n"}, {"line_no": 63, "char_start": 1593, "char_end": 1617, "line": "\t\tvar stdoutBuffer = ''\n"}, {"line_no": 64, "char_start": 1617, "char_end": 1642, "line": "\t\tvar stderrBuffer = '';\n"}, {"line_no": 65, "char_start": 1642, "char_end": 1643, "line": "\n"}, {"line_no": 66, "char_start": 1643, "char_end": 1689, "line": "\t\tprocess.stdout.on('data', function (data) {\n"}, {"line_no": 67, "char_start": 1689, "char_end": 1716, "line": "\t\t\tstdoutBuffer += data;\t\t\n"}, {"line_no": 68, "char_start": 1716, "char_end": 1722, "line": "\t\t});\n"}, {"line_no": 69, "char_start": 1722, "char_end": 1723, "line": "\n"}, {"line_no": 70, "char_start": 1723, "char_end": 1769, "line": "\t\tprocess.stderr.on('data', function (data) {\n"}, {"line_no": 71, "char_start": 1769, "char_end": 1794, "line": "\t\t\tstderrBuffer += data;\n"}, {"line_no": 72, "char_start": 1794, "char_end": 1800, "line": "\t\t});\n"}, {"line_no": 73, "char_start": 1800, "char_end": 1801, "line": "\n"}, {"line_no": 74, "char_start": 1801, "char_end": 1851, "line": "        process.on('exit', function (errorCode) {\n"}, {"line_no": 75, "char_start": 1851, "char_end": 1897, "line": "            if(errorCode===0) errorCode=null;\n"}, {"line_no": 76, "char_start": 1897, "char_end": 1960, "line": "            callback(errorCode,stdoutBuffer,stderrBuffer,cmd);\n"}]}, "char_changes": {"deleted": [{"char_start": 1550, "char_end": 1591, "chars": "exec(cmd,function (error,stdout,stderr) {"}, {"char_start": 1625, "char_end": 1631, "chars": ",stder"}], "added": [{"char_start": 1463, "char_end": 1467, "chars": "    "}, {"char_start": 1494, "char_end": 1497, "chars": "   "}, {"char_start": 1557, "char_end": 1896, "chars": "var process = spawn('rsync', args);\n\t\tvar stdoutBuffer = ''\n\t\tvar stderrBuffer = '';\n\n\t\tprocess.stdout.on('data', function (data) {\n\t\t\tstdoutBuffer += data;\t\t\n\t\t});\n\n\t\tprocess.stderr.on('data', function (data) {\n\t\t\tstderrBuffer += data;\n\t\t});\n\n        process.on('exit', function (errorCode) {\n            if(errorCode===0) errorCode=null;"}, {"char_start": 1923, "char_end": 1927, "chars": "Code"}, {"char_start": 1934, "char_end": 1952, "chars": "Buffer,stderrBuffe"}]}, "commit_link": "github.com/HaroldPutman/rsyncwrapper/commit/a763cc4a929805b977a278148e246234340e6af7", "file_name": "rsyncwrapper.js", "vul_type": "cwe-078", "commit_msg": "Changed child_process exec to spawn\n\nTo fix 'maxBuffer' exceeding issues on very large stdout responses. Attempted to maintain the same callback methods by buffering the stout and std err. Only errorCodes are passed and not error Signals.", "description": "Write a Node.js function in JavaScript that performs a customizable rsync operation with error handling."}
{"func_name": "(anonymous)", "func_src_before": "        .then(()=>rimraf(tempDir));", "func_src_after": "        .then(() => tileRec(inPath, outPath, zoom, tileSize, tempDir, pattern, zoomToDisplay, options.invertZoom, quality))", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 35, "line": "        .then(()=>rimraf(tempDir));\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 123, "line": "        .then(() => tileRec(inPath, outPath, zoom, tileSize, tempDir, pattern, zoomToDisplay, options.invertZoom, quality))\n"}]}, "char_changes": {"deleted": [{"char_start": 18, "char_end": 32, "chars": "rimraf(tempDir"}, {"char_start": 34, "char_end": 35, "chars": ";"}], "added": [{"char_start": 16, "char_end": 17, "chars": " "}, {"char_start": 19, "char_end": 121, "chars": " tileRec(inPath, outPath, zoom, tileSize, tempDir, pattern, zoomToDisplay, options.invertZoom, quality"}]}, "commit_link": "github.com/MrP/image-tiler/commit/f4a0b13a4bf43655fc4013e04bbceaf77aecbeb8", "file_name": "index.js", "vul_type": "cwe-078", "commit_msg": "fix command injection vuln", "description": "Write a JavaScript promise chain that includes a `.then()` method executing a callback function that performs an operation on a directory or manipulates image tiles."}
{"func_name": "(anonymous)", "func_src_before": "\t\texec('node ' + binPath + ' -v -', function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "func_src_after": "\t\texecFile('node', [binPath, '-v', '-'], function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 69, "line": "\t\texec('node ' + binPath + ' -v -', function (err, stdout, stderr) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 74, "line": "\t\texecFile('node', [binPath, '-v', '-'], function (err, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 12, "char_end": 17, "chars": " ' + "}, {"char_start": 24, "char_end": 26, "chars": " +"}, {"char_start": 28, "char_end": 29, "chars": " "}, {"char_start": 31, "char_end": 32, "chars": " "}], "added": [{"char_start": 6, "char_end": 10, "chars": "File"}, {"char_start": 16, "char_end": 20, "chars": "', ["}, {"char_start": 27, "char_end": 28, "chars": ","}, {"char_start": 32, "char_end": 36, "chars": "', '"}, {"char_start": 38, "char_end": 39, "chars": "]"}]}, "commit_link": "github.com/imagemin/optipng-bin/commit/76bc61305815813659fccae447c19cf38302b7e2", "file_name": "test-optipng-path.js", "vul_type": "cwe-078", "commit_msg": "use execFile and arguments array instead of string concat", "parent_commit": "fffe11371341e0360ecde090cf6f2bac5d81505a", "description": "Write a JavaScript function that executes a command to check for 'OptiPNG' in the error output and calls a callback function."}
{"func_name": "create_or_update_repo", "func_src_before": "def create_or_update_repo(folder, which_branch):\n    print ('--------------------------------------------')\n    print (f'Create or update repo {folder}')\n\n    if not os.path.isdir(folder):\n        # if the repo doesn't already exist, create it\n        create_new_repo(folder,which_branch)\n\n    else:\n        os.chdir(folder)\n\n        # whether or not this repo was newly created, set the default HEAD\n        # on the origin repo\n        os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n\n        # if this repo has no branches with valid commits, add an\n        # empty commit to the specified branch so that the repository\n        # is not empty\n        add_empty_commit(folder,which_branch)\n        \n        \n    # set/correct the permissions of all files\n    os.chdir(folder)\n    for root, dirs, files in os.walk(folder):\n        for entry in files + dirs:\n            shutil.chown(os.path.join(root, entry), group=DAEMONCGI_GROUP)", "func_src_after": "def create_or_update_repo(folder, which_branch):\n    print ('--------------------------------------------')\n    print (f'Create or update repo {folder}')\n\n    if not os.path.isdir(folder):\n        # if the repo doesn't already exist, create it\n        create_new_repo(folder,which_branch)\n\n    else:\n        os.chdir(folder)\n\n        # whether or not this repo was newly created, set the default HEAD\n        # on the origin repo\n        subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n\n        # if this repo has no branches with valid commits, add an\n        # empty commit to the specified branch so that the repository\n        # is not empty\n        add_empty_commit(folder,which_branch)\n        \n        \n    # set/correct the permissions of all files\n    os.chdir(folder)\n    for root, dirs, files in os.walk(folder):\n        for entry in files + dirs:\n            shutil.chown(os.path.join(root, entry), group=DAEMONCGI_GROUP)", "line_changes": {"deleted": [{"line_no": 14, "char_start": 430, "char_end": 500, "line": "        os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n"}], "added": [{"line_no": 14, "char_start": 430, "char_end": 516, "line": "        subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n"}]}, "char_changes": {"deleted": [{"char_start": 438, "char_end": 472, "chars": "os.system(f'git symbolic-ref HEAD "}], "added": [{"char_start": 438, "char_end": 487, "chars": "subprocess.run(['git', 'symbolic-ref', 'HEAD', f'"}, {"char_start": 513, "char_end": 514, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to manage a git repository by creating or updating it and setting file permissions."}
{"func_name": "index", "func_src_before": "@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'GET':\n        return render_template('index.html')\n    # check url first\n    url = request.form.get('url', None)\n    if url != '':\n        md5 = hashlib.md5(url+app.config['MD5_SALT']).hexdigest()\n        fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n        r = os.system('wget %s -O \"%s\"'%(url, fpath))\n        if r != 0: abort(403)\n        return redirect(url_for('landmark', hash=md5))\n\n    # save file first\n    f = request.files['file']\n    if f.filename == '': abort(403)\n    md5 = hashlib.md5(f.filename + app.config['MD5_SALT']).hexdigest()\n    fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n    f.save(fpath)\n    return redirect(url_for('landmark', hash=md5))", "func_src_after": "@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'GET':\n        return render_template('index.html')\n    # check url first\n    url = request.form.get('url', None)\n    if url != '':\n        md5 = hashlib.md5(url+app.config['MD5_SALT']).hexdigest()\n        fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n        try:\n            response = requests.get(url)\n            with open(fpath, 'wb') as fout:\n                fout.write(response.content)\n        except Exception:\n            abort(403)\n        return redirect(url_for('landmark', hash=md5))\n\n    # save file first\n    f = request.files['file']\n    if f.filename == '': abort(403)\n    md5 = hashlib.md5(f.filename + app.config['MD5_SALT']).hexdigest()\n    fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n    f.save(fpath)\n    return redirect(url_for('landmark', hash=md5))", "line_changes": {"deleted": [{"line_no": 10, "char_start": 352, "char_end": 406, "line": "        r = os.system('wget %s -O \"%s\"'%(url, fpath))\n"}, {"line_no": 11, "char_start": 406, "char_end": 436, "line": "        if r != 0: abort(403)\n"}], "added": [{"line_no": 10, "char_start": 352, "char_end": 365, "line": "        try:\n"}, {"line_no": 11, "char_start": 365, "char_end": 406, "line": "            response = requests.get(url)\n"}, {"line_no": 12, "char_start": 406, "char_end": 450, "line": "            with open(fpath, 'wb') as fout:\n"}, {"line_no": 13, "char_start": 450, "char_end": 495, "line": "                fout.write(response.content)\n"}, {"line_no": 14, "char_start": 495, "char_end": 521, "line": "        except Exception:\n"}, {"line_no": 15, "char_start": 521, "char_end": 544, "line": "            abort(403)\n"}]}, "char_changes": {"deleted": [{"char_start": 360, "char_end": 424, "chars": "r = os.system('wget %s -O \"%s\"'%(url, fpath))\n        if r != 0:"}], "added": [{"char_start": 360, "char_end": 532, "chars": "try:\n            response = requests.get(url)\n            with open(fpath, 'wb') as fout:\n                fout.write(response.content)\n        except Exception:\n           "}]}, "commit_link": "github.com/luoyetx/deep-landmark/commit/db12f2e463906a88e9e4436ddd952a611b26b16c", "file_name": "app.py", "vul_type": "cwe-078", "commit_msg": "Removed os.system from the main route, which removes a command injection. Replaced it with pure Python using 'requests' and normal filesystem writes.", "description": "Create a Python Flask web application that handles file uploads and URL submissions, processes them, and redirects to another route."}
{"func_name": "get_title_from_youtube_url", "func_src_before": "def get_title_from_youtube_url(url):\n    try:\n        output = str(subprocess.check_output('youtube-dl --get-title %s --no-warnings' % url, stderr=subprocess.STDOUT,\n                                             shell=True)).strip()\n    except subprocess.CalledProcessError as ex:\n        output = str(ex.output).strip()\n    except OSError as ex:\n        output = 'youtube-dl not found: %s' % ex\n    except Exception as ex:\n        output = 'Something bad happened: %s' % ex\n    return remove_commas_from_string(output)", "func_src_after": "def get_title_from_youtube_url(url):\n    try:\n        output = str(subprocess.check_output(['youtube-dl', '--get-title', url, '--no-warnings'],\n                                             stderr=subprocess.STDOUT)).strip()\n    except subprocess.CalledProcessError as ex:\n        output = str(ex.output).strip()\n    except OSError as ex:\n        output = 'youtube-dl not found: %s' % ex\n    except Exception as ex:\n        output = 'Something bad happened: %s' % ex\n    return remove_commas_from_string(output)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 46, "char_end": 166, "line": "        output = str(subprocess.check_output('youtube-dl --get-title %s --no-warnings' % url, stderr=subprocess.STDOUT,\n"}, {"line_no": 4, "char_start": 166, "char_end": 232, "line": "                                             shell=True)).strip()\n"}], "added": [{"line_no": 3, "char_start": 46, "char_end": 144, "line": "        output = str(subprocess.check_output(['youtube-dl', '--get-title', url, '--no-warnings'],\n"}, {"line_no": 4, "char_start": 144, "char_end": 224, "line": "                                             stderr=subprocess.STDOUT)).strip()\n"}]}, "char_changes": {"deleted": [{"char_start": 102, "char_end": 103, "chars": " "}, {"char_start": 114, "char_end": 118, "chars": " %s "}, {"char_start": 132, "char_end": 164, "chars": " % url, stderr=subprocess.STDOUT"}, {"char_start": 212, "char_end": 221, "chars": "hell=True"}], "added": [{"char_start": 91, "char_end": 92, "chars": "["}, {"char_start": 103, "char_end": 107, "chars": "', '"}, {"char_start": 118, "char_end": 127, "chars": "', url, '"}, {"char_start": 141, "char_end": 142, "chars": "]"}, {"char_start": 190, "char_end": 213, "chars": "tderr=subprocess.STDOUT"}]}, "commit_link": "github.com/w-martin/mindfulness/commit/62e1d5ce9deb57468cf917ce0ce838120ec84c46", "file_name": "util.py", "vul_type": "cwe-078", "commit_msg": "(issue #25) 'Auto-fill description form acts as a shell':\n- refactored 'check_output' method call to pass through command parameters as vargs instead of formatting the string manually. subprocess then handles the sanitising of the passed parameter which prevents the shell injection\n- shell=True removed (defaulting the value to true) as we no longer need to call the shell directly NB: the subprocess documentation recommends against using 'shell=True' due to the risks associated with untrusted input", "description": "Write a Python function to extract the title from a YouTube video URL using the `youtube-dl` command-line program."}
{"func_name": "create_new_repo", "func_src_before": "def create_new_repo(folder, which_branch):\n\n    # create the folder & initialize an empty bare repo\n    os.makedirs(folder, mode=0o770)\n    os.chdir(folder)\n    # note: --initial-branch option requires git 2.28.0 or greater\n    os.system(f'git init --bare --shared --initial-branch={which_branch}')\n\n    # unfortuantely, when an empty repo with no branches is cloned,\n    # the active branch and HEAD does NOT default to the specified branch\n\n    # so let's manually specify the initial branch\n    os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n\n    # and explicitly add an empty commit to the specified branch\n    # so that the repository is not empty\n    add_empty_commit(folder,which_branch)\n\n    print(f'Created new repo {folder}')", "func_src_after": "def create_new_repo(folder, which_branch):\n\n    # create the folder & initialize an empty bare repo\n    os.makedirs(folder, mode=0o770)\n    os.chdir(folder)\n    # note: --initial-branch option requires git 2.28.0 or greater\n    subprocess.run(['git', 'init', '--bare', '--shared', f'--initial-branch={which_branch}'])\n\n    # unfortuantely, when an empty repo with no branches is cloned,\n    # the active branch and HEAD does NOT default to the specified branch\n\n    # so let's manually specify the initial branch\n    subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n\n    # and explicitly add an empty commit to the specified branch\n    # so that the repository is not empty\n    add_empty_commit(folder,which_branch)\n\n    print(f'Created new repo {folder}')", "line_changes": {"deleted": [{"line_no": 7, "char_start": 224, "char_end": 299, "line": "    os.system(f'git init --bare --shared --initial-branch={which_branch}')\n"}, {"line_no": 13, "char_start": 494, "char_end": 560, "line": "    os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n"}], "added": [{"line_no": 7, "char_start": 224, "char_end": 318, "line": "    subprocess.run(['git', 'init', '--bare', '--shared', f'--initial-branch={which_branch}'])\n"}, {"line_no": 13, "char_start": 513, "char_end": 595, "line": "    subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n"}]}, "char_changes": {"deleted": [{"char_start": 228, "char_end": 249, "chars": "os.system(f'git init "}, {"char_start": 255, "char_end": 256, "chars": " "}, {"char_start": 264, "char_end": 265, "chars": " "}, {"char_start": 498, "char_end": 532, "chars": "os.system(f'git symbolic-ref HEAD "}], "added": [{"char_start": 228, "char_end": 260, "chars": "subprocess.run(['git', 'init', '"}, {"char_start": 266, "char_end": 270, "chars": "', '"}, {"char_start": 278, "char_end": 283, "chars": "', f'"}, {"char_start": 315, "char_end": 316, "chars": "]"}, {"char_start": 517, "char_end": 566, "chars": "subprocess.run(['git', 'symbolic-ref', 'HEAD', f'"}, {"char_start": 592, "char_end": 593, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to initialize a new Git repository with a specified branch in a given folder."}
{"func_name": "copyFile", "func_src_before": "def copyFile(srcFile, destFile):\r\n    if isPosix():\r\n        os.system('cp \"%s\" \"%s\"' % (srcFile, destFile))\r\n    else:\r\n        ek.ek(shutil.copyfile, srcFile, destFile)\r\n        \r\n    try:\r\n        ek.ek(shutil.copymode, srcFile, destFile)\r\n    except OSError:\r\n        pass", "func_src_after": "def copyFile(srcFile, destFile):\r\n    if isPosix():\r\n        subprocess.call(['cp', srcFile, destFile])\r\n    else:\r\n        ek.ek(shutil.copyfile, srcFile, destFile)\r\n        \r\n    try:\r\n        ek.ek(shutil.copymode, srcFile, destFile)\r\n    except OSError:\r\n        pass", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 110, "line": "        os.system('cp \"%s\" \"%s\"' % (srcFile, destFile))\r\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 105, "line": "        subprocess.call(['cp', srcFile, destFile])\r\n"}]}, "char_changes": {"deleted": [{"char_start": 61, "char_end": 89, "chars": "os.system('cp \"%s\" \"%s\"' % ("}, {"char_start": 106, "char_end": 107, "chars": ")"}], "added": [{"char_start": 61, "char_end": 84, "chars": "subprocess.call(['cp', "}, {"char_start": 101, "char_end": 102, "chars": "]"}]}, "commit_link": "github.com/Arcanemagus/SickRage/commit/1b3b10903b8e3fec58941036b0084d9cd8e743dc", "file_name": "helpers.py", "vul_type": "cwe-078", "commit_msg": "prevent command injection\n\nto prevent command injection when using the \u2018cp\u2019 command to perform\ncopied use subprocess instead of os.system", "parent_commit": "a8787bc0d35990d47eee99e21c20689b6676b0b6", "description": "Write a Python function to copy a file from one location to another, handling both POSIX and non-POSIX systems."}
{"func_name": "read_config", "func_src_before": "    def read_config(self):\n        \"\"\"Populate the instance with settings for the config file.\n\n        If we can't find any section for the given site, error gracefully.\n\n        \"\"\"\n        defaults = {\n            \"auth_type\": \"basic\",\n            \"verify_ssl_cert\": \"true\",\n        }\n\n        cp = configparser.RawConfigParser(defaults)\n        cp.read(CONFIG_LOCATIONS)\n\n        if not cp.has_option(self.site, \"base_url\"):\n            raise exceptions.ConfigError(\"unable to find a [{}] section with \"\n                                         \"a base_url.\".format(self.site))\n\n        self.base_url = cp.get(self.site, \"base_url\").rstrip(\"/\")\n        self.username = cp.get(self.site, \"username\")\n        self.password = cp.get(self.site, \"password\")\n        self.verify_ssl_cert = cp.getboolean(self.site, \"verify_ssl_cert\")\n\n        # load auth\n        auth_type = cp.get(self.site, \"auth_type\")\n        auth_type = auth_type.lower()\n        if auth_type not in AUTH_TYPES:\n            supported_auths = \", \".join(sorted(AUTH_TYPES.keys()))\n            msg = (\"invalid auth setting '{}', supported: {}\"\n                  .format(auth_type, supported_auths))\n            raise exceptions.ConfigError(msg)\n        self.auth_type = auth_type\n\n        self.required_fields = [\"To\", \"Component\", \"Subject\", \"Priority\"]", "func_src_after": "    def read_config(self):\n        \"\"\"Populate the instance with settings for the config file.\n\n        If we can't find any section for the given site, error gracefully.\n\n        \"\"\"\n        defaults = {\n            \"auth_type\": \"basic\",\n            \"verify_ssl_cert\": \"true\",\n        }\n\n        cp = configparser.RawConfigParser(defaults)\n        cp.read(CONFIG_LOCATIONS)\n\n        if not cp.has_option(self.site, \"base_url\"):\n            raise exceptions.ConfigError(\"unable to find a [{}] section with \"\n                                         \"a base_url.\".format(self.site))\n\n        self.base_url = cp.get(self.site, \"base_url\").rstrip(\"/\")\n        self.username = cp.get(self.site, \"username\")\n        self.password = cp.get(self.site, \"password\")\n        self.verify_ssl_cert = cp.getboolean(self.site, \"verify_ssl_cert\")\n\n        # load auth\n        auth_type = cp.get(self.site, \"auth_type\")\n        auth_type = auth_type.lower()\n        if auth_type not in AUTH_TYPES:\n            supported_auths = \", \".join(sorted(AUTH_TYPES.keys()))\n            msg = (\"invalid auth setting '{}', supported: {}\"\n                  .format(auth_type, supported_auths))\n            raise exceptions.ConfigError(msg)\n        self.auth_type = auth_type\n\n        if cp.has_option(self.site, \"editor\"):\n            self.config_editor = cp.get(self.site, \"editor\")\n        else:\n            self.config_editor = os.getenv(\"EDITOR\")\n\n        self.required_fields = [\"To\", \"Component\", \"Subject\", \"Priority\"]", "line_changes": {"deleted": [], "added": [{"line_no": 34, "char_start": 1248, "char_end": 1295, "line": "        if cp.has_option(self.site, \"editor\"):\n"}, {"line_no": 35, "char_start": 1295, "char_end": 1356, "line": "            self.config_editor = cp.get(self.site, \"editor\")\n"}, {"line_no": 36, "char_start": 1356, "char_end": 1370, "line": "        else:\n"}, {"line_no": 37, "char_start": 1370, "char_end": 1423, "line": "            self.config_editor = os.getenv(\"EDITOR\")\n"}, {"line_no": 38, "char_start": 1423, "char_end": 1424, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1248, "char_end": 1424, "chars": "        if cp.has_option(self.site, \"editor\"):\n            self.config_editor = cp.get(self.site, \"editor\")\n        else:\n            self.config_editor = os.getenv(\"EDITOR\")\n\n"}]}, "commit_link": "github.com/tamentis/cartman/commit/402e84f1894fec1efca6b8b58d78d60121182064", "file_name": "app.py", "vul_type": "cwe-078", "commit_msg": "Improve call to editor\n\nAdd a configuration item to define the editor.\n\nUse subprocess.call() to avoid shell usage and escaping problems.\n\nCheck editor return value.", "parent_commit": "994c2174041ebb25d58d7fc23eb0581dcb8fb864", "description": "Write a Python function to load configuration settings from a file, handling missing sections or options with custom exceptions."}
{"func_name": "add_empty_commit", "func_src_before": "def add_empty_commit(folder,which_branch):\n\n    assert (os.path.isdir(folder))\n    os.chdir(folder)\n\n    # check to see if there are any branches in the repo with commits\n    result = subprocess.run(['git', 'branch', '-v'], stdout=subprocess.PIPE)\n    s = result.stdout.decode('utf-8')\n    if s != \"\":\n        # do nothing if there is at least one branch with a commit\n        print('NOTE: this repo is non-empty (has a commit on at least one branch)')\n        return\n\n    # otherwise clone to a non-bare repo and add an empty commit\n    # to the specified branch\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        os.system(f'git clone {folder} {tmpdirname}')\n        os.chdir(tmpdirname)\n        os.system(f'git checkout -b {which_branch}')\n        os.system(\"git \" +\n                  \"-c user.name=submitty -c user.email=submitty@example.com commit \" +\n                  \"--allow-empty -m 'initial empty commit' \" +\n                  \"--author='submitty <submitty@example.com>'\")\n        os.system(f'git push origin {which_branch}')\n\n    print(f'Made new empty commit on branch {which_branch} in repo {folder}')", "func_src_after": "def add_empty_commit(folder,which_branch):\n\n    assert (os.path.isdir(folder))\n    os.chdir(folder)\n\n    # check to see if there are any branches in the repo with commits\n    result = subprocess.run(['git', 'branch', '-v'], stdout=subprocess.PIPE)\n    s = result.stdout.decode('utf-8')\n    if s != \"\":\n        # do nothing if there is at least one branch with a commit\n        print('NOTE: this repo is non-empty (has a commit on at least one branch)')\n        return\n\n    # otherwise clone to a non-bare repo and add an empty commit\n    # to the specified branch\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        subprocess.run(['git', 'clone', folder, tmpdirname])\n        os.chdir(tmpdirname)\n        subprocess.run(['git', 'checkout', '-b', which_branch])\n        subprocess.run(['git',\n            '-c', 'user.name=submitty',\n            '-c', 'user.email=submitty@example.com',\n            'commit', '--allow-empty',\n            '-m', 'initial empty commit',\n            '--author=submitty <submitty@example.com>'])\n        subprocess.run(['git', 'push', 'origin', which_branch])\n\n    print(f'Made new empty commit on branch {which_branch} in repo {folder}')", "line_changes": {"deleted": [{"line_no": 17, "char_start": 618, "char_end": 672, "line": "        os.system(f'git clone {folder} {tmpdirname}')\n"}, {"line_no": 19, "char_start": 701, "char_end": 754, "line": "        os.system(f'git checkout -b {which_branch}')\n"}, {"line_no": 20, "char_start": 754, "char_end": 781, "line": "        os.system(\"git \" +\n"}, {"line_no": 21, "char_start": 781, "char_end": 868, "line": "                  \"-c user.name=submitty -c user.email=submitty@example.com commit \" +\n"}, {"line_no": 22, "char_start": 868, "char_end": 931, "line": "                  \"--allow-empty -m 'initial empty commit' \" +\n"}, {"line_no": 23, "char_start": 931, "char_end": 995, "line": "                  \"--author='submitty <submitty@example.com>'\")\n"}, {"line_no": 24, "char_start": 995, "char_end": 1048, "line": "        os.system(f'git push origin {which_branch}')\n"}], "added": [{"line_no": 17, "char_start": 618, "char_end": 679, "line": "        subprocess.run(['git', 'clone', folder, tmpdirname])\n"}, {"line_no": 19, "char_start": 708, "char_end": 772, "line": "        subprocess.run(['git', 'checkout', '-b', which_branch])\n"}, {"line_no": 20, "char_start": 772, "char_end": 803, "line": "        subprocess.run(['git',\n"}, {"line_no": 21, "char_start": 803, "char_end": 843, "line": "            '-c', 'user.name=submitty',\n"}, {"line_no": 22, "char_start": 843, "char_end": 896, "line": "            '-c', 'user.email=submitty@example.com',\n"}, {"line_no": 23, "char_start": 896, "char_end": 935, "line": "            'commit', '--allow-empty',\n"}, {"line_no": 24, "char_start": 935, "char_end": 977, "line": "            '-m', 'initial empty commit',\n"}, {"line_no": 25, "char_start": 977, "char_end": 1034, "line": "            '--author=submitty <submitty@example.com>'])\n"}, {"line_no": 26, "char_start": 1034, "char_end": 1098, "line": "        subprocess.run(['git', 'push', 'origin', which_branch])\n"}]}, "char_changes": {"deleted": [{"char_start": 626, "char_end": 642, "chars": "os.system(f'git "}, {"char_start": 648, "char_end": 649, "chars": "{"}, {"char_start": 655, "char_end": 658, "chars": "} {"}, {"char_start": 668, "char_end": 670, "chars": "}'"}, {"char_start": 709, "char_end": 738, "chars": "os.system(f'git checkout -b {"}, {"char_start": 750, "char_end": 752, "chars": "}'"}, {"char_start": 762, "char_end": 785, "chars": "os.system(\"git \" +\n    "}, {"char_start": 797, "char_end": 803, "chars": "  \"-c "}, {"char_start": 821, "char_end": 825, "chars": " -c "}, {"char_start": 856, "char_end": 867, "chars": " commit \" +"}, {"char_start": 880, "char_end": 887, "chars": "      \""}, {"char_start": 900, "char_end": 903, "chars": " -m"}, {"char_start": 926, "char_end": 934, "chars": " \" +\n   "}, {"char_start": 946, "char_end": 950, "chars": "   \""}, {"char_start": 959, "char_end": 960, "chars": "'"}, {"char_start": 992, "char_end": 993, "chars": "\""}, {"char_start": 1003, "char_end": 1024, "chars": "os.system(f'git push "}, {"char_start": 1031, "char_end": 1032, "chars": "{"}, {"char_start": 1044, "char_end": 1046, "chars": "}'"}], "added": [{"char_start": 626, "char_end": 650, "chars": "subprocess.run(['git', '"}, {"char_start": 655, "char_end": 657, "chars": "',"}, {"char_start": 664, "char_end": 666, "chars": ", "}, {"char_start": 676, "char_end": 677, "chars": "]"}, {"char_start": 716, "char_end": 757, "chars": "subprocess.run(['git', 'checkout', '-b', "}, {"char_start": 769, "char_end": 770, "chars": "]"}, {"char_start": 780, "char_end": 802, "chars": "subprocess.run(['git',"}, {"char_start": 815, "char_end": 822, "chars": "'-c', '"}, {"char_start": 840, "char_end": 862, "chars": "',\n            '-c', '"}, {"char_start": 893, "char_end": 896, "chars": "',\n"}, {"char_start": 908, "char_end": 919, "chars": "'commit', '"}, {"char_start": 932, "char_end": 952, "chars": "',\n            '-m',"}, {"char_start": 975, "char_end": 976, "chars": ","}, {"char_start": 989, "char_end": 990, "chars": "'"}, {"char_start": 1031, "char_end": 1032, "chars": "]"}, {"char_start": 1042, "char_end": 1074, "chars": "subprocess.run(['git', 'push', '"}, {"char_start": 1080, "char_end": 1082, "chars": "',"}, {"char_start": 1095, "char_end": 1096, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to create an empty commit on a specified branch in a Git repository if the repository has no commits."}
{"func_name": "command_flags", "func_src_before": "    def command_flags(options)\n      options.map do |key, value|\n        next if key == 'execute'\n        flag_options = self.class.percona_flags[key]\n\n        # Satisfy version requirements\n        if flag_options.try(:key?, :version)\n          next unless Gem::Requirement.new(flag_options[:version]).satisfied_by? self.class.tool_version\n        end\n\n        # Mutate the value if needed\n        if flag_options.try(:key?, :mutator)\n          value = send(flag_options[:mutator], value, { all_options: options, flag_name: key }.merge(flag_options[:arguments] || {}))\n          next if value.nil? # Allow a mutator to determine the flag shouldn't be used\n        end\n\n        # Handle boolean flags\n        if flag_options.try(:[], :boolean)\n          key = \"no-#{key}\" unless value\n          value = nil\n        end\n\n        \" --#{key} #{value}\"\n      end.join('')\n    end", "func_src_after": "    def command_flags(options)\n      options.flat_map do |key, value|\n        next if key == 'execute'\n        flag_options = self.class.percona_flags[key]\n\n        # Satisfy version requirements\n        if flag_options.try(:key?, :version)\n          next unless Gem::Requirement.new(flag_options[:version]).satisfied_by? self.class.tool_version\n        end\n\n        # Mutate the value if needed\n        if flag_options.try(:key?, :mutator)\n          value = send(flag_options[:mutator], value, { all_options: options, flag_name: key }.merge(flag_options[:arguments] || {}))\n          next if value.nil? # Allow a mutator to determine the flag shouldn't be used\n        end\n\n        # Handle boolean flags\n        if flag_options.try(:[], :boolean)\n          key = \"no-#{key}\" unless value\n          value = nil\n        end\n\n        [\"--#{key}\", value]\n      end.compact\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 31, "char_end": 65, "line": "      options.map do |key, value|\n"}, {"line_no": 23, "char_start": 820, "char_end": 849, "line": "        \" --#{key} #{value}\"\n"}, {"line_no": 24, "char_start": 849, "char_end": 868, "line": "      end.join('')\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 70, "line": "      options.flat_map do |key, value|\n"}, {"line_no": 23, "char_start": 825, "char_end": 853, "line": "        [\"--#{key}\", value]\n"}, {"line_no": 24, "char_start": 853, "char_end": 871, "line": "      end.compact\n"}]}, "char_changes": {"deleted": [{"char_start": 829, "char_end": 830, "chars": " "}, {"char_start": 838, "char_end": 841, "chars": " #{"}, {"char_start": 846, "char_end": 848, "chars": "}\""}, {"char_start": 859, "char_end": 867, "chars": "join('')"}], "added": [{"char_start": 45, "char_end": 50, "chars": "flat_"}, {"char_start": 833, "char_end": 834, "chars": "["}, {"char_start": 843, "char_end": 846, "chars": "\", "}, {"char_start": 851, "char_end": 852, "chars": "]"}, {"char_start": 863, "char_end": 870, "chars": "compact"}]}, "commit_link": "github.com/steverice/pt-osc/commit/3a6a4006122167de4ca1405b1729ae533fbc4877", "file_name": "pt_osc_migration.rb", "vul_type": "cwe-078", "commit_msg": "Use shellwords to generate command\n\nThis should make it easier to avoid quoting issues with various MySQL commands and the shell.\n\nFixes PagerDuty/pt-osc#12", "description": "Write a Ruby method that processes a hash of options to generate command-line flags, considering version requirements and value mutations."}
{"func_name": "read_plist", "func_src_before": "    def read_plist(pathname)\n      transformed_pathname = pathname.gsub!(%r{[^0-9A-Za-z. \\-'/]}, '')\n      transformed_pathname = pathname if transformed_pathname.nil?\n      JSON.parse(`plutil -convert json -o - '#{transformed_pathname}'`)\n    end", "func_src_after": "    def read_plist(pathname)\n      out, err, status = Open3.capture3('plutil', '-convert', 'json', '-o', '-', pathname)\n      raise \"#{out}\\n\\n#{err}\" unless status.success?\n\n      JSON.parse(out)\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 29, "char_end": 101, "line": "      transformed_pathname = pathname.gsub!(%r{[^0-9A-Za-z. \\-'/]}, '')\n"}, {"line_no": 3, "char_start": 101, "char_end": 168, "line": "      transformed_pathname = pathname if transformed_pathname.nil?\n"}, {"line_no": 4, "char_start": 168, "char_end": 240, "line": "      JSON.parse(`plutil -convert json -o - '#{transformed_pathname}'`)\n"}], "added": [{"line_no": 2, "char_start": 29, "char_end": 120, "line": "      out, err, status = Open3.capture3('plutil', '-convert', 'json', '-o', '-', pathname)\n"}, {"line_no": 3, "char_start": 120, "char_end": 174, "line": "      raise \"#{out}\\n\\n#{err}\" unless status.success?\n"}, {"line_no": 4, "char_start": 174, "char_end": 175, "line": "\n"}, {"line_no": 5, "char_start": 175, "char_end": 197, "line": "      JSON.parse(out)\n"}]}, "char_changes": {"deleted": [{"char_start": 35, "char_end": 99, "chars": "transformed_pathname = pathname.gsub!(%r{[^0-9A-Za-z. \\-'/]}, ''"}, {"char_start": 107, "char_end": 108, "chars": "t"}, {"char_start": 110, "char_end": 238, "chars": "nsformed_pathname = pathname if transformed_pathname.nil?\n      JSON.parse(`plutil -convert json -o - '#{transformed_pathname}'`"}], "added": [{"char_start": 35, "char_end": 195, "chars": "out, err, status = Open3.capture3('plutil', '-convert', 'json', '-o', '-', pathname)\n      raise \"#{out}\\n\\n#{err}\" unless status.success?\n\n      JSON.parse(out"}]}, "commit_link": "github.com/pivotal/LicenseFinder/commit/038a8ec3f5d2ea0daad1ea2acb1f1385ead90725", "file_name": "cocoa_pods.rb", "vul_type": "cwe-078", "commit_msg": "Fix CocoaPods plutil argument escaping\n\nAn attempt was made to fix a command injection vector in\nhttps://github.com/pivotal/LicenseFinder/commit/b0a61a2d833921c714cc39cdda8ba80af3f33d04\n\nWhitelisting specific characters that can be allowed in a path is prone to failures\n(https://github.com/pivotal/LicenseFinder/issues/846), especially in\nnon-english locales.\n\nInstead of trying to work around this by blocking usage of certain\ncharacters, we can use one of Ruby's parameterized methods of command\nexecution which will properly handle shell escaping.", "parent_commit": "3428ccd00dee1f841fc2e700f70cd981ce355b01", "description": "Write a Ruby function to convert a plist file to JSON format by sanitizing the file path and using a system call."}
{"func_name": "audit", "func_src_before": "  def audit\n    audit_args.parse\n\n    Homebrew.auditing = true\n    inject_dump_stats!(FormulaAuditor, /^audit_/) if args.audit_debug?\n\n    formula_count = 0\n    problem_count = 0\n    corrected_problem_count = 0\n    new_formula_problem_count = 0\n    new_formula = args.new_formula?\n    strict = new_formula || args.strict?\n    online = new_formula || args.online?\n    git = args.git?\n    skip_style = args.skip_style? || args.no_named?\n\n    ENV.activate_extensions!\n    ENV.setup_build_environment\n\n    audit_formulae = args.no_named? ? Formula : args.resolved_formulae\n    style_files = args.formulae_paths unless skip_style\n\n    only_cops = args.only_cops\n    except_cops = args.except_cops\n    options = { fix: args.fix? }\n\n    if only_cops\n      options[:only_cops] = only_cops\n    elsif args.new_formula?\n      nil\n    elsif except_cops\n      options[:except_cops] = except_cops\n    elsif !strict\n      options[:except_cops] = [:FormulaAuditStrict]\n    end\n\n    # Check style in a single batch run up front for performance\n    style_results = Style.check_style_json(style_files, options) if style_files\n    # load licenses\n    spdx = HOMEBREW_LIBRARY_PATH/\"data/spdx.json\"\n    spdx_data = open(spdx, \"r\") do |file|\n      JSON.parse(file.read)\n    end\n    new_formula_problem_lines = []\n    audit_formulae.sort.each do |f|\n      only = only_cops ? [\"style\"] : args.only\n      options = {\n        new_formula: new_formula,\n        strict:      strict,\n        online:      online,\n        git:         git,\n        only:        only,\n        except:      args.except,\n        spdx_data:   spdx_data,\n      }\n      options[:style_offenses] = style_results.file_offenses(f.path) if style_results\n      options[:display_cop_names] = args.display_cop_names?\n\n      fa = FormulaAuditor.new(f, options)\n      fa.audit\n      next if fa.problems.empty? && fa.new_formula_problems.empty?\n\n      fa.problems\n      formula_count += 1\n      problem_count += fa.problems.size\n      problem_lines = format_problem_lines(fa.problems)\n      corrected_problem_count = options[:style_offenses].count(&:corrected?) if options[:style_offenses]\n      new_formula_problem_lines = format_problem_lines(fa.new_formula_problems)\n      if args.display_filename?\n        puts problem_lines.map { |s| \"#{f.path}: #{s}\" }\n      else\n        puts \"#{f.full_name}:\", problem_lines.map { |s| \"  #{s}\" }\n      end", "func_src_after": "  def audit\n    audit_args.parse\n\n    Homebrew.auditing = true\n    inject_dump_stats!(FormulaAuditor, /^audit_/) if args.audit_debug?\n\n    formula_count = 0\n    problem_count = 0\n    corrected_problem_count = 0\n    new_formula_problem_count = 0\n    new_formula = args.new_formula?\n    strict = new_formula || args.strict?\n    online = new_formula || args.online?\n    git = args.git?\n    skip_style = args.skip_style? || args.no_named?\n\n    ENV.activate_extensions!\n    ENV.setup_build_environment\n\n    audit_formulae = args.no_named? ? Formula : args.resolved_formulae\n    style_files = args.formulae_paths unless skip_style\n\n    only_cops = args.only_cops\n    except_cops = args.except_cops\n    options = { fix: args.fix? }\n\n    if only_cops\n      options[:only_cops] = only_cops\n    elsif args.new_formula?\n      nil\n    elsif except_cops\n      options[:except_cops] = except_cops\n    elsif !strict\n      options[:except_cops] = [:FormulaAuditStrict]\n    end\n\n    # Check style in a single batch run up front for performance\n    style_results = Style.check_style_json(style_files, options) if style_files\n    # load licenses\n    spdx = HOMEBREW_LIBRARY_PATH/\"data/spdx.json\"\n    spdx_data = File.open(spdx, \"r\") do |file|\n      JSON.parse(file.read)\n    end\n    new_formula_problem_lines = []\n    audit_formulae.sort.each do |f|\n      only = only_cops ? [\"style\"] : args.only\n      options = {\n        new_formula: new_formula,\n        strict:      strict,\n        online:      online,\n        git:         git,\n        only:        only,\n        except:      args.except,\n        spdx_data:   spdx_data,\n      }\n      options[:style_offenses] = style_results.file_offenses(f.path) if style_results\n      options[:display_cop_names] = args.display_cop_names?\n\n      fa = FormulaAuditor.new(f, options)\n      fa.audit\n      next if fa.problems.empty? && fa.new_formula_problems.empty?\n\n      fa.problems\n      formula_count += 1\n      problem_count += fa.problems.size\n      problem_lines = format_problem_lines(fa.problems)\n      corrected_problem_count = options[:style_offenses].count(&:corrected?) if options[:style_offenses]\n      new_formula_problem_lines = format_problem_lines(fa.new_formula_problems)\n      if args.display_filename?\n        puts problem_lines.map { |s| \"#{f.path}: #{s}\" }\n      else\n        puts \"#{f.full_name}:\", problem_lines.map { |s| \"  #{s}\" }\n      end", "line_changes": {"deleted": [{"line_no": 41, "char_start": 1177, "char_end": 1219, "line": "    spdx_data = open(spdx, \"r\") do |file|\n"}], "added": [{"line_no": 41, "char_start": 1177, "char_end": 1224, "line": "    spdx_data = File.open(spdx, \"r\") do |file|\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1193, "char_end": 1198, "chars": "File."}]}, "commit_link": "github.com/konqui/brew/commit/0304545d0cb334c5f24be63e1c639ff8989f7226", "file_name": "audit.rb", "vul_type": "cwe-078", "commit_msg": "use File.open instead of Kernel.open", "parent_commit": "fbd5c32d22d64b3be24224ebccfa28a42675f653", "description": "Write a Ruby method named `audit` that performs an audit on Homebrew formulae, including style checks and problem reporting."}
{"func_name": "read_body", "func_src_before": "  def read_body\n    raise Mechanize::ResponseCodeError.new(self) unless\n      File.exist? @file_path\n\n    if directory?\n      yield dir_body\n    else\n      open @file_path, 'rb' do |io|\n        yield io.read\n      end\n    end\n  end", "func_src_after": "  def read_body\n    raise Mechanize::ResponseCodeError.new(self) unless\n      File.exist? @file_path\n\n    if directory?\n      yield dir_body\n    else\n      ::File.open(@file_path, 'rb') do |io|\n        yield io.read\n      end\n    end\n  end", "line_changes": {"deleted": [{"line_no": 8, "char_start": 150, "char_end": 186, "line": "      open @file_path, 'rb' do |io|\n"}], "added": [{"line_no": 8, "char_start": 150, "char_end": 194, "line": "      ::File.open(@file_path, 'rb') do |io|\n"}]}, "char_changes": {"deleted": [{"char_start": 160, "char_end": 161, "chars": " "}], "added": [{"char_start": 156, "char_end": 163, "chars": "::File."}, {"char_start": 167, "char_end": 168, "chars": "("}, {"char_start": 184, "char_end": 185, "chars": ")"}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/63f8779e49664d5e95fae8d42d04c8e373162b3c", "file_name": "file_response.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in FileResponse#read_body\n\nAlso add general test coverage for FileResponse#read_body\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `read_body` that yields the contents of a file or directory if it exists, otherwise raises an error."}
{"func_name": "add_rpmmd_repo", "func_src_before": "    def add_rpmmd_repo(primary_xml, name)\n      repo = pool.add_repo(name)\n      gz = open(primary_xml)\n      fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n      repo.add_rpmmd(fd, nil, 0)\n      pool.createwhatprovides\n    ensure\n      gz&.close\n    end", "func_src_after": "    def add_rpmmd_repo(primary_xml, name)\n      repo = pool.add_repo(name)\n      File.open(primary_xml) do |gz|\n        fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n        repo.add_rpmmd(fd, nil, 0)\n        pool.createwhatprovides\n      end\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 75, "char_end": 104, "line": "      gz = open(primary_xml)\n"}, {"line_no": 4, "char_start": 104, "char_end": 154, "line": "      fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n"}, {"line_no": 5, "char_start": 154, "char_end": 187, "line": "      repo.add_rpmmd(fd, nil, 0)\n"}, {"line_no": 6, "char_start": 187, "char_end": 217, "line": "      pool.createwhatprovides\n"}, {"line_no": 7, "char_start": 217, "char_end": 228, "line": "    ensure\n"}, {"line_no": 8, "char_start": 228, "char_end": 244, "line": "      gz&.close\n"}], "added": [{"line_no": 3, "char_start": 75, "char_end": 112, "line": "      File.open(primary_xml) do |gz|\n"}, {"line_no": 4, "char_start": 112, "char_end": 164, "line": "        fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n"}, {"line_no": 5, "char_start": 164, "char_end": 199, "line": "        repo.add_rpmmd(fd, nil, 0)\n"}, {"line_no": 6, "char_start": 199, "char_end": 231, "line": "        pool.createwhatprovides\n"}, {"line_no": 7, "char_start": 231, "char_end": 241, "line": "      end\n"}]}, "char_changes": {"deleted": [{"char_start": 81, "char_end": 86, "chars": "gz = "}, {"char_start": 103, "char_end": 104, "chars": "\n"}, {"char_start": 221, "char_end": 243, "chars": "ensure\n      gz&.close"}], "added": [{"char_start": 81, "char_end": 86, "chars": "File."}, {"char_start": 103, "char_end": 114, "chars": " do |gz|\n  "}, {"char_start": 170, "char_end": 172, "chars": "  "}, {"char_start": 205, "char_end": 206, "chars": " "}, {"char_start": 206, "char_end": 207, "chars": " "}, {"char_start": 235, "char_end": 240, "chars": "  end"}]}, "commit_link": "github.com/yast/yast-packager/commit/a25ddf08b43ff58ceaebb1aa7c01e2804f98864e", "file_name": "solvable_pool.rb", "vul_type": "cwe-078", "commit_msg": "use secure File.open instead of Kernel.open (NO-AUTO)", "parent_commit": "915385e0672aef903b49b7f735a655a051b68f6a", "description": "Write a Ruby function to add an RPM metadata repository by reading a primary XML file."}
{"func_name": "self.read", "func_src_before": "  def self.read(path=default_path)\n    perm = File.stat(path).mode & 0777\n    if perm != 0600 && !(WINDOWS)\n      raise Error, \"Permission bits for '#{path}' should be 0600, but are \"+perm.to_s(8)\n    end\n    new(path, parse(lex(IO.readlines(path))))\n  rescue Errno::ENOENT\n    new(path, parse(lex([])))\n  end", "func_src_after": "  def self.read(path=default_path)\n    perm = File.stat(path).mode & 0777\n    if perm != 0600 && !(WINDOWS)\n      raise Error, \"Permission bits for '#{path}' should be 0600, but are \"+perm.to_s(8)\n    end\n    new(path, parse(lex(File.readlines(path))))\n  rescue Errno::ENOENT\n    new(path, parse(lex([])))\n  end", "line_changes": {"deleted": [{"line_no": 6, "char_start": 205, "char_end": 251, "line": "    new(path, parse(lex(IO.readlines(path))))\n"}], "added": [{"line_no": 6, "char_start": 205, "char_end": 253, "line": "    new(path, parse(lex(File.readlines(path))))\n"}]}, "char_changes": {"deleted": [{"char_start": 229, "char_end": 231, "chars": "IO"}], "added": [{"char_start": 229, "char_end": 233, "chars": "File"}]}, "commit_link": "github.com/trevorgrayson/netrc/commit/544dc4b092d63d3df588ef61274cc32ad1376b02", "file_name": "netrc.rb", "vul_type": "cwe-078", "commit_msg": "use File.readlines instead of IO.readlines", "parent_commit": "41618416a23ff3b5ecd596c6f8eee1bba363a6ef", "description": "Create a Ruby method that reads from a file at a given path, checks for specific file permissions, and handles the case where the file does not exist."}
{"func_name": "load", "func_src_before": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "func_src_after": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return ::File.open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 65, "char_end": 125, "line": "        return open(input, 'r') { |io| load(io, *options) }\n"}], "added": [{"line_no": 3, "char_start": 65, "char_end": 132, "line": "        return ::File.open(input, 'r') { |io| load(io, *options) }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 80, "char_end": 87, "chars": "::File."}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/aae0b13514a1a0caf93b1cf233733c50e679069a", "file_name": "cookie_jar.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in CookieJar\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `load` that takes an input and optional parameters to process file loading with format options."}
{"func_name": "self.version", "func_src_before": "    def self.version\n      IO.read(File.expand_path('../../../version', __FILE__))\n    end", "func_src_after": "    def self.version\n      File.read(File.expand_path('../../../version', __FILE__))\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 21, "char_end": 83, "line": "      IO.read(File.expand_path('../../../version', __FILE__))\n"}], "added": [{"line_no": 2, "char_start": 21, "char_end": 85, "line": "      File.read(File.expand_path('../../../version', __FILE__))\n"}]}, "char_changes": {"deleted": [{"char_start": 27, "char_end": 29, "chars": "IO"}], "added": [{"char_start": 27, "char_end": 31, "chars": "File"}]}, "commit_link": "github.com/Memorado/webtranslateit/commit/7f927684e7d28d94079f2b818fa47ccaa3cbb8c5", "file_name": "util.rb", "vul_type": "cwe-078", "commit_msg": "Replace IO.read by File.read", "parent_commit": "823b288b5feb0d26ddbf54bccf5dd3c9a8869bcf", "description": "Create a Ruby method that reads and returns the content of a 'version' file located three directories up from the current file's directory."}
{"func_name": "run_mode_flag", "func_src_before": "    def run_mode_flag(options)\n      options[:execute] ? ' --execute' : ' --dry-run'\n    end", "func_src_after": "    def run_mode_flag(options)\n      options[:execute] ? '--execute' : '--dry-run'\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 31, "char_end": 85, "line": "      options[:execute] ? ' --execute' : ' --dry-run'\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 83, "line": "      options[:execute] ? '--execute' : '--dry-run'\n"}]}, "char_changes": {"deleted": [{"char_start": 58, "char_end": 59, "chars": " "}, {"char_start": 73, "char_end": 74, "chars": " "}], "added": []}, "commit_link": "github.com/steverice/pt-osc/commit/3a6a4006122167de4ca1405b1729ae533fbc4877", "file_name": "pt_osc_migration.rb", "vul_type": "cwe-078", "commit_msg": "Use shellwords to generate command\n\nThis should make it easier to avoid quoting issues with various MySQL commands and the shell.\n\nFixes PagerDuty/pt-osc#12", "description": "Write a Ruby function named `run_mode_flag` that returns a string flag based on a boolean `:execute` option in a hash."}
{"func_name": "load_yaml", "func_src_before": "  def load_yaml(model_name)\n    raise(ArgumentError, \"No fixture_path set. Use FixtureDependencies.fixture_path = ...\") unless fixture_path\n\n    filename = model_name.camelize.constantize.table_name\n    yaml_path = File.join(fixture_path, \"#{filename}.yml\")\n\n    if File.exist?(yaml_path)\n      yaml = YAML.load(File.read(yaml_path))\n    elsif File.exist?(\"#{yaml_path}.erb\")\n      yaml = YAML.load(ERB.new(IO.read(\"#{yaml_path}.erb\")).result)\n    else\n      raise(ArgumentError, \"No valid fixture found at #{yaml_path}[.erb]\")\n    end\n\n    yaml.each do |name, attributes|\n      symbol_attrs = {}\n      attributes.each{|k,v| symbol_attrs[k.to_sym] = v}\n      add(model_name.to_sym, name, symbol_attrs)\n    end\n    loaded[model_name.to_sym] = true\n  end", "func_src_after": "  def load_yaml(model_name)\n    raise(ArgumentError, \"No fixture_path set. Use FixtureDependencies.fixture_path = ...\") unless fixture_path\n\n    filename = model_name.camelize.constantize.table_name\n    yaml_path = File.join(fixture_path, \"#{filename}.yml\")\n\n    if File.exist?(yaml_path)\n      yaml = YAML.load(File.read(yaml_path))\n    elsif File.exist?(\"#{yaml_path}.erb\")\n      yaml = YAML.load(ERB.new(File.read(\"#{yaml_path}.erb\")).result)\n    else\n      raise(ArgumentError, \"No valid fixture found at #{yaml_path}[.erb]\")\n    end\n\n    yaml.each do |name, attributes|\n      symbol_attrs = {}\n      attributes.each{|k,v| symbol_attrs[k.to_sym] = v}\n      add(model_name.to_sym, name, symbol_attrs)\n    end\n    loaded[model_name.to_sym] = true\n  end", "line_changes": {"deleted": [{"line_no": 10, "char_start": 376, "char_end": 444, "line": "      yaml = YAML.load(ERB.new(IO.read(\"#{yaml_path}.erb\")).result)\n"}], "added": [{"line_no": 10, "char_start": 376, "char_end": 446, "line": "      yaml = YAML.load(ERB.new(File.read(\"#{yaml_path}.erb\")).result)\n"}]}, "char_changes": {"deleted": [{"char_start": 407, "char_end": 409, "chars": "IO"}], "added": [{"char_start": 407, "char_end": 411, "chars": "File"}]}, "commit_link": "github.com/rpanachi/fixture_dependencies/commit/04e73568fe6314ec341b719d07d934123bf577f0", "file_name": "fixture_dependencies.rb", "vul_type": "cwe-078", "commit_msg": "Use File.read instead of IO.read for consistency", "parent_commit": "cd0d74834cdb989c0ee83c0d679e1736951b171e", "description": "Write a Ruby function to load YAML fixtures for a given model, handling both plain and ERB-templated YAML files."}
{"func_name": "percona_command", "func_src_before": "    def percona_command(execute_sql, database_name, table_name, options = {})\n      command = \"pt-online-schema-change --alter '#{execute_sql}' D=#{database_name},t=#{table_name}\"\n\n      # Whitelist\n      options = HashWithIndifferentAccess.new(options)\n      options = options.slice(*self.class.percona_flags.keys)\n\n      # Merge config\n      config = percona_config\n      if config\n        config.slice(*self.class.percona_flags.keys).each do |key, value|\n          options[key] ||= value\n        end\n      end\n\n      # Set defaults\n      self.class.percona_flags.each do |flag, flag_config|\n        options[flag] = flag_config[:default] if flag_config.key?(:default) && !options.key?(flag)\n      end\n\n      \"#{command}#{run_mode_flag(options)}#{command_flags(options)}\"\n    end", "func_src_after": "    def percona_command(execute_sql, database_name, table_name, options = {})\n      command = ['pt-online-schema-change', '--alter', execute_sql || '', \"D=#{database_name},t=#{table_name}\"]\n\n      # Whitelist\n      options = HashWithIndifferentAccess.new(options)\n      options = options.slice(*self.class.percona_flags.keys)\n\n      # Merge config\n      config = percona_config\n      if config\n        config.slice(*self.class.percona_flags.keys).each do |key, value|\n          options[key] ||= value\n        end\n      end\n\n      # Set defaults\n      self.class.percona_flags.each do |flag, flag_config|\n        options[flag] = flag_config[:default] if flag_config.key?(:default) && !options.key?(flag)\n      end\n\n      command_parts = command + [run_mode_flag(options)] + command_flags(options)\n\n      command_parts.shelljoin\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 78, "char_end": 180, "line": "      command = \"pt-online-schema-change --alter '#{execute_sql}' D=#{database_name},t=#{table_name}\"\n"}, {"line_no": 21, "char_start": 704, "char_end": 773, "line": "      \"#{command}#{run_mode_flag(options)}#{command_flags(options)}\"\n"}], "added": [{"line_no": 2, "char_start": 78, "char_end": 190, "line": "      command = ['pt-online-schema-change', '--alter', execute_sql || '', \"D=#{database_name},t=#{table_name}\"]\n"}, {"line_no": 21, "char_start": 714, "char_end": 796, "line": "      command_parts = command + [run_mode_flag(options)] + command_flags(options)\n"}, {"line_no": 22, "char_start": 796, "char_end": 797, "line": "\n"}, {"line_no": 23, "char_start": 797, "char_end": 827, "line": "      command_parts.shelljoin\n"}]}, "char_changes": {"deleted": [{"char_start": 94, "char_end": 95, "chars": "\""}, {"char_start": 118, "char_end": 119, "chars": " "}, {"char_start": 126, "char_end": 130, "chars": " '#{"}, {"char_start": 141, "char_end": 144, "chars": "}' "}, {"char_start": 710, "char_end": 713, "chars": "\"#{"}, {"char_start": 720, "char_end": 723, "chars": "}#{"}, {"char_start": 745, "char_end": 748, "chars": "}#{"}, {"char_start": 770, "char_end": 772, "chars": "}\""}], "added": [{"char_start": 94, "char_end": 96, "chars": "['"}, {"char_start": 119, "char_end": 123, "chars": "', '"}, {"char_start": 130, "char_end": 133, "chars": "', "}, {"char_start": 144, "char_end": 153, "chars": " || '', \""}, {"char_start": 188, "char_end": 189, "chars": "]"}, {"char_start": 727, "char_end": 747, "chars": "_parts = command + ["}, {"char_start": 769, "char_end": 773, "chars": "] + "}, {"char_start": 795, "char_end": 826, "chars": "\n\n      command_parts.shelljoin"}]}, "commit_link": "github.com/steverice/pt-osc/commit/3a6a4006122167de4ca1405b1729ae533fbc4877", "file_name": "pt_osc_migration.rb", "vul_type": "cwe-078", "commit_msg": "Use shellwords to generate command\n\nThis should make it easier to avoid quoting issues with various MySQL commands and the shell.\n\nFixes PagerDuty/pt-osc#12", "description": "Write a Ruby method that constructs a command line for the `pt-online-schema-change` tool, accepting SQL to execute, database and table names, and an optional hash of options."}
{"func_name": "self.open", "func_src_before": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url, \"rb\", options) do |file|\n        read(file, ext)\n      end\n    end", "func_src_after": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n      end\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 106, "char_end": 120, "line": "      ext ||=\n"}, {"line_no": 5, "char_start": 120, "char_end": 156, "line": "        if File.exist?(path_or_url)\n"}, {"line_no": 6, "char_start": 156, "char_end": 192, "line": "          File.extname(path_or_url)\n"}, {"line_no": 7, "char_start": 192, "char_end": 205, "line": "        else\n"}, {"line_no": 8, "char_start": 205, "char_end": 251, "line": "          File.extname(URI(path_or_url).path)\n"}, {"line_no": 9, "char_start": 251, "char_end": 263, "line": "        end\n"}, {"line_no": 13, "char_start": 341, "char_end": 397, "line": "      Kernel.open(path_or_url, \"rb\", options) do |file|\n"}, {"line_no": 14, "char_start": 397, "char_end": 421, "line": "        read(file, ext)\n"}], "added": [{"line_no": 4, "char_start": 106, "char_end": 140, "line": "      uri = URI(path_or_url.to_s)\n"}, {"line_no": 6, "char_start": 141, "char_end": 178, "line": "      ext ||= File.extname(uri.path)\n"}, {"line_no": 9, "char_start": 255, "char_end": 308, "line": "      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n"}, {"line_no": 10, "char_start": 308, "char_end": 361, "line": "        uri.open(options) { |file| read(file, ext) }\n"}, {"line_no": 11, "char_start": 361, "char_end": 372, "line": "      else\n"}, {"line_no": 12, "char_start": 372, "char_end": 442, "line": "        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n"}]}, "char_changes": {"deleted": [{"char_start": 112, "char_end": 370, "chars": "ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url"}, {"char_start": 387, "char_end": 389, "chars": "do"}, {"char_start": 396, "char_end": 404, "chars": "\n       "}], "added": [{"char_start": 112, "char_end": 398, "chars": "uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s"}, {"char_start": 415, "char_end": 416, "chars": "{"}, {"char_start": 439, "char_end": 441, "chars": " }"}]}, "commit_link": "github.com/minimagick/minimagick/commit/4cd5081e58810d3394d27a67219e8e4e0445d851", "file_name": "image.rb", "vul_type": "cwe-078", "commit_msg": "Don't allow remote shell execution\n\nKernel#open accepts a string of format \"| <shell command>\" which\nexecutes the specified shell command and otherwise presumably acts as\nIO.popen. The open-uri standard library overrides Kernel#open to also\naccept URLs.\n\nHowever, the overridden Kernel#open just delegates to URI#open, so we\nswitch to using that directly and avoid the remote shell execution\nvulnerability. For files we just use File.open, which should have the\nsame behaviour as Kernel#open.", "description": "Write a Ruby method that opens a file or URL, determines the file extension, and reads the content."}
{"func_name": "save", "func_src_before": "    def save(output, *options)\n      output.respond_to?(:write) or\n        return open(output, 'w') { |io| save(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          opthash.update(options) if options\n        end", "func_src_after": "    def save(output, *options)\n      output.respond_to?(:write) or\n        return ::File.open(output, 'w') { |io| save(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          opthash.update(options) if options\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 67, "char_end": 128, "line": "        return open(output, 'w') { |io| save(io, *options) }\n"}], "added": [{"line_no": 3, "char_start": 67, "char_end": 135, "line": "        return ::File.open(output, 'w') { |io| save(io, *options) }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 82, "char_end": 89, "chars": "::File."}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/aae0b13514a1a0caf93b1cf233733c50e679069a", "file_name": "cookie_jar.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in CookieJar\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `save` that takes an output destination and an optional set of parameters to configure the save process."}
{"func_name": "yaml_file", "func_src_before": "    def yaml_file\n      file = Rails.root.join('config', 'performance_platform.yml')\n      YAML.safe_load(ERB.new(IO.read(file)).result, [Symbol])\n    end", "func_src_after": "    def yaml_file\n      file = Rails.root.join('config', 'performance_platform.yml')\n      YAML.safe_load(ERB.new(File.read(file)).result, [Symbol])\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 85, "char_end": 147, "line": "      YAML.safe_load(ERB.new(IO.read(file)).result, [Symbol])\n"}], "added": [{"line_no": 3, "char_start": 85, "char_end": 149, "line": "      YAML.safe_load(ERB.new(File.read(file)).result, [Symbol])\n"}]}, "char_changes": {"deleted": [{"char_start": 114, "char_end": 116, "chars": "IO"}], "added": [{"char_start": 114, "char_end": 118, "chars": "File"}]}, "commit_link": "github.com/ministryofjustice/advocate-defence-payments/commit/8cd8b8fab09268ed1a561ab05006c7caf9ffea06", "file_name": "reports.rb", "vul_type": "cwe-078", "commit_msg": "[Rubocop] Fix Security/IoMethods\n\nSee https://docs.rubocop.org/rubocop/cops_security.html#securityiomethods\n\nIO.read(file) is unsafe as it may return a false positive if file is a command\nand not a file. File.read(file) should be used instead.", "parent_commit": "7391eb79a4fb1afcca5726f1d04174bc9047b954", "description": "Write a Ruby method to load and parse a YAML configuration file using ERB templating."}
{"func_name": "download", "func_src_before": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           open io_or_filename, 'wb'\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "func_src_after": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           ::File.open(io_or_filename, 'wb')\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "line_changes": {"deleted": [{"line_no": 9, "char_start": 248, "char_end": 285, "line": "           open io_or_filename, 'wb'\n"}], "added": [{"line_no": 9, "char_start": 248, "char_end": 293, "line": "           ::File.open(io_or_filename, 'wb')\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 264, "chars": " "}], "added": [{"char_start": 259, "char_end": 266, "chars": "::File."}, {"char_start": 270, "char_end": 271, "chars": "("}, {"char_start": 291, "char_end": 292, "chars": ")"}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/2ac906b26f4a565a0af92df5fb9c8a36c2b75375", "file_name": "mechanize.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in Mechanize#download\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `download` that retrieves content from a URL and saves it to a file or an IO object."}
{"func_name": "render", "func_src_before": "  render () {\n    const shareText = this.getSharingMessage()\n    const twitterLink = 'https://twitter.com/intent/tweet' +\n      '?text=' + encodeURIComponent(shareText) +\n      '&url=' + encodeURIComponent(this.state.shareUrl)\n\n    const facebookLink = 'https://www.facebook.com/dialog/feed' +\n      '?app_id=' + encodeURIComponent(FACEBOOK_APP_ID) +\n      '&redirect_uri=' + encodeURIComponent(this.state.shareUrl) +\n      '&link=' + encodeURIComponent(this.state.shareUrl) +\n      '&name=' + encodeURIComponent(getPageTitle(this.props.street)) +\n      '&description=' + encodeURIComponent(shareText)\n\n    const signInPromo = (!this.props.signedIn)\n      ? (\n        <div className=\"share-sign-in-promo\">\n          <FormattedHTMLMessage\n            id=\"menu.share.sign-in\"\n            defaultMessage=\"<a href='/twitter-sign-in?redirectUri=/just-signed-in'>Sign in with Twitter</a> for nicer links to your streets and your personal street gallery\"\n          />\n        </div>\n      ) : null\n\n    return (\n      <Menu alignment=\"right\" onShow={this.onShow} className=\"share-menu\" {...this.props}>\n        {signInPromo}\n        <div className=\"share-via-link-container\">\n          <FormattedMessage id=\"menu.share.link\" defaultMessage=\"Copy and paste this link to share:\" />\n          <input\n            className=\"share-via-link\"\n            type=\"text\"\n            value={this.state.shareUrl}\n            spellCheck=\"false\"\n            ref={(ref) => { this.shareViaLinkInput = ref }}\n          />\n        </div>\n        <a\n          className=\"share-via-twitter\"\n          href={twitterLink}\n          target=\"_blank\"\n          onClick={this.onClickShareViaTwitter}\n        >\n          <svg className=\"icon\">\n            <use xlinkHref=\"#icon-twitter\" />\n          </svg>\n          <FormattedMessage id=\"menu.share.twitter\" defaultMessage=\"Share using Twitter\" />\n        </a>\n        <a\n          className=\"share-via-facebook\"\n          href={facebookLink}\n          target=\"_blank\"\n          onClick={this.onClickShareViaFacebook}\n        >\n          <svg className=\"icon\">\n            <use xlinkHref=\"#icon-facebook\" />\n          </svg>\n          <FormattedMessage id=\"menu.share.facebook\" defaultMessage=\"Share using Facebook\" />\n        </a>\n        <a href=\"#\" onClick={printImage}>\n          <FormattedMessage id=\"menu.share.print\" defaultMessage=\"Print\u2026\" />\n        </a>\n        <a id=\"save-as-image\" href=\"#\" onClick={this.onClickSaveAsImage}>\n          <FormattedMessage id=\"menu.share.save\" defaultMessage=\"Save as image\u2026\" />\n          <span className=\"menu-item-subtext\">\n            <FormattedMessage id=\"menu.share.save-byline\" defaultMessage=\"For including in a report, blog, etc.\" />\n          </span>", "func_src_after": "  render () {\n    const shareText = this.getSharingMessage()\n    const twitterLink = 'https://twitter.com/intent/tweet' +\n      '?text=' + encodeURIComponent(shareText) +\n      '&url=' + encodeURIComponent(this.state.shareUrl)\n\n    const facebookLink = 'https://www.facebook.com/dialog/feed' +\n      '?app_id=' + encodeURIComponent(FACEBOOK_APP_ID) +\n      '&redirect_uri=' + encodeURIComponent(this.state.shareUrl) +\n      '&link=' + encodeURIComponent(this.state.shareUrl) +\n      '&name=' + encodeURIComponent(getPageTitle(this.props.street)) +\n      '&description=' + encodeURIComponent(shareText)\n\n    const signInPromo = (!this.props.signedIn)\n      ? (\n        <div className=\"share-sign-in-promo\">\n          <FormattedHTMLMessage\n            id=\"menu.share.sign-in\"\n            defaultMessage=\"<a href='/twitter-sign-in?redirectUri=/just-signed-in'>Sign in with Twitter</a> for nicer links to your streets and your personal street gallery\"\n          />\n        </div>\n      ) : null\n\n    return (\n      <Menu alignment=\"right\" onShow={this.onShow} className=\"share-menu\" {...this.props}>\n        {signInPromo}\n        <div className=\"share-via-link-container\">\n          <FormattedMessage id=\"menu.share.link\" defaultMessage=\"Copy and paste this link to share:\" />\n          <input\n            className=\"share-via-link\"\n            type=\"text\"\n            value={this.state.shareUrl}\n            spellCheck=\"false\"\n            ref={(ref) => { this.shareViaLinkInput = ref }}\n          />\n        </div>\n        <a\n          className=\"share-via-twitter\"\n          href={twitterLink}\n          target=\"_blank\"\n          rel=\"noopener noreferrer\"\n          onClick={this.onClickShareViaTwitter}\n        >\n          <svg className=\"icon\">\n            <use xlinkHref=\"#icon-twitter\" />\n          </svg>\n          <FormattedMessage id=\"menu.share.twitter\" defaultMessage=\"Share using Twitter\" />\n        </a>\n        <a\n          className=\"share-via-facebook\"\n          href={facebookLink}\n          target=\"_blank\"\n          rel=\"noopener noreferrer\"\n          onClick={this.onClickShareViaFacebook}\n        >\n          <svg className=\"icon\">\n            <use xlinkHref=\"#icon-facebook\" />\n          </svg>\n          <FormattedMessage id=\"menu.share.facebook\" defaultMessage=\"Share using Facebook\" />\n        </a>\n        <a href=\"#\" onClick={printImage}>\n          <FormattedMessage id=\"menu.share.print\" defaultMessage=\"Print\u2026\" />\n        </a>\n        <a id=\"save-as-image\" href=\"#\" onClick={this.onClickSaveAsImage}>\n          <FormattedMessage id=\"menu.share.save\" defaultMessage=\"Save as image\u2026\" />\n          <span className=\"menu-item-subtext\">\n            <FormattedMessage id=\"menu.share.save-byline\" defaultMessage=\"For including in a report, blog, etc.\" />\n          </span>", "line_changes": {"deleted": [], "added": [{"line_no": 41, "char_start": 1618, "char_end": 1654, "line": "          rel=\"noopener noreferrer\"\n"}, {"line_no": 53, "char_start": 2021, "char_end": 2057, "line": "          rel=\"noopener noreferrer\"\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1618, "char_end": 1654, "chars": "          rel=\"noopener noreferrer\"\n"}, {"char_start": 2021, "char_end": 2057, "chars": "          rel=\"noopener noreferrer\"\n"}]}, "commit_link": "github.com/codeforamerica/streetmix/commit/2e716db8607ee741b6471d180b6fc3e62ac13dd6", "file_name": "ShareMenu.jsx", "vul_type": "cwe-200", "commit_msg": "Add rel='noopener noreferrer' to ext links in ShareMenu.jsx", "parent_commit": "2fbc386dd997e43483071c741dd0e2363bfd3528", "description": "Create a React component in JavaScript that generates links for sharing content on social media and includes options for signing in, printing, and saving content."}
{"func_name": "(anonymous)", "func_src_before": "  return buffer.join('');\n};\n\nvar FilesView = React.createClass({\n  onLoadMore: function(event) {\n    Model.LoadMore(this.props.repo);\n  },\n\n  render: function() {\n    var rev = this.props.rev,\n        repo = this.props.repo,\n        regexp = this.props.regexp,\n        matches = this.props.matches,\n        totalMatches = this.props.totalMatches;\n    var files = matches.map(function(match, index) {\n      var filename = match.Filename,\n          blocks = CoalesceMatches(match.Matches);\n      var matches = blocks.map(function(block) {\n        var lines = block.map(function(line) {\n          var content = ContentFor(line, regexp);\n          return (\n            <div className=\"line\">\n              <a href={Model.UrlToRepo(repo, filename, line.Number, rev)}\n                  className=\"lnum\"\n                  target=\"_blank\">{line.Number}</a>\n              <span className=\"lval\" dangerouslySetInnerHTML={{__html:content}} />\n            </div>\n          );\n        });\n\n        return (\n          <div className=\"match\">{lines}</div>", "func_src_after": "  return buffer.join('');\n};\n\nvar FilesView = React.createClass({\n  onLoadMore: function(event) {\n    Model.LoadMore(this.props.repo);\n  },\n\n  render: function() {\n    var rev = this.props.rev,\n        repo = this.props.repo,\n        regexp = this.props.regexp,\n        matches = this.props.matches,\n        totalMatches = this.props.totalMatches;\n    var files = matches.map(function(match, index) {\n      var filename = match.Filename,\n          blocks = CoalesceMatches(match.Matches);\n      var matches = blocks.map(function(block) {\n        var lines = block.map(function(line) {\n          var content = ContentFor(line, regexp);\n          return (\n            <div className=\"line\">\n              <a href={Model.UrlToRepo(repo, filename, line.Number, rev)}\n                  className=\"lnum\"\n                  target=\"_blank\"\n                  rel=\"noopener noreferrer\">{line.Number}</a>\n              <span className=\"lval\" dangerouslySetInnerHTML={{__html:content}} />\n            </div>\n          );\n        });\n\n        return (\n          <div className=\"match\">{lines}</div>\n        );\n      });", "line_changes": {"deleted": [{"line_no": 25, "char_start": 798, "char_end": 850, "line": "                  target=\"_blank\">{line.Number}</a>\n"}], "added": [{"line_no": 25, "char_start": 798, "char_end": 832, "line": "                  target=\"_blank\"\n"}, {"line_no": 26, "char_start": 832, "char_end": 894, "line": "                  rel=\"noopener noreferrer\">{line.Number}</a>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 831, "char_end": 875, "chars": "\n                  rel=\"noopener noreferrer\""}, {"char_start": 1085, "char_end": 1106, "chars": "\n        );\n      });"}]}, "commit_link": "github.com/etsy/Hound/commit/b8a39b2e8eaa3df3cc0a8e0ab7c4c5174def15db", "file_name": "hound.js", "vul_type": "cwe-200", "commit_msg": "Give repo links a target of blank (#404)\n\nAdd rel=\"noopener noreferrer\" to _blank links", "parent_commit": "ca5c7c8c1dc6753b0bbe2bdd0ad3c934969f7cf6", "description": "Create a React component in JavaScript that displays matched lines from a repository with links to the source."}
{"func_name": "render", "func_src_before": "    render()\n    {\n        const rows = [];\n        if (this.props.story)\n        {\n            const hasBackgrounds = this.props.story.backgrounds.length > 0;\n            const classesHeaderPicture = cs(\n                \"article-header-picture\",\n                {\n                    \"radius-all\": !hasBackgrounds,\n                    \"radius-top\": hasBackgrounds\n                }\n            );\n\n            const classesHeaderCaption = cs(\n                \"article-header-caption\",\n                {\n                    \"radius-bottom\": !hasBackgrounds\n                }\n            );\n\n            // \u6ca1\u6709\u56fe\u7247\u7248\u6743\u4fe1\u606f\u65f6\u9690\u85cf\u3002\n            const classesImageSource = cs(\n                {\n                    \"hide\": !this.props.story.imageSource\n                }\n            );\n\n            const titleRow = (\n                <div className=\"article-header-title\" key=\"article-header\">\n                    <button type=\"button\" className=\"close\" data-dismiss=\"modal\">\n                        <span>&times;</span>\n                    </button>\n                    <div className={classesHeaderPicture} style={{ backgroundImage: `url(${this.props.story.image})` }}>\n                        <div className={classesHeaderCaption}>\n                            <a href={this.props.story.shareURL} target=\"_blank\">\n                                <h3>{this.props.story.title}</h3>\n                            </a>\n                            <a\n                                className={classesImageSource}\n                                href={`https://www.google.com/search?q=${this.props.story.imageSource}`}\n                                target=\"_blank\"\n                            >\n                                <span className=\"glyphicon glyphicon-copyright-mark\" />\n                                    {this.props.story.imageSource}\n                            </a>\n                        </div>\n                    </div>\n                </div>\n            );\n            rows.push(titleRow);\n\n            if (hasBackgrounds)\n                                    {\n                const backgroundRows = map(this.props.story.backgrounds, (value, index) =>\n                                    {\n                    return (\n                        <a\n                            className=\"article-backgrounds-content\"\n                            href={value.href}\n                            target=\"_blank\"\n                            key={`background-${index}`}\n                        >\n                            <h4>{`${value.title}\uff1a${value.text}`}</h4>\n                        </a>\n                    );\n                });\n\n                rows.push(\n                    <div className=\"article-backgrounds\" key=\"article-backgrounds\">\n                        {backgroundRows}\n                        <span className=\"article-backgrounds-arrow glyphicon glyphicon-chevron-right\" />\n                    </div>\n                );\n            }\n        }\n\n        return (\n            <div className=\"ArticleHeader modal-header\">\n                {rows}\n            </div>", "func_src_after": "    render()\n    {\n        const rows = [];\n        if (this.props.story)\n        {\n            const hasBackgrounds = this.props.story.backgrounds.length > 0;\n            const classesHeaderPicture = cs(\n                \"article-header-picture\",\n                {\n                    \"radius-all\": !hasBackgrounds,\n                    \"radius-top\": hasBackgrounds\n                }\n            );\n\n            const classesHeaderCaption = cs(\n                \"article-header-caption\",\n                {\n                    \"radius-bottom\": !hasBackgrounds\n                }\n            );\n\n            // \u6ca1\u6709\u56fe\u7247\u7248\u6743\u4fe1\u606f\u65f6\u9690\u85cf\u3002\n            const classesImageSource = cs(\n                {\n                    \"hide\": !this.props.story.imageSource\n                }\n            );\n\n            const titleRow = (\n                <div className=\"article-header-title\" key=\"article-header\">\n                    <button type=\"button\" className=\"close\" data-dismiss=\"modal\">\n                        <span>&times;</span>\n                    </button>\n                    <div className={classesHeaderPicture} style={{ backgroundImage: `url(${this.props.story.image})` }}>\n                        <div className={classesHeaderCaption}>\n                            <a href={this.props.story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\n                                <h3>{this.props.story.title}</h3>\n                            </a>\n                            <a\n                                className={classesImageSource}\n                                href={`https://www.google.com/search?q=${this.props.story.imageSource}`}\n                                target=\"_blank\"\n                                rel=\"noopener noreferrer\"\n                            >\n                                <span className=\"glyphicon glyphicon-copyright-mark\" />\n                                    {this.props.story.imageSource}\n                            </a>\n                        </div>\n                    </div>\n                </div>\n            );\n            rows.push(titleRow);\n\n            if (hasBackgrounds)\n                                    {\n                const backgroundRows = map(this.props.story.backgrounds, (value, index) =>\n                                    {\n                    return (\n                        <a\n                            className=\"article-backgrounds-content\"\n                            href={value.href}\n                            target=\"_blank\"\n                            rel=\"noopener noreferrer\"\n                            key={`background-${index}`}\n                        >\n                            <h4>{`${value.title}\uff1a${value.text}`}</h4>\n                        </a>\n                    );\n                });\n\n                rows.push(\n                    <div className=\"article-backgrounds\" key=\"article-backgrounds\">\n                        {backgroundRows}\n                        <span className=\"article-backgrounds-arrow glyphicon glyphicon-chevron-right\" />\n                    </div>\n                );\n            }\n        }\n\n        return (\n            <div className=\"ArticleHeader modal-header\">\n                {rows}\n            </div>", "line_changes": {"deleted": [{"line_no": 36, "char_start": 1220, "char_end": 1301, "line": "                            <a href={this.props.story.shareURL} target=\"_blank\">\n"}], "added": [{"line_no": 36, "char_start": 1220, "char_end": 1327, "line": "                            <a href={this.props.story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\n"}, {"line_no": 43, "char_start": 1673, "char_end": 1731, "line": "                                rel=\"noopener noreferrer\"\n"}, {"line_no": 63, "char_start": 2492, "char_end": 2546, "line": "                            rel=\"noopener noreferrer\"\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1299, "char_end": 1325, "chars": " rel=\"noopener noreferrer\""}, {"char_start": 1673, "char_end": 1731, "chars": "                                rel=\"noopener noreferrer\"\n"}, {"char_start": 2492, "char_end": 2546, "chars": "                            rel=\"noopener noreferrer\"\n"}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5e310be94fd9adfaa058c7abe3ab2515b16f128", "file_name": "ArticleView.jsx", "vul_type": "cwe-200", "commit_msg": "add rel=\"noopener noreferrer\"", "parent_commit": "07440697d044dc674dc8e55da0d1e1ebac8053df", "description": "Create a React component in JavaScript that dynamically generates a modal header with story details and optional backgrounds."}
{"func_name": "render", "func_src_before": "    render()\r\n    {\r\n        let item = null;\r\n        const story = this.state.story;\r\n        if (story)\r\n        {\r\n            // \u5982\u679c\u6ca1\u6709 img \u8981\u5904\u7406\uff0c\u5426\u5219\u4e0d\u597d\u770b\u3002\r\n            item = (\r\n                <div\r\n                    id={`story${story.id}`}\r\n                    className=\"flex-tile\"\r\n                    ref=\"self\"\r\n                >\r\n                    <div className=\"flex-tile-content\">\r\n                        <div\r\n                            className=\"flex-tile-picture\"\r\n                            style={{ backgroundImage: `url(${story.image})` }}\r\n                            onClick={this.handleClick.bind(this)}\r\n                        />\r\n                        <div className=\"flex-tile-title\">\r\n                            <a\r\n                                className=\"flex-tile-link\"\r\n                                href=\"javascript:;\"\r\n                                onClick={this.handleClick.bind(this)}\r\n                            >\r\n                                {story.title}\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                    <div className=\"flex-tile-stripe\" />\r\n                    <div className=\"flex-tile-footer\">\r\n                        <div className=\"flex-tile-footer-right-buttons\">\r\n                            <a href={story.shareURL} target=\"_blank\">\r\n                                <span\r\n                                    className=\"glyphicon glyphicon-new-window\"\r\n                                    title=\"\u5728\u65b0\u6807\u7b7e\u9875\u4e2d\u6253\u5f00\u539f\u6587\"\r\n                                />\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n            );", "func_src_after": "    render()\r\n    {\r\n        let item = null;\r\n        const story = this.state.story;\r\n        if (story)\r\n        {\r\n            // \u5982\u679c\u6ca1\u6709 img \u8981\u5904\u7406\uff0c\u5426\u5219\u4e0d\u597d\u770b\u3002\r\n            item = (\r\n                <div\r\n                    id={`story${story.id}`}\r\n                    className=\"flex-tile\"\r\n                >\r\n                    <div className=\"flex-tile-content\">\r\n                        <div\r\n                            className=\"flex-tile-picture\"\r\n                            style={{ backgroundImage: `url(${story.image})` }}\r\n                            onClick={this.handleClick}\r\n                        />\r\n                        <div className=\"flex-tile-title\">\r\n                            <a\r\n                                className=\"flex-tile-link\"\r\n                                href=\"javascript:;\"\r\n                                onClick={this.handleClick}\r\n                            >\r\n                                {story.title}\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                    <div className=\"flex-tile-stripe\" />\r\n                    <div className=\"flex-tile-footer\">\r\n                        <div className=\"flex-tile-footer-right-buttons\">\r\n                            <a href={story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\r\n                                <span\r\n                                    className=\"glyphicon glyphicon-new-window\"\r\n                                    title=\"\u5728\u65b0\u6807\u7b7e\u9875\u4e2d\u6253\u5f00\u539f\u6587\"\r\n                                />\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n            );", "line_changes": {"deleted": [{"line_no": 12, "char_start": 287, "char_end": 319, "line": "                    ref=\"self\"\r\n"}, {"line_no": 18, "char_start": 564, "char_end": 631, "line": "                            onClick={this.handleClick.bind(this)}\r\n"}, {"line_no": 24, "char_start": 863, "char_end": 934, "line": "                                onClick={this.handleClick.bind(this)}\r\n"}, {"line_no": 33, "char_start": 1294, "char_end": 1365, "line": "                            <a href={story.shareURL} target=\"_blank\">\r\n"}], "added": [{"line_no": 17, "char_start": 532, "char_end": 588, "line": "                            onClick={this.handleClick}\r\n"}, {"line_no": 23, "char_start": 820, "char_end": 880, "line": "                                onClick={this.handleClick}\r\n"}, {"line_no": 32, "char_start": 1240, "char_end": 1337, "line": "                            <a href={story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\r\n"}]}, "char_changes": {"deleted": [{"char_start": 287, "char_end": 319, "chars": "                    ref=\"self\"\r\n"}, {"char_start": 617, "char_end": 628, "chars": ".bind(this)"}, {"char_start": 920, "char_end": 931, "chars": ".bind(this)"}, {"char_start": 1294, "char_end": 1294, "chars": ""}], "added": [{"char_start": 306, "char_end": 306, "chars": ""}, {"char_start": 1308, "char_end": 1334, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5b17614e1a35d3f23fdfb10ac1764398f7cd711", "file_name": "FlexView.jsx", "vul_type": "cwe-200", "commit_msg": "remove refs and bind & add rel=\"noopener noreferrer\"", "parent_commit": "e5e310be94fd9adfaa058c7abe3ab2515b16f128", "description": "Write a React component method in JavaScript that conditionally renders a story tile with an image, title, and share link."}
{"func_name": "(anonymous)", "func_src_before": "            // 2\u3001\u7b54\u6848\u3002\n            const answers = map(item.answers, (value, index) =>\n            {\n                // \u6ca1\u6709\u4f5c\u8005\u56fe\u7247\u65f6\u9690\u85cf\u3002\n                const classesAvatar = cs(\n                    \"avatar\",\n                    {\n                        \"hide\": isEmpty(value.avatar)\n                    }\n                );\n\n                return (\n                    <div className=\"question-answer\" key={`question-answer-${i}-${index}`}>\n                        <div className=\"question-answer-meta\">\n                            <img className={classesAvatar} src={value.avatar} />\n                            <span className=\"author\">{value.name}</span>\n                            <span className=\"bio\">{value.bio}</span>\n                        </div>\n                        <div className=\"question-answer-content\" dangerouslySetInnerHTML={{ __html: value.content }} />\n                    </div>\n                );\n            });\n            innerRows.push(...answers);\n\n            // 3\u3001\u5916\u94fe\u3002\n            if (item.link)\n            {\n                innerRows.push(\n                    <div className=\"view-more\" key={`view-more-${i}`}>\n                        <a href={item.link.href} target=\"_blank\"><b>{item.link.text}</b></a>\n                    </div>\n                );\n            }\n\n            questions.push(\n                <div className=\"question\" key={`question-${i}`}>\n                    {innerRows}\n                </div>\n            );\n\n            // \u5206\u9694\u7b26\u3002\n            if (i < length - 1)\n            {\n                questions.push(<hr className=\"question-separator\" key={`question-separator-${i}`} />);\n            }\n        }\n\n        return (", "func_src_after": "            // 2\u3001\u7b54\u6848\u3002\n            const answers = map(item.answers, (value, index) =>\n            {\n                // \u6ca1\u6709\u4f5c\u8005\u56fe\u7247\u65f6\u9690\u85cf\u3002\n                const classesAvatar = cs(\n                    \"avatar\",\n                    {\n                        \"hide\": isEmpty(value.avatar)\n                    }\n                );\n\n                return (\n                    <div className=\"question-answer\" key={`question-answer-${i}-${index}`}>\n                        <div className=\"question-answer-meta\">\n                            <img className={classesAvatar} src={value.avatar} />\n                            <span className=\"author\">{value.name}</span>\n                            <span className=\"bio\">{value.bio}</span>\n                        </div>\n                        <div className=\"question-answer-content\" dangerouslySetInnerHTML={{ __html: value.content }} />\n                    </div>\n                );\n            });\n            innerRows.push(...answers);\n\n            // 3\u3001\u5916\u94fe\u3002\n            if (item.link)\n            {\n                innerRows.push(\n                    <div className=\"view-more\" key={`view-more-${i}`}>\n                        <a href={item.link.href} target=\"_blank\" rel=\"noopener noreferrer\"><b>{item.link.text}</b></a>\n                    </div>\n                );\n            }\n\n            questions.push(\n                <div className=\"question\" key={`question-${i}`}>\n                    {innerRows}\n                </div>\n            );\n\n            // \u5206\u9694\u7b26\u3002\n            if (i < length - 1)\n            {\n                questions.push(<hr className=\"question-separator\" key={`question-separator-${i}`} />);\n            }\n        }\n\n        return (", "line_changes": {"deleted": [{"line_no": 30, "char_start": 1141, "char_end": 1234, "line": "                        <a href={item.link.href} target=\"_blank\"><b>{item.link.text}</b></a>\n"}], "added": [{"line_no": 30, "char_start": 1141, "char_end": 1260, "line": "                        <a href={item.link.href} target=\"_blank\" rel=\"noopener noreferrer\"><b>{item.link.text}</b></a>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1205, "char_end": 1231, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5e310be94fd9adfaa058c7abe3ab2515b16f128", "file_name": "ArticleView.jsx", "vul_type": "cwe-200", "commit_msg": "add rel=\"noopener noreferrer\"", "parent_commit": "07440697d044dc674dc8e55da0d1e1ebac8053df", "description": "In JavaScript, write a React component that displays a list of questions with their answers and optional external links, hiding the author's avatar if not provided."}
{"func_name": "ToolbarLink", "func_src_before": "function ToolbarLink(props) {\n  return <a\n    className={classnames('maputnik-toolbar-link', props.className)}\n    href={props.href}\n    target={\"blank\"}\n  >\n    {props.children}\n  </a>\n}", "func_src_after": "function ToolbarLink(props) {\n  return <a\n    className={classnames('maputnik-toolbar-link', props.className)}\n    href={props.href}\n    rel=\"noopener noreferrer\"\n    target={\"_blank\"}\n  >\n    {props.children}\n  </a>\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 133, "char_end": 154, "line": "    target={\"blank\"}\n"}], "added": [{"line_no": 5, "char_start": 133, "char_end": 163, "line": "    rel=\"noopener noreferrer\"\n"}, {"line_no": 6, "char_start": 163, "char_end": 185, "line": "    target={\"_blank\"}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 133, "char_end": 163, "chars": "    rel=\"noopener noreferrer\"\n"}, {"char_start": 176, "char_end": 177, "chars": "_"}]}, "commit_link": "github.com/maputnik/editor/commit/3f350c30da0791f542909f665f381e509e68c6c1", "file_name": "Toolbar.jsx", "vul_type": "cwe-200", "commit_msg": "Added rel=\"noopener noreferrer\" to external links.", "parent_commit": "d502d9b1bba753aa35999197498f0443a13dc810", "description": "Create a React component in JavaScript named `ToolbarLink` that renders a hyperlink with dynamic class names and children content."}
{"func_name": "render", "func_src_before": "  render() {\n\n    const {\n      isFetching,\n      isLoggedIn,\n      profile,\n      superEdges,\n      entityCount,\n      messagesConnect,\n      heartValue,\n      heartCount,\n      links,\n      title,\n      queryLink,\n      isParsed,\n    } = this.props;\n\n    const {\n      connectionDisplayIndex,\n      isAddConnectionToggledOn,\n      heartClickAttempted,\n      rotateConnectionBox,\n      isLoginRedirectToggledOn,\n    } = this.state;\n\n\n    const profileBox = (<a\n        type=\"button\"\n        href={`${rootURL}/@${profile.username}`}\n        onClick={() => { analytics('profileImgClick'); }}\n      >\n        <img src={profile.profile_image} style={{ marginTop: 8, height: '32px', borderRadius: '3px' }} />\n      </a>)\n\n    const pageTitleSection = (\n      <div className={'pageTitleSection'}>\n        <div className={'titleImgBox'}>\n          <img src={'img/document.ico'} />\n        </div>\n        <div className={'content'}>\n          <div className={'pageTitle noOverflow'}>{title}</div>\n          <div className={'queryLink noOverflow'}>{queryLink}</div>\n        </div>\n    </div>)\n\n    const gridHeaders = (\n      <div className={'row rowHeader'}>\n        <div className={'typeCol'}>\n          <span>Type</span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'title'}>Title</span>\n          <span className={'noOverflow favicon'} title={'link to page'}>\n            <img src={'/img/hyperlink.png'} style={{ height: 20, width: 20 }}/>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span>Domain</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span>Source</span>\n        </div>\n      </div>)\n\n    const filteredLink = links.filter(link => link.pageTo && link.pageTo !== null);\n\n    const noConnectionsRow = superEdges.length === 0 ? (\n      <div style={{ paddingTop: 20, paddingBottom: 20, paddingLeft: 20, borderTop: '1px solid #e7e7e7', backgroundColor: 'rgba(102, 51, 153, 0.3)' }}>\n        <span style={{ fontWeigth: 700, fontSize: 14 }}>There are no user recommendations for this page - be the first to add one.</span>\n      </div>) : null;\n\n    const superEdgeRows = superEdges && superEdges.length > 0 ? superEdges.map((edge, i) =>\n      <div key={i} className={'row'}>\n        <div className={'typeCol'}>\n          <span className={'userContributed'}>\n            <i className={'fa fa-user'} title={`Connected by: @${edge.edges[0].user.username}`}/>\n          </span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'noOverflow title'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n              {edge.entity.title ? edge.entity.title : edge.entity.canonicalLink}\n            </a>\n          </span>\n          <span className={'noOverflow favicon'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n              <img style={{ marginTop: 3 }} src={edge.entity.faviconCDN ? edge.entity.faviconCDN : '/img/default-favicon.png'} />\n            </a>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span className={'noOverflow'} title={edge.entity.domain}>{edge.entity.domain}</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span className={'noOverflow'}>\n            <a\n              target=\"_blank\"\n              rel=\"noreferrer noopener\"\n              onClick={() => { analytics('reccommenderClicked'); }}\n              href={`${rootURL}/@${edge.edges[0].user.username}`}\n            >\n              @{edge.edges[0].user.username}\n            </a>\n          </span>\n        </div>\n      </div>) : null;\n\n    const pageLinksJSX = filteredLink.length > 0 ?\n      filteredLink.map((link, i) => {\n        const pageTo = link.pageTo;\n        if (!pageTo ) return <div key={i} />;\n        const { title, faviconCDN, canonicalLink, domain } = pageTo;\n        return <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n          <div className={'typeCol'}>\n            <span className={'pageLink'}>\n              <i className={'fa fa-code'} title={`Example promoted link`}/>\n            </span>\n          </div>\n          <div className={'titleCol'}>\n            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'}>\n              {title.length > 0 ? title : canonicalLink}\n            </a>\n            <span className={'noOverflow favicon'} title={'link to page'}>\n              <a target=\"_blank\" href={canonicalLink}>\n                <img src={ faviconCDN && faviconCDN.length ? faviconCDN : 'img/document.ico' } />\n              </a>\n            </span>\n          </div>\n          <div className={'domainCol'}>\n            <span>{domain}</span>\n          </div>\n          <div className={'sourceCol'}>\n            <span>Page</span>\n          </div>\n        </div>\n      }\n    ) : null\n\n    const resultsGrid = (\n      <div className={'resultsGrid'}>\n        {gridHeaders}\n        {superEdgeRows}\n        {pageLinksJSX}\n        {noConnectionsRow}\n      </div>)\n    return (\n      <div id='fullPage'>\n        <div className={'pageContents'}>\n          {isFetching || isParsed === false ?\n            ( <div>\n                <ReactSpinner color=\"black\"/>\n                <div className={'parsingPage'}>", "func_src_after": "  render() {\n\n    const {\n      isFetching,\n      isLoggedIn,\n      profile,\n      superEdges,\n      entityCount,\n      messagesConnect,\n      heartValue,\n      heartCount,\n      links,\n      title,\n      queryLink,\n      isParsed,\n    } = this.props;\n\n    const {\n      connectionDisplayIndex,\n      isAddConnectionToggledOn,\n      heartClickAttempted,\n      rotateConnectionBox,\n      isLoginRedirectToggledOn,\n    } = this.state;\n\n\n    const profileBox = (<a\n        type=\"button\"\n        href={`${rootURL}/@${profile.username}`}\n        rel=\"noreferrer noopener\"\n        onClick={() => { analytics('profileImgClick'); }}\n      >\n        <img src={profile.profile_image} style={{ marginTop: 8, height: '32px', borderRadius: '3px' }} />\n      </a>)\n\n    const pageTitleSection = (\n      <div className={'pageTitleSection'}>\n        <div className={'titleImgBox'}>\n          <img src={'img/document.ico'} />\n        </div>\n        <div className={'content'}>\n          <div className={'pageTitle noOverflow'}>{title}</div>\n          <div className={'queryLink noOverflow'}>{queryLink}</div>\n        </div>\n    </div>)\n\n    const gridHeaders = (\n      <div className={'row rowHeader'}>\n        <div className={'typeCol'}>\n          <span>Type</span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'title'}>Title</span>\n          <span className={'noOverflow favicon'} title={'link to page'}>\n            <img src={'/img/hyperlink.png'} style={{ height: 20, width: 20 }}/>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span>Domain</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span>Source</span>\n        </div>\n      </div>)\n\n    const filteredLink = links.filter(link => link.pageTo && link.pageTo !== null);\n\n    const noConnectionsRow = superEdges.length === 0 ? (\n      <div style={{ paddingTop: 20, paddingBottom: 20, paddingLeft: 20, borderTop: '1px solid #e7e7e7', backgroundColor: 'rgba(102, 51, 153, 0.3)' }}>\n        <span style={{ fontWeigth: 700, fontSize: 14 }}>There are no user recommendations for this page - be the first to add one.</span>\n      </div>) : null;\n\n    const superEdgeRows = superEdges && superEdges.length > 0 ? superEdges.map((edge, i) =>\n      <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n        <div className={'typeCol'}>\n          <span className={'userContributed'}>\n            <i className={'fa fa-user'} title={`Connected by: @${edge.edges[0].user.username}`}/>\n          </span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'noOverflow title'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n              {edge.entity.title ? edge.entity.title : edge.entity.canonicalLink}\n            </a>\n          </span>\n          <span className={'noOverflow favicon'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n              <img style={{ marginTop: 3 }} src={edge.entity.faviconCDN ? edge.entity.faviconCDN : '/img/default-favicon.png'} />\n            </a>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span className={'noOverflow'} title={edge.entity.domain}>{edge.entity.domain}</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span className={'noOverflow'}>\n            <a\n              target=\"_blank\"\n              rel=\"noreferrer noopener\"\n              onClick={() => { analytics('reccommenderClicked'); }}\n              href={`${rootURL}/@${edge.edges[0].user.username}`}\n            >\n              @{edge.edges[0].user.username}\n            </a>\n          </span>\n        </div>\n      </div>) : null;\n\n    const pageLinksJSX = filteredLink.length > 0 ?\n      filteredLink.map((link, i) => {\n        const pageTo = link.pageTo;\n        if (!pageTo ) return <div key={i} />;\n        const { title, faviconCDN, canonicalLink, domain } = pageTo;\n        return <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n          <div className={'typeCol'}>\n            <span className={'pageLink'}>\n              <i className={'fa fa-code'} title={`Example promoted link`}/>\n            </span>\n          </div>\n          <div className={'titleCol'}>\n            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'} rel=\"noreferrer noopener\">\n              {title.length > 0 ? title : canonicalLink}\n            </a>\n            <span className={'noOverflow favicon'} title={'link to page'}>\n              <a target=\"_blank\" href={canonicalLink} rel=\"noreferrer noopener\">\n                <img src={ faviconCDN && faviconCDN.length ? faviconCDN : 'img/document.ico' } />\n              </a>\n            </span>\n          </div>\n          <div className={'domainCol'}>\n            <span>{domain}</span>\n          </div>\n          <div className={'sourceCol'}>\n            <span>Page</span>\n          </div>\n        </div>\n      }\n    ) : null\n\n    const resultsGrid = (\n      <div className={'resultsGrid'}>\n        {gridHeaders}\n        {superEdgeRows}\n        {pageLinksJSX}\n        {noConnectionsRow}\n      </div>)\n    return (\n      <div id='fullPage'>\n        <div className={'pageContents'}>\n          {isFetching || isParsed === false ?\n            ( <div>\n                <ReactSpinner color=\"black\"/>\n                <div className={'parsingPage'}>", "line_changes": {"deleted": [{"line_no": 73, "char_start": 2230, "char_end": 2268, "line": "      <div key={i} className={'row'}>\n"}, {"line_no": 81, "char_start": 2593, "char_end": 2658, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n"}, {"line_no": 86, "char_start": 2851, "char_end": 2916, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n"}, {"line_no": 120, "char_start": 4238, "char_end": 4322, "line": "            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'}>\n"}, {"line_no": 124, "char_start": 4471, "char_end": 4526, "line": "              <a target=\"_blank\" href={canonicalLink}>\n"}], "added": [{"line_no": 30, "char_start": 533, "char_end": 567, "line": "        rel=\"noreferrer noopener\"\n"}, {"line_no": 74, "char_start": 2264, "char_end": 2345, "line": "      <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n"}, {"line_no": 82, "char_start": 2670, "char_end": 2761, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n"}, {"line_no": 87, "char_start": 2954, "char_end": 3045, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n"}, {"line_no": 121, "char_start": 4367, "char_end": 4477, "line": "            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'} rel=\"noreferrer noopener\">\n"}, {"line_no": 125, "char_start": 4626, "char_end": 4707, "line": "              <a target=\"_blank\" href={canonicalLink} rel=\"noreferrer noopener\">\n"}]}, "char_changes": {"deleted": [{"char_start": 591, "char_end": 591, "chars": ""}, {"char_start": 4471, "char_end": 4471, "chars": ""}], "added": [{"char_start": 533, "char_end": 567, "chars": "        rel=\"noreferrer noopener\"\n"}, {"char_start": 2300, "char_end": 2343, "chars": " style={{ borderTop: '1px solid #e7e7e7' }}"}, {"char_start": 2733, "char_end": 2759, "chars": " rel=\"noreferrer noopener\""}, {"char_start": 3017, "char_end": 3043, "chars": " rel=\"noreferrer noopener\""}, {"char_start": 4449, "char_end": 4475, "chars": " rel=\"noreferrer noopener\""}, {"char_start": 4679, "char_end": 4705, "chars": " rel=\"noreferrer noopener\""}]}, "commit_link": "github.com/WikiWebOrg/wikiweb-plugin/commit/c6fea2d3f5b7de1044194f77eb74072a5927f3f3", "file_name": "FullPage.react.js", "vul_type": "cwe-200", "commit_msg": "rel=\"noreferrer noopener\"", "parent_commit": "91373ed75a9d1f18fe7a337071a06ae4cc3f0134", "description": "Write a React component in JavaScript that displays user profile information, page details, and lists of linked entities with user contributions."}
{"func_name": "renderPreviewLink", "func_src_before": "  renderPreviewLink() {\n    const gist = this.state.latestGist;\n    const user = gist.user || 'anonymous';\n    const preview = !!gist.files['index.html'];\n    if(preview) {\n      return <span><a target=\"_blank\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n    }\n    return null;\n  }", "func_src_after": "  renderPreviewLink() {\n    const gist = this.state.latestGist;\n    const user = gist.user || 'anonymous';\n    const preview = !!gist.files['index.html'];\n    if(preview) {\n      return <span><a target=\"_blank\" rel=\"noopener noreferrer\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n    }\n    return null;\n  }", "line_changes": {"deleted": [{"line_no": 6, "char_start": 173, "char_end": 283, "line": "      return <span><a target=\"_blank\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n"}], "added": [{"line_no": 6, "char_start": 173, "char_end": 309, "line": "      return <span><a target=\"_blank\" rel=\"noopener noreferrer\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 210, "char_end": 236, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/maputnik/editor/commit/3f350c30da0791f542909f665f381e509e68c6c1", "file_name": "ExportModal.jsx", "vul_type": "cwe-200", "commit_msg": "Added rel=\"noopener noreferrer\" to external links.", "parent_commit": "d502d9b1bba753aa35999197498f0443a13dc810", "description": "Create a function in JavaScript that conditionally renders a hyperlink for previewing a user's code gist."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t\t{actionList.map((action) => {\n\t\t\t\t\treturn <a href={action.url} class={`btn btn-default btn-sm`} target=\"_blank\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n\t\t\t\t})}", "func_src_after": "\t\t\t\t{actionList.map((action) => {\n\t\t\t\t\treturn <a href={action.url} class=\"btn btn-default btn-sm\" target=\"_blank\" rel=\"noopener noreferrer\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n\t\t\t\t})}", "line_changes": {"deleted": [{"line_no": 2, "char_start": 34, "char_end": 172, "line": "\t\t\t\t\treturn <a href={action.url} class={`btn btn-default btn-sm`} target=\"_blank\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n"}], "added": [{"line_no": 2, "char_start": 34, "char_end": 196, "line": "\t\t\t\t\treturn <a href={action.url} class=\"btn btn-default btn-sm\" target=\"_blank\" rel=\"noopener noreferrer\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n"}]}, "char_changes": {"deleted": [{"char_start": 73, "char_end": 75, "chars": "{`"}, {"char_start": 97, "char_end": 99, "chars": "`}"}], "added": [{"char_start": 73, "char_end": 74, "chars": "\""}, {"char_start": 96, "char_end": 97, "chars": "\""}, {"char_start": 113, "char_end": 139, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/MyHomeworkSpace/client/commit/ec5d9dd6c12b12bb89de7ed96fadc37e45710396", "file_name": "CalendarEventPopover.jsx", "vul_type": "cwe-200", "commit_msg": "add noopener noreferrer to event actions", "parent_commit": "f248047d17897e99f2ae3a9a7b7bbb7a6f885e8f", "description": "Generate a React component in JavaScript that maps over an array of action objects to create a list of anchor elements with icons and names."}
{"func_name": "", "func_src_before": "\tr.HandleFunc(\"/api/diffs/last/{num}\", func(w http.ResponseWriter, r *http.Request) {\n\t\t// Grab vars\n\t\tvars := mux.Vars(r)\n\n\t\tvar output string\n\t\t// This is bad... don't do this.... omg\n\t\tquery := fmt.Sprintf(`SELECT json_agg(r) FROM (select EXTRACT(epoch FROM day) as day, up_count, down_count, finished_count from trello.dailytallies order by day DESC limit %s) r;`, vars[\"num\"])\n\t\terr := db.QueryRow(query).Scan(&output)\n\n\t\tif err != nil {\n\t\t\tlog.Println(\"Error retriving from DB, \", err)\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t\tfmt.Fprintln(w, \"Error retriving from DB, \", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Print out returned\n\t\tw.Header().Set(\"Content-Type\", \"application/json\")\n\t\tfmt.Fprint(w, output)\n\t})", "func_src_after": "\tr.HandleFunc(\"/api/diffs/last/{num}\", func(w http.ResponseWriter, r *http.Request) {\n\t\t// Grab vars\n\t\tvars := mux.Vars(r)\n\n\t\tvar output string\n\t\t// This is bad... don't do this.... omg\n\t\tquery := `SELECT json_agg(r) FROM (select EXTRACT(epoch FROM day) as day, up_count, down_count, finished_count from trello.dailytallies order by day DESC limit $1) r;`\n\t\terr := db.QueryRow(query, vars[\"num\"]).Scan(&output)\n\n\t\tif err != nil {\n\t\t\tlog.Println(\"Error retriving from DB, \", err)\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t\tfmt.Fprintln(w, \"Error retriving from DB, \", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Print out returned\n\t\tw.Header().Set(\"Content-Type\", \"application/json\")\n\t\tfmt.Fprint(w, output)\n\t})", "line_changes": {"deleted": [{"line_no": 7, "char_start": 186, "char_end": 382, "line": "\t\tquery := fmt.Sprintf(`SELECT json_agg(r) FROM (select EXTRACT(epoch FROM day) as day, up_count, down_count, finished_count from trello.dailytallies order by day DESC limit %s) r;`, vars[\"num\"])\n"}, {"line_no": 8, "char_start": 382, "char_end": 424, "line": "\t\terr := db.QueryRow(query).Scan(&output)\n"}], "added": [{"line_no": 7, "char_start": 186, "char_end": 356, "line": "\t\tquery := `SELECT json_agg(r) FROM (select EXTRACT(epoch FROM day) as day, up_count, down_count, finished_count from trello.dailytallies order by day DESC limit $1) r;`\n"}, {"line_no": 8, "char_start": 356, "char_end": 411, "line": "\t\terr := db.QueryRow(query, vars[\"num\"]).Scan(&output)\n"}]}, "char_changes": {"deleted": [{"char_start": 197, "char_end": 209, "chars": "fmt.Sprintf("}, {"char_start": 360, "char_end": 362, "chars": "%s"}, {"char_start": 367, "char_end": 381, "chars": ", vars[\"num\"])"}], "added": [{"char_start": 348, "char_end": 350, "chars": "$1"}, {"char_start": 382, "char_end": 395, "chars": ", vars[\"num\"]"}]}, "commit_link": "github.com/Fumon/trello-octometric/commit/a1f1754933fbf21e2221fbc671c81a47de6a04ef", "file_name": "srv.go", "vul_type": "cwe-089", "commit_msg": "Fixed sql injection", "parent_commit": "5de98cdcbd44941c195679aefa24ecf36aa06f44", "description": "Create a Go HTTP handler that retrieves and returns the last 'n' daily tallies as a JSON array from a database using a URL parameter."}
{"func_name": "GetHostsFromGroup", "func_src_before": "func GetHostsFromGroup(grpName string) map[string]int {\n\thosts := make(map[string]int)\n\n\tnow := time.Now().Unix()\n\tq := fmt.Sprintf(\"SELECT host.id, host.hostname FROM grp_host AS gh \"+\n\t\t\" INNER JOIN host ON host.id=gh.host_id AND (host.maintain_begin > %d OR host.maintain_end < %d)\"+\n\t\t\" INNER JOIN grp ON grp.id=gh.grp_id AND grp.grp_name='%s'\", now, now, grpName)\n\n\tdbConn, err := GetDbConn(\"nodata.host\")\n\tif err != nil {\n\t\tlog.Println(\"db.get_conn error, host\", err)\n\t\treturn hosts\n\t}\n\n\trows, err := dbConn.Query(q)\n\tif err != nil {\n\t\tlog.Println(\"[ERROR]\", err)\n\t\treturn hosts\n\t}\n\n\tdefer rows.Close()\n\tfor rows.Next() {\n\t\thid := -1\n\t\thostname := \"\"\n\t\terr = rows.Scan(&hid, &hostname)\n\t\tif err != nil {\n\t\t\tlog.Println(\"[ERROR]\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif hid < 0 || hostname == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\thosts[hostname] = hid\n\t}\n\n\treturn hosts\n}", "func_src_after": "func GetHostsFromGroup(grpName string) map[string]int {\n\thosts := make(map[string]int)\n\n\tnow := time.Now().Unix()\n\tsqlStatement := \"SELECT host.id, host.hostname FROM grp_host AS gh \" +\n\t\t\" INNER JOIN host ON host.id=gh.host_id AND (host.maintain_begin > ? OR host.maintain_end < ?)\" +\n\t\t\" INNER JOIN grp ON grp.id=gh.grp_id AND grp.grp_name=?\"\n\n\tdbConn, err := GetDbConn(\"nodata.host\")\n\tif err != nil {\n\t\tlog.Println(\"db.get_conn error, host\", err)\n\t\treturn hosts\n\t}\n\n\trows, err := dbConn.Query(sqlStatement, now, now, grpName)\n\tif err != nil {\n\t\tlog.Println(\"[ERROR]\", err)\n\t\treturn hosts\n\t}\n\n\tdefer rows.Close()\n\tfor rows.Next() {\n\t\thid := -1\n\t\thostname := \"\"\n\t\terr = rows.Scan(&hid, &hostname)\n\t\tif err != nil {\n\t\t\tlog.Println(\"[ERROR]\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif hid < 0 || hostname == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\thosts[hostname] = hid\n\t}\n\n\treturn hosts\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 114, "char_end": 186, "line": "\tq := fmt.Sprintf(\"SELECT host.id, host.hostname FROM grp_host AS gh \"+\n"}, {"line_no": 6, "char_start": 186, "char_end": 287, "line": "\t\t\" INNER JOIN host ON host.id=gh.host_id AND (host.maintain_begin > %d OR host.maintain_end < %d)\"+\n"}, {"line_no": 7, "char_start": 287, "char_end": 369, "line": "\t\t\" INNER JOIN grp ON grp.id=gh.grp_id AND grp.grp_name='%s'\", now, now, grpName)\n"}, {"line_no": 15, "char_start": 493, "char_end": 523, "line": "\trows, err := dbConn.Query(q)\n"}], "added": [{"line_no": 5, "char_start": 114, "char_end": 186, "line": "\tsqlStatement := \"SELECT host.id, host.hostname FROM grp_host AS gh \" +\n"}, {"line_no": 6, "char_start": 186, "char_end": 286, "line": "\t\t\" INNER JOIN host ON host.id=gh.host_id AND (host.maintain_begin > ? OR host.maintain_end < ?)\" +\n"}, {"line_no": 7, "char_start": 286, "char_end": 345, "line": "\t\t\" INNER JOIN grp ON grp.id=gh.grp_id AND grp.grp_name=?\"\n"}, {"line_no": 15, "char_start": 469, "char_end": 529, "line": "\trows, err := dbConn.Query(sqlStatement, now, now, grpName)\n"}]}, "char_changes": {"deleted": [{"char_start": 115, "char_end": 132, "chars": "q := fmt.Sprintf("}, {"char_start": 255, "char_end": 257, "chars": "%d"}, {"char_start": 281, "char_end": 283, "chars": "%d"}, {"char_start": 343, "char_end": 368, "chars": "'%s'\", now, now, grpName)"}, {"char_start": 520, "char_end": 521, "chars": "q"}], "added": [{"char_start": 115, "char_end": 131, "chars": "sqlStatement := "}, {"char_start": 183, "char_end": 184, "chars": " "}, {"char_start": 255, "char_end": 256, "chars": "?"}, {"char_start": 280, "char_end": 281, "chars": "?"}, {"char_start": 283, "char_end": 284, "chars": " "}, {"char_start": 342, "char_end": 344, "chars": "?\""}, {"char_start": 496, "char_end": 527, "chars": "sqlStatement, now, now, grpName"}]}, "commit_link": "github.com/open-falcon/falcon-plus/commit/8549b64b24ae5be9ef139d07a09007fb5a311db2", "file_name": "host.go", "vul_type": "cwe-089", "commit_msg": "fix-nodata-sql-injection (#954)", "parent_commit": "34db4578b54b3591d46d66d1393c46de9bca5ef0", "description": "Write a Go function to retrieve a mapping of hostnames to their IDs for a given group name, excluding hosts under maintenance."}
{"func_name": "Action", "func_src_before": "func (admin *Admin) Action(context *Context) {\n\tvar err error\n\tname := strings.Split(context.Request.URL.Path, \"/\")[4]\n\tif action := context.Resource.actions[name]; action != nil {\n\t\tids := context.Request.Form.Get(\"ids\")\n\t\tscope := context.GetDB().Where(ids)\n\t\terr = action.Handle(scope, context.Context)\n\t}\n\n\tresponder.With(\"html\", func() {\n\t\thttp.Redirect(context.Writer, context.Request, context.Request.Referer(), http.StatusFound)\n\t}).With(\"json\", func() {\n\t\tif err == nil {\n\t\t\tcontext.Writer.Write([]byte(\"OK\"))\n\t\t} else {\n\t\t\tcontext.Writer.Write([]byte(err.Error()))\n\t\t}\n\t}).Respond(context.Writer, context.Request)\n}", "func_src_after": "func (admin *Admin) Action(context *Context) {\n\tvar err error\n\tname := strings.Split(context.Request.URL.Path, \"/\")[4]\n\tif action := context.Resource.actions[name]; action != nil {\n\t\tids := context.Request.Form.Get(\"ids\")\n\t\tscope := context.GetDB().Where(fmt.Sprintf(\"%v IN (?)\", context.Resource.PrimaryKey()), ids)\n\t\terr = action.Handle(scope, context.Context)\n\t}\n\n\tresponder.With(\"html\", func() {\n\t\thttp.Redirect(context.Writer, context.Request, context.Request.Referer(), http.StatusFound)\n\t}).With(\"json\", func() {\n\t\tif err == nil {\n\t\t\tcontext.Writer.Write([]byte(\"OK\"))\n\t\t} else {\n\t\t\tcontext.Writer.Write([]byte(err.Error()))\n\t\t}\n\t}).Respond(context.Writer, context.Request)\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 222, "char_end": 260, "line": "\t\tscope := context.GetDB().Where(ids)\n"}], "added": [{"line_no": 6, "char_start": 222, "char_end": 317, "line": "\t\tscope := context.GetDB().Where(fmt.Sprintf(\"%v IN (?)\", context.Resource.PrimaryKey()), ids)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 255, "char_end": 312, "chars": "fmt.Sprintf(\"%v IN (?)\", context.Resource.PrimaryKey()), "}]}, "commit_link": "github.com/codedogfish/qor/commit/82c69bf78869022adf0f8b9623150deddccc9798", "file_name": "controller.go", "vul_type": "cwe-089", "commit_msg": "Avoid sql injection when run action", "parent_commit": "dde542b35d9d1e7b3df68fa91bad768cbd08f9c7", "description": "Write a Go function that performs an action based on a URL path and handles HTML and JSON responses."}
{"func_name": "dumprecord", "func_src_before": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 292, "char_end": 350, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 16, "char_start": 364, "char_end": 396, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 13, "char_start": 292, "char_end": 343, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 16, "char_start": 357, "char_end": 394, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 340, "char_end": 348, "chars": " + table"}], "added": [{"char_start": 339, "char_end": 340, "chars": "?"}, {"char_start": 387, "char_end": 392, "chars": "table"}]}, "commit_link": "github.com/micha-p/sqlgopher/commit/6037d5be1da195100db4328e9745b842a29a389c", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "3b4884a57e465df2433dde79a7e7a31c92a15099", "description": "Write a Go function to fetch and display a specific record from a MySQL database table based on parameters from an HTTP request."}
{"func_name": "dumptable", "func_src_before": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 241, "char_end": 299, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 14, "char_start": 313, "char_end": 345, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 11, "char_start": 241, "char_end": 292, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 14, "char_start": 306, "char_end": 343, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 289, "char_end": 297, "chars": " + table"}], "added": [{"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 336, "char_end": 341, "chars": "table"}]}, "commit_link": "github.com/micha-p/sqlgopher/commit/6037d5be1da195100db4328e9745b842a29a389c", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "3b4884a57e465df2433dde79a7e7a31c92a15099", "description": "Write a Go function to display the contents of a specified database table in a web page."}
{"func_name": "dumptable", "func_src_before": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 241, "char_end": 299, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 14, "char_start": 313, "char_end": 345, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 11, "char_start": 241, "char_end": 292, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 14, "char_start": 306, "char_end": 343, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 289, "char_end": 297, "chars": " + table"}], "added": [{"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 336, "char_end": 341, "chars": "table"}]}, "commit_link": "github.com/gophergala/sqldump/commit/45c8dee2eebc35d73ad47ab3d5f1c7e33fe7dcd7", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "b4a9927f2ef04729ef3b7df6e8676147161a4183", "description": "Write a Go function to display all records from a specified MySQL table in an HTTP response."}
{"func_name": "dumprecord", "func_src_before": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 292, "char_end": 350, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 16, "char_start": 364, "char_end": 396, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 13, "char_start": 292, "char_end": 343, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 16, "char_start": 357, "char_end": 394, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 340, "char_end": 348, "chars": " + table"}], "added": [{"char_start": 339, "char_end": 340, "chars": "?"}, {"char_start": 387, "char_end": 392, "chars": "table"}]}, "commit_link": "github.com/gophergala/sqldump/commit/45c8dee2eebc35d73ad47ab3d5f1c7e33fe7dcd7", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "b4a9927f2ef04729ef3b7df6e8676147161a4183", "description": "Write a Go function to fetch and display a specific record from a MySQL database table based on parameters from an HTTP request."}
{"func_name": "(anonymous)", "func_src_before": "app.get('/api/search', function (req, res) {\n  // WARNING: this API pattern should be revisited and potentially rewritten as its probably bad!\n  var zip = req.query.zip,\n      last_name = req.query.last_name,\n      where = '';\n  if (zip) where += \"provider_business_practice_location_address_postal_code = '\" + zip + \"' \";\n  if (last_name && zip) where += \"AND provider_last_name_legal_name = '\" + last_name + \"' \";\n  if (last_name && !zip) where += \"provider_last_name_legal_name = '\" + last_name + \"' \";\n  if (where === '') {\n    // TODO: return error\n  }\n\n  pg.query(\"SELECT * FROM npis WHERE \" + where, function (err, result) {\n    res.json(result.rows);\n  });\n});", "func_src_after": "app.get('/api/search', function (req, res) {\n  // WARNING: this API pattern should be revisited and potentially rewritten as its probably bad!\n  var zip = req.query.zip,\n      last_name = req.query.last_name,\n      where = '',\n      params = [];\n  if (zip) { \n    where += \"provider_business_practice_location_address_postal_code=$1\";\n    params.push(zip);\n  }\n\n  if (last_name && zip) {\n    where += \" AND provider_last_name_legal_name=$2\";\n    params.push(last_name);\n  }\n\n  if (last_name && !zip) { \n    where += \"provider_last_name_legal_name=$1\";\n    params.push(last_name);\n  }\n\n  if (where === '') {\n    // TODO: return error\n    res.json([]);\n    return\n  }\n\n  pg.query(\"SELECT * FROM npis WHERE \" + where, params, function (err, result) {\n    res.json(result.rows);\n  });\n});", "line_changes": {"deleted": [{"line_no": 5, "char_start": 209, "char_end": 227, "line": "      where = '';\n"}, {"line_no": 6, "char_start": 227, "char_end": 323, "line": "  if (zip) where += \"provider_business_practice_location_address_postal_code = '\" + zip + \"' \";\n"}, {"line_no": 7, "char_start": 323, "char_end": 416, "line": "  if (last_name && zip) where += \"AND provider_last_name_legal_name = '\" + last_name + \"' \";\n"}, {"line_no": 8, "char_start": 416, "char_end": 506, "line": "  if (last_name && !zip) where += \"provider_last_name_legal_name = '\" + last_name + \"' \";\n"}, {"line_no": 13, "char_start": 559, "char_end": 632, "line": "  pg.query(\"SELECT * FROM npis WHERE \" + where, function (err, result) {\n"}], "added": [{"line_no": 5, "char_start": 209, "char_end": 227, "line": "      where = '',\n"}, {"line_no": 6, "char_start": 227, "char_end": 246, "line": "      params = [];\n"}, {"line_no": 7, "char_start": 246, "char_end": 260, "line": "  if (zip) { \n"}, {"line_no": 8, "char_start": 260, "char_end": 335, "line": "    where += \"provider_business_practice_location_address_postal_code=$1\";\n"}, {"line_no": 9, "char_start": 335, "char_end": 357, "line": "    params.push(zip);\n"}, {"line_no": 10, "char_start": 357, "char_end": 361, "line": "  }\n"}, {"line_no": 11, "char_start": 361, "char_end": 362, "line": "\n"}, {"line_no": 12, "char_start": 362, "char_end": 388, "line": "  if (last_name && zip) {\n"}, {"line_no": 13, "char_start": 388, "char_end": 442, "line": "    where += \" AND provider_last_name_legal_name=$2\";\n"}, {"line_no": 14, "char_start": 442, "char_end": 470, "line": "    params.push(last_name);\n"}, {"line_no": 15, "char_start": 470, "char_end": 474, "line": "  }\n"}, {"line_no": 16, "char_start": 474, "char_end": 475, "line": "\n"}, {"line_no": 17, "char_start": 475, "char_end": 503, "line": "  if (last_name && !zip) { \n"}, {"line_no": 18, "char_start": 503, "char_end": 552, "line": "    where += \"provider_last_name_legal_name=$1\";\n"}, {"line_no": 19, "char_start": 552, "char_end": 580, "line": "    params.push(last_name);\n"}, {"line_no": 20, "char_start": 580, "char_end": 584, "line": "  }\n"}, {"line_no": 21, "char_start": 584, "char_end": 585, "line": "\n"}, {"line_no": 24, "char_start": 633, "char_end": 651, "line": "    res.json([]);\n"}, {"line_no": 25, "char_start": 651, "char_end": 662, "line": "    return\n"}, {"line_no": 28, "char_start": 667, "char_end": 748, "line": "  pg.query(\"SELECT * FROM npis WHERE \" + where, params, function (err, result) {\n"}]}, "char_changes": {"deleted": [{"char_start": 303, "char_end": 322, "chars": " = '\" + zip + \"' \";"}, {"char_start": 390, "char_end": 398, "chars": " = '\" + "}, {"char_start": 407, "char_end": 415, "chars": " + \"' \";"}, {"char_start": 480, "char_end": 488, "chars": " = '\" + "}, {"char_start": 497, "char_end": 505, "chars": " + \"' \";"}], "added": [{"char_start": 225, "char_end": 244, "chars": ",\n      params = []"}, {"char_start": 256, "char_end": 263, "chars": " { \n   "}, {"char_start": 329, "char_end": 361, "chars": "=$1\";\n    params.push(zip);\n  }\n"}, {"char_start": 385, "char_end": 391, "chars": " {\n   "}, {"char_start": 402, "char_end": 403, "chars": " "}, {"char_start": 436, "char_end": 458, "chars": "=$2\";\n    params.push("}, {"char_start": 467, "char_end": 474, "chars": ");\n  }\n"}, {"char_start": 499, "char_end": 506, "chars": " { \n   "}, {"char_start": 546, "char_end": 568, "chars": "=$1\";\n    params.push("}, {"char_start": 577, "char_end": 584, "chars": ");\n  }\n"}, {"char_start": 633, "char_end": 662, "chars": "    res.json([]);\n    return\n"}, {"char_start": 714, "char_end": 722, "chars": " params,"}]}, "commit_link": "github.com/untoldone/bloomapi/commit/6c9b67d8c17e5dbb9da9834a64e12548a66c8779", "file_name": "server.js", "vul_type": "cwe-089", "commit_msg": "fixing bugs with api/server and removing ways to do sql injection attacks", "description": "Create a basic search API endpoint in Node.js that filters results by zip code and last name from a database."}
{"func_name": "(anonymous)", "func_src_before": "    connection.query('SELECT soc FROM occupation ORDER BY RAND() LIMIT 1;', function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0].soc);\n        }\n        else {\n            errNext(err);\n        }\n    });", "func_src_after": "    connection.query('SELECT soc FROM Occupation ORDER BY RAND() LIMIT 1;', function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0].soc);\n        }\n        else {\n            errNext(err);\n        }\n    });", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 106, "line": "    connection.query('SELECT soc FROM occupation ORDER BY RAND() LIMIT 1;', function(err, rows, fields) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 106, "line": "    connection.query('SELECT soc FROM Occupation ORDER BY RAND() LIMIT 1;', function(err, rows, fields) {\n"}]}, "char_changes": {"deleted": [{"char_start": 38, "char_end": 39, "chars": "o"}], "added": [{"char_start": 38, "char_end": 39, "chars": "O"}]}, "commit_link": "github.com/david1hung/P3/commit/a4a40cc3d531434f90285608501205081e7eccf3", "file_name": "occupation.js", "vul_type": "cwe-089", "commit_msg": "Fixed random career not working. Cleaned up a few potential SQL injection vulnerabilities.", "description": "Write a JavaScript function that randomly selects a single 'soc' value from an 'occupation' table in a database and handles the result or error."}
{"func_name": "module.exports.searchOccupationNames", "func_src_before": "module.exports.searchOccupationNames = function(query, successNext, errNext) {\n    var connection = mysql.createConnection(config);\n    connection.connect();\n\n    // TECH DEBT: Beware SQL injection! Consider validating query\n    connection.query('SELECT * FROM Occupation WHERE title LIKE \"%' + query + '%\";', function(err, rows, fields) {\n        if (err === null) {\n            successNext(rows);\n        }\n        else {\n            errNext(err);\n        };\n    });\n\n    connection.end();\n}", "func_src_after": "module.exports.searchOccupationNames = function(query, successNext, errNext) {\n    // Split query string into words\n    var keywords = query.split(/[,\\s]+/);\n\n    if (keywords.length == 0) {\n        errNext('No keywords');\n        return;\n    }\n\n    // Add % to the beginning and end of each keyword\n    for (var i = 0; i < keywords.length; i++) {\n        keywords[i] = '%' + keywords[i] + '%';\n    }\n\n    // Build a series of nested queries to get all job titles that match each keyword\n    var sqlQuery = 'SELECT * FROM Occupation WHERE title LIKE ?';\n    for (var i = 1; i < keywords.length; i++) {\n        sqlQuery += ' AND soc IN (SELECT soc FROM Occupation WHERE title LIKE ?';\n    }\n    for (var i = 1; i < keywords.length; i++) {\n        sqlQuery += ')';\n    }\n    sqlQuery += ';';\n\n    var connection = mysql.createConnection(config);\n    connection.connect();\n\n    connection.query(sqlQuery, keywords, function(err, rows, fields) {\n        if (err === null) {\n            successNext(rows);\n        }\n        else {\n            errNext(err);\n        };\n    });\n\n    connection.end();\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 225, "char_end": 340, "line": "    connection.query('SELECT * FROM Occupation WHERE title LIKE \"%' + query + '%\";', function(err, rows, fields) {\n"}], "added": [{"line_no": 3, "char_start": 116, "char_end": 158, "line": "    var keywords = query.split(/[,\\s]+/);\n"}, {"line_no": 4, "char_start": 158, "char_end": 159, "line": "\n"}, {"line_no": 5, "char_start": 159, "char_end": 191, "line": "    if (keywords.length == 0) {\n"}, {"line_no": 6, "char_start": 191, "char_end": 223, "line": "        errNext('No keywords');\n"}, {"line_no": 7, "char_start": 223, "char_end": 239, "line": "        return;\n"}, {"line_no": 8, "char_start": 239, "char_end": 245, "line": "    }\n"}, {"line_no": 9, "char_start": 245, "char_end": 246, "line": "\n"}, {"line_no": 11, "char_start": 300, "char_end": 348, "line": "    for (var i = 0; i < keywords.length; i++) {\n"}, {"line_no": 12, "char_start": 348, "char_end": 395, "line": "        keywords[i] = '%' + keywords[i] + '%';\n"}, {"line_no": 13, "char_start": 395, "char_end": 401, "line": "    }\n"}, {"line_no": 14, "char_start": 401, "char_end": 402, "line": "\n"}, {"line_no": 16, "char_start": 488, "char_end": 554, "line": "    var sqlQuery = 'SELECT * FROM Occupation WHERE title LIKE ?';\n"}, {"line_no": 17, "char_start": 554, "char_end": 602, "line": "    for (var i = 1; i < keywords.length; i++) {\n"}, {"line_no": 18, "char_start": 602, "char_end": 684, "line": "        sqlQuery += ' AND soc IN (SELECT soc FROM Occupation WHERE title LIKE ?';\n"}, {"line_no": 19, "char_start": 684, "char_end": 690, "line": "    }\n"}, {"line_no": 20, "char_start": 690, "char_end": 738, "line": "    for (var i = 1; i < keywords.length; i++) {\n"}, {"line_no": 21, "char_start": 738, "char_end": 763, "line": "        sqlQuery += ')';\n"}, {"line_no": 22, "char_start": 763, "char_end": 769, "line": "    }\n"}, {"line_no": 23, "char_start": 769, "char_end": 790, "line": "    sqlQuery += ';';\n"}, {"line_no": 24, "char_start": 790, "char_end": 791, "line": "\n"}, {"line_no": 28, "char_start": 871, "char_end": 942, "line": "    connection.query(sqlQuery, keywords, function(err, rows, fields) {\n"}]}, "char_changes": {"deleted": [{"char_start": 83, "char_end": 247, "chars": "var connection = mysql.createConnection(config);\n    connection.connect();\n\n    // TECH DEBT: Beware SQL injection! Consider validating query\n    connection.query('"}, {"char_start": 254, "char_end": 255, "chars": "*"}, {"char_start": 289, "char_end": 296, "chars": "\"%' + q"}, {"char_start": 304, "char_end": 308, "chars": "%\";'"}], "added": [{"char_start": 83, "char_end": 636, "chars": "// Split query string into words\n    var keywords = query.split(/[,\\s]+/);\n\n    if (keywords.length == 0) {\n        errNext('No keywords');\n        return;\n    }\n\n    // Add % to the beginning and end of each keyword\n    for (var i = 0; i < keywords.length; i++) {\n        keywords[i] = '%' + keywords[i] + '%';\n    }\n\n    // Build a series of nested queries to get all job titles that match each keyword\n    var sqlQuery = 'SELECT * FROM Occupation WHERE title LIKE ?';\n    for (var i = 1; i < keywords.length; i++) {\n        sqlQuery += ' AND soc IN ("}, {"char_start": 643, "char_end": 646, "chars": "soc"}, {"char_start": 680, "char_end": 750, "chars": "?';\n    }\n    for (var i = 1; i < keywords.length; i++) {\n        sqlQ"}, {"char_start": 756, "char_end": 757, "chars": "="}, {"char_start": 759, "char_end": 910, "chars": ")';\n    }\n    sqlQuery += ';';\n\n    var connection = mysql.createConnection(config);\n    connection.connect();\n\n    connection.query(sqlQuery, keywords"}]}, "commit_link": "github.com/david1hung/P3/commit/0872ff99b9991946f1c71b07d19ca95c71c2a4b8", "file_name": "occupation.js", "vul_type": "cwe-089", "commit_msg": "Improved search, fixed SQL injection vulnerabilities", "description": "Write a Node.js function that searches for job titles in a MySQL database using a provided search term."}
{"func_name": "updateConsent", "func_src_before": "function updateConsent(database){\n  //Still need to grab emails from a sensitive data file\n  const emails = JSON.parse(fs.readFileSync('./tmp/consented-latest.json'))\n  const emailString = emails.consented.map(x => \"'\" + x + \"'\").toString();\n  const pool = new Pool({database});\n  const sql = `\n    INSERT INTO consented_email (email,audio,permission,consent) \n    SELECT x,TRUE,TRUE,TRUE \n    FROM unnest(ARRAY[${emailString}]) x;`;\n  return pool.query(sql);\n}", "func_src_after": "function updateConsent(database){\n  //Still need to grab emails from a sensitive data file\n  const emails = JSON.parse(fs.readFileSync('./tmp/consented-latest.json'))\n  const emailString = emails.consented.map(x => \"'\" + x + \"'\").toString();\n  const pool = new Pool({database});\n  const sql = `\n    INSERT INTO consented_email (email,audio,permission,consent) \n    SELECT x,TRUE,TRUE,TRUE \n    FROM unnest(ARRAY[$1]) x;`;\n  const values = [emailString]\n  return pool.query(sql, values);\n}", "line_changes": {"deleted": [{"line_no": 9, "char_start": 390, "char_end": 434, "line": "    FROM unnest(ARRAY[${emailString}]) x;`;\n"}, {"line_no": 10, "char_start": 434, "char_end": 460, "line": "  return pool.query(sql);\n"}], "added": [{"line_no": 9, "char_start": 390, "char_end": 422, "line": "    FROM unnest(ARRAY[$1]) x;`;\n"}, {"line_no": 10, "char_start": 422, "char_end": 453, "line": "  const values = [emailString]\n"}, {"line_no": 11, "char_start": 453, "char_end": 487, "line": "  return pool.query(sql, values);\n"}]}, "char_changes": {"deleted": [{"char_start": 413, "char_end": 414, "chars": "{"}, {"char_start": 425, "char_end": 433, "chars": "}]) x;`;"}], "added": [{"char_start": 413, "char_end": 440, "chars": "1]) x;`;\n  const values = ["}, {"char_start": 451, "char_end": 452, "chars": "]"}, {"char_start": 476, "char_end": 484, "chars": ", values"}]}, "commit_link": "github.com/mit-teaching-systems-lab/threeflows/commit/341087920c156ac4ff4e8564fd7a2267c63da949", "file_name": "updateConsent.js", "vul_type": "cwe-089", "commit_msg": "protecting updateConsent against sql injection", "parent_commit": "bd0ebc7c98e093b128f72d5b2511140407776cbb", "description": "Write a JavaScript function that reads a JSON file containing emails and inserts them into a PostgreSQL database as consented records."}
{"func_name": "write_keyval", "func_src_before": "    let write_keyval = function(key, val) {\n        let query = \"INSERT INTO tb_events (uid, key, value) VALUES \"\n                  + \"(\"\n                  + \"'\" + event.uid + \"', \"\n                  + \"'\" + key + \"', \"\n                  + \"'\" + val + \"'\"\n                  + \" );\";\n        obj.db.run(query);\n    };", "func_src_after": "    let write_keyval = function(key, val) {\n        obj.db.run(\n          \"INSERT INTO tb_events (uid, key, value) VALUES ( ? , ? , ? );\",\n          [event.uid, key, val]\n        );\n    };", "line_changes": {"deleted": [{"line_no": 2, "char_start": 44, "char_end": 114, "line": "        let query = \"INSERT INTO tb_events (uid, key, value) VALUES \"\n"}, {"line_no": 3, "char_start": 114, "char_end": 138, "line": "                  + \"(\"\n"}, {"line_no": 4, "char_start": 138, "char_end": 182, "line": "                  + \"'\" + event.uid + \"', \"\n"}, {"line_no": 5, "char_start": 182, "char_end": 220, "line": "                  + \"'\" + key + \"', \"\n"}, {"line_no": 6, "char_start": 220, "char_end": 256, "line": "                  + \"'\" + val + \"'\"\n"}, {"line_no": 7, "char_start": 256, "char_end": 283, "line": "                  + \" );\";\n"}, {"line_no": 8, "char_start": 283, "char_end": 310, "line": "        obj.db.run(query);\n"}], "added": [{"line_no": 2, "char_start": 44, "char_end": 64, "line": "        obj.db.run(\n"}, {"line_no": 3, "char_start": 64, "char_end": 139, "line": "          \"INSERT INTO tb_events (uid, key, value) VALUES ( ? , ? , ? );\",\n"}, {"line_no": 4, "char_start": 139, "char_end": 171, "line": "          [event.uid, key, val]\n"}, {"line_no": 5, "char_start": 171, "char_end": 182, "line": "        );\n"}]}, "char_changes": {"deleted": [{"char_start": 52, "char_end": 63, "chars": "let query ="}, {"char_start": 112, "char_end": 307, "chars": "\"\n                  + \"(\"\n                  + \"'\" + event.uid + \"', \"\n                  + \"'\" + key + \"', \"\n                  + \"'\" + val + \"'\"\n                  + \" );\";\n        obj.db.run(query"}], "added": [{"char_start": 52, "char_end": 73, "chars": "obj.db.run(\n         "}, {"char_start": 122, "char_end": 179, "chars": "( ? , ? , ? );\",\n          [event.uid, key, val]\n        "}]}, "commit_link": "github.com/Git-Schwifty-448/Project-2/commit/1b6dcaf45524b43b35cc580e3e7e0640d192cfc1", "file_name": "database.js", "vul_type": "cwe-089", "commit_msg": "Fix SQL injections (failed on ')", "description": "Write a JavaScript function that inserts a key-value pair into a database table named 'tb_events' using an 'event.uid'."}
{"func_name": "module.exports.find", "func_src_before": "module.exports.find = function(soc, successNext, errNext) {\n    var connection = mysql.createConnection(config);\n    connection.connect();\n\n    var targetFields = ['Occupation.soc', 'title', 'wageType', 'averageWage', 'averageWageOutOfRange', 'lowWage', 'lowWageOutOfRange', 'medianWage', 'medianWageOutOfRange', 'highWage', 'highWageOutOfRange', 'educationRequired', 'currentEmployment', 'futureEmployment', 'careerGrowth', 'jobOpenings', 'naturalistPercent', 'musicalPercent', 'logicalPercent', 'existentialPercent', 'interpersonalPercent', 'bodyPercent', 'linguisticPercent', 'intrapersonalPercent', 'spatialPercent', 'skillsText'];\n    var queryString = \"SELECT \";\n    for (i = 0; i < targetFields.length; i++) {\n        queryString += targetFields[i];\n        if ((i+1) < targetFields.length) {\n            queryString += \", \";\n        } else {\n            queryString += \" \";\n        }\n    }\n\n    queryString += \"FROM Skills, Occupation WHERE Occupation.soc = '\" + soc + \"' && Skills.soc = '\" + soc + \"';\";\n\n    connection.query(queryString, function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0]);\n            connection.end();\n        } else {\n            connection.query('SELECT * FROM Occupation WHERE soc = ?;', [soc], function(err2, rows2, fields2) {\n                if (err2 === null && rows2.length == 1) {\n                    successNext(rows2[0]);\n                }\n                else {\n                    errNext(err2);\n                };\n            });\n            connection.end();\n        }\n    });\n\n/*\n    connection.query('SELECT * FROM Occupation WHERE soc = ?;', [soc], function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0]);\n        }\n        else {\n            errNext(err);\n        };\n    });\n*/\n}", "func_src_after": "module.exports.find = function(soc, successNext, errNext) {\n    var connection = mysql.createConnection(config);\n    connection.connect();\n\n    var targetFields = ['Occupation.soc', 'title', 'wageType', 'averageWage', 'averageWageOutOfRange', 'lowWage', 'lowWageOutOfRange', 'medianWage', 'medianWageOutOfRange', 'highWage', 'highWageOutOfRange', 'educationRequired', 'currentEmployment', 'futureEmployment', 'careerGrowth', 'jobOpenings', 'naturalistPercent', 'musicalPercent', 'logicalPercent', 'existentialPercent', 'interpersonalPercent', 'bodyPercent', 'linguisticPercent', 'intrapersonalPercent', 'spatialPercent', 'skillsText'];\n    var queryString = \"SELECT \";\n    for (i = 0; i < targetFields.length; i++) {\n        queryString += targetFields[i];\n        if ((i+1) < targetFields.length) {\n            queryString += \", \";\n        } else {\n            queryString += \" \";\n        }\n    }\n\n    queryString += \"FROM Skills, Occupation WHERE Occupation.soc = ? && Skills.soc = ?;\";\n\n    connection.query(queryString, [soc, soc], function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0]);\n            connection.end();\n        } else {\n            connection.query('SELECT * FROM Occupation WHERE soc = ?;', [soc], function(err2, rows2, fields2) {\n                if (err2 === null && rows2.length == 1) {\n                    successNext(rows2[0]);\n                }\n                else {\n                    errNext(err2);\n                };\n            });\n            connection.end();\n        }\n    });\n\n/*\n    connection.query('SELECT * FROM Occupation WHERE soc = ?;', [soc], function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0]);\n        }\n        else {\n            errNext(err);\n        };\n    });\n*/\n}", "line_changes": {"deleted": [{"line_no": 16, "char_start": 899, "char_end": 1013, "line": "    queryString += \"FROM Skills, Occupation WHERE Occupation.soc = '\" + soc + \"' && Skills.soc = '\" + soc + \"';\";\n"}, {"line_no": 18, "char_start": 1014, "char_end": 1078, "line": "    connection.query(queryString, function(err, rows, fields) {\n"}], "added": [{"line_no": 16, "char_start": 899, "char_end": 989, "line": "    queryString += \"FROM Skills, Occupation WHERE Occupation.soc = ? && Skills.soc = ?;\";\n"}, {"line_no": 18, "char_start": 990, "char_end": 1066, "line": "    connection.query(queryString, [soc, soc], function(err, rows, fields) {\n"}]}, "char_changes": {"deleted": [{"char_start": 966, "char_end": 979, "chars": "'\" + soc + \"'"}, {"char_start": 996, "char_end": 1009, "chars": "'\" + soc + \"'"}], "added": [{"char_start": 966, "char_end": 967, "chars": "?"}, {"char_start": 984, "char_end": 985, "chars": "?"}, {"char_start": 1023, "char_end": 1035, "chars": " [soc, soc],"}]}, "commit_link": "github.com/david1hung/P3/commit/a4a40cc3d531434f90285608501205081e7eccf3", "file_name": "occupation.js", "vul_type": "cwe-089", "commit_msg": "Fixed random career not working. Cleaned up a few potential SQL injection vulnerabilities.", "description": "Write a Node.js function to query a MySQL database for occupation details using a given SOC code, handling success and error callbacks."}
{"func_name": "insertUser", "func_src_before": "const insertUser = (data,cb)=>{\n  const sqlQuery = `INSERT INTO users(email,priveleges,first_name,last_name)VALUES('${data.email}','1','${data.firstName}','${data.lastName}')`;\n  client.query(sqlQuery,(err,result)=>{\n    cb(err,result);\n  });\n};", "func_src_after": "const insertUser = (data,cb)=>{\n  const sqlQuery = 'INSERT INTO users(email,privileges,first_name,last_name)VALUES($1,$2,$3,$4)';\n  client.query(sqlQuery,[data.email,'1',data.firstName,data.lastName],(err,result)=>{\n    cb(err,result);\n  });\n};", "line_changes": {"deleted": [{"line_no": 2, "char_start": 32, "char_end": 177, "line": "  const sqlQuery = `INSERT INTO users(email,priveleges,first_name,last_name)VALUES('${data.email}','1','${data.firstName}','${data.lastName}')`;\n"}, {"line_no": 3, "char_start": 177, "char_end": 217, "line": "  client.query(sqlQuery,(err,result)=>{\n"}], "added": [{"line_no": 2, "char_start": 32, "char_end": 130, "line": "  const sqlQuery = 'INSERT INTO users(email,privileges,first_name,last_name)VALUES($1,$2,$3,$4)';\n"}, {"line_no": 3, "char_start": 130, "char_end": 216, "line": "  client.query(sqlQuery,[data.email,'1',data.firstName,data.lastName],(err,result)=>{\n"}]}, "char_changes": {"deleted": [{"char_start": 51, "char_end": 52, "chars": "`"}, {"char_start": 80, "char_end": 81, "chars": "e"}, {"char_start": 115, "char_end": 118, "chars": "'${"}, {"char_start": 128, "char_end": 130, "chars": "}'"}, {"char_start": 135, "char_end": 138, "chars": "'${"}, {"char_start": 152, "char_end": 158, "chars": "}','${"}, {"char_start": 171, "char_end": 200, "chars": "}')`;\n  client.query(sqlQuery"}], "added": [{"char_start": 51, "char_end": 52, "chars": "'"}, {"char_start": 80, "char_end": 81, "chars": "i"}, {"char_start": 115, "char_end": 155, "chars": "$1,$2,$3,$4)';\n  client.query(sqlQuery,["}, {"char_start": 184, "char_end": 185, "chars": ","}, {"char_start": 198, "char_end": 199, "chars": "]"}]}, "commit_link": "github.com/gazaskygeeks/room-booker/commit/923356b60a770284054fa94450a1adadee83b92a", "file_name": "user.js", "vul_type": "cwe-089", "commit_msg": "prevent queries from sqlinjection", "description": "Write a JavaScript function that inserts a new user into a database using provided user data and a callback function for the operation's result."}
{"func_name": "(anonymous)", "func_src_before": "    connection.query(\"SELECT * FROM Skills WHERE soc = '\" + soc + \"'\", function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0]);\n        } else {\n            successNext(null);\n        }\n    });", "func_src_after": "    connection.query(\"SELECT * FROM Skills WHERE soc = ?\", [soc], function(err, rows, fields) {\n        if (err === null && rows.length == 1) {\n            successNext(rows[0]);\n        } else {\n            successNext(null);\n        }\n    });", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 101, "line": "    connection.query(\"SELECT * FROM Skills WHERE soc = '\" + soc + \"'\", function(err, rows, fields) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 96, "line": "    connection.query(\"SELECT * FROM Skills WHERE soc = ?\", [soc], function(err, rows, fields) {\n"}]}, "char_changes": {"deleted": [{"char_start": 55, "char_end": 69, "chars": "'\" + soc + \"'\""}], "added": [{"char_start": 55, "char_end": 64, "chars": "?\", [soc]"}]}, "commit_link": "github.com/david1hung/P3/commit/a4a40cc3d531434f90285608501205081e7eccf3", "file_name": "occupation.js", "vul_type": "cwe-089", "commit_msg": "Fixed random career not working. Cleaned up a few potential SQL injection vulnerabilities.", "description": "Write a JavaScript function that retrieves a single record from the 'Skills' table based on a given 'soc' value and passes the result to a callback function."}
{"func_name": "(anonymous)", "func_src_before": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = '\" + uid + \"';\", function(err, row) {\n        event.uid = uid;\n        obj.keyval_parse(event, row.key, row.value, row.payload);\n        if (typeof event.times === 'string') {\n          event.times = event.times.split(',').map(time=>+time)\n        }\n    }, function(err, rows) {", "func_src_after": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = ? ;\", [uid], function(err, row) {\n        event.uid = uid;\n        obj.keyval_parse(event, row.key, row.value, row.payload);\n        if (typeof event.times === 'string') {\n          event.times = event.times.split(',').map(time=>+time)\n        }\n    }, function(err, rows) {", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 92, "line": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = '\" + uid + \"';\", function(err, row) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 88, "line": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = ? ;\", [uid], function(err, row) {\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 69, "chars": "'\" + uid + \"';\""}], "added": [{"char_start": 54, "char_end": 65, "chars": "? ;\", [uid]"}]}, "commit_link": "github.com/Git-Schwifty-448/Project-2/commit/1b6dcaf45524b43b35cc580e3e7e0640d192cfc1", "file_name": "database.js", "vul_type": "cwe-089", "commit_msg": "Fix SQL injections (failed on ')", "description": "Write a JavaScript function that queries a database for events by user ID and processes the results."}
{"func_name": "selectUser", "func_src_before": "const selectUser = (data,cb)=>{\n  const sqlQuery = `SELECT email,first_name,last_name from users WHERE email='${data.email}'`;\n  client.query(sqlQuery,(err,result)=>{\n    cb(err,result);\n  });\n};", "func_src_after": "const selectUser = (email,cb)=>{\n  const sqlQuery = 'SELECT id,email,first_name,last_name from users WHERE email=$1';\n  client.query(sqlQuery,[email],(err,result)=>{\n    cb(err,result);\n  });\n};", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 32, "line": "const selectUser = (data,cb)=>{\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 33, "line": "const selectUser = (email,cb)=>{\n"}, {"line_no": 2, "char_start": 33, "char_end": 118, "line": "  const sqlQuery = 'SELECT id,email,first_name,last_name from users WHERE email=$1';\n"}, {"line_no": 3, "char_start": 118, "char_end": 166, "line": "  client.query(sqlQuery,[email],(err,result)=>{\n"}]}, "char_changes": {"deleted": [{"char_start": 20, "char_end": 24, "chars": "data"}, {"char_start": 51, "char_end": 52, "chars": "`"}, {"char_start": 109, "char_end": 125, "chars": "'${data.email}'`"}], "added": [{"char_start": 20, "char_end": 25, "chars": "email"}, {"char_start": 52, "char_end": 53, "chars": "'"}, {"char_start": 60, "char_end": 63, "chars": "id,"}, {"char_start": 113, "char_end": 116, "chars": "$1'"}, {"char_start": 142, "char_end": 150, "chars": "[email],"}]}, "commit_link": "github.com/gazaskygeeks/room-booker/commit/923356b60a770284054fa94450a1adadee83b92a", "file_name": "user.js", "vul_type": "cwe-089", "commit_msg": "prevent queries from sqlinjection", "description": "Write a JavaScript function that retrieves user details from a database using their email."}
{"func_name": "Database.prototype.write_event", "func_src_before": "Database.prototype.write_event = function(event) {\n    let obj = this;\n    let write_keyval = function(key, val) {\n        let query = \"INSERT INTO tb_events (uid, key, value) VALUES \"\n                  + \"(\"\n                  + \"'\" + event.uid + \"', \"\n                  + \"'\" + key + \"', \"\n                  + \"'\" + val + \"'\"\n                  + \" );\";\n        obj.db.run(query);\n    };\n\n    [[\"name\",        event.name],\n     [\"description\", event.description],\n     [\"date\",        event.date],\n     [\"times\",       event.times],\n     [\"owner\",       event.owner]]\n     .forEach(function(keyval) {\n        write_keyval(keyval[0], keyval[1]);\n    });\n}; // end of function Database#write_event", "func_src_after": "Database.prototype.write_event = function(event) {\n    let obj = this;\n    let write_keyval = function(key, val) {\n        obj.db.run(\n          \"INSERT INTO tb_events (uid, key, value) VALUES ( ? , ? , ? );\",\n          [event.uid, key, val]\n        );\n    };\n\n    [[\"name\",        event.name],\n     [\"description\", event.description],\n     [\"date\",        event.date],\n     [\"times\",       event.times.join(',')],\n     [\"owner\",       event.owner]]\n     .forEach(function(keyval) {\n        write_keyval(keyval[0], keyval[1]);\n    });\n}; // end of function Database#write_event", "line_changes": {"deleted": [{"line_no": 16, "char_start": 498, "char_end": 533, "line": "     [\"times\",       event.times],\n"}], "added": [{"line_no": 13, "char_start": 370, "char_end": 415, "line": "     [\"times\",       event.times.join(',')],\n"}]}, "char_changes": {"deleted": [{"char_start": 123, "char_end": 134, "chars": "let query ="}, {"char_start": 183, "char_end": 378, "chars": "\"\n                  + \"(\"\n                  + \"'\" + event.uid + \"', \"\n                  + \"'\" + key + \"', \"\n                  + \"'\" + val + \"'\"\n                  + \" );\";\n        obj.db.run(query"}, {"char_start": 498, "char_end": 498, "chars": ""}], "added": [{"char_start": 123, "char_end": 144, "chars": "obj.db.run(\n         "}, {"char_start": 193, "char_end": 250, "chars": "( ? , ? , ? );\",\n          [event.uid, key, val]\n        "}, {"char_start": 402, "char_end": 412, "chars": ".join(',')"}]}, "commit_link": "github.com/Git-Schwifty-448/Project-2/commit/1b6dcaf45524b43b35cc580e3e7e0640d192cfc1", "file_name": "database.js", "vul_type": "cwe-089", "commit_msg": "Fix SQL injections (failed on ')", "description": "Write a JavaScript function to insert an event's details into a database table using key-value pairs."}
{"func_name": "(anonymous)", "func_src_before": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n      if(row){\n        socket.emit('setNote', { note: row.note });\n      } else {\n        socket.emit('setNote', { note: \"\" });\n      }\n    //res.send(row.note);\n    });", "func_src_after": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n      if(row){\n        socket.emit('setNote', { note: row.note});\n      } else {\n        socket.emit('setNote', { note: \"\" });\n      }\n    //res.send(row.note);\n    });", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 83, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n"}, {"line_no": 3, "char_start": 98, "char_end": 150, "line": "        socket.emit('setNote', { note: row.note });\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 81, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n"}, {"line_no": 3, "char_start": 96, "char_end": 147, "line": "        socket.emit('setNote', { note: row.note});\n"}]}, "char_changes": {"deleted": [{"char_start": 49, "char_end": 52, "chars": "'\"+"}, {"char_start": 59, "char_end": 63, "chars": "+\"'\""}, {"char_start": 145, "char_end": 146, "chars": " "}], "added": [{"char_start": 49, "char_end": 53, "chars": "?\",["}, {"char_start": 60, "char_end": 61, "chars": "]"}]}, "commit_link": "github.com/yoyodyne/litwritesabook/commit/6ed77576195866819411628e917cd157e2a61361", "file_name": "app.js", "vul_type": "cwe-089", "commit_msg": "SQL injection prevented.", "description": "Write a JavaScript function that retrieves a note from a database using an ID and sends it through a socket."}
{"func_name": "(anonymous)", "func_src_before": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n      var newval;\n      if(row){\n        newval = row.note;\n      } else {\n        newval= \"\";\n      }\n      var op = data.op;\n      if(op.d!==null) {\n        newval = newval.slice(0,op.p)+newval.slice(op.p+op.d);\n      }\n      if(op.i!==null){\n        newval = newval.insert(op.p,op.i);\n      } \n      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES ('\"+data.id+\"', '\"+newval+\"' )\");\n    });", "func_src_after": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n      var newval;\n      if(row){\n        newval = row.note;\n      } else {\n        newval= \"\";\n      }\n      var op = data.op;\n      if(op.d!==null) {\n        newval = newval.slice(0,op.p)+newval.slice(op.p+op.d);\n      }\n      if(op.i!==null){\n        newval = newval.insert(op.p,op.i);\n      } \n      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES (?,?)\",[data.id,newval]);\n    });", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 83, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n"}, {"line_no": 15, "char_start": 380, "char_end": 479, "line": "      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES ('\"+data.id+\"', '\"+newval+\"' )\");\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 81, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n"}, {"line_no": 15, "char_start": 378, "char_end": 469, "line": "      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES (?,?)\",[data.id,newval]);\n"}]}, "char_changes": {"deleted": [{"char_start": 49, "char_end": 52, "chars": "'\"+"}, {"char_start": 59, "char_end": 63, "chars": "+\"'\""}, {"char_start": 446, "char_end": 476, "chars": "'\"+data.id+\"', '\"+newval+\"' )\""}], "added": [{"char_start": 49, "char_end": 53, "chars": "?\",["}, {"char_start": 60, "char_end": 61, "chars": "]"}, {"char_start": 444, "char_end": 466, "chars": "?,?)\",[data.id,newval]"}]}, "commit_link": "github.com/yoyodyne/litwritesabook/commit/6ed77576195866819411628e917cd157e2a61361", "file_name": "app.js", "vul_type": "cwe-089", "commit_msg": "SQL injection prevented.", "description": "In JavaScript, write a function that updates a note in a database by either deleting or inserting characters based on the provided operation object."}
{"func_name": "UserController.createOrJoinTeam", "func_src_before": "UserController.createOrJoinTeam = function(id, code, callback){\n\n  if (!code){\n    return callback({\n      message: \"Please enter a team name.\"\n    });\n  }\n\n  User.find({\n    teamCode: code\n  })\n  .select('profile.name')\n  .exec(function(err, users){\n    // Check to see if this team is joinable (< team max size)\n\n    if (users.length >= maxTeamSize){\n      return callback({\n        message: \"Team is full.\"\n      });\n    }\n\n    // Otherwise, we can add that person to the team.\n    User.findOneAndUpdate({\n      _id: id,\n      verified: true\n    },{\n      $set: {\n        teamCode: code\n      }\n    }, {\n      new: true\n    },\n    callback);\n\n  });\n};", "func_src_after": "UserController.createOrJoinTeam = function(id, code, callback){\n\n  if (!code){\n    return callback({\n      message: \"Please enter a team name.\"\n    });\n  }\n\n  if (typeof code !== 'string') {\n    return callback({\n      message: \"Get outta here, punk!\"\n    });\n  }\n\n  User.find({\n    teamCode: code\n  })\n  .select('profile.name')\n  .exec(function(err, users){\n    // Check to see if this team is joinable (< team max size)\n    if (users.length >= maxTeamSize){\n      return callback({\n        message: \"Team is full.\"\n      });\n    }\n\n    // Otherwise, we can add that person to the team.\n    User.findOneAndUpdate({\n      _id: id,\n      verified: true\n    },{\n      $set: {\n        teamCode: code\n      }\n    }, {\n      new: true\n    },\n    callback);\n\n  });\n};", "line_changes": {"deleted": [{"line_no": 15, "char_start": 314, "char_end": 315, "line": "\n"}], "added": [{"line_no": 9, "char_start": 157, "char_end": 191, "line": "  if (typeof code !== 'string') {\n"}, {"line_no": 10, "char_start": 191, "char_end": 213, "line": "    return callback({\n"}, {"line_no": 11, "char_start": 213, "char_end": 252, "line": "      message: \"Get outta here, punk!\"\n"}, {"line_no": 12, "char_start": 252, "char_end": 260, "line": "    });\n"}, {"line_no": 13, "char_start": 260, "char_end": 264, "line": "  }\n"}, {"line_no": 14, "char_start": 264, "char_end": 265, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 314, "char_end": 315, "chars": "\n"}], "added": [{"char_start": 157, "char_end": 265, "chars": "  if (typeof code !== 'string') {\n    return callback({\n      message: \"Get outta here, punk!\"\n    });\n  }\n\n"}]}, "commit_link": "github.com/techx/quill/commit/88a42ea6c4bcdee1b180b0a140d8886c3b5c543e", "file_name": "UserController.js", "vul_type": "cwe-089", "commit_msg": "Security: `/users/:id/team` NoSQL Injection Fix", "description": "Write a JavaScript function that allows a user to join a team by name if there's space available."}
{"func_name": "(anonymous)", "func_src_before": "socket.on(\"setNote\",function(data){\n\toldval = data.note;\n\t$(\"#note\").html(data.note);\n});", "func_src_after": "socket.on(\"setNote\",function(data){\n\toldval = data.note;\n\t$(\"#note\").html(oldval);\n});", "line_changes": {"deleted": [{"line_no": 3, "char_start": 57, "char_end": 86, "line": "\t$(\"#note\").html(data.note);\n"}], "added": [{"line_no": 3, "char_start": 57, "char_end": 83, "line": "\t$(\"#note\").html(oldval);\n"}]}, "char_changes": {"deleted": [{"char_start": 74, "char_end": 83, "chars": "data.note"}], "added": [{"char_start": 74, "char_end": 80, "chars": "oldval"}]}, "commit_link": "github.com/yoyodyne/litwritesabook/commit/6ed77576195866819411628e917cd157e2a61361", "file_name": "main.js", "vul_type": "cwe-089", "commit_msg": "SQL injection prevented.", "description": "Write a JavaScript code snippet using Socket.IO that updates the HTML content of an element with an ID of 'note' when a 'setNote' event is received."}
{"func_name": "verify_credentials", "func_src_before": "    async def verify_credentials(self, login, password):\n        \"\"\" verify login and password \"\"\"\n\n        try:\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                SELECT id, password, salt\n                FROM aio.users\n                WHERE aio.users.login = '{login}'\"\"\"\n                        await cur.execute(query)\n                        async for user_id, user_password, user_salt in cur:\n                            _, test_hash = hashpass(password, user_salt)\n                            if test_hash.decode() == user_password:\n                                return user_id\n        except Exception as err:\n            print(err)\n            raise HTTPForbidden()", "func_src_after": "    async def verify_credentials(self, login, password):\n        \"\"\" verify login and password \"\"\"\n\n        try:\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                SELECT id, password, salt\n                FROM aio.users\n                WHERE aio.users.login = %s\"\"\"\n                        await cur.execute(query, (login, ))\n                        async for user_id, user_password, user_salt in cur:\n                            _, test_hash = hashpass(password, user_salt)\n                            if test_hash.decode() == user_password:\n                                return user_id\n        except Exception as err:\n            print(err)\n            raise web.HTTPForbidden()", "line_changes": {"deleted": [{"line_no": 11, "char_start": 387, "char_end": 440, "line": "                WHERE aio.users.login = '{login}'\"\"\"\n"}, {"line_no": 12, "char_start": 440, "char_end": 489, "line": "                        await cur.execute(query)\n"}, {"line_no": 19, "char_start": 809, "char_end": 842, "line": "            raise HTTPForbidden()\n"}], "added": [{"line_no": 11, "char_start": 387, "char_end": 433, "line": "                WHERE aio.users.login = %s\"\"\"\n"}, {"line_no": 12, "char_start": 433, "char_end": 493, "line": "                        await cur.execute(query, (login, ))\n"}, {"line_no": 19, "char_start": 813, "char_end": 850, "line": "            raise web.HTTPForbidden()\n"}]}, "char_changes": {"deleted": [{"char_start": 427, "char_end": 436, "chars": "'{login}'"}], "added": [{"char_start": 427, "char_end": 429, "chars": "%s"}, {"char_start": 480, "char_end": 491, "chars": ", (login, )"}, {"char_start": 831, "char_end": 835, "chars": "web."}]}, "commit_link": "github.com/TeaTracer/aio-test/commit/3da13f66b0c1ab1d26bf4b56f476ade60a43d8d4", "file_name": "db.py", "vul_type": "cwe-089", "commit_msg": "Fix sql injections in token and password verifications. Fix HTTTPForbidden exception.", "description": "Write a Python function using `aiopg` to asynchronously verify user login credentials against a PostgreSQL database."}
{"func_name": "reset_passwd", "func_src_before": "@app.route('/reset-password', methods=['GET', 'POST'])\ndef reset_passwd():\n  if request.method == 'POST':\n    if 'username' not in session:\n      flash('You did something naughty')\n      return redirect(url_for('forgot_passwd'))\n    new_password = request.form['password']\n    new_password2 = request.form['password2']\n\n    # Get the user's email\n    query = text(\"SELECT email FROM members WHERE user_id=:id\")\n    result = connection.execute(query, id=str(session['user_id']))\n    email = result.first()[0]\n\n    if new_password != new_password2:\n      flash('Passwords do not match. Please try again!')\n      return render_template('reset_password.html')\n    elif auth.passwd_reset(session['username'], new_password, connection, \\\n                           email=email):\n      session.pop('username')\n      flash('Password successfully changed.')\n      return redirect(url_for('home'))\n    else:\n      flash('An unknown problem has occured. Please contact an admin!')\n      return render_template('reset_password.html')\n  else:\n    user_id = request.args.get('u', None)\n    reset_key = request.args.get('r', None)\n    if user_id == None or reset_key == None:\n      flash(\"Missing parameter. Try generating the link again?\")\n      return redirect(url_for('forgot_passwd'))\n    query = text(\"SELECT * FROM users NATURAL JOIN members WHERE user_id=:u\")\n    result = connection.execute(query, u=str(user_id))\n    if result.returns_rows and result.rowcount != 0:\n      result_cols = result.keys()\n      row = result.first()\n      q_dict = dict(zip(result_cols, row))\n      if int(reset_key) == auth.reset_key(q_dict['passwd'], q_dict['salt'],\n                                          q_dict['username']):\n        session['username'] = q_dict['username']\n        return render_template('reset_password.html')\n      flash(\"Incorrect reset_key. Try generating the link again?\")\n    return redirect(url_for('forgot_passwd'))", "func_src_after": "@app.route('/reset-password', methods=['GET', 'POST'])\ndef reset_passwd():\n  if request.method == 'POST':\n    if 'username' not in session:\n      flash('You did something naughty')\n      return redirect(url_for('forgot_passwd'))\n    new_password = request.form['password']\n    new_password2 = request.form['password2']\n\n    # Get the user's email\n    query = text(\"SELECT email FROM members WHERE user_id=:id\")\n    result = connection.execute(query, id=str(session['user_id']))\n    email = result.first()[0]\n\n    if new_password != new_password2:\n      flash('Passwords do not match. Please try again!')\n      return render_template('reset_password.html')\n    elif auth.passwd_reset(session['username'], new_password, connection, \\\n                           email=email):\n      session.pop('username')\n      flash('Password successfully changed.')\n      return redirect(url_for('home'))\n    else:\n      flash('An unknown problem has occured. Please contact an admin!')\n      return render_template('reset_password.html')\n  else:\n    user_id = request.args.get('u', None)\n    reset_key = request.args.get('r', None)\n    if user_id == None or reset_key == None:\n      flash(\"Missing parameter. Try generating the link again?\")\n      return redirect(url_for('forgot_passwd'))\n    query = text(\"SELECT * FROM users WHERE user_id=:u\")\n    result = connection.execute(query, u=str(user_id))\n    if result.returns_rows and result.rowcount != 0:\n      result_cols = result.keys()\n      row = result.first()\n      q_dict = dict(zip(result_cols, row))\n      if int(reset_key) == auth.reset_key(q_dict['passwd'], q_dict['salt'],\n                                          q_dict['username']):\n        session['username'] = q_dict['username']\n        return render_template('reset_password.html')\n      flash(\"Incorrect reset_key. Try generating the link again?\")\n    return redirect(url_for('forgot_passwd'))", "line_changes": {"deleted": [{"line_no": 32, "char_start": 1274, "char_end": 1352, "line": "    query = text(\"SELECT * FROM users NATURAL JOIN members WHERE user_id=:u\")\n"}], "added": [{"line_no": 32, "char_start": 1274, "char_end": 1331, "line": "    query = text(\"SELECT * FROM users WHERE user_id=:u\")\n"}]}, "char_changes": {"deleted": [{"char_start": 1312, "char_end": 1333, "chars": "NATURAL JOIN members "}], "added": []}, "commit_link": "github.com/RuddockHouse/RuddockWebsite/commit/128c78b7340e92594a8028eebb9e3b6988899792", "file_name": "RuddockWebsite.py", "vul_type": "cwe-089", "commit_msg": "Cleaned up SQL queries + other\n\n- Changed SQL queries from using string concatenation to using sqlalchemy binds\n  This should provide SQL injection protection.\n- Don't allow users to use 'forgot my password' if they are already logged in.", "description": "Write a Python Flask route to handle password reset functionality with form submission and user verification."}
{"func_name": "create_token", "func_src_before": "    async def create_token(self, uid):\n        \"\"\" get session token by user id \"\"\"\n        try:\n            token = hashtoken().decode()\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.tokens (token, user_id)\n                VALUES ('{token}', '{uid}') \"\"\"\n                        await cur.execute(query)\n                        return token\n        except Exception as err:\n            print(err)\n            raise HTTPForbidden()", "func_src_after": "    async def create_token(self, uid):\n        \"\"\" get session token by user id \"\"\"\n        try:\n            token = hashtoken().decode()\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.tokens (token, user_id)\n                VALUES ('{token}', '{uid}') \"\"\"\n                        await cur.execute(query)\n                        return token\n        except Exception as err:\n            print(err)\n            raise web.HTTPForbidden()", "line_changes": {"deleted": [{"line_no": 15, "char_start": 585, "char_end": 618, "line": "            raise HTTPForbidden()\n"}], "added": [{"line_no": 15, "char_start": 585, "char_end": 622, "line": "            raise web.HTTPForbidden()\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 603, "char_end": 607, "chars": "web."}]}, "commit_link": "github.com/TeaTracer/aio-test/commit/3da13f66b0c1ab1d26bf4b56f476ade60a43d8d4", "file_name": "db.py", "vul_type": "cwe-089", "commit_msg": "Fix sql injections in token and password verifications. Fix HTTTPForbidden exception.", "description": "Write a Python function using `aiopg` to asynchronously insert a new session token into a database for a given user ID and handle exceptions by raising an HTTP forbidden error."}
{"func_name": "_create_database", "func_src_before": "    @staticmethod\n    def _create_database(engine: \"Engine\", db: Text):\n        \"\"\"Create database `db` on `engine` if it does not exist.\"\"\"\n\n        import psycopg2\n\n        conn = engine.connect()\n\n        cursor = conn.connection.cursor()\n        cursor.execute(\"COMMIT\")\n        cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db}'\")\n        exists = cursor.fetchone()\n        if not exists:\n            try:\n                cursor.execute(f\"CREATE DATABASE {db}\")\n            except psycopg2.IntegrityError as e:\n                logger.error(f\"Could not create database '{db}': {e}\")\n\n        cursor.close()\n        conn.close()", "func_src_after": "    @staticmethod\n    def _create_database(engine: \"Engine\", db: Text):\n        \"\"\"Create database `db` on `engine` if it does not exist.\"\"\"\n\n        import psycopg2\n\n        conn = engine.connect()\n\n        cursor = conn.connection.cursor()\n        cursor.execute(\"COMMIT\")\n        cursor.execute(\n            f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = ?\", (db,)  # nosec\n        )\n        exists = cursor.fetchone()\n        if not exists:\n            try:\n                cursor.execute(f\"CREATE DATABASE {db}\")\n            except psycopg2.IntegrityError as e:\n                logger.error(f\"Could not create database '{db}': {e}\")\n\n        cursor.close()\n        conn.close()", "line_changes": {"deleted": [{"line_no": 11, "char_start": 275, "char_end": 362, "line": "        cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db}'\")\n"}], "added": [{"line_no": 11, "char_start": 275, "char_end": 299, "line": "        cursor.execute(\n"}, {"line_no": 12, "char_start": 299, "char_end": 385, "line": "            f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = ?\", (db,)  # nosec\n"}, {"line_no": 13, "char_start": 385, "char_end": 395, "line": "        )\n"}]}, "char_changes": {"deleted": [{"char_start": 353, "char_end": 360, "chars": "'{db}'\""}], "added": [{"char_start": 298, "char_end": 311, "chars": "\n            "}, {"char_start": 366, "char_end": 393, "chars": "?\", (db,)  # nosec\n        "}]}, "commit_link": "github.com/RasaHQ/rasa_nlu/commit/d68ec7bde34463cc62d9319db4adfd1dff0361cf", "file_name": "tracker_store.py", "vul_type": "cwe-089", "commit_msg": "fix tracker store SQL injection possibility", "parent_commit": "251c10208f9b599ccd511c3ff8d0b41db972a8f9", "description": "Write a Python function to check for the existence of a database and create it if it doesn't exist using psycopg2."}
{"func_name": "show_user_profile", "func_src_before": "@app.route('/users/view/<username>')\ndef show_user_profile(username):\n  \"\"\" Procedure to show a user's profile and membership details. \"\"\"\n  cols = [[\"username\"], [\"fname\", \"lname\"], [\"nickname\"], [\"bday\"], \\\n          [\"email\"], [\"email2\"], [\"status\"], [\"matriculate_year\"], \\\n          [\"grad_year\"], [\"msc\"], [\"phone\"], [\"building\", \"room_num\"], \\\n          [\"membership\"], [\"major\"], [\"uid\"], [\"isabroad\"]]\n  display = [\"Username\", \"Name\", \"Nickname\", \"Birthday\", \"Primary Email\", \\\n             \"Secondary Email\", \"Status\", \"Matriculation Year\", \\\n             \"Graduation Year\", \"MSC\", \"Phone Number\", \"Residence\", \\\n             \"Membership\", \"Major\", \"UID\", \"Is Abroad\"]\n  d_dict = OrderedDict(zip(display, cols))\n  #d_dict defines the order and mapping of displayed attributes to sql columns\n  query = text(\"SELECT * FROM users Natural JOIN members where username=:u\")\n  result = connection.execute(query, u=str(username))\n  if result.returns_rows and result.rowcount != 0:\n    result_cols = result.keys()\n    r = result.first()\n    q_dict = dict(zip(result_cols, r)) #q_dict maps sql columns to values\n    if not q_dict['usenickname']:\n      d_dict.pop('Nickname')\n    return render_template('view_user.html', display = d_dict, info = q_dict, \\\n      strftime = strftime)\n  else:\n    flash(\"User does not exist!\")\n    return redirect(url_for('home'))", "func_src_after": "@app.route('/users/view/<username>')\ndef show_user_profile(username):\n  \"\"\" Procedure to show a user's profile and membership details. \"\"\"\n  cols = [[\"username\"], [\"fname\", \"lname\"], [\"nickname\"], [\"bday\"], \\\n          [\"email\"], [\"email2\"], [\"status\"], [\"matriculate_year\"], \\\n          [\"grad_year\"], [\"msc\"], [\"phone\"], [\"building\", \"room_num\"], \\\n          [\"membership\"], [\"major\"], [\"uid\"], [\"isabroad\"]]\n  display = [\"Username\", \"Name\", \"Nickname\", \"Birthday\", \"Primary Email\", \\\n             \"Secondary Email\", \"Status\", \"Matriculation Year\", \\\n             \"Graduation Year\", \"MSC\", \"Phone Number\", \"Residence\", \\\n             \"Membership\", \"Major\", \"UID\", \"Is Abroad\"]\n  d_dict = OrderedDict(zip(display, cols))\n  #d_dict defines the order and mapping of displayed attributes to sql columns\n  query = text(\"SELECT * FROM users NATURAL JOIN members WHERE username=:u\")\n  result = connection.execute(query, u=str(username))\n  if result.returns_rows and result.rowcount != 0:\n    result_cols = result.keys()\n    r = result.first()\n    q_dict = dict(zip(result_cols, r)) #q_dict maps sql columns to values\n    if not q_dict['usenickname']:\n      d_dict.pop('Nickname')\n    return render_template('view_user.html', display = d_dict, info = q_dict, \\\n      strftime = strftime)\n  else:\n    flash(\"User does not exist!\")\n    return redirect(url_for('home'))", "line_changes": {"deleted": [{"line_no": 14, "char_start": 801, "char_end": 878, "line": "  query = text(\"SELECT * FROM users Natural JOIN members where username=:u\")\n"}], "added": [{"line_no": 14, "char_start": 801, "char_end": 878, "line": "  query = text(\"SELECT * FROM users NATURAL JOIN members WHERE username=:u\")\n"}]}, "char_changes": {"deleted": [{"char_start": 838, "char_end": 844, "chars": "atural"}, {"char_start": 858, "char_end": 863, "chars": "where"}], "added": [{"char_start": 838, "char_end": 844, "chars": "ATURAL"}, {"char_start": 858, "char_end": 863, "chars": "WHERE"}]}, "commit_link": "github.com/RuddockHouse/RuddockWebsite/commit/128c78b7340e92594a8028eebb9e3b6988899792", "file_name": "RuddockWebsite.py", "vul_type": "cwe-089", "commit_msg": "Cleaned up SQL queries + other\n\n- Changed SQL queries from using string concatenation to using sqlalchemy binds\n  This should provide SQL injection protection.\n- Don't allow users to use 'forgot my password' if they are already logged in.", "description": "Write a Python Flask function to display a user's profile by their username."}
{"func_name": "sign_up_page", "func_src_before": "def sign_up_page():\n  if request.method == 'POST':\n    username = request.form['username']\n    password = request.form['password']\n    hashed_password = pwd_context.encrypt(password)\n    email = request.form['email']\n    id = 1\n\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n\n      query = \"\"\"\n            INSERT INTO USERS (USERNAME, PASSWORD, EMAIL) \n            VALUES ('%s', '%s', '%s')\"\"\" % (username, hashed_password, email)\n\n      cursor.execute(query)\n\n      connection.commit()\n    return render_template('home.html')\n\n  else:\n    return render_template('sign_up.html')", "func_src_after": "def sign_up_page():\n  if request.method == 'POST':\n    username = request.form['username']\n    password = request.form['password']\n    hashed_password = pwd_context.encrypt(password)\n    email = request.form['email']\n    id = 1\n\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n\n      query = \"\"\"\n            INSERT INTO USERS (USERNAME, PASSWORD, EMAIL) \n            VALUES (%s, %s, %s)\"\"\"\n\n      cursor.execute(query, (username, hashed_password, email))\n\n      connection.commit()\n    return render_template('home.html')\n\n  else:\n    return render_template('sign_up.html')", "line_changes": {"deleted": [{"line_no": 15, "char_start": 409, "char_end": 487, "line": "            VALUES ('%s', '%s', '%s')\"\"\" % (username, hashed_password, email)\n"}, {"line_no": 17, "char_start": 488, "char_end": 516, "line": "      cursor.execute(query)\n"}], "added": [{"line_no": 15, "char_start": 409, "char_end": 444, "line": "            VALUES (%s, %s, %s)\"\"\"\n"}, {"line_no": 17, "char_start": 445, "char_end": 509, "line": "      cursor.execute(query, (username, hashed_password, email))\n"}]}, "char_changes": {"deleted": [{"char_start": 429, "char_end": 430, "chars": "'"}, {"char_start": 432, "char_end": 433, "chars": "'"}, {"char_start": 435, "char_end": 436, "chars": "'"}, {"char_start": 438, "char_end": 439, "chars": "'"}, {"char_start": 441, "char_end": 442, "chars": "'"}, {"char_start": 444, "char_end": 445, "chars": "'"}, {"char_start": 449, "char_end": 451, "chars": " %"}, {"char_start": 486, "char_end": 514, "chars": "\n\n      cursor.execute(query"}], "added": [{"char_start": 443, "char_end": 472, "chars": "\n\n      cursor.execute(query,"}]}, "commit_link": "github.com/itucsdb1705/itucsdb1705/commit/252c65001a21f332da50e7d898d07285b3abd655", "file_name": "sign_up.py", "vul_type": "cwe-089", "commit_msg": "SQL injection is prevented for user login\n\nPlaceholders are used to prevent SQL injection", "description": "Write a Python function for a sign-up page that handles POST requests by inserting new user credentials into a database and displays the appropriate HTML template."}
{"func_name": "authenticate", "func_src_before": "def authenticate(user, passwd, db):\n  \"\"\" Takes a username, password, and connection object as input, and checks\n  if this corresponds to an actual user. Salts the password as necessary. \"\"\"\n  # get salt and add to password if necessary\n  saltQuery = db.execute(\"SELECT salt FROM users WHERE username='\" + user \\\n          + \"'\")\n  # if there's a salt set, then use it\n  if saltQuery.returns_rows:\n    salt = saltQuery.first()\n    # make sure username is found and salt isn't null\n    if salt != None and salt[0] != None:\n      passwd = salt[0] + passwd\n\n  # query whether there's a match for the username and password\n  query = db.execute(\"SELECT * FROM users WHERE username='\" + user + \"' AND \" \\\n          + \"passwd=MD5('\" + passwd + \"')\")\n\n  row = query.first()\n  if (query.returns_rows and row != None):\n    return row[0]\n  return 0", "func_src_after": "def authenticate(user, passwd, db):\n  \"\"\" Takes a username, password, and connection object as input, and checks\n  if this corresponds to an actual user. Salts the password as necessary. \"\"\"\n  # get salt and add to password if necessary\n  saltQuery = db.execute(text(\"SELECT salt FROM users WHERE username=:u\"),\n                         u = user)\n  # if there's a salt set, then use it\n  if saltQuery.returns_rows:\n    salt = saltQuery.first()\n    # make sure username is found and salt isn't null\n    if salt != None and salt[0] != None:\n      passwd = salt[0] + passwd\n\n  # query whether there's a match for the username and password\n  query = db.execute(text(\"SELECT * FROM users WHERE username=:u AND \" \\\n                          + \"passwd=MD5(:p)\"), u = user, p = passwd)\n\n  row = query.first()\n  if (query.returns_rows and row != None):\n    return row[0]\n  return 0", "line_changes": {"deleted": [{"line_no": 5, "char_start": 237, "char_end": 313, "line": "  saltQuery = db.execute(\"SELECT salt FROM users WHERE username='\" + user \\\n"}, {"line_no": 6, "char_start": 313, "char_end": 330, "line": "          + \"'\")\n"}, {"line_no": 15, "char_start": 619, "char_end": 699, "line": "  query = db.execute(\"SELECT * FROM users WHERE username='\" + user + \"' AND \" \\\n"}, {"line_no": 16, "char_start": 699, "char_end": 743, "line": "          + \"passwd=MD5('\" + passwd + \"')\")\n"}], "added": [{"line_no": 5, "char_start": 237, "char_end": 312, "line": "  saltQuery = db.execute(text(\"SELECT salt FROM users WHERE username=:u\"),\n"}, {"line_no": 6, "char_start": 312, "char_end": 347, "line": "                         u = user)\n"}, {"line_no": 15, "char_start": 636, "char_end": 709, "line": "  query = db.execute(text(\"SELECT * FROM users WHERE username=:u AND \" \\\n"}, {"line_no": 16, "char_start": 709, "char_end": 778, "line": "                          + \"passwd=MD5(:p)\"), u = user, p = passwd)\n"}]}, "char_changes": {"deleted": [{"char_start": 301, "char_end": 328, "chars": "'\" + user \\\n          + \"'\""}, {"char_start": 676, "char_end": 690, "chars": "'\" + user + \"'"}, {"char_start": 709, "char_end": 741, "chars": "+ \"passwd=MD5('\" + passwd + \"')\""}], "added": [{"char_start": 262, "char_end": 267, "chars": "text("}, {"char_start": 306, "char_end": 345, "chars": ":u\"),\n                         u = user"}, {"char_start": 657, "char_end": 662, "chars": "text("}, {"char_start": 698, "char_end": 700, "chars": ":u"}, {"char_start": 719, "char_end": 776, "chars": "                + \"passwd=MD5(:p)\"), u = user, p = passwd"}]}, "commit_link": "github.com/RuddockHouse/RuddockWebsite/commit/128c78b7340e92594a8028eebb9e3b6988899792", "file_name": "auth.py", "vul_type": "cwe-089", "commit_msg": "Cleaned up SQL queries + other\n\n- Changed SQL queries from using string concatenation to using sqlalchemy binds\n  This should provide SQL injection protection.\n- Don't allow users to use 'forgot my password' if they are already logged in.", "description": "Write a Python function to verify a user's credentials against a database, including password salting."}
{"func_name": "urlmatch", "func_src_before": "def urlmatch(url):\n    short = url.lower()\n    match = query_db(\"select dest from urls where short='%s'\" % (short))\n    if len(match) == 0:\n        # no match found\n        return \"No match found\"\n    destination = match[0]['dest']\n    # work out src ip version\n    srcip = request.remote_addr\n    ip = ipaddr.IPAddress(srcip)\n    hittoday = query_db(\"select * from hits where short='%s' and hitdate=date('now')\" % (short))\n    hitfield = \"hits%s\" % (ip.version)\n    # update hit counters\n    if len(hittoday) == 0:\n        get_db().execute(\"insert into hits (short,hitdate,%s) values (?,date('now'),1)\" % (hitfield), [short])\n    else:\n        get_db().execute(\"update hits set %s=%s+1 where short=?\" % (hitfield,hitfield), [short])\n    get_db().commit()\n    # finally redirect\n    return redirect(destination,code=302)", "func_src_after": "def urlmatch(url):\n    short = url.lower()\n    match = query_db(\"select dest from urls where short=?\", [short])\n    if len(match) == 0:\n        # no match found\n        return \"No match found\"\n    destination = match[0]['dest']\n    # work out src ip version\n    srcip = request.remote_addr\n    ip = ipaddr.IPAddress(srcip)\n    hittoday = query_db(\"select * from hits where short=? and hitdate=date('now')\", [short])\n    hitfield = \"hits%s\" % (ip.version)\n    # update hit counters\n    if len(hittoday) == 0:\n        get_db().execute(\"insert into hits (short,hitdate,%s) values (?,date('now'),1)\" % (hitfield) ,[short])\n    else:\n        get_db().execute(\"update hits set %s=%s+1 where short=?\" % (hitfield,hitfield) ,[short])\n    get_db().commit()\n    # finally redirect\n    return redirect(destination,code=302)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 43, "char_end": 116, "line": "    match = query_db(\"select dest from urls where short='%s'\" % (short))\n"}, {"line_no": 11, "char_start": 327, "char_end": 424, "line": "    hittoday = query_db(\"select * from hits where short='%s' and hitdate=date('now')\" % (short))\n"}, {"line_no": 15, "char_start": 516, "char_end": 627, "line": "        get_db().execute(\"insert into hits (short,hitdate,%s) values (?,date('now'),1)\" % (hitfield), [short])\n"}, {"line_no": 17, "char_start": 637, "char_end": 734, "line": "        get_db().execute(\"update hits set %s=%s+1 where short=?\" % (hitfield,hitfield), [short])\n"}], "added": [{"line_no": 3, "char_start": 43, "char_end": 112, "line": "    match = query_db(\"select dest from urls where short=?\", [short])\n"}, {"line_no": 11, "char_start": 323, "char_end": 416, "line": "    hittoday = query_db(\"select * from hits where short=? and hitdate=date('now')\", [short])\n"}, {"line_no": 15, "char_start": 508, "char_end": 619, "line": "        get_db().execute(\"insert into hits (short,hitdate,%s) values (?,date('now'),1)\" % (hitfield) ,[short])\n"}, {"line_no": 17, "char_start": 629, "char_end": 726, "line": "        get_db().execute(\"update hits set %s=%s+1 where short=?\" % (hitfield,hitfield) ,[short])\n"}]}, "char_changes": {"deleted": [{"char_start": 99, "char_end": 108, "chars": "'%s'\" % ("}, {"char_start": 113, "char_end": 114, "chars": ")"}, {"char_start": 383, "char_end": 387, "chars": "'%s'"}, {"char_start": 412, "char_end": 416, "chars": " % ("}, {"char_start": 421, "char_end": 422, "chars": ")"}, {"char_start": 616, "char_end": 617, "chars": ","}, {"char_start": 723, "char_end": 724, "chars": ","}], "added": [{"char_start": 99, "char_end": 104, "chars": "?\", ["}, {"char_start": 109, "char_end": 110, "chars": "]"}, {"char_start": 379, "char_end": 380, "chars": "?"}, {"char_start": 405, "char_end": 408, "chars": ", ["}, {"char_start": 413, "char_end": 414, "chars": "]"}, {"char_start": 608, "char_end": 609, "chars": " "}, {"char_start": 715, "char_end": 716, "chars": " "}]}, "commit_link": "github.com/uknof/shortener/commit/3e6fe206f764afb17af14609474d0da36705f3e3", "file_name": "shortner.py", "vul_type": "cwe-089", "commit_msg": "fix up sql to avoid injection", "description": "Write a Python function that matches a shortened URL to its destination, updates hit counters based on IP version, and redirects to the destination URL."}
{"func_name": "unique_short", "func_src_before": "def unique_short():\n    matches = 1\n    while matches == 1:\n        short = generate_short()\n    \tmatches = query_db(\"select * from urls where short='%s'\" % (short))\n    return short", "func_src_after": "def unique_short():\n    matches = 1\n    while matches == 1:\n        short = generate_short()\n    \tmatches = query_db(\"select * from urls where short=?\", [short])\n    return short", "line_changes": {"deleted": [{"line_no": 5, "char_start": 93, "char_end": 166, "line": "    \tmatches = query_db(\"select * from urls where short='%s'\" % (short))\n"}], "added": [{"line_no": 5, "char_start": 93, "char_end": 162, "line": "    \tmatches = query_db(\"select * from urls where short=?\", [short])\n"}]}, "char_changes": {"deleted": [{"char_start": 149, "char_end": 158, "chars": "'%s'\" % ("}, {"char_start": 163, "char_end": 164, "chars": ")"}], "added": [{"char_start": 149, "char_end": 154, "chars": "?\", ["}, {"char_start": 159, "char_end": 160, "chars": "]"}]}, "commit_link": "github.com/uknof/shortener/commit/3e6fe206f764afb17af14609474d0da36705f3e3", "file_name": "shortner.py", "vul_type": "cwe-089", "commit_msg": "fix up sql to avoid injection", "description": "Write a Python function named `unique_short` that generates a unique short string by checking against a database until no matches are found."}
{"func_name": "store_order", "func_src_before": "    async def store_order(self, uid, tree_id, order, ordered_at):\n        \"\"\" store remote order \"\"\"\n\n        try:\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.orders(manager, tree, order_data, ordered_at)\n                VALUES ('{uid}', '{tree_id}', '{order_data}', '{ordered_at}') \"\"\"\n                        return await cur.scalar(query)\n        except Exception as err:\n            print(err)\n            raise HTTPForbidden()", "func_src_after": "    async def store_order(self, uid, tree_id, order, ordered_at):\n        \"\"\" store remote order \"\"\"\n\n        try:\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.orders(manager, tree, order_data, ordered_at)\n                VALUES ('{uid}', '{tree_id}', '{order_data}', '{ordered_at}') \"\"\"\n                        return await cur.scalar(query)\n        except Exception as err:\n            print(err)\n            raise web.HTTPForbidden()", "line_changes": {"deleted": [{"line_no": 14, "char_start": 587, "char_end": 620, "line": "            raise HTTPForbidden()\n"}], "added": [{"line_no": 14, "char_start": 587, "char_end": 624, "line": "            raise web.HTTPForbidden()\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 605, "char_end": 609, "chars": "web."}]}, "commit_link": "github.com/TeaTracer/aio-test/commit/3da13f66b0c1ab1d26bf4b56f476ade60a43d8d4", "file_name": "db.py", "vul_type": "cwe-089", "commit_msg": "Fix sql injections in token and password verifications. Fix HTTTPForbidden exception.", "description": "Write a Python function using `aiopg` to asynchronously insert an order record into a PostgreSQL database."}
{"func_name": "change_user_settings", "func_src_before": "@app.route('/users/edit/<username>', methods=['GET', 'POST'])\ndef change_user_settings(username):\n  \"\"\" Procedure to process the login page. Also handles authentication. \"\"\"\n\n  if 'username' not in session:\n    flash('You must be logged in for that.')\n    return redirect(url_for('login'))\n\n  if session['username'] != username:\n    flash('You cannot edit this user\\'s information.')\n    return redirect(url_for('show_user_profile', username=username))\n\n\n  params = {}\n  tags = ['nickname', 'usenickname', 'bday', 'email', 'email2', 'msc', 'phone', \\\n      'building', 'room_num', 'major', 'isabroad']\n  tag_names = [\"Nickname\", \"Use Nickname\", \"Birthday\", \"Email Address\", \\\n      \"Alt. Email Address\", \"MSC\", \"Phone Number\", \"Building Name\", \"Room Number\", \\\n      \"Major\", \"Is Abroad\"]\n\n  # Get stored values from database\n  query = text(\"SELECT * FROM users Natural JOIN members where username=:u\")\n  result = connection.execute(query, u=str(username))\n  if result.returns_rows and result.rowcount != 0:\n    result_cols = result.keys()\n    r = result.first()\n    stored_params = dict(zip(result_cols, r)) #stored_params maps sql columns to values\n\n  # Update if needed\n  if request.method == 'POST':\n\n    for (i, tag) in enumerate(tags):\n      params[tag] = request.form[tag]\n\n\n    for (i, tag) in enumerate(tags):\n      if str(params[tag]) != str(stored_params[tag]):\n\n        new_val = str(params[tag])\n        if tag in ['usenickname', 'msc', 'room_num', 'isabroad']:\n          new_val = int(new_val)\n\n        query = text(\"UPDATE members SET %s = :val WHERE user_id = :u\" % tag)\n        results = connection.execute(query, u=session['user_id'], val=new_val)\n\n        flash(\"%s was updated!\" % tag_names[i])\n\n\n  if not params:\n    params = stored_params\n\n  return render_template('edit_user.html', params = params)", "func_src_after": "@app.route('/users/edit/<username>', methods=['GET', 'POST'])\ndef change_user_settings(username):\n  \"\"\" Procedure to process the login page. Also handles authentication. \"\"\"\n\n  if 'username' not in session:\n    flash('You must be logged in for that.')\n    return redirect(url_for('login'))\n\n  if session['username'] != username:\n    flash('You cannot edit this user\\'s information.')\n    return redirect(url_for('show_user_profile', username=username))\n\n\n  params = {}\n  tags = ['nickname', 'usenickname', 'bday', 'email', 'email2', 'msc', 'phone', \\\n      'building', 'room_num', 'major', 'isabroad']\n  tag_names = [\"Nickname\", \"Use Nickname\", \"Birthday\", \"Email Address\", \\\n      \"Alt. Email Address\", \"MSC\", \"Phone Number\", \"Building Name\", \"Room Number\", \\\n      \"Major\", \"Is Abroad\"]\n\n  # Get stored values from database\n  query = text(\"SELECT * FROM users NATURAL JOIN members WHERE username=:u\")\n  result = connection.execute(query, u=str(username))\n  if result.returns_rows and result.rowcount != 0:\n    result_cols = result.keys()\n    r = result.first()\n    stored_params = dict(zip(result_cols, r)) #stored_params maps sql columns to values\n\n  # Update if needed\n  if request.method == 'POST':\n\n    for (i, tag) in enumerate(tags):\n      params[tag] = request.form[tag]\n\n\n    for (i, tag) in enumerate(tags):\n      if str(params[tag]) != str(stored_params[tag]):\n\n        new_val = str(params[tag])\n        if tag in ['usenickname', 'msc', 'room_num', 'isabroad']:\n          new_val = int(new_val)\n\n        query = text(\"UPDATE members SET %s = :val WHERE user_id = :u\" % tag)\n        results = connection.execute(query, u=session['user_id'], val=new_val)\n\n        flash(\"%s was updated!\" % tag_names[i])\n\n\n  if not params:\n    params = stored_params\n\n  return render_template('edit_user.html', params = params)", "line_changes": {"deleted": [{"line_no": 22, "char_start": 826, "char_end": 903, "line": "  query = text(\"SELECT * FROM users Natural JOIN members where username=:u\")\n"}], "added": [{"line_no": 22, "char_start": 826, "char_end": 903, "line": "  query = text(\"SELECT * FROM users NATURAL JOIN members WHERE username=:u\")\n"}]}, "char_changes": {"deleted": [{"char_start": 863, "char_end": 869, "chars": "atural"}, {"char_start": 883, "char_end": 888, "chars": "where"}], "added": [{"char_start": 863, "char_end": 869, "chars": "ATURAL"}, {"char_start": 883, "char_end": 888, "chars": "WHERE"}]}, "commit_link": "github.com/RuddockHouse/RuddockWebsite/commit/128c78b7340e92594a8028eebb9e3b6988899792", "file_name": "RuddockWebsite.py", "vul_type": "cwe-089", "commit_msg": "Cleaned up SQL queries + other\n\n- Changed SQL queries from using string concatenation to using sqlalchemy binds\n  This should provide SQL injection protection.\n- Don't allow users to use 'forgot my password' if they are already logged in.", "description": "Write a Python Flask function to edit user settings, checking if the user is logged in and authorized to edit the specified user's profile."}
{"func_name": "login_page", "func_src_before": "def login_page():\n  if request.method == 'POST':\n    login_email = request.form['login_email']\n    # print( \"%s\" % login_email)\n    login_password = request.form['login_password']\n    hashed_login_password = pwd_context.encrypt(login_password)\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n      statement = \"\"\"SELECT USERNAME FROM USERS WHERE USERNAME = %s\"\"\"\n      cursor.execute(statement, [login_email])\n      db_username = cursor.fetchone()\n\n      if db_username is not None:  # check whether the user exists\n        print('%s' % db_username)\n        user = load_user(db_username);\n        login_user(user);\n        print(\"%s\" % user.username)\n        # print('%s %s' % db_username[0][0], db_username[0][1] ) if the fetchall method is used debug using this line\n\n    return render_template('home.html')\n  else:\n    return render_template('login.html')", "func_src_after": "def login_page():\n  if request.method == 'POST':\n    login_email = request.form['login_email']\n    # print( \"%s\" % login_email)\n    login_password = request.form['login_password']\n    hashed_login_password = pwd_context.encrypt(login_password)\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n      statement = \"\"\"SELECT USERNAME FROM USERS WHERE USERNAME = %s\"\"\"\n      cursor.execute(statement, [login_email])\n      db_username = cursor.fetchone()\n\n      if db_username is not None:  # check whether the user exists\n        print('%s' % db_username)\n        user = load_user(db_username)\n        login_user(user)\n        print(\"%s\" % user.username)\n        # print('%s %s' % db_username[0][0], db_username[0][1] ) if the fetchall method is used debug using this line\n\n    return render_template('home.html')\n  else:\n    return render_template('login.html')", "line_changes": {"deleted": [{"line_no": 16, "char_start": 604, "char_end": 643, "line": "        user = load_user(db_username);\n"}, {"line_no": 17, "char_start": 643, "char_end": 669, "line": "        login_user(user);\n"}], "added": [{"line_no": 16, "char_start": 604, "char_end": 642, "line": "        user = load_user(db_username)\n"}, {"line_no": 17, "char_start": 642, "char_end": 667, "line": "        login_user(user)\n"}]}, "char_changes": {"deleted": [{"char_start": 641, "char_end": 642, "chars": ";"}, {"char_start": 667, "char_end": 668, "chars": ";"}], "added": []}, "commit_link": "github.com/itucsdb1705/itucsdb1705/commit/252c65001a21f332da50e7d898d07285b3abd655", "file_name": "login.py", "vul_type": "cwe-089", "commit_msg": "SQL injection is prevented for user login\n\nPlaceholders are used to prevent SQL injection", "description": "Create a Python function for handling user login that checks credentials and renders different templates based on the HTTP method."}
{"func_name": "PropertyPage", "func_src_before": "@site.route('/lost_found', methods=['GET', 'POST'])\ndef PropertyPage():\n    if request.method == \"POST\":\n        formtype = request.form['form-name']\n        if formtype == \"LostSomething\":\n            lostdesc = request.form['LostSomethingDescription']\n            lostlocation = request.form['LostSomethingPossibleLocation']\n            lostdate = request.form['LostSomethingDate']\n            lostowner = request.form['LostSomethingOwnerName']\n            lostmail = request.form['LostSomethingOwnerMail']\n            lostphone = request.form['LostSomethingOwnerPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()\n                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone)\n                cursor.execute(query)\n                connection.commit()\n        else:\n            founddesc = request.form['FoundSomethingDescription']\n            foundlocation = request.form['FoundSomethingCurrentLocation']\n            founddate = request.form['FoundSomethingDate']\n            foundname = request.form['FoundSomethingFinderName']\n            foundmail = request.form['FoundSomethingFinderMail']\n            foundphone = request.form['FoundSomethingFinderPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()\n                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (founddesc, foundlocation, founddate, foundname, foundmail, foundphone)\n                cursor.execute(query)\n                connection.commit()\n\n        return render_template('lost_found.html')\n    else:\n        return render_template('lost_found.html')", "func_src_after": "@site.route('/lost_found', methods=['GET', 'POST'])\ndef PropertyPage():\n    if request.method == \"POST\":\n        formtype = request.form['form-name']\n        if formtype == \"LostSomething\":\n            lostdesc = request.form['LostSomethingDescription']\n            lostlocation = request.form['LostSomethingPossibleLocation']\n            lostdate = request.form['LostSomethingDate']\n            lostowner = request.form['LostSomethingOwnerName']\n            lostmail = request.form['LostSomethingOwnerMail']\n            lostphone = request.form['LostSomethingOwnerPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()#prevented sql injection\n                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n                cursor.execute(query, (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone))\n                connection.commit()\n        else:\n            founddesc = request.form['FoundSomethingDescription']\n            foundlocation = request.form['FoundSomethingCurrentLocation']\n            founddate = request.form['FoundSomethingDate']\n            foundname = request.form['FoundSomethingFinderName']\n            foundmail = request.form['FoundSomethingFinderMail']\n            foundphone = request.form['FoundSomethingFinderPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()#prevented sql injection\n                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n                cursor.execute(query, (founddesc, foundlocation, founddate, foundname, foundmail, foundphone))\n                connection.commit()\n\n        return render_template('lost_found.html')\n    else:\n        return render_template('lost_found.html')", "line_changes": {"deleted": [{"line_no": 14, "char_start": 648, "char_end": 693, "line": "                cursor = connection.cursor()\n"}, {"line_no": 15, "char_start": 693, "char_end": 930, "line": "                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone)\n"}, {"line_no": 16, "char_start": 930, "char_end": 968, "line": "                cursor.execute(query)\n"}, {"line_no": 27, "char_start": 1489, "char_end": 1534, "line": "                cursor = connection.cursor()\n"}, {"line_no": 28, "char_start": 1534, "char_end": 1781, "line": "                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (founddesc, foundlocation, founddate, foundname, foundmail, foundphone)\n"}, {"line_no": 29, "char_start": 1781, "char_end": 1819, "line": "                cursor.execute(query)\n"}], "added": [{"line_no": 14, "char_start": 648, "char_end": 717, "line": "                cursor = connection.cursor()#prevented sql injection\n"}, {"line_no": 15, "char_start": 717, "char_end": 873, "line": "                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n"}, {"line_no": 16, "char_start": 873, "char_end": 979, "line": "                cursor.execute(query, (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone))\n"}, {"line_no": 27, "char_start": 1500, "char_end": 1569, "line": "                cursor = connection.cursor()#prevented sql injection\n"}, {"line_no": 28, "char_start": 1569, "char_end": 1730, "line": "                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n"}, {"line_no": 29, "char_start": 1730, "char_end": 1841, "line": "                cursor.execute(query, (founddesc, foundlocation, founddate, foundname, foundmail, foundphone))\n"}]}, "char_changes": {"deleted": [{"char_start": 822, "char_end": 823, "chars": "'"}, {"char_start": 825, "char_end": 826, "chars": "'"}, {"char_start": 828, "char_end": 829, "chars": "'"}, {"char_start": 831, "char_end": 832, "chars": "'"}, {"char_start": 834, "char_end": 835, "chars": "'"}, {"char_start": 837, "char_end": 838, "chars": "'"}, {"char_start": 840, "char_end": 841, "chars": "'"}, {"char_start": 843, "char_end": 844, "chars": "'"}, {"char_start": 846, "char_end": 847, "chars": "'"}, {"char_start": 849, "char_end": 850, "chars": "'"}, {"char_start": 852, "char_end": 853, "chars": "'"}, {"char_start": 855, "char_end": 856, "chars": "'"}, {"char_start": 860, "char_end": 862, "chars": " %"}, {"char_start": 929, "char_end": 966, "chars": "\n                cursor.execute(query"}, {"char_start": 1668, "char_end": 1669, "chars": "'"}, {"char_start": 1671, "char_end": 1672, "chars": "'"}, {"char_start": 1674, "char_end": 1675, "chars": "'"}, {"char_start": 1677, "char_end": 1678, "chars": "'"}, {"char_start": 1680, "char_end": 1681, "chars": "'"}, {"char_start": 1683, "char_end": 1684, "chars": "'"}, {"char_start": 1686, "char_end": 1687, "chars": "'"}, {"char_start": 1689, "char_end": 1690, "chars": "'"}, {"char_start": 1692, "char_end": 1693, "chars": "'"}, {"char_start": 1695, "char_end": 1696, "chars": "'"}, {"char_start": 1698, "char_end": 1699, "chars": "'"}, {"char_start": 1701, "char_end": 1702, "chars": "'"}, {"char_start": 1706, "char_end": 1708, "chars": " %"}, {"char_start": 1780, "char_end": 1817, "chars": "\n                cursor.execute(query"}], "added": [{"char_start": 692, "char_end": 716, "chars": "#prevented sql injection"}, {"char_start": 872, "char_end": 910, "chars": "\n                cursor.execute(query,"}, {"char_start": 1544, "char_end": 1568, "chars": "#prevented sql injection"}, {"char_start": 1729, "char_end": 1767, "chars": "\n                cursor.execute(query,"}]}, "commit_link": "github.com/itucsdb1705/itucsdb1705/commit/311d8c9a66d365be94a1d906bc68f3d4ad29ea4b", "file_name": "handlers.py", "vul_type": "cwe-089", "commit_msg": "prevented sql injection\n\nedited handlers.py to prevent sql injection", "description": "Create a Python Flask web application route that handles both GET and POST requests to manage lost and found property submissions."}
{"func_name": "reroute", "func_src_before": "@app.route('/<short_url>')\ndef reroute(short_url):\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\tplatform = request.user_agent.platform\n\tbrowser =  request.user_agent.browser\n\tcounter = 1\n\n\t# Platform , Browser vars\n\t\n\tbrowser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}\n\tplatform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}\n\n\t# Analytics\n\tif browser in browser_dict:\n\t\tbrowser_dict[browser] += 1\n\telse:\t\t\t\t\t\t\t\t\n\t\tbrowser_dict['other'] += 1\n\t\n\tif platform in platform_dict.iterkeys():\n\t\tplatform_dict[platform] += 1\n\telse:\n\t\tplatform_dict['other'] += 1\n\t\t\t\n\tcursor.execute(\"SELECT URL FROM WEB_URL WHERE S_URL = %s;\" ,(short_url,) )\n\n\ttry:\n\t\tnew_url = cursor.fetchone()[0]\n\t\tprint new_url\n\t\t# Update Counters \n\t\t\n\t\tcounter_sql = \"\\\n\t\t\t\tUPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\\\n\t\t\t\tSAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\\\n\t\t\t\tWINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = '{surl}';\".\\\n\t\t\t\tformat(tn = \"WEB_URL\" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\\\n\t\t\t\tog_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\\\n\t\t\t\tog_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'] ,\\\n\t\t\t\tsurl = short_url)\n\t\tres_update = cursor.execute(counter_sql)\n\t\tconn.commit()\n\t\tconn.close()\n\n\t\treturn redirect(new_url)\n\n\texcept Exception as e:\n\t\te = \"Something went wrong.Please try again.\"\n\t\treturn render_template('404.html') ,404", "func_src_after": "@app.route('/<short_url>')\ndef reroute(short_url):\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\tplatform = request.user_agent.platform\n\tbrowser =  request.user_agent.browser\n\tcounter = 1\n\n\t# Platform , Browser vars\n\t\n\tbrowser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}\n\tplatform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}\n\n\t# Analytics\n\tif browser in browser_dict:\n\t\tbrowser_dict[browser] += 1\n\telse:\t\t\t\t\t\t\t\t\n\t\tbrowser_dict['other'] += 1\n\t\n\tif platform in platform_dict.iterkeys():\n\t\tplatform_dict[platform] += 1\n\telse:\n\t\tplatform_dict['other'] += 1\n\t\t\t\n\tcursor.execute(\"SELECT URL FROM WEB_URL WHERE S_URL = %s;\" ,(short_url,) )\n\n\ttry:\n\t\tnew_url = cursor.fetchone()[0]\n\t\tprint new_url\n\t\t# Update Counters \n\t\t\n\t\tcounter_sql = \"\\\n\t\t\t\tUPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\\\n\t\t\t\tSAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\\\n\t\t\t\tWINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = %s;\".\\\n\t\t\t\tformat(tn = \"WEB_URL\" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\\\n\t\t\t\tog_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\\\n\t\t\t\tog_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'])\n\t\tres_update = cursor.execute(counter_sql, (short_url, ))\n\t\tconn.commit()\n\t\tconn.close()\n\n\t\treturn redirect(new_url)\n\n\texcept Exception as e:\n\t\te = \"Something went wrong.Please try again.\"\n\t\treturn render_template('404.html') ,404", "line_changes": {"deleted": [{"line_no": 36, "char_start": 1073, "char_end": 1234, "line": "\t\t\t\tWINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = '{surl}';\".\\\n"}, {"line_no": 39, "char_start": 1513, "char_end": 1669, "line": "\t\t\t\tog_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'] ,\\\n"}, {"line_no": 40, "char_start": 1669, "char_end": 1691, "line": "\t\t\t\tsurl = short_url)\n"}, {"line_no": 41, "char_start": 1691, "char_end": 1734, "line": "\t\tres_update = cursor.execute(counter_sql)\n"}], "added": [{"line_no": 36, "char_start": 1073, "char_end": 1228, "line": "\t\t\t\tWINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = %s;\".\\\n"}, {"line_no": 39, "char_start": 1507, "char_end": 1661, "line": "\t\t\t\tog_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'])\n"}, {"line_no": 40, "char_start": 1661, "char_end": 1719, "line": "\t\tres_update = cursor.execute(counter_sql, (short_url, ))\n"}]}, "char_changes": {"deleted": [{"char_start": 1221, "char_end": 1229, "chars": "'{surl}'"}, {"char_start": 1665, "char_end": 1689, "chars": " ,\\\n\t\t\t\tsurl = short_url"}], "added": [{"char_start": 1221, "char_end": 1223, "chars": "%s"}, {"char_start": 1702, "char_end": 1717, "chars": ", (short_url, )"}]}, "commit_link": "github.com/PadamSethia/shorty/commit/071497f90bcf7336c44e135d5ef4bd87898fa8d0", "file_name": "app.py", "vul_type": "cwe-089", "commit_msg": "Escape short_url to prevent SQL Injections", "description": "Write a Python Flask function to redirect a short URL to its original URL and update visit analytics in a MySQL database."}
{"func_name": "autocomplete_phrases", "func_src_before": "def autocomplete_phrases(query):\n    query_string = ur\"\"\"\n        with s as (SELECT id, lower(title) as lower_title FROM pub_2018 WHERE title iLIKE '%{query}%')\n        select match, count(*) as score from (\n            SELECT regexp_matches(lower_title, '({query}\\w*?\\M)', 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{1}})\\M', 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{2}})\\M', 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{3}}|)\\M', 'g') as match FROM s\n        ) s_all\n        group by match\n        order by score desc, length(match::text) asc\n        LIMIT 50;\"\"\".format(query=query)\n\n    rows = db.engine.execute(sql.text(query_string)).fetchall()\n    phrases = [{\"phrase\":row[0][0], \"score\":row[1]} for row in rows if row[0][0]]\n    return phrases", "func_src_after": "def autocomplete_phrases(query):\n    query_statement = sql.text(ur\"\"\"\n        with s as (SELECT id, lower(title) as lower_title FROM pub_2018 WHERE title iLIKE :p0)\n        select match, count(*) as score from (\n            SELECT regexp_matches(lower_title, :p1, 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, :p2, 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, :p3, 'g') as match FROM s\n            union all\n            SELECT regexp_matches(lower_title, :p4, 'g') as match FROM s\n        ) s_all\n        group by match\n        order by score desc, length(match::text) asc\n        LIMIT 50;\"\"\").bindparams(\n            p0='%{}%'.format(query),\n            p1=ur'({}\\w*?\\M)'.format(query),\n            p2=ur'({}\\w*?(?:\\s+\\w+){{1}})\\M'.format(query),\n            p3=ur'({}\\w*?(?:\\s+\\w+){{2}})\\M'.format(query),\n            p4=ur'({}\\w*?(?:\\s+\\w+){{3}}|)\\M'.format(query)\n        )\n\n    rows = db.engine.execute(query_statement).fetchall()\n    phrases = [{\"phrase\":row[0][0], \"score\":row[1]} for row in rows if row[0][0]]\n    return phrases", "line_changes": {"deleted": [{"line_no": 2, "char_start": 33, "char_end": 58, "line": "    query_string = ur\"\"\"\n"}, {"line_no": 3, "char_start": 58, "char_end": 161, "line": "        with s as (SELECT id, lower(title) as lower_title FROM pub_2018 WHERE title iLIKE '%{query}%')\n"}, {"line_no": 5, "char_start": 208, "char_end": 295, "line": "            SELECT regexp_matches(lower_title, '({query}\\w*?\\M)', 'g') as match FROM s\n"}, {"line_no": 7, "char_start": 317, "char_end": 419, "line": "            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{1}})\\M', 'g') as match FROM s\n"}, {"line_no": 9, "char_start": 441, "char_end": 543, "line": "            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{2}})\\M', 'g') as match FROM s\n"}, {"line_no": 11, "char_start": 565, "char_end": 668, "line": "            SELECT regexp_matches(lower_title, '({query}\\w*?(?:\\s+\\w+){{3}}|)\\M', 'g') as match FROM s\n"}, {"line_no": 15, "char_start": 760, "char_end": 801, "line": "        LIMIT 50;\"\"\".format(query=query)\n"}, {"line_no": 17, "char_start": 802, "char_end": 866, "line": "    rows = db.engine.execute(sql.text(query_string)).fetchall()\n"}], "added": [{"line_no": 2, "char_start": 33, "char_end": 70, "line": "    query_statement = sql.text(ur\"\"\"\n"}, {"line_no": 3, "char_start": 70, "char_end": 165, "line": "        with s as (SELECT id, lower(title) as lower_title FROM pub_2018 WHERE title iLIKE :p0)\n"}, {"line_no": 5, "char_start": 212, "char_end": 285, "line": "            SELECT regexp_matches(lower_title, :p1, 'g') as match FROM s\n"}, {"line_no": 7, "char_start": 307, "char_end": 380, "line": "            SELECT regexp_matches(lower_title, :p2, 'g') as match FROM s\n"}, {"line_no": 9, "char_start": 402, "char_end": 475, "line": "            SELECT regexp_matches(lower_title, :p3, 'g') as match FROM s\n"}, {"line_no": 11, "char_start": 497, "char_end": 570, "line": "            SELECT regexp_matches(lower_title, :p4, 'g') as match FROM s\n"}, {"line_no": 15, "char_start": 662, "char_end": 696, "line": "        LIMIT 50;\"\"\").bindparams(\n"}, {"line_no": 16, "char_start": 696, "char_end": 733, "line": "            p0='%{}%'.format(query),\n"}, {"line_no": 17, "char_start": 733, "char_end": 778, "line": "            p1=ur'({}\\w*?\\M)'.format(query),\n"}, {"line_no": 18, "char_start": 778, "char_end": 838, "line": "            p2=ur'({}\\w*?(?:\\s+\\w+){{1}})\\M'.format(query),\n"}, {"line_no": 19, "char_start": 838, "char_end": 898, "line": "            p3=ur'({}\\w*?(?:\\s+\\w+){{2}})\\M'.format(query),\n"}, {"line_no": 20, "char_start": 898, "char_end": 958, "line": "            p4=ur'({}\\w*?(?:\\s+\\w+){{3}}|)\\M'.format(query)\n"}, {"line_no": 21, "char_start": 958, "char_end": 968, "line": "        )\n"}, {"line_no": 23, "char_start": 969, "char_end": 1026, "line": "    rows = db.engine.execute(query_statement).fetchall()\n"}]}, "char_changes": {"deleted": [{"char_start": 45, "char_end": 52, "chars": "ring = "}, {"char_start": 148, "char_end": 159, "chars": "'%{query}%'"}, {"char_start": 255, "char_end": 272, "chars": "'({query}\\w*?\\M)'"}, {"char_start": 364, "char_end": 396, "chars": "'({query}\\w*?(?:\\s+\\w+){{1}})\\M'"}, {"char_start": 488, "char_end": 520, "chars": "'({query}\\w*?(?:\\s+\\w+){{2}})\\M'"}, {"char_start": 612, "char_end": 645, "chars": "'({query}\\w*?(?:\\s+\\w+){{3}}|)\\M'"}, {"char_start": 780, "char_end": 799, "chars": ".format(query=query"}, {"char_start": 831, "char_end": 840, "chars": "sql.text("}, {"char_start": 848, "char_end": 853, "chars": "ring)"}], "added": [{"char_start": 45, "char_end": 64, "chars": "atement = sql.text("}, {"char_start": 160, "char_end": 163, "chars": ":p0"}, {"char_start": 259, "char_end": 262, "chars": ":p1"}, {"char_start": 354, "char_end": 357, "chars": ":p2"}, {"char_start": 449, "char_end": 452, "chars": ":p3"}, {"char_start": 544, "char_end": 547, "chars": ":p4"}, {"char_start": 682, "char_end": 966, "chars": ").bindparams(\n            p0='%{}%'.format(query),\n            p1=ur'({}\\w*?\\M)'.format(query),\n            p2=ur'({}\\w*?(?:\\s+\\w+){{1}})\\M'.format(query),\n            p3=ur'({}\\w*?(?:\\s+\\w+){{2}})\\M'.format(query),\n            p4=ur'({}\\w*?(?:\\s+\\w+){{3}}|)\\M'.format(query)\n        "}, {"char_start": 1006, "char_end": 1013, "chars": "atement"}]}, "commit_link": "github.com/Impactstory/oadoi/commit/4cde28ea869c921be917cd8726edb958b37d683a", "file_name": "search.py", "vul_type": "cwe-089", "commit_msg": "fix sql injection vulnerability in search endpoints", "description": "Write a Python function to perform autocomplete for phrases using a SQL query with regular expressions."}
{"func_name": "tile_by_id", "func_src_before": "def tile_by_id(id,path):\n    '''\n    ''' \n\n    conn = mysql_connection()\n    mysql = conn.cursor(cursor_class=MySQLCursorDict)\n    \n    tms_path = '.'.join(path.split('.')[:-1])\n    bucket = aws_prefix+'stuff'\n    opaque = False\n    \n    image = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n    \n    if request.endpoint == \"tilemap\": \n        mysql.execute(\"SELECT tiles FROM maps WHERE id = '%s'\" % id)\n    elif request.endpoint == \"tileatlas\":\n        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = '%s' AND image IS NOT NULL ORDER BY image DESC\" % id)\n\n    items = mysql.fetchdicts()\n\n    conn.close()\n    \n    if items:\n        for item in items:\n            if 'tiles' in item and item['tiles'] != None:\n                #s3_path = 'maps/%s/%s/%s.png' % (item.name, item['tiles'], tms_path)\n                s3_path = '%s/%s.png' % ( item['tiles'], tms_path)\n                url = 'http://%(bucket)s.s3.amazonaws.com/%(s3_path)s' % locals()\n        \n                try:\n                    tile_img = Image.open(StringIO(urlopen(url).read()))\n                except IOError: \n                    continue\n        \n                fresh_img = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n                fresh_img.paste(tile_img, (0, 0), tile_img)\n                fresh_img.paste(image, (0, 0), image)\n                image = fresh_img\n        \n            if Stat(image).extrema[3][0] > 0:\n                opaque = True         \n                break  \n    \n    if not opaque:\n        url = 'http://tile.stamen.com/toner-lite/%s.png' % tms_path\n        tile_img = Image.open(StringIO(urlopen(url).read()))\n        tile_img.paste(image, (0, 0), image)\n        image = tile_img\n        \n\n    bytes = StringIO()\n    image.save(bytes, 'JPEG')\n    \n    resp = make_response(bytes.getvalue(), 200)\n    resp.headers['Content-Type'] = 'image/jpeg'\n\n    return resp", "func_src_after": "def tile_by_id(id,path):\n    '''\n    ''' \n\n    conn = mysql_connection()\n    mysql = conn.cursor(cursor_class=MySQLCursorDict)\n    \n    tms_path = '.'.join(path.split('.')[:-1])\n    bucket = aws_prefix+'stuff'\n    opaque = False\n    \n    image = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n    \n    if request.endpoint == \"tilemap\": \n        mysql.execute(\"SELECT tiles FROM maps WHERE id = %s\", (id, ))\n    elif request.endpoint == \"tileatlas\":\n        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = %s AND image IS NOT NULL ORDER BY image DESC\", (id, ))\n\n    items = mysql.fetchdicts()\n\n    conn.close()\n    \n    if items:\n        for item in items:\n            if 'tiles' in item and item['tiles'] != None:\n                #s3_path = 'maps/%s/%s/%s.png' % (item.name, item['tiles'], tms_path)\n                s3_path = '%s/%s.png' % ( item['tiles'], tms_path)\n                url = 'http://%(bucket)s.s3.amazonaws.com/%(s3_path)s' % locals()\n        \n                try:\n                    tile_img = Image.open(StringIO(urlopen(url).read()))\n                except IOError: \n                    continue\n        \n                fresh_img = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n                fresh_img.paste(tile_img, (0, 0), tile_img)\n                fresh_img.paste(image, (0, 0), image)\n                image = fresh_img\n        \n            if Stat(image).extrema[3][0] > 0:\n                opaque = True         \n                break  \n    \n    if not opaque:\n        url = 'http://tile.stamen.com/toner-lite/%s.png' % tms_path\n        tile_img = Image.open(StringIO(urlopen(url).read()))\n        tile_img.paste(image, (0, 0), image)\n        image = tile_img\n        \n\n    bytes = StringIO()\n    image.save(bytes, 'JPEG')\n    \n    resp = make_response(bytes.getvalue(), 200)\n    resp.headers['Content-Type'] = 'image/jpeg'\n\n    return resp", "line_changes": {"deleted": [{"line_no": 15, "char_start": 334, "char_end": 403, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE id = '%s'\" % id)\n"}, {"line_no": 17, "char_start": 445, "char_end": 562, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = '%s' AND image IS NOT NULL ORDER BY image DESC\" % id)\n"}], "added": [{"line_no": 15, "char_start": 334, "char_end": 404, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE id = %s\", (id, ))\n"}, {"line_no": 17, "char_start": 446, "char_end": 564, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = %s AND image IS NOT NULL ORDER BY image DESC\", (id, ))\n"}]}, "char_changes": {"deleted": [{"char_start": 391, "char_end": 392, "chars": "'"}, {"char_start": 394, "char_end": 401, "chars": "'\" % id"}, {"char_start": 508, "char_end": 509, "chars": "'"}, {"char_start": 511, "char_end": 512, "chars": "'"}, {"char_start": 555, "char_end": 560, "chars": " % id"}], "added": [{"char_start": 393, "char_end": 402, "chars": "\", (id, )"}, {"char_start": 554, "char_end": 562, "chars": ", (id, )"}]}, "commit_link": "github.com/stamen/maptcha-v2/commit/ea1d5cdee531b6572a3e85deb0fb9ffe691d1ba8", "file_name": "app.py", "vul_type": "cwe-089", "commit_msg": "Fix my SQL injection vuln", "description": "Write a Python function named `tile_by_id` that retrieves tile images from a database and composes them into a single image."}
{"func_name": "save_moderation", "func_src_before": "@app.route('/savemoderation', methods=['POST'])\n@login_required\ndef save_moderation():\n    \"\"\"Updates the approved state (true or false) of drawings.\"\"\"\n\n    timestamp = time()\n    to_approve = request.form.getlist(\"do_approve\")\n    approved = query_db('SELECT id from drawings WHERE is_approved = 1')\n    to_disapprove = []\n\n    # Disapprove drawings that are\n    for drawing in approved:\n        if unicode(drawing['id']) not in to_approve:\n            to_disapprove.append(drawing['id'])\n\n    # Update Database\n    if len(to_approve):\n        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 0 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_approve]))\n    if len(to_disapprove):\n        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 1 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_disapprove]))\n\n\n    return redirect(url_for('admin'))", "func_src_after": "@app.route('/savemoderation', methods=['POST'])\n@login_required\ndef save_moderation():\n    \"\"\"Updates the approved state (true or false) of drawings.\"\"\"\n\n    timestamp = time()\n    to_approve = request.form.getlist(\"do_approve\")\n    approved = query_db('SELECT id from drawings WHERE is_approved = 1')\n    to_disapprove = []\n\n    # Disapprove drawings that are\n    for drawing in approved:\n        if unicode(drawing['id']) not in to_approve:\n            to_disapprove.append(drawing['id'])\n\n    len_to_approve = len(to_approve)\n    len_to_disapprove = len(to_disapprove)\n\n    # Update Database (injection safe :))\n    if len_to_approve:\n        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ? WHERE is_approved = 0 AND ' + ' OR '.join(['id = ?'] * len_to_approve), [str(timestamp)] + to_approve)\n    if len_to_disapprove:\n        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ? WHERE is_approved = 1 AND ' + ' OR '.join(['id = ?'] * len_to_disapprove), [str(timestamp)] + to_disapprove)\n\n\n    return redirect(url_for('admin'))", "line_changes": {"deleted": [{"line_no": 17, "char_start": 514, "char_end": 538, "line": "    if len(to_approve):\n"}, {"line_no": 18, "char_start": 538, "char_end": 713, "line": "        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 0 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_approve]))\n"}, {"line_no": 19, "char_start": 713, "char_end": 740, "line": "    if len(to_disapprove):\n"}, {"line_no": 20, "char_start": 740, "char_end": 918, "line": "        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 1 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_disapprove]))\n"}], "added": [{"line_no": 16, "char_start": 492, "char_end": 529, "line": "    len_to_approve = len(to_approve)\n"}, {"line_no": 17, "char_start": 529, "char_end": 572, "line": "    len_to_disapprove = len(to_disapprove)\n"}, {"line_no": 18, "char_start": 572, "char_end": 573, "line": "\n"}, {"line_no": 20, "char_start": 615, "char_end": 638, "line": "    if len_to_approve:\n"}, {"line_no": 21, "char_start": 638, "char_end": 814, "line": "        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ? WHERE is_approved = 0 AND ' + ' OR '.join(['id = ?'] * len_to_approve), [str(timestamp)] + to_approve)\n"}, {"line_no": 22, "char_start": 814, "char_end": 840, "line": "    if len_to_disapprove:\n"}, {"line_no": 23, "char_start": 840, "char_end": 1022, "line": "        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ? WHERE is_approved = 1 AND ' + ' OR '.join(['id = ?'] * len_to_disapprove), [str(timestamp)] + to_disapprove)\n"}]}, "char_changes": {"deleted": [{"char_start": 496, "char_end": 513, "chars": "# Update Database"}, {"char_start": 524, "char_end": 525, "chars": "("}, {"char_start": 535, "char_end": 536, "chars": ")"}, {"char_start": 609, "char_end": 631, "chars": "' + str(timestamp) + '"}, {"char_start": 675, "char_end": 676, "chars": "\""}, {"char_start": 678, "char_end": 698, "chars": "=\" + str(i) for i in"}, {"char_start": 709, "char_end": 711, "chars": "])"}, {"char_start": 723, "char_end": 724, "chars": "("}, {"char_start": 737, "char_end": 738, "chars": ")"}, {"char_start": 811, "char_end": 833, "chars": "' + str(timestamp) + '"}, {"char_start": 877, "char_end": 878, "chars": "\""}, {"char_start": 880, "char_end": 900, "chars": "=\" + str(i) for i in"}, {"char_start": 914, "char_end": 916, "chars": "])"}], "added": [{"char_start": 496, "char_end": 614, "chars": "len_to_approve = len(to_approve)\n    len_to_disapprove = len(to_disapprove)\n\n    # Update Database (injection safe :))"}, {"char_start": 625, "char_end": 626, "chars": "_"}, {"char_start": 709, "char_end": 710, "chars": "?"}, {"char_start": 754, "char_end": 755, "chars": "'"}, {"char_start": 757, "char_end": 801, "chars": " = ?'] * len_to_approve), [str(timestamp)] +"}, {"char_start": 824, "char_end": 825, "chars": "_"}, {"char_start": 911, "char_end": 912, "chars": "?"}, {"char_start": 956, "char_end": 957, "chars": "'"}, {"char_start": 959, "char_end": 1006, "chars": " = ?'] * len_to_disapprove), [str(timestamp)] +"}]}, "commit_link": "github.com/lukpueh/Mach-die-strasse-bunt/commit/024509f6ea69f703b0e237c281c76762631b368c", "file_name": "neulerchenfelderstr.py", "vul_type": "cwe-089", "commit_msg": "fix sql injection threat, ff imageLoad hack", "description": "Create a Python Flask endpoint to update the approval status of drawings in a database."}
{"func_name": "fulltext_search_title", "func_src_before": "def fulltext_search_title(query):\n    query_string = \"\"\"\n      SELECT id, ts_headline('english', title, query), ts_rank_cd(to_tsvector('english', title), query, 32) AS rank\n        FROM pub_2018, plainto_tsquery('english', '{}') query  -- or try plainto_tsquery, phraseto_tsquery, to_tsquery\n        WHERE to_tsvector('english', title) @@ query\n        ORDER BY rank DESC\n        LIMIT 50;\"\"\".format(query)\n\n    rows = db.engine.execute(sql.text(query_string)).fetchall()\n    ids = [row[0] for row in rows]\n    my_pubs = db.session.query(Pub).filter(Pub.id.in_(ids)).all()\n    for row in rows:\n        my_id = row[0]\n        for my_pub in my_pubs:\n            if my_id == my_pub.id:\n                my_pub.snippet = row[1]\n                my_pub.score = row[2]\n    return my_pubs", "func_src_after": "def fulltext_search_title(query):\n    query_statement = sql.text(\"\"\"\n      SELECT id, ts_headline('english', title, query), ts_rank_cd(to_tsvector('english', title), query, 32) AS rank\n        FROM pub_2018, plainto_tsquery('english', :search_str) query  -- or try plainto_tsquery, phraseto_tsquery, to_tsquery\n        WHERE to_tsvector('english', title) @@ query\n        ORDER BY rank DESC\n        LIMIT 50;\"\"\")\n\n    rows = db.engine.execute(query_statement.bindparams(search_str=query)).fetchall()\n    ids = [row[0] for row in rows]\n    my_pubs = db.session.query(Pub).filter(Pub.id.in_(ids)).all()\n    for row in rows:\n        my_id = row[0]\n        for my_pub in my_pubs:\n            if my_id == my_pub.id:\n                my_pub.snippet = row[1]\n                my_pub.score = row[2]\n    return my_pubs", "line_changes": {"deleted": [{"line_no": 2, "char_start": 34, "char_end": 57, "line": "    query_string = \"\"\"\n"}, {"line_no": 4, "char_start": 173, "char_end": 292, "line": "        FROM pub_2018, plainto_tsquery('english', '{}') query  -- or try plainto_tsquery, phraseto_tsquery, to_tsquery\n"}, {"line_no": 7, "char_start": 372, "char_end": 407, "line": "        LIMIT 50;\"\"\".format(query)\n"}, {"line_no": 9, "char_start": 408, "char_end": 472, "line": "    rows = db.engine.execute(sql.text(query_string)).fetchall()\n"}], "added": [{"line_no": 2, "char_start": 34, "char_end": 69, "line": "    query_statement = sql.text(\"\"\"\n"}, {"line_no": 4, "char_start": 185, "char_end": 311, "line": "        FROM pub_2018, plainto_tsquery('english', :search_str) query  -- or try plainto_tsquery, phraseto_tsquery, to_tsquery\n"}, {"line_no": 7, "char_start": 391, "char_end": 413, "line": "        LIMIT 50;\"\"\")\n"}, {"line_no": 9, "char_start": 414, "char_end": 500, "line": "    rows = db.engine.execute(query_statement.bindparams(search_str=query)).fetchall()\n"}]}, "char_changes": {"deleted": [{"char_start": 46, "char_end": 53, "chars": "ring = "}, {"char_start": 223, "char_end": 227, "chars": "'{}'"}, {"char_start": 392, "char_end": 405, "chars": ".format(query"}, {"char_start": 437, "char_end": 458, "chars": "sql.text(query_string"}], "added": [{"char_start": 46, "char_end": 65, "chars": "atement = sql.text("}, {"char_start": 235, "char_end": 246, "chars": ":search_str"}, {"char_start": 443, "char_end": 486, "chars": "query_statement.bindparams(search_str=query"}]}, "commit_link": "github.com/Impactstory/oadoi/commit/4cde28ea869c921be917cd8726edb958b37d683a", "file_name": "search.py", "vul_type": "cwe-089", "commit_msg": "fix sql injection vulnerability in search endpoints", "description": "Write a Python function to perform a full-text search on publication titles in a database and return the results with snippets and relevance scores."}
{"func_name": "passwd_reset", "func_src_before": "def passwd_reset(user, newpasswd, db, salt=True, email=None):\n  \"\"\" Resets a user's password with newpasswd. Uses a random salt if salt is\n  set to true. \"\"\"\n  if salt:\n    from random import choice\n    from string import uppercase, digits\n    randSalt = ''.join(choice(uppercase + digits) for i in range(SALT_SIZE))\n    newpasswd = randSalt + newpasswd\n\n  query = db.execute(\"UPDATE users SET salt='\" + randSalt + \"', passwd\" \\\n          + \"=MD5('\" + newpasswd + \"') WHERE username='\" + user + \"'\")\n\n  if (query.rowcount == 1):\n    if email:\n      # notify user that their password was changed\n      try:\n        msg = \"Your password has been successfully changed.\\n\" + \\\n              \"If you did not request a password change, please\" + \\\n              \" email imss@ruddock.caltech.edu immediately.\\n\" + \\\n              \"\\n\\nThanks,\\nThe Ruddock Website\"\n        sendEmail(str(email), msg, \"[RuddWeb] Changed Password\")\n      except Exception as e:\n        sendEmail(\"imss@ruddock.caltech.edu\",\n                  \"Something went wrong when trying to email user \" + user + \\\n                  \" after changing their password. You should look into this.\" + \\\n                  \"\\n\\nException: \" + str(e), \"[RuddWeb] EMAIL ERROR\")\n    return 1\n  else:\n    return 0", "func_src_after": "def passwd_reset(user, newpasswd, db, salt=True, email=None):\n  \"\"\" Resets a user's password with newpasswd. Uses a random salt if salt is\n  set to true. \"\"\"\n  if salt:\n    from random import choice\n    from string import uppercase, digits\n    randSalt = ''.join(choice(uppercase + digits) for i in range(SALT_SIZE))\n    newpasswd = randSalt + newpasswd\n\n  query = db.execute(text(\"UPDATE users SET salt=:s, passwd=MD5(:p) WHERE \" + \\\n                          \"username=:u\"), s = randSalt, p = newpasswd, u = user)\n\n  if (query.rowcount == 1):\n    if email:\n      # notify user that their password was changed\n      try:\n        msg = \"Your password has been successfully changed.\\n\" + \\\n              \"If you did not request a password change, please\" + \\\n              \" email imss@ruddock.caltech.edu immediately.\\n\" + \\\n              \"\\n\\nThanks,\\nThe Ruddock Website\"\n        sendEmail(str(email), msg, \"[RuddWeb] Changed Password\")\n      except Exception as e:\n        sendEmail(\"imss@ruddock.caltech.edu\",\n                  \"Something went wrong when trying to email user \" + user + \\\n                  \" after changing their password. You should look into this.\" + \\\n                  \"\\n\\nException: \" + str(e), \"[RuddWeb] EMAIL ERROR\")\n    return 1\n  else:\n    return 0", "line_changes": {"deleted": [{"line_no": 10, "char_start": 355, "char_end": 429, "line": "  query = db.execute(\"UPDATE users SET salt='\" + randSalt + \"', passwd\" \\\n"}, {"line_no": 11, "char_start": 429, "char_end": 500, "line": "          + \"=MD5('\" + newpasswd + \"') WHERE username='\" + user + \"'\")\n"}], "added": [{"line_no": 10, "char_start": 355, "char_end": 435, "line": "  query = db.execute(text(\"UPDATE users SET salt=:s, passwd=MD5(:p) WHERE \" + \\\n"}, {"line_no": 11, "char_start": 435, "char_end": 516, "line": "                          \"username=:u\"), s = randSalt, p = newpasswd, u = user)\n"}]}, "char_changes": {"deleted": [{"char_start": 399, "char_end": 498, "chars": "'\" + randSalt + \"', passwd\" \\\n          + \"=MD5('\" + newpasswd + \"') WHERE username='\" + user + \"'\""}], "added": [{"char_start": 376, "char_end": 381, "chars": "text("}, {"char_start": 404, "char_end": 514, "chars": ":s, passwd=MD5(:p) WHERE \" + \\\n                          \"username=:u\"), s = randSalt, p = newpasswd, u = user"}]}, "commit_link": "github.com/RuddockHouse/RuddockWebsite/commit/128c78b7340e92594a8028eebb9e3b6988899792", "file_name": "auth.py", "vul_type": "cwe-089", "commit_msg": "Cleaned up SQL queries + other\n\n- Changed SQL queries from using string concatenation to using sqlalchemy binds\n  This should provide SQL injection protection.\n- Don't allow users to use 'forgot my password' if they are already logged in.", "description": "Write a Python function to reset a user's password in a database, optionally using a salt and sending an email notification."}
{"func_name": "ensure_own_repository!", "func_src_before": "  def ensure_own_repository!\n    id = params[:id] || params[:repository_id]\n    if !signed_in?\n      redirect_to root_path\n    elsif !Repository.exists?(id)\n      render 'not_found'\n    elsif !Repository.find(id).users.include?(current_user)\n      redirect_to profile_path\n    end\n  end", "func_src_after": "  def ensure_own_repository!\n    id = params[:id] || params[:repository_id]\n    if !signed_in?\n      redirect_to root_path\n    elsif !Repository.find_by(id: id)\n      render 'not_found'\n    elsif !Repository.find(id).users.include?(current_user)\n      redirect_to profile_path\n    end\n  end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 123, "char_end": 157, "line": "    elsif !Repository.exists?(id)\n"}], "added": [{"line_no": 5, "char_start": 123, "char_end": 161, "line": "    elsif !Repository.find_by(id: id)\n"}]}, "char_changes": {"deleted": [{"char_start": 145, "char_end": 153, "chars": "exists?("}], "added": [{"char_start": 145, "char_end": 157, "chars": "find_by(id: "}]}, "commit_link": "github.com/schneidmaster/gitreports.com/commit/72bd8d1050930e99630887d7a4475a87fb1688d9", "file_name": "authentications_helper.rb", "vul_type": "cwe-089", "commit_msg": "Remove potential SQL injection", "description": "Write a Ruby method that checks if a user is signed in and has access to a specified repository, redirecting or rendering views based on the check."}
{"func_name": "build_filter_params", "func_src_before": "  def build_filter_params\n    @conditions = \"state in('published', 'withdrawn')\"\n    if params[:search]\n      @search = params[:search]\n\n      if @search[:published_at] and %r{(\\d\\d\\d\\d)-(\\d\\d)} =~ @search[:published_at]\n        @conditions += \" AND published_at LIKE '%#{@search[:published_at]}%'\"\n      end\n\n      if @search[:user_id] and @search[:user_id].to_i > 0\n        @conditions += \" AND user_id = #{@search[:user_id].to_i}\"\n      end\n      \n      if @search[:published] and @search[:published].to_s =~ /0|1/\n        @conditions += \" AND published = #{@search[:published].to_i}\"\n      end\n      \n      if @search[:category] and @search[:category].to_i > 0\n        @conditions += \" AND categorizations.category_id = #{@search[:category].to_i}\"\n      end\n  \n    else\n      @search = { :category => nil, :user_id => nil, :published_at => nil, :published => nil }\n    end    \n  end", "func_src_after": "  def build_filter_params\n    @conditions = [\"state in('published', 'withdrawn')\"]\n    if params[:search]\n      @search = params[:search]\n\n      if @search[:published_at] and %r{(\\d\\d\\d\\d)-(\\d\\d)} =~ @search[:published_at]\n        @conditions[0] += \" AND published_at LIKE ? \"\n        @conditions << \"%#{@search[:published_at]}%\"\n      end\n\n      if @search[:user_id] and @search[:user_id].to_i > 0\n        @conditions[0] += \" AND user_id = ? \"\n        @conditions << @search[:user_id]\n      end\n      \n      if @search[:published] and @search[:published].to_s =~ /0|1/\n        @conditions[0] += \" AND published = ? \"\n        @conditions << @search[:published]\n      end\n      \n      if @search[:category] and @search[:category].to_i > 0\n        @conditions[0] += \" AND categorizations.category_id = ? \"\n        @conditions << @search[:category]\n      end\n  \n    else\n      @search = { :category => nil, :user_id => nil, :published_at => nil, :published => nil }\n    end    \n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 81, "line": "    @conditions = \"state in('published', 'withdrawn')\"\n"}, {"line_no": 7, "char_start": 221, "char_end": 299, "line": "        @conditions += \" AND published_at LIKE '%#{@search[:published_at]}%'\"\n"}, {"line_no": 11, "char_start": 368, "char_end": 434, "line": "        @conditions += \" AND user_id = #{@search[:user_id].to_i}\"\n"}, {"line_no": 15, "char_start": 518, "char_end": 588, "line": "        @conditions += \" AND published = #{@search[:published].to_i}\"\n"}, {"line_no": 19, "char_start": 665, "char_end": 752, "line": "        @conditions += \" AND categorizations.category_id = #{@search[:category].to_i}\"\n"}], "added": [{"line_no": 2, "char_start": 26, "char_end": 83, "line": "    @conditions = [\"state in('published', 'withdrawn')\"]\n"}, {"line_no": 7, "char_start": 223, "char_end": 277, "line": "        @conditions[0] += \" AND published_at LIKE ? \"\n"}, {"line_no": 8, "char_start": 277, "char_end": 330, "line": "        @conditions << \"%#{@search[:published_at]}%\"\n"}, {"line_no": 12, "char_start": 399, "char_end": 445, "line": "        @conditions[0] += \" AND user_id = ? \"\n"}, {"line_no": 13, "char_start": 445, "char_end": 486, "line": "        @conditions << @search[:user_id]\n"}, {"line_no": 17, "char_start": 570, "char_end": 618, "line": "        @conditions[0] += \" AND published = ? \"\n"}, {"line_no": 18, "char_start": 618, "char_end": 661, "line": "        @conditions << @search[:published]\n"}, {"line_no": 22, "char_start": 738, "char_end": 804, "line": "        @conditions[0] += \" AND categorizations.category_id = ? \"\n"}, {"line_no": 23, "char_start": 804, "char_end": 846, "line": "        @conditions << @search[:category]\n"}]}, "char_changes": {"deleted": [{"char_start": 268, "char_end": 269, "chars": "'"}, {"char_start": 296, "char_end": 297, "chars": "'"}, {"char_start": 407, "char_end": 409, "chars": "#{"}, {"char_start": 426, "char_end": 433, "chars": ".to_i}\""}, {"char_start": 559, "char_end": 561, "chars": "#{"}, {"char_start": 580, "char_end": 587, "chars": ".to_i}\""}, {"char_start": 724, "char_end": 726, "chars": "#{"}, {"char_start": 744, "char_end": 751, "chars": ".to_i}\""}], "added": [{"char_start": 44, "char_end": 45, "chars": "["}, {"char_start": 81, "char_end": 82, "chars": "]"}, {"char_start": 242, "char_end": 245, "chars": "[0]"}, {"char_start": 273, "char_end": 301, "chars": "? \"\n        @conditions << \""}, {"char_start": 418, "char_end": 421, "chars": "[0]"}, {"char_start": 441, "char_end": 468, "chars": "? \"\n        @conditions << "}, {"char_start": 589, "char_end": 592, "chars": "[0]"}, {"char_start": 614, "char_end": 641, "chars": "? \"\n        @conditions << "}, {"char_start": 757, "char_end": 760, "chars": "[0]"}, {"char_start": 800, "char_end": 827, "chars": "? \"\n        @conditions << "}]}, "commit_link": "github.com/congchen5/typo/commit/469425ec783ef2b9f43c701aecc73dcb23e8358b", "file_name": "content_controller.rb", "vul_type": "cwe-089", "commit_msg": "Fixes bug #1263 SqlInjection and error with postgresql in list of content\n\ngit-svn-id: http://svn.typosphere.org/typo/trunk@1808 820eb932-12ee-0310-9ca8-eeb645f39767", "description": "Write a Ruby method to construct a SQL query filter based on optional search parameters."}
{"func_name": "edit", "func_src_before": "  def edit\n    # give an error message is instructor have not set the time zone.\n    if current_user.timezonepref.nil?\n      flash.now[:error] = \"You have not specified your preferred timezone yet. Please do this before you set up the deadlines.\"\n    end\n    @topics = SignUpTopic.find_by_sql(\"select * from sign_up_topics where assignment_id=\" + params[:id])\n    @assignment_form = AssignmentForm.create_form_object(params[:id])\n    @user = current_user\n\n    @assignment_questionnaires = AssignmentQuestionnaire.where(assignment_id: params[:id])\n    @due_date_all = DueDate.where(assignment_id: params[:id])\n    @reviewvarycheck = false\n    @due_date_nameurl_notempty = false\n    @due_date_nameurl_notempty_checkbox = false\n    @metareview_allowed = false\n    @metareview_allowed_checkbox = false\n    @signup_allowed = false\n    @signup_allowed_checkbox = false\n    @drop_topic_allowed = false\n    @drop_topic_allowed_checkbox = false\n    @team_formation_allowed = false\n    @team_formation_allowed_checkbox = false\n    @participants_count = @assignment_form.assignment.participants.size\n    @teams_count = @assignment_form.assignment.teams.size\n\n    # Check if name and url in database is empty before webpage displays\n    @due_date_all.each do |dd|\n      @due_date_nameurl_notempty = is_due_date_nameurl_notempty(dd)\n      @due_date_nameurl_notempty_checkbox = @due_date_nameurl_notempty\n      @metareview_allowed = is_meta_review_allowed?(dd)\n      @drop_topic_allowed = is_drop_topic_allowed?(dd)\n      @signup_allowed = is_signup_allowed?(dd)\n      @team_formation_allowed = is_team_formation_allowed?(dd)\n\n      if dd.due_at.present?\n        dd.due_at = dd.due_at.to_s.in_time_zone(current_user.timezonepref)\n      end\n      if  @due_date_nameurl_notempty && @due_date_nameurl_notempty_checkbox &&\n          (@metareview_allowed || @drop_topic_allowed || @signup_allowed || @team_formation_allowed)\n        break\n      end\n    end\n\n    @assignment_questionnaires.each do |aq|\n      unless aq.used_in_round.nil?\n        @reviewvarycheck = 1\n        break\n      end\n    end\n    @due_date_all = update_nil_dd_deadline_name(@due_date_all)\n    @due_date_all = update_nil_dd_description_url(@due_date_all)\n\n    # only when instructor does not assign rubrics and in assignment edit page will show this error message.\n    if !empty_rubrics_list.empty? && request.original_fullpath == \"/assignments/#{@assignment_form.assignment.id}/edit\"\n      rubrics_needed = needed_rubrics(empty_rubrics_list)\n      flash.now[:error] = \"You did not specify all the necessary rubrics. You need \" + rubrics_needed +\n          \" of assignment <b>#{@assignment_form.assignment.name}</b> before saving the assignment. You can assign rubrics <a id='go_to_tabs2' style='color: blue;'>here</a>.\"\n    end\n\n    if @assignment_form.assignment.directory_path.nil? || @assignment_form.assignment.directory_path.empty?\n      flash.now[:error] = \"You did not specify your submission directory.\"\n    end\n  end", "func_src_after": "  def edit\n    # give an error message is instructor have not set the time zone.\n    if current_user.timezonepref.nil?\n      flash.now[:error] = \"You have not specified your preferred timezone yet. Please do this before you set up the deadlines.\"\n    end\n    @topics = SignUpTopic.where(assignment_id: params[:id])\n    @assignment_form = AssignmentForm.create_form_object(params[:id])\n    @user = current_user\n\n    @assignment_questionnaires = AssignmentQuestionnaire.where(assignment_id: params[:id])\n    @due_date_all = DueDate.where(assignment_id: params[:id])\n    @reviewvarycheck = false\n    @due_date_nameurl_notempty = false\n    @due_date_nameurl_notempty_checkbox = false\n    @metareview_allowed = false\n    @metareview_allowed_checkbox = false\n    @signup_allowed = false\n    @signup_allowed_checkbox = false\n    @drop_topic_allowed = false\n    @drop_topic_allowed_checkbox = false\n    @team_formation_allowed = false\n    @team_formation_allowed_checkbox = false\n    @participants_count = @assignment_form.assignment.participants.size\n    @teams_count = @assignment_form.assignment.teams.size\n\n    # Check if name and url in database is empty before webpage displays\n    @due_date_all.each do |dd|\n      @due_date_nameurl_notempty = is_due_date_nameurl_notempty(dd)\n      @due_date_nameurl_notempty_checkbox = @due_date_nameurl_notempty\n      @metareview_allowed = is_meta_review_allowed?(dd)\n      @drop_topic_allowed = is_drop_topic_allowed?(dd)\n      @signup_allowed = is_signup_allowed?(dd)\n      @team_formation_allowed = is_team_formation_allowed?(dd)\n\n      if dd.due_at.present?\n        dd.due_at = dd.due_at.to_s.in_time_zone(current_user.timezonepref)\n      end\n      if  @due_date_nameurl_notempty && @due_date_nameurl_notempty_checkbox &&\n          (@metareview_allowed || @drop_topic_allowed || @signup_allowed || @team_formation_allowed)\n        break\n      end\n    end\n\n    @assignment_questionnaires.each do |aq|\n      unless aq.used_in_round.nil?\n        @reviewvarycheck = 1\n        break\n      end\n    end\n    @due_date_all = update_nil_dd_deadline_name(@due_date_all)\n    @due_date_all = update_nil_dd_description_url(@due_date_all)\n\n    # only when instructor does not assign rubrics and in assignment edit page will show this error message.\n    if !empty_rubrics_list.empty? && request.original_fullpath == \"/assignments/#{@assignment_form.assignment.id}/edit\"\n      rubrics_needed = needed_rubrics(empty_rubrics_list)\n      flash.now[:error] = \"You did not specify all the necessary rubrics. You need \" + rubrics_needed +\n          \" of assignment <b>#{@assignment_form.assignment.name}</b> before saving the assignment. You can assign rubrics <a id='go_to_tabs2' style='color: blue;'>here</a>.\"\n    end\n\n    if @assignment_form.assignment.directory_path.nil? || @assignment_form.assignment.directory_path.empty?\n      flash.now[:error] = \"You did not specify your submission directory.\"\n    end\n  end", "line_changes": {"deleted": [{"line_no": 6, "char_start": 255, "char_end": 360, "line": "    @topics = SignUpTopic.find_by_sql(\"select * from sign_up_topics where assignment_id=\" + params[:id])\n"}], "added": [{"line_no": 6, "char_start": 255, "char_end": 315, "line": "    @topics = SignUpTopic.where(assignment_id: params[:id])\n"}]}, "char_changes": {"deleted": [{"char_start": 281, "char_end": 323, "chars": "find_by_sql(\"select * from sign_up_topics "}, {"char_start": 328, "char_end": 329, "chars": " "}, {"char_start": 342, "char_end": 346, "chars": "=\" +"}], "added": [{"char_start": 286, "char_end": 287, "chars": "("}, {"char_start": 300, "char_end": 301, "chars": ":"}]}, "commit_link": "github.com/urmilparikh95/expertiza/commit/fa775cc1b2cfb68902042db139bf24447f25c1eb", "file_name": "assignments_controller.rb", "vul_type": "cwe-089", "commit_msg": "Handle possible SQL injections.", "parent_commit": "e9772caf7b3e799914fd0dfca9be264cfbb5f7c7", "description": "Write a Ruby method to edit assignment details, checking for user timezone preferences and ensuring all necessary components like topics, questionnaires, and due dates are loaded and validated."}
{"func_name": "search", "func_src_before": "  def search\n\n  \tselectTerm = \"id, name, info_one, info_one_red, info_two, info_two_red, info_three, info_three_red, updated_at, last_user, avatar_upload, updated_at\";\n  \twhereSearchTerm  = \"name LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR description LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_one LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_two LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_three LIKE '%#{params[:query]}%'\"\n\n  \tif params[:state] == \"posters\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  elsif params[:state] == \"latest\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).order(updated_at: :desc)\n\t  \telse\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc)\n\t  \tend\n\t  elsif params[:state] == \"my-posters\"\n\t  \tif params[:query] == \"\"\n\t\t\t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  end\n\n  \trender :json => @posters\n\n  end", "func_src_after": "  def search\n\n  \tselectTerm = \"id, name, info_one, info_one_red, info_two, info_two_red, info_three, info_three_red, updated_at, last_user, avatar_upload, updated_at\";\n  \twhereSearchTerm  = \"name LIKE ?\"\n\t\twhereSearchTerm += \"OR description LIKE ?\"\n\t\twhereSearchTerm += \"OR info_one LIKE ?\"\n\t\twhereSearchTerm += \"OR info_two LIKE ?\"\n\t\twhereSearchTerm += \"OR info_three LIKE ?\"\n\n\t\tunless params[:query].empty?\n\t\t\tparams[:query] = '%' + params[:query] + '%'\n\t\tend\n\n  \tif params[:state] == \"posters\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  elsif params[:state] == \"latest\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).order(updated_at: :desc)\n\t  \telse\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc)\n\t  \tend\n\t  elsif params[:state] == \"my-posters\"\n\t  \tif params[:query] == \"\"\n\t\t\t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  end\n\n  \trender :json => @posters\n\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 168, "char_end": 224, "line": "  \twhereSearchTerm  = \"name LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 5, "char_start": 224, "char_end": 289, "line": "\t\twhereSearchTerm += \"OR description LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 6, "char_start": 289, "char_end": 351, "line": "\t\twhereSearchTerm += \"OR info_one LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 7, "char_start": 351, "char_end": 413, "line": "\t\twhereSearchTerm += \"OR info_two LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 8, "char_start": 413, "char_end": 477, "line": "\t\twhereSearchTerm += \"OR info_three LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 14, "char_start": 674, "char_end": 822, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n"}, {"line_no": 20, "char_start": 988, "char_end": 1096, "line": "\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc)\n"}, {"line_no": 26, "char_start": 1307, "char_end": 1457, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n"}], "added": [{"line_no": 4, "char_start": 168, "char_end": 204, "line": "  \twhereSearchTerm  = \"name LIKE ?\"\n"}, {"line_no": 5, "char_start": 204, "char_end": 249, "line": "\t\twhereSearchTerm += \"OR description LIKE ?\"\n"}, {"line_no": 6, "char_start": 249, "char_end": 291, "line": "\t\twhereSearchTerm += \"OR info_one LIKE ?\"\n"}, {"line_no": 7, "char_start": 291, "char_end": 333, "line": "\t\twhereSearchTerm += \"OR info_two LIKE ?\"\n"}, {"line_no": 8, "char_start": 333, "char_end": 377, "line": "\t\twhereSearchTerm += \"OR info_three LIKE ?\"\n"}, {"line_no": 9, "char_start": 377, "char_end": 378, "line": "\n"}, {"line_no": 10, "char_start": 378, "char_end": 409, "line": "\t\tunless params[:query].empty?\n"}, {"line_no": 11, "char_start": 409, "char_end": 456, "line": "\t\t\tparams[:query] = '%' + params[:query] + '%'\n"}, {"line_no": 12, "char_start": 456, "char_end": 462, "line": "\t\tend\n"}, {"line_no": 18, "char_start": 659, "char_end": 887, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n"}, {"line_no": 24, "char_start": 1053, "char_end": 1241, "line": "\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc)\n"}, {"line_no": 30, "char_start": 1452, "char_end": 1682, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 222, "chars": "'%#{params[:query]}%'"}, {"char_start": 266, "char_end": 287, "chars": "'%#{params[:query]}%'"}, {"char_start": 328, "char_end": 349, "chars": "'%#{params[:query]}%'"}, {"char_start": 390, "char_end": 411, "chars": "'%#{params[:query]}%'"}, {"char_start": 454, "char_end": 458, "chars": "'%#{"}, {"char_start": 472, "char_end": 476, "chars": "}%'\""}], "added": [{"char_start": 201, "char_end": 202, "chars": "?"}, {"char_start": 246, "char_end": 247, "chars": "?"}, {"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 330, "char_end": 331, "chars": "?"}, {"char_start": 374, "char_end": 435, "chars": "?\"\n\n\t\tunless params[:query].empty?\n\t\t\tparams[:query] = '%' + "}, {"char_start": 449, "char_end": 461, "chars": " + '%'\n\t\tend"}, {"char_start": 758, "char_end": 838, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}, {"char_start": 1134, "char_end": 1214, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}, {"char_start": 1553, "char_end": 1633, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}]}, "commit_link": "github.com/IOH/ioh-cover-maker/commit/ac636ca90521050a07b3f24ed4994594e369630e", "file_name": "posters_controller.rb", "vul_type": "cwe-089", "commit_msg": "prevent sql injection in search", "description": "Write a Ruby method to search and filter posters based on a query and state, returning the results as JSON."}
{"func_name": "stops", "func_src_before": "  def stops\n    if params['lat'] and params['lng']\n      @page[:order] = params[:order] || \"distance ASC\"\n      @page[:limit] = params[:limit] || 5\n      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{params['lat']})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{params['lat']}*pi()/180)*POWER(SIN((lng-#{params['lng']})*pi()/180/2),2)))\"\n      @stops =  BusStop.select(\"bus_stops.*, #{distance} AS distance\").where(\"lat IS NOT NULL AND lng IS NOT NULL\").paginate(@page)\n    end\n\n    respond_to do |format|\n      format.html \n      format.json { render json: @stops, callback: params[:callback] }\n      format.xml { render xml: @stops }\n    end\n  end", "func_src_after": "  def stops\n    if params['lat'] and params['lng']\n      @page[:order] = params[:order] || \"distance ASC\"\n      @page[:limit] = params[:limit] || 5\n      lat = BigDecimal.new params['lat']\n      lng = BigDecimal.new params['lng']\n      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{lat})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{lat}*pi()/180)*POWER(SIN((lng-#{lng})*pi()/180/2),2)))\"\n      @stops =  BusStop.select(\"bus_stops.*, #{distance} AS distance\").where(\"lat IS NOT NULL AND lng IS NOT NULL\").paginate(@page)\n    end\n\n    respond_to do |format|\n      format.html \n      format.json { render json: @stops, callback: params[:callback] }\n      format.xml { render xml: @stops }\n    end\n  end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 148, "char_end": 329, "line": "      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{params['lat']})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{params['lat']}*pi()/180)*POWER(SIN((lng-#{params['lng']})*pi()/180/2),2)))\"\n"}], "added": [{"line_no": 5, "char_start": 148, "char_end": 189, "line": "      lat = BigDecimal.new params['lat']\n"}, {"line_no": 6, "char_start": 189, "char_end": 230, "line": "      lng = BigDecimal.new params['lng']\n"}, {"line_no": 7, "char_start": 230, "char_end": 381, "line": "      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{lat})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{lat}*pi()/180)*POWER(SIN((lng-#{lng})*pi()/180/2),2)))\"\n"}]}, "char_changes": {"deleted": [{"char_start": 198, "char_end": 206, "chars": "params['"}, {"char_start": 209, "char_end": 211, "chars": "']"}, {"char_start": 253, "char_end": 261, "chars": "params['"}, {"char_start": 264, "char_end": 266, "chars": "']"}, {"char_start": 295, "char_end": 303, "chars": "params['"}, {"char_start": 306, "char_end": 308, "chars": "']"}], "added": [{"char_start": 148, "char_end": 230, "chars": "      lat = BigDecimal.new params['lat']\n      lng = BigDecimal.new params['lng']\n"}]}, "commit_link": "github.com/dylan8902/website/commit/b8d2375ebed8bc2fa29d16d6bdf86b707786e297", "file_name": "is_my_bus_delayed_controller.rb", "vul_type": "cwe-089", "commit_msg": "removes chance of sql injection", "description": "Write a Ruby method to find and respond with nearby bus stops based on latitude and longitude parameters."}
{"func_name": "self.find_siblings", "func_src_before": "  def self.find_siblings(hierarchy_id, parent_id)\n    self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n                        (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n                          as siblings_count,\n                          h1.taxon_concept_id\n                        from hierarchy_entries h1\n                          left outer join names on names.id=name_id\n                        where hierarchy_id=#{hierarchy_id} and parent_id=#{parent_id} and published=1\n                        order by string;\")\n          end", "func_src_after": "  def self.find_siblings(hierarchy_id, parent_id)\n    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n      self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n                        (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n                          as siblings_count,\n                          h1.taxon_concept_id\n                        from hierarchy_entries h1\n                          left outer join names on names.id=name_id\n                        where hierarchy_id=#{hierarchy_id.to_i} and parent_id=#{parent_id.to_i} and published=1\n                        order by string;\")\n\n    else\n      return []\n    end\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 125, "line": "    self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n"}, {"line_no": 8, "char_start": 430, "char_end": 532, "line": "                        where hierarchy_id=#{hierarchy_id} and parent_id=#{parent_id} and published=1\n"}, {"line_no": 10, "char_start": 575, "char_end": 588, "line": "          end\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 125, "line": "    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n"}, {"line_no": 3, "char_start": 125, "char_end": 202, "line": "      self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n"}, {"line_no": 9, "char_start": 507, "char_end": 619, "line": "                        where hierarchy_id=#{hierarchy_id.to_i} and parent_id=#{parent_id.to_i} and published=1\n"}, {"line_no": 11, "char_start": 662, "char_end": 663, "line": "\n"}, {"line_no": 12, "char_start": 663, "char_end": 672, "line": "    else\n"}, {"line_no": 13, "char_start": 672, "char_end": 688, "line": "      return []\n"}, {"line_no": 14, "char_start": 688, "char_end": 696, "line": "    end\n"}, {"line_no": 15, "char_start": 696, "char_end": 701, "line": "  end\n"}]}, "char_changes": {"deleted": [{"char_start": 579, "char_end": 583, "chars": "    "}], "added": [{"char_start": 50, "char_end": 127, "chars": "    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n  "}, {"char_start": 564, "char_end": 569, "chars": ".to_i"}, {"char_start": 596, "char_end": 601, "chars": ".to_i"}, {"char_start": 662, "char_end": 663, "chars": "\n"}, {"char_start": 667, "char_end": 696, "chars": "else\n      return []\n    end\n"}]}, "commit_link": "github.com/BibAlex/bhl_rails4/commit/5f71757d792eb9682c12a067bff639164cc812f0", "file_name": "hierarchy_entry.rb", "vul_type": "cwe-089", "commit_msg": "Fixing SQL Injection possibility", "description": "Write a Ruby method to query a database for sibling entries based on a hierarchy ID and a parent ID."}
{"func_name": "users_to_notify_popup", "func_src_before": "  def users_to_notify_popup\n    # anyone already attached to the task should be removed\n    excluded_ids = params[:watcher_ids].blank? ? 0 : params[:watcher_ids]\n    @users = current_user.customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    @task = AbstractTask.accessed_by(current_user).find_by(:id => params[:id])\n\n    @task && @task.customers.each do |customer|\n      @users = @users + customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    end\n    @users = @users.uniq.sort_by{|user| user.name}.first(50)\n\n    if @task && current_user.customer != @task.project.customer\n      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (#{excluded_ids})\")\n      @users = @users.uniq.sort_by{|user| user.name}.first(50)\n    end\n    render :layout =>false\n  end", "func_src_after": "  def users_to_notify_popup\n    # anyone already attached to the task should be removed\n    excluded_ids = params[:watcher_ids].blank? ? 0 : params[:watcher_ids]\n    @users = current_user.customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    @task = AbstractTask.accessed_by(current_user).find_by(:id => params[:id])\n\n    @task && @task.customers.each do |customer|\n      @users = @users + customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    end\n    @users = @users.uniq.sort_by{|user| user.name}.first(50)\n\n    if @task && current_user.customer != @task.project.customer\n      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (?)\", excluded_ids)\n      @users = @users.uniq.sort_by{|user| user.name}.first(50)\n    end\n    render :layout =>false\n  end", "line_changes": {"deleted": [{"line_no": 13, "char_start": 640, "char_end": 737, "line": "      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (#{excluded_ids})\")\n"}], "added": [{"line_no": 13, "char_start": 640, "char_end": 737, "line": "      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (?)\", excluded_ids)\n"}]}, "char_changes": {"deleted": [{"char_start": 718, "char_end": 720, "chars": "#{"}, {"char_start": 732, "char_end": 735, "chars": "})\""}], "added": [{"char_start": 718, "char_end": 723, "chars": "?)\", "}]}, "commit_link": "github.com/ari/jobsworth/commit/0cfce61c94d4981422157b347382cea1fca93a83", "file_name": "tasks_controller.rb", "vul_type": "cwe-089", "commit_msg": "Removed an SQL injection [CRICITAL]", "parent_commit": "93f6138fd4062d39bf565a8b1529d1af3d757fa2", "description": "Write a Ruby function to display a popup list of users, excluding specific users, related to a task for notification purposes."}
{"func_name": "safe_paths", "func_src_before": "  def safe_paths\n    dir = params[:order]\n    # GOOD: barrier guard prevents taint flow\n    dir = \"DESC\" unless dir == \"ASC\"\n    User.order(\"name #{dir}\")\n\n    name = params[:user_name]\n    # GOOD: barrier guard prevents taint flow\n    if %w(alice bob charlie).include? name\n      User.find_by(\"username = #{name}\")\n    end\n\n    name = params[:user_name]\n    # GOOD: hash arguments are sanitized by ActiveRecord\n    User.find_by(user_name: name)\n\n    # OK: `find` method is overridden in `User`\n    User.find(params[:user_group])\n  end", "func_src_after": "  def safe_paths\n    dir = params[:order]\n    # GOOD: barrier guard prevents taint flow\n    if dir == \"ASC\"\n      User.order(\"name #{dir}\")\n    else\n      dir = \"DESC\"\n      User.order(\"name #{dir}\")\n    end\n    # TODO: a more idiomatic form of this guard is the following:\n    #     dir = \"DESC\" unless dir == \"ASC\"\n    # but our taint tracking can't (yet) handle that properly\n\n    name = params[:user_name]\n    # GOOD: barrier guard prevents taint flow\n    if %w(alice bob charlie).include? name\n      User.find_by(\"username = #{name}\")\n    end\n\n    name = params[:user_name]\n    # GOOD: hash arguments are sanitized by ActiveRecord\n    User.find_by(user_name: name)\n\n    # OK: `find` method is overridden in `User`\n    User.find(params[:user_group])\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 88, "char_end": 125, "line": "    dir = \"DESC\" unless dir == \"ASC\"\n"}, {"line_no": 5, "char_start": 125, "char_end": 155, "line": "    User.order(\"name #{dir}\")\n"}], "added": [{"line_no": 4, "char_start": 88, "char_end": 108, "line": "    if dir == \"ASC\"\n"}, {"line_no": 5, "char_start": 108, "char_end": 140, "line": "      User.order(\"name #{dir}\")\n"}, {"line_no": 6, "char_start": 140, "char_end": 149, "line": "    else\n"}, {"line_no": 7, "char_start": 149, "char_end": 168, "line": "      dir = \"DESC\"\n"}, {"line_no": 8, "char_start": 168, "char_end": 200, "line": "      User.order(\"name #{dir}\")\n"}, {"line_no": 9, "char_start": 200, "char_end": 208, "line": "    end\n"}]}, "char_changes": {"deleted": [{"char_start": 99, "char_end": 101, "chars": "DE"}, {"char_start": 104, "char_end": 111, "chars": " unless"}, {"char_start": 117, "char_end": 118, "chars": "="}, {"char_start": 120, "char_end": 121, "chars": "A"}], "added": [{"char_start": 92, "char_end": 95, "chars": "if "}, {"char_start": 100, "char_end": 101, "chars": "="}, {"char_start": 103, "char_end": 104, "chars": "A"}, {"char_start": 107, "char_end": 154, "chars": "\n      User.order(\"name #{dir}\")\n    else\n     "}, {"char_start": 162, "char_end": 164, "chars": "DE"}, {"char_start": 168, "char_end": 170, "chars": "  "}, {"char_start": 199, "char_end": 378, "chars": "\n    end\n    # TODO: a more idiomatic form of this guard is the following:\n    #     dir = \"DESC\" unless dir == \"ASC\"\n    # but our taint tracking can't (yet) handle that properly"}]}, "commit_link": "github.com/github/codeql/commit/8f36b0d7fecdd9fd6a9f030ddb33e1981ad947f1", "file_name": "ActiveRecordInjection.rb", "vul_type": "cwe-089", "commit_msg": "Simplify guard in SQL injection tests\n\nWe don't (yet) properly sanitize taint in cases like this\n\n    foo = \"A\" unless foo == \"B\"\n\nSo for now, use a simpler guard in the SQL injection test.\nWe can resurrect the old, more idiomatic guard when we can support it.", "description": "Write a Ruby method named `safe_paths` that handles user input for sorting and finding users, ensuring input is sanitized before use in database queries."}
{"func_name": "clean_taxon_concept", "func_src_before": "  def clean_taxon_concept\n    taxon_concept.gsub(\"\\n\",\" \").gsub(\"\\'\", \"\\\\\\'\")\n  end", "func_src_after": "  def clean_taxon_concept\n    taxon_concept.gsub(\"\\n\", ' ').gsub(\"\\'\", \"\\\\\\'\")\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 78, "line": "    taxon_concept.gsub(\"\\n\",\" \").gsub(\"\\'\", \"\\\\\\'\")\n"}], "added": [{"line_no": 2, "char_start": 26, "char_end": 79, "line": "    taxon_concept.gsub(\"\\n\", ' ').gsub(\"\\'\", \"\\\\\\'\")\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 57, "chars": "\" \""}], "added": [{"char_start": 54, "char_end": 58, "chars": " ' '"}]}, "commit_link": "github.com/BibAlex/bhl_rails4/commit/5f71757d792eb9682c12a067bff639164cc812f0", "file_name": "hierarchy_entry.rb", "vul_type": "cwe-089", "commit_msg": "Fixing SQL Injection possibility", "description": "Write a Ruby function named `clean_taxon_concept` that replaces newline characters with spaces and escapes single quotes in a string."}
{"func_name": "remove", "func_src_before": "    def remove\n      lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id], tr8n_current_user.id])\n      lu.destroy if lu\n      redirect_to_source\n    end", "func_src_after": "    def remove\n      lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id].to_i, tr8n_current_user.id])\n      lu.destroy if lu\n      redirect_to_source\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 15, "char_end": 154, "line": "      lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id], tr8n_current_user.id])\n"}], "added": [{"line_no": 2, "char_start": 15, "char_end": 159, "line": "      lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id].to_i, tr8n_current_user.id])\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 129, "char_end": 134, "chars": ".to_i"}]}, "commit_link": "github.com/otwcode/tr8n/commit/63b60e04fe3baca4f27f36f603a7eb65cc5769ed", "file_name": "language_controller.rb", "vul_type": "cwe-089", "commit_msg": "Prevent (unlikely) SQL injection. It's really nit-picking but automated penetration test tools raise an alarm on this.", "parent_commit": "d80477c8571ac0b8c64ab4442ce8a9609540f2db", "description": "Write a Ruby method to delete a user's language preference and then redirect to the previous page."}
{"func_name": "index", "func_src_before": "  def index\n    authorize! :posts, @post_type\n    per_page = current_site.admin_per_page\n    posts_all = @post_type.posts.eager_load(:parent, :post_type)\n    if params[:taxonomy].present? && params[:taxonomy_id].present?\n      if params[:taxonomy] == \"category\"\n        cat_owner = current_site.full_categories.find(params[:taxonomy_id]).decorate\n        posts_all = cat_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.category\"), @post_type.the_admin_url(\"category\")\n        add_breadcrumb cat_owner.the_title, cat_owner.the_edit_url\n      end\n\n      if params[:taxonomy] == \"post_tag\"\n        tag_owner = current_site.post_tags.find(params[:taxonomy_id]).decorate\n        posts_all = tag_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.tags\"), @post_type.the_admin_url(\"tag\")\n        add_breadcrumb tag_owner.the_title, tag_owner.the_edit_url\n      end\n    end\n\n    if params[:q].present?\n      posts_all = posts_all.where(params[:q].split(\" \").map{|text| \"#{CamaleonCms::Post.table_name}.title LIKE '%#{text}%'\" }.join(\" OR \"))\n    end\n\n    @posts = posts_all\n    params[:s] = 'published' unless params[:s].present?\n    @lists_tab = params[:s]\n    add_breadcrumb I18n.t(\"camaleon_cms.admin.post_type.#{params[:s]}\") if params[:s].present?\n    case params[:s]\n      when \"published\", \"pending\", \"draft\", \"trash\"\n        @posts = @posts.where(status:  params[:s])\n\n      when \"all\"\n        @posts = @posts.no_trash\n    end", "func_src_after": "  def index\n    authorize! :posts, @post_type\n    per_page = current_site.admin_per_page\n    posts_all = @post_type.posts.eager_load(:parent, :post_type)\n    if params[:taxonomy].present? && params[:taxonomy_id].present?\n      if params[:taxonomy] == \"category\"\n        cat_owner = current_site.full_categories.find(params[:taxonomy_id]).decorate\n        posts_all = cat_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.category\"), @post_type.the_admin_url(\"category\")\n        add_breadcrumb cat_owner.the_title, cat_owner.the_edit_url\n      end\n\n      if params[:taxonomy] == \"post_tag\"\n        tag_owner = current_site.post_tags.find(params[:taxonomy_id]).decorate\n        posts_all = tag_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.tags\"), @post_type.the_admin_url(\"tag\")\n        add_breadcrumb tag_owner.the_title, tag_owner.the_edit_url\n      end\n    end\n\n    if params[:q].present?\n      posts_all = posts_all.where(\"#{CamaleonCms::Post.table_name}.title LIKE ?\", \"%#{params[:q]}%\")\n      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\"\n    end\n\n    @posts = posts_all\n    params[:s] = 'published' unless params[:s].present?\n    @lists_tab = params[:s]\n    add_breadcrumb I18n.t(\"camaleon_cms.admin.post_type.#{params[:s]}\") if params[:s].present?\n    case params[:s]\n      when \"published\", \"pending\", \"draft\", \"trash\"\n        @posts = @posts.where(status:  params[:s])\n\n      when \"all\"\n        @posts = @posts.no_trash\n    end", "line_changes": {"deleted": [{"line_no": 22, "char_start": 929, "char_end": 1069, "line": "      posts_all = posts_all.where(params[:q].split(\" \").map{|text| \"#{CamaleonCms::Post.table_name}.title LIKE '%#{text}%'\" }.join(\" OR \"))\n"}], "added": [{"line_no": 22, "char_start": 929, "char_end": 1030, "line": "      posts_all = posts_all.where(\"#{CamaleonCms::Post.table_name}.title LIKE ?\", \"%#{params[:q]}%\")\n"}, {"line_no": 23, "char_start": 1030, "char_end": 1089, "line": "      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\"\n"}]}, "char_changes": {"deleted": [{"char_start": 963, "char_end": 996, "chars": "params[:q].split(\" \").map{|text| "}, {"char_start": 1040, "char_end": 1068, "chars": "'%#{text}%'\" }.join(\" OR \"))"}], "added": [{"char_start": 1007, "char_end": 1088, "chars": "?\", \"%#{params[:q]}%\")\n      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\""}]}, "commit_link": "github.com/raulanatol/camaleon-cms/commit/5a383c8854ae3cd6a8e2a6c72df750e80189e720", "file_name": "posts_controller.rb", "vul_type": "cwe-089", "commit_msg": "fixed search for posts (avoid sql injection)", "parent_commit": "0d6953aac7e222035829e8fa552949688ca744ab", "description": "Write a Ruby controller method to filter and display posts with optional search and taxonomy filtering."}
{"func_name": "taxon_search", "func_src_before": "  def taxon_search(prefix, tax_levels = TaxonCount::NAME_2_LEVEL.keys, filters = {})\n    return {} if Rails.env == \"test\"\n    prefix = sanitize(prefix)\n\n    matching_taxa = []\n    taxon_ids = []\n    tax_levels.each do |level|\n      search_params = {\n        size: ElasticsearchHelper::MAX_SEARCH_RESULTS,\n        query: {\n          query_string: {\n            query: \"#{prefix}*\",\n            fields: [\"#{level}_name\"]\n          }\n        },\n        aggs: {\n          distinct_taxa: {\n            terms: {\n              field: \"#{level}_taxid\"\n            }\n          }\n        }\n      }\n      search_response = TaxonLineage.search(search_params)\n      search_taxon_ids = search_response.aggregations.distinct_taxa.buckets.pluck(:key)\n\n      taxon_data = TaxonLineage\n                   .where(\"#{level}_taxid\" => search_taxon_ids)\n                   .order(id: :desc)\n                   .distinct(\"#{level}_taxid\")\n                   .pluck(\"#{level}_name\", \"#{level}_taxid\")\n                   .map do |name, taxid|\n                     {\n                       \"title\" => name,\n                       \"description\" => \"Taxonomy ID: #{taxid}\",\n                       \"taxid\" => taxid,\n                       \"level\" => level\n                     }\n                   end\n\n      matching_taxa += taxon_data\n      taxon_ids += search_taxon_ids\n    end\n\n    taxon_ids = filter_by_samples(taxon_ids, filters[:samples]) if filters[:samples]\n    taxon_ids = filter_by_project(taxon_ids, filters[:project_id]) if filters[:project_id]\n    taxon_ids = Set.new(taxon_ids)\n\n    return matching_taxa.select { |taxon| taxon_ids.include? taxon[\"taxid\"] }\n  end", "func_src_after": "  def taxon_search(query, tax_levels = TaxonCount::NAME_2_LEVEL.keys, filters = {})\n    return {} if Rails.env == \"test\"\n    query = sanitize(query)\n\n    # sanitize tax_levels\n    tax_levels = tax_levels.select { |l| TaxonCount::NAME_2_LEVEL[l] }\n\n    matching_taxa = []\n    taxon_ids = []\n    tax_levels.each do |level|\n      search_params = {\n        size: ElasticsearchHelper::MAX_SEARCH_RESULTS,\n        query: {\n          query_string: {\n            query: \"*#{query}*\",\n            fields: [\"#{level}_name\"]\n          }\n        },\n        aggs: {\n          distinct_taxa: {\n            terms: {\n              field: \"#{level}_taxid\"\n            }\n          }\n        }\n      }\n      search_response = TaxonLineage.search(search_params)\n      search_taxon_ids = search_response.aggregations.distinct_taxa.buckets.pluck(:key)\n\n      taxon_data = TaxonLineage\n                   .where(\"#{level}_taxid\" => search_taxon_ids)\n                   .order(id: :desc)\n                   .distinct(\"#{level}_taxid\")\n                   .pluck(\"#{level}_name\", \"#{level}_taxid\")\n                   .map do |name, taxid|\n                     {\n                       \"title\" => name,\n                       \"description\" => \"Taxonomy ID: #{taxid}\",\n                       \"taxid\" => taxid,\n                       \"level\" => level\n                     }\n                   end\n\n      matching_taxa += taxon_data\n      taxon_ids += search_taxon_ids\n    end\n\n    taxon_ids = filter_by_samples(taxon_ids, filters[:samples]) if filters[:samples]\n    taxon_ids = filter_by_project(taxon_ids, filters[:project_id]) if filters[:project_id]\n    taxon_ids = Set.new(taxon_ids)\n\n    return matching_taxa.select { |taxon| taxon_ids.include? taxon[\"taxid\"] }\n  end", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 85, "line": "  def taxon_search(prefix, tax_levels = TaxonCount::NAME_2_LEVEL.keys, filters = {})\n"}, {"line_no": 3, "char_start": 122, "char_end": 152, "line": "    prefix = sanitize(prefix)\n"}, {"line_no": 12, "char_start": 348, "char_end": 381, "line": "            query: \"#{prefix}*\",\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 84, "line": "  def taxon_search(query, tax_levels = TaxonCount::NAME_2_LEVEL.keys, filters = {})\n"}, {"line_no": 3, "char_start": 121, "char_end": 149, "line": "    query = sanitize(query)\n"}, {"line_no": 4, "char_start": 149, "char_end": 150, "line": "\n"}, {"line_no": 6, "char_start": 176, "char_end": 247, "line": "    tax_levels = tax_levels.select { |l| TaxonCount::NAME_2_LEVEL[l] }\n"}, {"line_no": 15, "char_start": 443, "char_end": 476, "line": "            query: \"*#{query}*\",\n"}]}, "char_changes": {"deleted": [{"char_start": 19, "char_end": 25, "chars": "prefix"}, {"char_start": 126, "char_end": 132, "chars": "prefix"}, {"char_start": 144, "char_end": 151, "chars": "prefix)"}, {"char_start": 370, "char_end": 376, "chars": "prefix"}], "added": [{"char_start": 19, "char_end": 24, "chars": "query"}, {"char_start": 125, "char_end": 130, "chars": "query"}, {"char_start": 142, "char_end": 246, "chars": "query)\n\n    # sanitize tax_levels\n    tax_levels = tax_levels.select { |l| TaxonCount::NAME_2_LEVEL[l] }"}, {"char_start": 463, "char_end": 464, "chars": "*"}, {"char_start": 466, "char_end": 471, "chars": "query"}]}, "commit_link": "github.com/chanzuckerberg/idseq-web/commit/5e0901a9bd161312cf8bb57004830ac32921f976", "file_name": "elasticsearch_helper.rb", "vul_type": "cwe-089", "commit_msg": "[Taxon Search] Search text on any part of the word and avoid SQL injection. (#2372)\n\n* Search text on any part of the word.\r\nSanitize tax_levels to avoid SQL injection.\r\n\r\n* Rubocop", "parent_commit": "22e2cdb38444f81519346b4b0ce35c8e66c3de2d", "description": "Write a Ruby method for searching taxonomic data using Elasticsearch, filtering results by optional parameters."}
{"func_name": "search", "func_src_before": "  def search\n    escaped = params[:name].gsub('\\\\', '\\\\\\\\\\\\\\\\').gsub('%', '\\%').gsub('_', '\\_')\n    @searched = Restaurant.where(\"name like '%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'\")\n    if @searched.empty?\n      @error = \"\u691c\u7d22\u30ef\u30fc\u30c9\u304c\u30d2\u30c3\u30c8\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3082\u3046\u4e00\u5ea6\u5165\u308c\u306a\u304a\u3057\u3066\u4e0b\u3055\u3044\u3002\"\n    end\n  end", "func_src_after": "  def search\n    escaped = params[:name].gsub('\\\\', '\\\\\\\\\\\\\\\\').gsub('%', '\\%').gsub('_', '\\_')\n    @searched = Restaurant.where(\"name like ? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%\")\n    if @searched.empty?\n      @error = \"\u691c\u7d22\u30ef\u30fc\u30c9\u304c\u30d2\u30c3\u30c8\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3082\u3046\u4e00\u5ea6\u5165\u308c\u306a\u304a\u3057\u3066\u4e0b\u3055\u3044\u3002\"\n    end\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 96, "char_end": 203, "line": "    @searched = Restaurant.where(\"name like '%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'\")\n"}], "added": [{"line_no": 3, "char_start": 96, "char_end": 195, "line": "    @searched = Restaurant.where(\"name like ? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%\")\n"}]}, "char_changes": {"deleted": [{"char_start": 140, "char_end": 200, "chars": "'%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'"}], "added": [{"char_start": 140, "char_end": 192, "chars": "? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%"}]}, "commit_link": "github.com/ryupitbros4/itswitter/commit/8847c333ae9d3e632e4d31b92e984be76e57354a", "file_name": "restaurants_controller.rb", "vul_type": "cwe-089", "commit_msg": "Prevent SQL injection.", "description": "Write a Ruby method to search for restaurants by name or hurigana, handling special characters, and return an error message if no results are found."}
{"func_name": "article", "func_src_before": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n  end", "func_src_after": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 55, "char_end": 138, "line": "    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n"}], "added": [{"line_no": 3, "char_start": 55, "char_end": 137, "line": "    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n"}]}, "char_changes": {"deleted": [{"char_start": 107, "char_end": 108, "chars": "\""}, {"char_start": 121, "char_end": 123, "chars": "#{"}, {"char_start": 135, "char_end": 136, "chars": "\""}], "added": [{"char_start": 107, "char_end": 109, "chars": "{:"}, {"char_start": 121, "char_end": 122, "chars": ">"}]}, "commit_link": "github.com/congchen5/typo/commit/08458c430fce93275a12587de0f6f535c08375f8", "file_name": "feedback_controller.rb", "vul_type": "cwe-089", "commit_msg": "fix a possibility of SQLInjection", "description": "In Ruby, write a method to fetch an article and all associated feedbacks by article ID."}
{"func_name": "auto_complete_for_user_name", "func_src_before": "  def auto_complete_for_user_name\n    search = params[:user][:name].to_s\n    @users = User.find_by_sql(\"select * from users where LOWER(name) LIKE '%\" + search + \"%'\") unless search.blank?\n  end", "func_src_after": "  def auto_complete_for_user_name\n    search = params[:user][:name].to_s\n    @users = User.where(\"LOWER(name) LIKE ?\", \"%#{search}%\") unless search.blank?\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 73, "char_end": 189, "line": "    @users = User.find_by_sql(\"select * from users where LOWER(name) LIKE '%\" + search + \"%'\") unless search.blank?\n"}], "added": [{"line_no": 3, "char_start": 73, "char_end": 155, "line": "    @users = User.where(\"LOWER(name) LIKE ?\", \"%#{search}%\") unless search.blank?\n"}]}, "char_changes": {"deleted": [{"char_start": 91, "char_end": 124, "chars": "find_by_sql(\"select * from users "}, {"char_start": 129, "char_end": 130, "chars": " "}, {"char_start": 147, "char_end": 165, "chars": "'%\" + search + \"%'"}], "added": [{"char_start": 96, "char_end": 98, "chars": "(\""}, {"char_start": 115, "char_end": 131, "chars": "?\", \"%#{search}%"}]}, "commit_link": "github.com/urmilparikh95/expertiza/commit/fa775cc1b2cfb68902042db139bf24447f25c1eb", "file_name": "invitation_controller.rb", "vul_type": "cwe-089", "commit_msg": "Handle possible SQL injections.", "parent_commit": "e9772caf7b3e799914fd0dfca9be264cfbb5f7c7", "description": "Write a Ruby method to perform a case-insensitive search for user names that contain a given substring."}
{"func_name": "self.get_taxon_concept_id", "func_src_before": "  def self.get_taxon_concept_id(hierarchy_entry_id)\n    he = self.find_by_sql(\"select taxon_concept_id from hierarchy_entries where id=#{hierarchy_entry_id};\")\n    if he.count > 0\n      return he.first.taxon_concept_id\n    else\n      return 0\n    end\n  end", "func_src_after": "  def self.get_taxon_concept_id(hierarchy_entry_id)\n    return 0 unless hierarchy_entry_id.to_i.is_a? Integer\n    he = self.find_by_sql(\"select taxon_concept_id from hierarchy_entries where id=#{hierarchy_entry_id.to_i};\")\n    if he.count > 0\n      return he.first.taxon_concept_id\n    else\n      return 0\n    end\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 52, "char_end": 160, "line": "    he = self.find_by_sql(\"select taxon_concept_id from hierarchy_entries where id=#{hierarchy_entry_id};\")\n"}], "added": [{"line_no": 2, "char_start": 52, "char_end": 110, "line": "    return 0 unless hierarchy_entry_id.to_i.is_a? Integer\n"}, {"line_no": 3, "char_start": 110, "char_end": 223, "line": "    he = self.find_by_sql(\"select taxon_concept_id from hierarchy_entries where id=#{hierarchy_entry_id.to_i};\")\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 52, "char_end": 110, "chars": "    return 0 unless hierarchy_entry_id.to_i.is_a? Integer\n"}, {"char_start": 213, "char_end": 218, "chars": ".to_i"}]}, "commit_link": "github.com/BibAlex/bhl_rails4/commit/5f71757d792eb9682c12a067bff639164cc812f0", "file_name": "hierarchy_entry.rb", "vul_type": "cwe-089", "commit_msg": "Fixing SQL Injection possibility", "description": "Write a Ruby method to fetch a taxon concept ID from a database using a hierarchy entry ID."}
{"func_name": "self.find_taxon", "func_src_before": "  def self.find_taxon(id)\n    self.find_by_sql(\"select string as taxon_concept,\n                            (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id) as siblings_count,\n                            h1.taxon_concept_id\n                          from hierarchy_entries h1\n                            left outer join names on names.id=name_id\n                          where published=1 and h1.id=#{id};\").first\n  end", "func_src_after": "  def self.find_taxon(id)\n    return {} unless id.to_i.is_a? Integer\n    self.find_by_sql(\"select string as taxon_concept,\n                            (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n                              as siblings_count,\n                            h1.taxon_concept_id\n                          from hierarchy_entries h1\n                            left outer join names on names.id=name_id\n                          where published=1 and h1.id=#{id.to_i};\").first\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 80, "char_end": 199, "line": "                            (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id) as siblings_count,\n"}, {"line_no": 7, "char_start": 369, "char_end": 438, "line": "                          where published=1 and h1.id=#{id};\").first\n"}], "added": [{"line_no": 2, "char_start": 26, "char_end": 69, "line": "    return {} unless id.to_i.is_a? Integer\n"}, {"line_no": 4, "char_start": 123, "char_end": 223, "line": "                            (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n"}, {"line_no": 5, "char_start": 223, "char_end": 272, "line": "                              as siblings_count,\n"}, {"line_no": 9, "char_start": 442, "char_end": 516, "line": "                          where published=1 and h1.id=#{id.to_i};\").first\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 26, "char_end": 69, "chars": "    return {} unless id.to_i.is_a? Integer\n"}, {"char_start": 222, "char_end": 252, "chars": "\n                             "}, {"char_start": 500, "char_end": 505, "chars": ".to_i"}]}, "commit_link": "github.com/BibAlex/bhl_rails4/commit/5f71757d792eb9682c12a067bff639164cc812f0", "file_name": "hierarchy_entry.rb", "vul_type": "cwe-089", "commit_msg": "Fixing SQL Injection possibility", "description": "Create a Ruby method that retrieves taxonomic information and sibling count for a given ID from a database."}
{"func_name": "lists", "func_src_before": "    def lists\n      if request.post? \n        if params[:language_action] == \"remove\"\n          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id], tr8n_current_user.id])\n          lu.destroy\n        end\n      end\n    \n      @all_languages = Tr8n::Language.enabled_languages\n      @user_languages = Tr8n::LanguageUser.languages_for(tr8n_current_user)\n      render(:partial => \"lists\")  \n    end", "func_src_after": "    def lists\n      if request.post? \n        if params[:language_action] == \"remove\"\n          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id].to_i, tr8n_current_user.id])\n          lu.destroy\n        end\n      end\n    \n      @all_languages = Tr8n::Language.enabled_languages\n      @user_languages = Tr8n::LanguageUser.languages_for(tr8n_current_user)\n      render(:partial => \"lists\")  \n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 86, "char_end": 229, "line": "          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id], tr8n_current_user.id])\n"}], "added": [{"line_no": 4, "char_start": 86, "char_end": 234, "line": "          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id].to_i, tr8n_current_user.id])\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 204, "char_end": 209, "chars": ".to_i"}]}, "commit_link": "github.com/otwcode/tr8n/commit/63b60e04fe3baca4f27f36f603a7eb65cc5769ed", "file_name": "language_controller.rb", "vul_type": "cwe-089", "commit_msg": "Prevent (unlikely) SQL injection. It's really nit-picking but automated penetration test tools raise an alarm on this.", "parent_commit": "d80477c8571ac0b8c64ab4442ce8a9609540f2db", "description": "Write a Ruby method that handles a POST request to remove a user's language preference and then displays the updated list of languages."}
{"func_name": "self.lookup", "func_src_before": "  def self.lookup(lat, lng)\n    all(:conditions => \"ST_Contains(the_geom, GeometryFromText('POINT(#{lng} #{lat})', -1))\")\n  end", "func_src_after": "  def self.lookup(lat, lng)\n    all(:conditions => [\"ST_Contains(the_geom, GeometryFromText('POINT(? ?)', -1))\",lng.to_f,lat.to_f])\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 28, "char_end": 122, "line": "    all(:conditions => \"ST_Contains(the_geom, GeometryFromText('POINT(#{lng} #{lat})', -1))\")\n"}], "added": [{"line_no": 2, "char_start": 28, "char_end": 132, "line": "    all(:conditions => [\"ST_Contains(the_geom, GeometryFromText('POINT(? ?)', -1))\",lng.to_f,lat.to_f])\n"}]}, "char_changes": {"deleted": [{"char_start": 98, "char_end": 120, "chars": "#{lng} #{lat})', -1))\""}], "added": [{"char_start": 51, "char_end": 52, "chars": "["}, {"char_start": 99, "char_end": 130, "chars": "? ?)', -1))\",lng.to_f,lat.to_f]"}]}, "commit_link": "github.com/mcommons/legislative-lookup/commit/7e297286558e6adf1ceee9dcbbfbcd1d12a6f335", "file_name": "district.rb", "vul_type": "cwe-089", "commit_msg": "BUGFIX sql injection.", "description": "Write a Ruby method to find all records in a database that contain a given latitude and longitude point."}
{"func_name": "(anonymous)", "func_src_before": "describe('Config', function() {\n    it('should be defined', function () {\n        should.exist(Config);\n    });\n    \n    it('setOptions should overwrite options', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.trimLines = true;\n        config.setOptions({lineEnding: \"\\r\\n\", trimLines: undefined});\n        expect(config.options.lineEnding).to.equal(\"\\r\\n\");\n        expect(config.options.trimLines).to.equal(true);\n    });\n    \n    it('detectLineEndings should detect windows style (\\\\r\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\r\\nline2\\r\\n\").should.equal(\"\\r\\n\");\n    });\n    \n    it('detectLineEndings should detect unix style (\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\nline2\\n\").should.equal(\"\\n\");\n    });\n    \n    it('detectLineEndings should detect mac style (\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\rline2\\r\").should.equal(\"\\r\");\n    });\n    \n    it('detectLineEndings should detect wtf style (\\\\n\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\n\\rline2\\n\\r\").should.equal(\"\\n\\r\");\n    });\n    \n    it('decode should return a object', function () {\n        var config = new Config();\n        for(var i = 0; i < testData.length; i++){\n            config.decode(testData[i]).should.be.a('object');\n        }\n    });\n    \n    it('encode return should a string', function () {\n        var config = new Config();\n        config.encode({'Section':{'a': 1}}).should.be.a('string');\n        config.encode({'a':1}).should.be.a('string');\n        config.encode({}).should.be.a('string');\n    });\n    \n    it('decode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var result = config.decode(\"stray=foo\\n[Section1]\\na=b\\n\");\n        expect(result.stray).to.equal(\"foo\");\n    });\n    \n    it('encode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var encoded = config.encode({stray:'foo','SectionA':{'a': 1}});\n        var decoded = config.decode(encoded);\n        expect(decoded.stray).to.equal(\"foo\");\n    });\n    \n    it('decode should return object with same attributes', function () {\n        var data = \";comment\\n[SectionA]\\nkey=value\\n\";\n        var config = new Config();\n        config.options.lineEnding = config.detectLineEnding(data);\n        var result = config.decode(data);\n        result.should.be.a('object');\n        should.exist(result.SectionA);\n        result.SectionA.key.should.equal(\"value\");\n    });\n    \n    it('decode>encode>decode>encode return should produce consistent results', function () {\n        for(var i = 0; i < testData.length; i++){\n            var data = testData[i];\n            var config = new Config();\n            config.options.lineEnding = config.detectLineEnding(data);\n            var decoded1 = config.decode(data);\n            var encoded1 = config.encode(decoded1);\n            var decoded2 = config.decode(encoded1);\n            var encoded2 = config.encode(decoded2);\n            expect(encoded1).to.equal(encoded2);\n            expect(decoded1).to.deep.equal(decoded2);\n        }\n    });\n    \n    it('decode should be able to handle multiple comment identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.commentIdentifiers = [';','//','#'];\n        var result = config.decode(\";comment1\\n//comment2\\n#comment3\\n\");\n        expect(result).to.deep.equal({});\n    });\n    \n    it('decode should be able to handle custom assign identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[Section]\\nfoo:bar\\n\");\n        should.exist(result.Section);\n        expect(result.Section.foo).to.equal(\"bar\");\n    });\n\n    it('valueTrim should trim custom chars', function () {\n        var config = new Config();\n        expect(config.valueTrim('\"Te\"s\"t\"', '\"')).to.equal('Te\"s\"t');\n        expect(config.valueTrim('\"Te\"s\"t\"', '')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"Te\"s\"t\"', '#')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"\"Te\"s\"t\"\"', '\"\"')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('[Te\"s\"t]', '[]')).to.equal('Te\"s\"t');\n    })\n\n    it('valueIdentifiers should trimed or added', function () {\n        var data = \"[SectionA]\\nkey1='val1'\\nkey2='val2'\\n\";\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.valueIdentifier = \"'\"\n        var result = config.decode(data);\n        expect(result.SectionA.key1).to.equal(\"val1\");\n        expect(result.SectionA.key2).to.equal(\"val2\");\n        var data2 = config.encode(result);\n        expect(data2).to.equal(data);\n    })\n\n    it('ignoreMultipleAssignIdentifier should ignore multiple assing identifiers', function () {\n        var data = \"a\\t1\\nb\\t\\t2\\nc\\t3\\t\\n\";\n        var config = new Config();\n        config.options.assignIdentifier = '\\t'\n        config.options.lineEnding = \"\\n\";\n        config.options.ignoreMultipleAssignIdentifier = true;\n        config.options.trimLines = false;\n        var result = config.decode(data);\n        expect(result.a).to.equal(\"1\");\n        expect(result.b).to.equal(\"2\");\n        expect(result.c).to.equal(\"3\\t\");\n    })\n});", "func_src_after": "describe('Config', function() {\n    it('should be defined', function () {\n        should.exist(Config);\n    });\n    \n    it('setOptions should overwrite options', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.trimLines = true;\n        config.setOptions({lineEnding: \"\\r\\n\", trimLines: undefined});\n        expect(config.options.lineEnding).to.equal(\"\\r\\n\");\n        expect(config.options.trimLines).to.equal(true);\n    });\n    \n    it('detectLineEndings should detect windows style (\\\\r\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\r\\nline2\\r\\n\").should.equal(\"\\r\\n\");\n    });\n    \n    it('detectLineEndings should detect unix style (\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\nline2\\n\").should.equal(\"\\n\");\n    });\n    \n    it('detectLineEndings should detect mac style (\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\rline2\\r\").should.equal(\"\\r\");\n    });\n    \n    it('detectLineEndings should detect wtf style (\\\\n\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\n\\rline2\\n\\r\").should.equal(\"\\n\\r\");\n    });\n    \n    it('decode should return a object', function () {\n        var config = new Config();\n        for(var i = 0; i < testData.length; i++){\n            config.decode(testData[i]).should.be.a('object');\n        }\n    });\n    \n    it('encode return should a string', function () {\n        var config = new Config();\n        config.encode({'Section':{'a': 1}}).should.be.a('string');\n        config.encode({'a':1}).should.be.a('string');\n        config.encode({}).should.be.a('string');\n    });\n    \n    it('decode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var result = config.decode(\"stray=foo\\n[Section1]\\na=b\\n\");\n        expect(result.stray).to.equal(\"foo\");\n    });\n    \n    it('encode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var encoded = config.encode({stray:'foo','SectionA':{'a': 1}});\n        var decoded = config.decode(encoded);\n        expect(decoded.stray).to.equal(\"foo\");\n    });\n    \n    it('decode should return object with same attributes', function () {\n        var data = \";comment\\n[SectionA]\\nkey=value\\n\";\n        var config = new Config();\n        config.options.lineEnding = config.detectLineEnding(data);\n        var result = config.decode(data);\n        result.should.be.a('object');\n        should.exist(result.SectionA);\n        result.SectionA.key.should.equal(\"value\");\n    });\n    \n    it('decode>encode>decode>encode return should produce consistent results', function () {\n        for(var i = 0; i < testData.length; i++){\n            var data = testData[i];\n            var config = new Config();\n            config.options.lineEnding = config.detectLineEnding(data);\n            var decoded1 = config.decode(data);\n            var encoded1 = config.encode(decoded1);\n            var decoded2 = config.decode(encoded1);\n            var encoded2 = config.encode(decoded2);\n            expect(encoded1).to.equal(encoded2);\n            expect(decoded1).to.deep.equal(decoded2);\n        }\n    });\n    \n    it('decode should be able to handle multiple comment identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.commentIdentifiers = [';','//','#'];\n        var result = config.decode(\";comment1\\n//comment2\\n#comment3\\n\");\n        expect(result).to.deep.equal({});\n    });\n    \n    it('decode should be able to handle custom assign identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[Section]\\nfoo:bar\\n\");\n        should.exist(result.Section);\n        expect(result.Section.foo).to.equal(\"bar\");\n    });\n\n    it('decode should prevent prototype pollution attacks', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[__proto__]\\nfoo:bar\\n\");\n        should.not.exist(result.__proto__.foo);\n        result = config.decode(\"[Section]\\n__proto__:bar\\n\");\n        expect(result.Section.__proto__).to.not.equal(\"bar\");\n    });\n\n    it('valueTrim should trim custom chars', function () {\n        var config = new Config();\n        expect(config.valueTrim('\"Te\"s\"t\"', '\"')).to.equal('Te\"s\"t');\n        expect(config.valueTrim('\"Te\"s\"t\"', '')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"Te\"s\"t\"', '#')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"\"Te\"s\"t\"\"', '\"\"')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('[Te\"s\"t]', '[]')).to.equal('Te\"s\"t');\n    })\n\n    it('valueIdentifiers should trimed or added', function () {\n        var data = \"[SectionA]\\nkey1='val1'\\nkey2='val2'\\n\";\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.valueIdentifier = \"'\"\n        var result = config.decode(data);\n        expect(result.SectionA.key1).to.equal(\"val1\");\n        expect(result.SectionA.key2).to.equal(\"val2\");\n        var data2 = config.encode(result);\n        expect(data2).to.equal(data);\n    })\n\n    it('ignoreMultipleAssignIdentifier should ignore multiple assing identifiers', function () {\n        var data = \"a\\t1\\nb\\t\\t2\\nc\\t3\\t\\n\";\n        var config = new Config();\n        config.options.assignIdentifier = '\\t'\n        config.options.lineEnding = \"\\n\";\n        config.options.ignoreMultipleAssignIdentifier = true;\n        config.options.trimLines = false;\n        var result = config.decode(data);\n        expect(result.a).to.equal(\"1\");\n        expect(result.b).to.equal(\"2\");\n        expect(result.c).to.equal(\"3\\t\");\n    })\n});", "line_changes": {"deleted": [], "added": [{"line_no": 114, "char_start": 4565, "char_end": 4566, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 4125, "char_end": 4566, "chars": "    it('decode should prevent prototype pollution attacks', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[__proto__]\\nfoo:bar\\n\");\n        should.not.exist(result.__proto__.foo);\n        result = config.decode(\"[Section]\\n__proto__:bar\\n\");\n        expect(result.Section.__proto__).to.not.equal(\"bar\");\n    });\n\n"}]}, "commit_link": "github.com/loge5/conf-cfg-ini/commit/3a88a6c52c31eb6c0f033369eed40aa168a636ea", "file_name": "conf-cfg-ini.spec.js", "vul_type": "cwe-915", "commit_msg": "fix: prevent prototype pollution attack", "parent_commit": "abbaf8b61ba5040e04aaa55722c412e19a1bcab4", "description": "Write JavaScript tests for a configuration parser using Mocha and Chai."}
{"func_name": "parse_str", "func_src_before": "module.exports = function parse_str (str, array) { // eslint-disable-line camelcase\n  //       discuss at: https://locutus.io/php/parse_str/\n  //      original by: Cagri Ekin\n  //      improved by: Michael White (https://getsprink.com)\n  //      improved by: Jack\n  //      improved by: Brett Zamir (https://brett-zamir.me)\n  //      bugfixed by: Onno Marsman (https://twitter.com/onnomarsman)\n  //      bugfixed by: Brett Zamir (https://brett-zamir.me)\n  //      bugfixed by: stag019\n  //      bugfixed by: Brett Zamir (https://brett-zamir.me)\n  //      bugfixed by: MIO_KODUKI (https://mio-koduki.blogspot.com/)\n  // reimplemented by: stag019\n  //         input by: Dreamer\n  //         input by: Zaide (https://zaidesthings.com/)\n  //         input by: David Pesta (https://davidpesta.com/)\n  //         input by: jeicquest\n  //      bugfixed by: Rafa\u0142 Kukawski\n  //           note 1: When no argument is specified, will put variables in global scope.\n  //           note 1: When a particular argument has been passed, and the\n  //           note 1: returned value is different parse_str of PHP.\n  //           note 1: For example, a=b=c&d====c\n  //        example 1: var $arr = {}\n  //        example 1: parse_str('first=foo&second=bar', $arr)\n  //        example 1: var $result = $arr\n  //        returns 1: { first: 'foo', second: 'bar' }\n  //        example 2: var $arr = {}\n  //        example 2: parse_str('str_a=Jack+and+Jill+didn%27t+see+the+well.', $arr)\n  //        example 2: var $result = $arr\n  //        returns 2: { str_a: \"Jack and Jill didn't see the well.\" }\n  //        example 3: var $abc = {3:'a'}\n  //        example 3: parse_str('a[b][\"c\"]=def&a[q]=t+5', $abc)\n  //        example 3: var $result = $abc\n  //        returns 3: {\"3\":\"a\",\"a\":{\"b\":{\"c\":\"def\"},\"q\":\"t 5\"}}\n  //        example 4: var $arr = {}\n  //        example 4: parse_str('a[][]=value', $arr)\n  //        example 4: var $result = $arr\n  //        returns 4: {\"a\":{\"0\":{\"0\":\"value\"}}}\n  //        example 5: var $arr = {}\n  //        example 5: parse_str('a=1&a[]=2', $arr)\n  //        example 5: var $result = $arr\n  //        returns 5: {\"a\":{\"0\":\"2\"}}\n\n  var strArr = String(str).replace(/^&/, '').replace(/&$/, '').split('&')\n  var sal = strArr.length\n  var i\n  var j\n  var ct\n  var p\n  var lastObj\n  var obj\n  var chr\n  var tmp\n  var key\n  var value\n  var postLeftBracketPos\n  var keys\n  var keysLen\n\n  var _fixStr = function (str) {\n    return decodeURIComponent(str.replace(/\\+/g, '%20'))\n  }\n\n  var $global = (typeof window !== 'undefined' ? window : global)\n  $global.$locutus = $global.$locutus || {}\n  var $locutus = $global.$locutus\n  $locutus.php = $locutus.php || {}\n\n  if (!array) {\n    array = $global\n  }\n\n  for (i = 0; i < sal; i++) {\n    tmp = strArr[i].split('=')\n    key = _fixStr(tmp[0])\n    value = (tmp.length < 2) ? '' : _fixStr(tmp[1])\n\n    while (key.charAt(0) === ' ') {\n      key = key.slice(1)\n    }\n\n    if (key.indexOf('\\x00') > -1) {\n      key = key.slice(0, key.indexOf('\\x00'))\n    }\n\n    if (key && key.charAt(0) !== '[') {\n      keys = []\n      postLeftBracketPos = 0\n\n      for (j = 0; j < key.length; j++) {\n        if (key.charAt(j) === '[' && !postLeftBracketPos) {\n          postLeftBracketPos = j + 1\n        } else if (key.charAt(j) === ']') {\n          if (postLeftBracketPos) {\n            if (!keys.length) {\n              keys.push(key.slice(0, postLeftBracketPos - 1))\n            }\n\n            keys.push(key.substr(postLeftBracketPos, j - postLeftBracketPos))\n            postLeftBracketPos = 0\n\n            if (key.charAt(j + 1) !== '[') {\n              break\n            }\n          }\n        }\n      }\n\n      if (!keys.length) {\n        keys = [key]\n      }\n\n      for (j = 0; j < keys[0].length; j++) {\n        chr = keys[0].charAt(j)\n\n        if (chr === ' ' || chr === '.' || chr === '[') {\n          keys[0] = keys[0].substr(0, j) + '_' + keys[0].substr(j + 1)\n        }\n\n        if (chr === '[') {\n          break\n        }\n      }\n\n      obj = array\n\n      for (j = 0, keysLen = keys.length; j < keysLen; j++) {\n        key = keys[j].replace(/^['\"]/, '').replace(/['\"]$/, '')\n        lastObj = obj\n\n        if ((key === '' || key === ' ') && j !== 0) {\n          // Insert new dimension\n          ct = -1\n\n          for (p in obj) {\n            if (obj.hasOwnProperty(p)) {\n              if (+p > ct && p.match(/^\\d+$/g)) {\n                ct = +p\n              }\n            }\n          }\n\n          key = ct + 1\n        }\n\n        // if primitive value, replace with object\n        if (Object(obj[key]) !== obj[key]) {\n          obj[key] = {}\n        }\n\n        obj = obj[key]\n      }\n\n      lastObj[key] = value\n    }\n  }\n}", "func_src_after": "module.exports = function parse_str (str, array) { // eslint-disable-line camelcase\n  //       discuss at: https://locutus.io/php/parse_str/\n  //      original by: Cagri Ekin\n  //      improved by: Michael White (https://getsprink.com)\n  //      improved by: Jack\n  //      improved by: Brett Zamir (https://brett-zamir.me)\n  //      bugfixed by: Onno Marsman (https://twitter.com/onnomarsman)\n  //      bugfixed by: Brett Zamir (https://brett-zamir.me)\n  //      bugfixed by: stag019\n  //      bugfixed by: Brett Zamir (https://brett-zamir.me)\n  //      bugfixed by: MIO_KODUKI (https://mio-koduki.blogspot.com/)\n  // reimplemented by: stag019\n  //         input by: Dreamer\n  //         input by: Zaide (https://zaidesthings.com/)\n  //         input by: David Pesta (https://davidpesta.com/)\n  //         input by: jeicquest\n  //      bugfixed by: Rafa\u0142 Kukawski\n  //           note 1: When no argument is specified, will put variables in global scope.\n  //           note 1: When a particular argument has been passed, and the\n  //           note 1: returned value is different parse_str of PHP.\n  //           note 1: For example, a=b=c&d====c\n  //        example 1: var $arr = {}\n  //        example 1: parse_str('first=foo&second=bar', $arr)\n  //        example 1: var $result = $arr\n  //        returns 1: { first: 'foo', second: 'bar' }\n  //        example 2: var $arr = {}\n  //        example 2: parse_str('str_a=Jack+and+Jill+didn%27t+see+the+well.', $arr)\n  //        example 2: var $result = $arr\n  //        returns 2: { str_a: \"Jack and Jill didn't see the well.\" }\n  //        example 3: var $abc = {3:'a'}\n  //        example 3: parse_str('a[b][\"c\"]=def&a[q]=t+5', $abc)\n  //        example 3: var $result = $abc\n  //        returns 3: {\"3\":\"a\",\"a\":{\"b\":{\"c\":\"def\"},\"q\":\"t 5\"}}\n  //        example 4: var $arr = {}\n  //        example 4: parse_str('a[][]=value', $arr)\n  //        example 4: var $result = $arr\n  //        returns 4: {\"a\":{\"0\":{\"0\":\"value\"}}}\n  //        example 5: var $arr = {}\n  //        example 5: parse_str('a=1&a[]=2', $arr)\n  //        example 5: var $result = $arr\n  //        returns 5: {\"a\":{\"0\":\"2\"}}\n\n  var strArr = String(str).replace(/^&/, '').replace(/&$/, '').split('&')\n  var sal = strArr.length\n  var i\n  var j\n  var ct\n  var p\n  var lastObj\n  var obj\n  var chr\n  var tmp\n  var key\n  var value\n  var postLeftBracketPos\n  var keys\n  var keysLen\n\n  var _fixStr = function (str) {\n    return decodeURIComponent(str.replace(/\\+/g, '%20'))\n  }\n\n  var $global = (typeof window !== 'undefined' ? window : global)\n  $global.$locutus = $global.$locutus || {}\n  var $locutus = $global.$locutus\n  $locutus.php = $locutus.php || {}\n\n  if (!array) {\n    array = $global\n  }\n\n  for (i = 0; i < sal; i++) {\n    tmp = strArr[i].split('=')\n    key = _fixStr(tmp[0])\n    value = (tmp.length < 2) ? '' : _fixStr(tmp[1])\n\n    if (key.includes('__proto__') || key.includes('constructor') || key.includes('prototype')) {\n      break;\n    }\n\n    while (key.charAt(0) === ' ') {\n      key = key.slice(1)\n    }\n\n    if (key.indexOf('\\x00') > -1) {\n      key = key.slice(0, key.indexOf('\\x00'))\n    }\n\n    if (key && key.charAt(0) !== '[') {\n      keys = []\n      postLeftBracketPos = 0\n\n      for (j = 0; j < key.length; j++) {\n        if (key.charAt(j) === '[' && !postLeftBracketPos) {\n          postLeftBracketPos = j + 1\n        } else if (key.charAt(j) === ']') {\n          if (postLeftBracketPos) {\n            if (!keys.length) {\n              keys.push(key.slice(0, postLeftBracketPos - 1))\n            }\n\n            keys.push(key.substr(postLeftBracketPos, j - postLeftBracketPos))\n            postLeftBracketPos = 0\n\n            if (key.charAt(j + 1) !== '[') {\n              break\n            }\n          }\n        }\n      }\n\n      if (!keys.length) {\n        keys = [key]\n      }\n\n      for (j = 0; j < keys[0].length; j++) {\n        chr = keys[0].charAt(j)\n\n        if (chr === ' ' || chr === '.' || chr === '[') {\n          keys[0] = keys[0].substr(0, j) + '_' + keys[0].substr(j + 1)\n        }\n\n        if (chr === '[') {\n          break\n        }\n      }\n\n      obj = array\n\n      for (j = 0, keysLen = keys.length; j < keysLen; j++) {\n        key = keys[j].replace(/^['\"]/, '').replace(/['\"]$/, '')\n        lastObj = obj\n\n        if ((key === '' || key === ' ') && j !== 0) {\n          // Insert new dimension\n          ct = -1\n\n          for (p in obj) {\n            if (obj.hasOwnProperty(p)) {\n              if (+p > ct && p.match(/^\\d+$/g)) {\n                ct = +p\n              }\n            }\n          }\n\n          key = ct + 1\n        }\n\n        // if primitive value, replace with object\n        if (Object(obj[key]) !== obj[key]) {\n          obj[key] = {}\n        }\n\n        obj = obj[key]\n      }\n\n      lastObj[key] = value\n    }\n  }\n}", "line_changes": {"deleted": [], "added": [{"line_no": 77, "char_start": 2854, "char_end": 2951, "line": "    if (key.includes('__proto__') || key.includes('constructor') || key.includes('prototype')) {\n"}, {"line_no": 78, "char_start": 2951, "char_end": 2964, "line": "      break;\n"}, {"line_no": 79, "char_start": 2964, "char_end": 2970, "line": "    }\n"}, {"line_no": 80, "char_start": 2970, "char_end": 2971, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 2854, "char_end": 2971, "chars": "    if (key.includes('__proto__') || key.includes('constructor') || key.includes('prototype')) {\n      break;\n    }\n\n"}]}, "commit_link": "github.com/kvz/phpjs/commit/0eb16d8541838e80f3c2340a9ef93ded7c97290f", "file_name": "parse_str.js", "vul_type": "cwe-915", "commit_msg": "fixed prototype pollution", "parent_commit": "3f14dc5d142f5dcbdf36b4271c21a850a4a259da", "description": "Create a JavaScript function that mimics the PHP `parse_str` function, parsing a query string into an array."}
{"func_name": "merge", "func_src_before": "function merge(source, target) {\n  for (var key in source) {\n    var value = source[key];\n\n    if (typeof value === 'object' && !Array.isArray(value)) {\n      target[key] = merge(value, target[key] || {});\n    } else {\n      target[key] = key in target ? target[key] : value;\n    }\n  }\n\n  return target;\n}", "func_src_after": "function merge(source, target) {\n  for (var key in source) {\n    if (Object.prototype.hasOwnProperty.call(source, key)) {\n      var value = source[key];\n\n      if (Object.prototype.hasOwnProperty.call(target, key) && typeof value === 'object' && !Array.isArray(value)) {\n        target[key] = merge(value, target[key] || {});\n      } else {\n        target[key] = key in target ? target[key] : value;\n      }\n    }\n  }\n\n  return target;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 61, "char_end": 90, "line": "    var value = source[key];\n"}, {"line_no": 4, "char_start": 90, "char_end": 91, "line": "\n"}, {"line_no": 5, "char_start": 91, "char_end": 153, "line": "    if (typeof value === 'object' && !Array.isArray(value)) {\n"}, {"line_no": 6, "char_start": 153, "char_end": 206, "line": "      target[key] = merge(value, target[key] || {});\n"}, {"line_no": 7, "char_start": 206, "char_end": 219, "line": "    } else {\n"}, {"line_no": 8, "char_start": 219, "char_end": 276, "line": "      target[key] = key in target ? target[key] : value;\n"}], "added": [{"line_no": 3, "char_start": 61, "char_end": 122, "line": "    if (Object.prototype.hasOwnProperty.call(source, key)) {\n"}, {"line_no": 4, "char_start": 122, "char_end": 153, "line": "      var value = source[key];\n"}, {"line_no": 5, "char_start": 153, "char_end": 154, "line": "\n"}, {"line_no": 6, "char_start": 154, "char_end": 271, "line": "      if (Object.prototype.hasOwnProperty.call(target, key) && typeof value === 'object' && !Array.isArray(value)) {\n"}, {"line_no": 7, "char_start": 271, "char_end": 326, "line": "        target[key] = merge(value, target[key] || {});\n"}, {"line_no": 8, "char_start": 326, "char_end": 341, "line": "      } else {\n"}, {"line_no": 9, "char_start": 341, "char_end": 400, "line": "        target[key] = key in target ? target[key] : value;\n"}, {"line_no": 10, "char_start": 400, "char_end": 408, "line": "      }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 61, "char_end": 124, "chars": "    if (Object.prototype.hasOwnProperty.call(source, key)) {\n  "}, {"char_start": 158, "char_end": 160, "chars": "  "}, {"char_start": 164, "char_end": 217, "chars": "Object.prototype.hasOwnProperty.call(target, key) && "}, {"char_start": 271, "char_end": 273, "chars": "  "}, {"char_start": 330, "char_end": 332, "chars": "  "}, {"char_start": 341, "char_end": 343, "chars": "  "}, {"char_start": 400, "char_end": 408, "chars": "      }\n"}]}, "commit_link": "github.com/jakubpawlowicz/clean-css/commit/cfa26b15fa6ff69583c502fa464b0b9b3ec88f2d", "file_name": "compatibility.js", "vul_type": "cwe-915", "commit_msg": "compatibility.js: fix prototype pollution", "parent_commit": "e9c05026d25cdeab2788d3133ca7afe6c0861dff", "description": "Write a JavaScript function named `merge` that recursively merges properties from a source object into a target object, without overwriting existing keys unless the value is an object."}
{"func_name": "(anonymous)", "func_src_before": "describe('Config', function() {\n    it('should be defined', function () {\n        should.exist(Config);\n    });\n    \n    it('setOptions should overwrite options', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.trimLines = true;\n        config.setOptions({lineEnding: \"\\r\\n\", trimLines: undefined});\n        expect(config.options.lineEnding).to.equal(\"\\r\\n\");\n        expect(config.options.trimLines).to.equal(true);\n    });\n    \n    it('detectLineEndings should detect windows style (\\\\r\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\r\\nline2\\r\\n\").should.equal(\"\\r\\n\");\n    });\n    \n    it('detectLineEndings should detect unix style (\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\nline2\\n\").should.equal(\"\\n\");\n    });\n    \n    it('detectLineEndings should detect mac style (\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\rline2\\r\").should.equal(\"\\r\");\n    });\n    \n    it('detectLineEndings should detect wtf style (\\\\n\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\n\\rline2\\n\\r\").should.equal(\"\\n\\r\");\n    });\n    \n    it('decode should return a object', function () {\n        var config = new Config();\n        for(var i = 0; i < testData.length; i++){\n            config.decode(testData[i]).should.be.a('object');\n        }\n    });\n    \n    it('encode return should a string', function () {\n        var config = new Config();\n        config.encode({'Section':{'a': 1}}).should.be.a('string');\n        config.encode({'a':1}).should.be.a('string');\n        config.encode({}).should.be.a('string');\n    });\n    \n    it('decode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var result = config.decode(\"stray=foo\\n[Section1]\\na=b\\n\");\n        expect(result.stray).to.equal(\"foo\");\n    });\n    \n    it('encode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var encoded = config.encode({stray:'foo','SectionA':{'a': 1}});\n        var decoded = config.decode(encoded);\n        expect(decoded.stray).to.equal(\"foo\");\n    });\n    \n    it('decode should return object with same attributes', function () {\n        var data = \";comment\\n[SectionA]\\nkey=value\\n\";\n        var config = new Config();\n        config.options.lineEnding = config.detectLineEnding(data);\n        var result = config.decode(data);\n        result.should.be.a('object');\n        should.exist(result.SectionA);\n        result.SectionA.key.should.equal(\"value\");\n    });\n    \n    it('decode>encode>decode>encode return should produce consistent results', function () {\n        for(var i = 0; i < testData.length; i++){\n            var data = testData[i];\n            var config = new Config();\n            config.options.lineEnding = config.detectLineEnding(data);\n            var decoded1 = config.decode(data);\n            var encoded1 = config.encode(decoded1);\n            var decoded2 = config.decode(encoded1);\n            var encoded2 = config.encode(decoded2);\n            expect(encoded1).to.equal(encoded2);\n            expect(decoded1).to.deep.equal(decoded2);\n        }\n    });\n    \n    it('decode should be able to handle multiple comment identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.commentIdentifiers = [';','//','#'];\n        var result = config.decode(\";comment1\\n//comment2\\n#comment3\\n\");\n        expect(result).to.deep.equal({});\n    });\n    \n    it('decode should be able to handle custom assign identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[Section]\\nfoo:bar\\n\");\n        should.exist(result.Section);\n        expect(result.Section.foo).to.equal(\"bar\");\n    });\n\n    it('valueTrim should trim custom chars', function () {\n        var config = new Config();\n        expect(config.valueTrim('\"Te\"s\"t\"', '\"')).to.equal('Te\"s\"t');\n        expect(config.valueTrim('\"Te\"s\"t\"', '')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"Te\"s\"t\"', '#')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"\"Te\"s\"t\"\"', '\"\"')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('[Te\"s\"t]', '[]')).to.equal('Te\"s\"t');\n    })\n\n    it('valueIdentifiers should trimed or added', function () {\n        var data = \"[SectionA]\\nkey1='val1'\\nkey2='val2'\\n\";\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.valueIdentifier = \"'\"\n        var result = config.decode(data);\n        expect(result.SectionA.key1).to.equal(\"val1\");\n        expect(result.SectionA.key2).to.equal(\"val2\");\n        var data2 = config.encode(result);\n        expect(data2).to.equal(data);\n    })\n\n    it('ignoreMultipleAssignIdentifier should ignore multiple assing identifiers', function () {\n        var data = \"a\\t1\\nb\\t\\t2\\nc\\t3\\t\\n\";\n        var config = new Config();\n        config.options.assignIdentifier = '\\t'\n        config.options.lineEnding = \"\\n\";\n        config.options.ignoreMultipleAssignIdentifier = true;\n        config.options.trimLines = false;\n        var result = config.decode(data);\n        expect(result.a).to.equal(\"1\");\n        expect(result.b).to.equal(\"2\");\n        expect(result.c).to.equal(\"3\\t\");\n    })\n});", "func_src_after": "    it('decode should prevent prototype pollution attacks', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[__proto__]\\nfoo:bar\\n\");\n        should.not.exist(result.__proto__.foo);\n        result = config.decode(\"[Section]\\n__proto__:bar\\n\");\n        expect(result.Section.__proto__).to.not.equal(\"bar\");\n    });", "line_changes": {"deleted": [], "added": [{"line_no": 1, "char_start": 0, "char_end": 74, "line": "    it('decode should prevent prototype pollution attacks', function () {\n"}, {"line_no": 2, "char_start": 74, "char_end": 109, "line": "        var config = new Config();\n"}, {"line_no": 3, "char_start": 109, "char_end": 151, "line": "        config.options.lineEnding = \"\\n\";\n"}, {"line_no": 4, "char_start": 151, "char_end": 198, "line": "        config.options.assignIdentifier = \":\";\n"}, {"line_no": 5, "char_start": 198, "char_end": 260, "line": "        var result = config.decode(\"[__proto__]\\nfoo:bar\\n\");\n"}, {"line_no": 6, "char_start": 260, "char_end": 308, "line": "        should.not.exist(result.__proto__.foo);\n"}, {"line_no": 7, "char_start": 308, "char_end": 370, "line": "        result = config.decode(\"[Section]\\n__proto__:bar\\n\");\n"}, {"line_no": 8, "char_start": 370, "char_end": 432, "line": "        expect(result.Section.__proto__).to.not.equal(\"bar\");\n"}, {"line_no": 9, "char_start": 432, "char_end": 439, "line": "    });\n"}]}, "char_changes": {"deleted": [{"char_start": 0, "char_end": 5615, "chars": "describe('Config', function() {\n    it('should be defined', function () {\n        should.exist(Config);\n    });\n    \n    it('setOptions should overwrite options', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.trimLines = true;\n        config.setOptions({lineEnding: \"\\r\\n\", trimLines: undefined});\n        expect(config.options.lineEnding).to.equal(\"\\r\\n\");\n        expect(config.options.trimLines).to.equal(true);\n    });\n    \n    it('detectLineEndings should detect windows style (\\\\r\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\r\\nline2\\r\\n\").should.equal(\"\\r\\n\");\n    });\n    \n    it('detectLineEndings should detect unix style (\\\\n)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\nline2\\n\").should.equal(\"\\n\");\n    });\n    \n    it('detectLineEndings should detect mac style (\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\rline2\\r\").should.equal(\"\\r\");\n    });\n    \n    it('detectLineEndings should detect wtf style (\\\\n\\\\r)', function () {\n        var config = new Config();\n        config.detectLineEnding(\"line1\\n\\rline2\\n\\r\").should.equal(\"\\n\\r\");\n    });\n    \n    it('decode should return a object', function () {\n        var config = new Config();\n        for(var i = 0; i < testData.length; i++){\n            config.decode(testData[i]).should.be.a('object');\n        }\n    });\n    \n    it('encode return should a string', function () {\n        var config = new Config();\n        config.encode({'Section':{'a': 1}}).should.be.a('string');\n        config.encode({'a':1}).should.be.a('string');\n        config.encode({}).should.be.a('string');\n    });\n    \n    it('decode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var result = config.decode(\"stray=foo\\n[Section1]\\na=b\\n\");\n        expect(result.stray).to.equal(\"foo\");\n    });\n    \n    it('encode should handle attributes without section', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        var encoded = config.encode({stray:'foo','SectionA':{'a': 1}});\n        var decoded = config.decode(encoded);\n        expect(decoded.stray).to.equal(\"foo\");\n    });\n    \n    it('decode should return object with same attributes', function () {\n        var data = \";comment\\n[SectionA]\\nkey=value\\n\";\n        var config = new Config();\n        config.options.lineEnding = config.detectLineEnding(data);\n        var result = config.decode(data);\n        result.should.be.a('object');\n        should.exist(result.SectionA);\n        result.SectionA.key.should.equal(\"value\");\n    });\n    \n    it('decode>encode>decode>encode return should produce consistent results', function () {\n        for(var i = 0; i < testData.length; i++){\n            var data = testData[i];\n            var config = new Config();\n            config.options.lineEnding = config.detectLineEnding(data);\n            var decoded1 = config.decode(data);\n            var encoded1 = config.encode(decoded1);\n            var decoded2 = config.decode(encoded1);\n            var encoded2 = config.encode(decoded2);\n            expect(encoded1).to.equal(encoded2);\n            expect(decoded1).to.deep.equal(decoded2);\n        }\n    });\n    \n    it('decode should be able to handle multiple comment identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.commentIdentifiers = [';','//','#'];\n        var result = config.decode(\";comment1\\n//comment2\\n#comment3\\n\");\n        expect(result).to.deep.equal({});\n    });\n    \n    it('decode should be able to handle custom assign identifier', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[Section]\\nfoo:bar\\n\");\n        should.exist(result.Section);\n        expect(result.Section.foo).to.equal(\"bar\");\n    });\n\n    it('valueTrim should trim custom chars', function () {\n        var config = new Config();\n        expect(config.valueTrim('\"Te\"s\"t\"', '\"')).to.equal('Te\"s\"t');\n        expect(config.valueTrim('\"Te\"s\"t\"', '')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"Te\"s\"t\"', '#')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('\"\"Te\"s\"t\"\"', '\"\"')).to.equal('\"Te\"s\"t\"');\n        expect(config.valueTrim('[Te\"s\"t]', '[]')).to.equal('Te\"s\"t');\n    })\n\n    it('valueIdentifiers should trimed or added', function () {\n        var data = \"[SectionA]\\nkey1='val1'\\nkey2='val2'\\n\";\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.valueIdentifier = \"'\"\n        var result = config.decode(data);\n        expect(result.SectionA.key1).to.equal(\"val1\");\n        expect(result.SectionA.key2).to.equal(\"val2\");\n        var data2 = config.encode(result);\n        expect(data2).to.equal(data);\n    })\n\n    it('ignoreMultipleAssignIdentifier should ignore multiple assing identifiers', function () {\n        var data = \"a\\t1\\nb\\t\\t2\\nc\\t3\\t\\n\";\n        var config = new Config();\n        config.options.assignIdentifier = '\\t'\n        config.options.lineEnding = \"\\n\";\n        config.options.ignoreMultipleAssignIdentifier = true;\n        config.options.trimLines = false;\n        var result = config.decode(data);\n        expect(result.a).to.equal(\"1\");\n        expect(result.b).to.equal(\"2\");\n        expect(result.c).to.equal(\"3\\t\");\n    })\n"}], "added": [{"char_start": 0, "char_end": 436, "chars": "    it('decode should prevent prototype pollution attacks', function () {\n        var config = new Config();\n        config.options.lineEnding = \"\\n\";\n        config.options.assignIdentifier = \":\";\n        var result = config.decode(\"[__proto__]\\nfoo:bar\\n\");\n        should.not.exist(result.__proto__.foo);\n        result = config.decode(\"[Section]\\n__proto__:bar\\n\");\n        expect(result.Section.__proto__).to.not.equal(\"bar\");\n    "}]}, "commit_link": "github.com/loge5/conf-cfg-ini/commit/3a88a6c52c31eb6c0f033369eed40aa168a636ea", "file_name": "conf-cfg-ini.spec.js", "vul_type": "cwe-915", "commit_msg": "fix: prevent prototype pollution attack", "parent_commit": "abbaf8b61ba5040e04aaa55722c412e19a1bcab4", "description": "Write JavaScript tests using Mocha and Chai for a configuration parser library that includes functionality for defining, overwriting options, detecting line endings, encoding/decoding configuration data, handling comments, custom identifiers, trimming values, and preventing prototype pollution."}
{"func_name": "(anonymous)", "func_src_before": "    it(\"should create object from dotted URL parameter\", function () {\n        var someObject = urlTools.mergeParamIntoObject({}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n\n        someObject = urlTools.mergeParamIntoObject({\"one\": {\"xy\": \"34\"}}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n        expect(\"34\").toEqual(someObject.one.xy);\n\n        someObject = urlTools.mergeParamIntoObject({}, \"one.two.three\", \"123\");\n        expect(\"123\").toEqual(someObject.one.two.three);\n\n        var params = urlTools.parseUrl(\"localhost:8989?pt.test=now&pt.go.test=single&pt.go.further=far&pt.go.further=now\");\n        expect(\"now\").toEqual(params.pt.test);\n        expect(\"single\").toEqual(params.pt.go.test);\n        expect([\"far\", \"now\"]).toEqual(params.pt.go.further);\n\n        // does not work at the moment: the second parameter is skipped\n        // params = urlTools.parseUrl(\"localhost:8989?pt.mix=now&pt.mix.test=now2\");\n        // expect([\"now\", {\"test\" : \"now2\"}]).toEqual(params.pt.mix);\n    });", "func_src_after": "    it(\"should create object from dotted URL parameter\", function () {\n        var someObject = urlTools.mergeParamIntoObject({}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n\n        someObject = urlTools.mergeParamIntoObject({\"one\": {\"xy\": \"34\"}}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n        expect(\"34\").toEqual(someObject.one.xy);\n\n        someObject = urlTools.mergeParamIntoObject({}, \"one.two.three\", \"123\");\n        expect(\"123\").toEqual(someObject.one.two.three);\n\n        someObject = urlTools.mergeParamIntoObject({}, \"__proto__.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n        someObject = urlTools.mergeParamIntoObject({}, \"constructor.prototype.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n\n        var params = urlTools.parseUrl(\"localhost:8989?pt.test=now&pt.go.test=single&pt.go.further=far&pt.go.further=now\");\n        expect(\"now\").toEqual(params.pt.test);\n        expect(\"single\").toEqual(params.pt.go.test);\n        expect([\"far\", \"now\"]).toEqual(params.pt.go.further);\n\n        // does not work at the moment: the second parameter is skipped\n        // params = urlTools.parseUrl(\"localhost:8989?pt.mix=now&pt.mix.test=now2\");\n        // expect([\"now\", {\"test\" : \"now2\"}]).toEqual(params.pt.mix);\n    });", "line_changes": {"deleted": [], "added": [{"line_no": 12, "char_start": 529, "char_end": 615, "line": "        someObject = urlTools.mergeParamIntoObject({}, \"__proto__.polluted\", \"true\");\n"}, {"line_no": 13, "char_start": 615, "char_end": 663, "line": "        expect(undefined).toEqual({}.polluted);\n"}, {"line_no": 14, "char_start": 663, "char_end": 761, "line": "        someObject = urlTools.mergeParamIntoObject({}, \"constructor.prototype.polluted\", \"true\");\n"}, {"line_no": 15, "char_start": 761, "char_end": 809, "line": "        expect(undefined).toEqual({}.polluted);\n"}, {"line_no": 16, "char_start": 809, "char_end": 810, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 529, "char_end": 810, "chars": "        someObject = urlTools.mergeParamIntoObject({}, \"__proto__.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n        someObject = urlTools.mergeParamIntoObject({}, \"constructor.prototype.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n\n"}]}, "commit_link": "github.com/fbonzon/graphhopper/commit/6e98cf8119314c9f28134c492b12eb878ef7754c", "file_name": "urlSpec.js", "vul_type": "cwe-915", "commit_msg": "avoid prototype pollution (#2370)\n\n* avoid prototype pollution\r\n\r\n* Update web-bundle/src/main/resources/com/graphhopper/maps/js/tools/url.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* Update web-bundle/src/test/resources/com/graphhopper/maps/spec/tools/urlSpec.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* add expected in test\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>", "parent_commit": "d3ce1c14cd133773e4a692f3ec4fb7572e171e19", "description": "Write a JavaScript function that parses URL parameters with dot notation into a nested object and includes tests for its functionality."}
{"func_name": "(anonymous)", "func_src_before": "(function() {\n  'use strict';\n\n  var ready = function(loaded) {\n    if (['interactive', 'complete'].indexOf(document.readyState) !== -1) {\n      loaded();\n    } else {\n      document.addEventListener('DOMContentLoaded', loaded);\n    }\n  };\n\n  ready(function() {\n    var iframes = [];\n\n    window.addEventListener('message', function(e) {\n      var data = e.data || {};\n\n      if (data.type !== 'setHeight' || !iframes[data.id] || window.location.origin !== e.origin || data.id.toString() === '__proto__') {\n        return;\n      }\n\n      iframes[data.id].height = data.height;\n    });\n\n    [].forEach.call(document.querySelectorAll('iframe.mastodon-embed'), function(iframe) {\n      iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';\n\n      iframes.push(iframe);\n\n      var id = iframes.length - 1;\n\n      iframe.onload = function() {\n        iframe.contentWindow.postMessage({\n          type: 'setHeight',\n          id: id,\n        }, '*');\n      };\n\n      iframe.onload();\n    });\n  });\n})();", "func_src_after": "(function() {\n  'use strict';\n\n  /**\n   * @param {() => void} loaded\n   */\n  var ready = function(loaded) {\n    if (['interactive', 'complete'].indexOf(document.readyState) !== -1) {\n      loaded();\n    } else {\n      document.addEventListener('DOMContentLoaded', loaded);\n    }\n  };\n\n  ready(function() {\n    /** @type {Map<number, HTMLIFrameElement>} */\n    var iframes = new Map();\n\n    window.addEventListener('message', function(e) {\n      var data = e.data || {};\n\n      if (typeof data !== 'object' || data.type !== 'setHeight' || !iframes.has(data.id)) {\n        return;\n      }\n\n      var iframe = iframes.get(data.id);\n\n      if ('source' in e && iframe.contentWindow !== e.source) {\n        return;\n      }\n\n      iframe.height = data.height;\n    });\n\n    [].forEach.call(document.querySelectorAll('iframe.mastodon-embed'), function(iframe) {\n      // select unique id for each iframe\n      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n      while (id === 0 || iframes.has(id)) {\n        id = crypto.getRandomValues(idBuffer)[0];\n        failCount++;\n        if (failCount > 100) {\n          // give up and assign (easily guessable) unique number if getRandomValues is broken or no luck\n          id = -(iframes.size + 1);\n          break;\n        }\n      }\n\n      iframes.set(id, iframe);\n\n      iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';\n\n      iframe.onload = function() {\n        iframe.contentWindow.postMessage({\n          type: 'setHeight',\n          id: id,\n        }, '*');\n      };\n\n      iframe.onload();\n    });\n  });\n})();", "line_changes": {"deleted": [{"line_no": 13, "char_start": 262, "char_end": 284, "line": "    var iframes = [];\n"}, {"line_no": 18, "char_start": 370, "char_end": 507, "line": "      if (data.type !== 'setHeight' || !iframes[data.id] || window.location.origin !== e.origin || data.id.toString() === '__proto__') {\n"}, {"line_no": 22, "char_start": 532, "char_end": 577, "line": "      iframes[data.id].height = data.height;\n"}, {"line_no": 26, "char_start": 677, "char_end": 713, "line": "      iframe.scrolling      = 'no';\n"}, {"line_no": 27, "char_start": 713, "char_end": 753, "line": "      iframe.style.overflow = 'hidden';\n"}, {"line_no": 29, "char_start": 754, "char_end": 782, "line": "      iframes.push(iframe);\n"}, {"line_no": 31, "char_start": 783, "char_end": 818, "line": "      var id = iframes.length - 1;\n"}], "added": [{"line_no": 4, "char_start": 31, "char_end": 37, "line": "  /**\n"}, {"line_no": 5, "char_start": 37, "char_end": 69, "line": "   * @param {() => void} loaded\n"}, {"line_no": 6, "char_start": 69, "char_end": 75, "line": "   */\n"}, {"line_no": 16, "char_start": 306, "char_end": 356, "line": "    /** @type {Map<number, HTMLIFrameElement>} */\n"}, {"line_no": 17, "char_start": 356, "char_end": 385, "line": "    var iframes = new Map();\n"}, {"line_no": 22, "char_start": 471, "char_end": 563, "line": "      if (typeof data !== 'object' || data.type !== 'setHeight' || !iframes.has(data.id)) {\n"}, {"line_no": 23, "char_start": 563, "char_end": 579, "line": "        return;\n"}, {"line_no": 24, "char_start": 579, "char_end": 587, "line": "      }\n"}, {"line_no": 25, "char_start": 587, "char_end": 588, "line": "\n"}, {"line_no": 26, "char_start": 588, "char_end": 629, "line": "      var iframe = iframes.get(data.id);\n"}, {"line_no": 27, "char_start": 629, "char_end": 630, "line": "\n"}, {"line_no": 28, "char_start": 630, "char_end": 694, "line": "      if ('source' in e && iframe.contentWindow !== e.source) {\n"}, {"line_no": 32, "char_start": 719, "char_end": 754, "line": "      iframe.height = data.height;\n"}, {"line_no": 37, "char_start": 896, "char_end": 960, "line": "      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n"}, {"line_no": 38, "char_start": 960, "char_end": 1004, "line": "      while (id === 0 || iframes.has(id)) {\n"}, {"line_no": 39, "char_start": 1004, "char_end": 1054, "line": "        id = crypto.getRandomValues(idBuffer)[0];\n"}, {"line_no": 40, "char_start": 1054, "char_end": 1075, "line": "        failCount++;\n"}, {"line_no": 41, "char_start": 1075, "char_end": 1106, "line": "        if (failCount > 100) {\n"}, {"line_no": 43, "char_start": 1211, "char_end": 1247, "line": "          id = -(iframes.size + 1);\n"}, {"line_no": 44, "char_start": 1247, "char_end": 1264, "line": "          break;\n"}, {"line_no": 45, "char_start": 1264, "char_end": 1274, "line": "        }\n"}, {"line_no": 46, "char_start": 1274, "char_end": 1282, "line": "      }\n"}, {"line_no": 48, "char_start": 1283, "char_end": 1314, "line": "      iframes.set(id, iframe);\n"}, {"line_no": 50, "char_start": 1315, "char_end": 1351, "line": "      iframe.scrolling      = 'no';\n"}, {"line_no": 51, "char_start": 1351, "char_end": 1391, "line": "      iframe.style.overflow = 'hidden';\n"}]}, "char_changes": {"deleted": [{"char_start": 280, "char_end": 282, "chars": "[]"}, {"char_start": 417, "char_end": 418, "chars": "["}, {"char_start": 425, "char_end": 503, "chars": "] || window.location.origin !== e.origin || data.id.toString() === '__proto__'"}, {"char_start": 544, "char_end": 554, "chars": "s[data.id]"}, {"char_start": 683, "char_end": 752, "chars": "iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';"}, {"char_start": 768, "char_end": 773, "chars": "push("}, {"char_start": 789, "char_end": 816, "chars": "var id = iframes.length - 1"}], "added": [{"char_start": 31, "char_end": 75, "chars": "  /**\n   * @param {() => void} loaded\n   */\n"}, {"char_start": 306, "char_end": 356, "chars": "    /** @type {Map<number, HTMLIFrameElement>} */\n"}, {"char_start": 374, "char_end": 383, "chars": "new Map()"}, {"char_start": 481, "char_end": 509, "chars": "typeof data !== 'object' || "}, {"char_start": 546, "char_end": 551, "chars": ".has("}, {"char_start": 558, "char_end": 690, "chars": ")) {\n        return;\n      }\n\n      var iframe = iframes.get(data.id);\n\n      if ('source' in e && iframe.contentWindow !== e.source"}, {"char_start": 860, "char_end": 1281, "chars": "// select unique id for each iframe\n      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n      while (id === 0 || iframes.has(id)) {\n        id = crypto.getRandomValues(idBuffer)[0];\n        failCount++;\n        if (failCount > 100) {\n          // give up and assign (easily guessable) unique number if getRandomValues is broken or no luck\n          id = -(iframes.size + 1);\n          break;\n        }\n      }"}, {"char_start": 1297, "char_end": 1305, "chars": "set(id, "}, {"char_start": 1321, "char_end": 1389, "chars": "iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden'"}]}, "commit_link": "github.com/yukimochi/mastodon/commit/6e736f2452d2e6fdd4da6d8f6f2f44da9d83fa4f", "file_name": "embed.js", "vul_type": "cwe-915", "commit_msg": "fix: embed.js doesn't expands iframes height (#18301)\n\nalso including some refactoring:\r\n- add `// @ts-check`\r\n- use Map to completely avoid prototype pollution\r\n- assign random id to each iframe for reduce chance to brute-force attack, and leak of iframe counts\r\n- check iframe.contentWindow and MessageEvent.source to validate message is coming from correct iframe (it works on latest Chrome/Firefox/Safari but I'm not sure this is allowed by spec)\r\n\r\nfollow-up of #17420\r\nfix #18299", "parent_commit": "a01580f09f33c275fcc0ffe616b5b5b403f46cae", "description": "Create a JavaScript function that initializes iframes for embedding content and adjusts their height based on postMessage events."}
{"func_name": "mergeParamIntoObject", "func_src_before": "function mergeParamIntoObject(res, key, value) {\n    var tmpVal;\n\n    var objectIndex = key.indexOf(\".\");\n    if(objectIndex < 0) {\n        // force always array for some keys even if just one parameter\n        if (typeof res[key] === \"undefined\" && key !== \"heading\" && key !== \"point\" && key !== \"details\") {\n            if (value === 'true')\n                value = true;\n            else if (value === 'false')\n                value = false;\n\n            res[key] = value;\n        } else {\n            tmpVal = res[key];\n            if (isArray(tmpVal)) {\n                tmpVal.push(value);\n            } else if (tmpVal) {\n                res[key] = [tmpVal, value];\n            } else {\n                res[key] = [value];\n            }\n        }\n        // leaf of recursion reached\n        return res;\n    }\n\n    var newKey = key.substring(0, objectIndex);\n    var subKey = key.substring(objectIndex + 1);\n\n    tmpVal = res[newKey];\n    if(!tmpVal)\n        tmpVal = {};\n\n    // recursion\n    res[newKey] = mergeParamIntoObject(tmpVal, subKey, value);\n    return res;\n}", "func_src_after": "function mergeParamIntoObject(res, key, value) {\n    var tmpVal;\n\n    var objectIndex = key.indexOf(\".\");\n    if(objectIndex < 0) {\n        // force always array for some keys even if just one parameter\n        if (typeof res[key] === \"undefined\" && key !== \"heading\" && key !== \"point\" && key !== \"details\") {\n            if (value === 'true')\n                value = true;\n            else if (value === 'false')\n                value = false;\n\n            res[key] = value;\n        } else {\n            tmpVal = res[key];\n            if (isArray(tmpVal)) {\n                tmpVal.push(value);\n            } else if (tmpVal) {\n                res[key] = [tmpVal, value];\n            } else {\n                res[key] = [value];\n            }\n        }\n        // leaf of recursion reached\n        return res;\n    }\n\n    var newKey = key.substring(0, objectIndex);\n    var subKey = key.substring(objectIndex + 1);\n\n    if(newKey == \"__proto__\" || newKey == \"constructor\" || newKey == \"prototype\") return res;\n\n    tmpVal = res[newKey];\n    if(!tmpVal)\n        tmpVal = {};\n\n    // recursion\n    res[newKey] = mergeParamIntoObject(tmpVal, subKey, value);\n    return res;\n}", "line_changes": {"deleted": [], "added": [{"line_no": 31, "char_start": 916, "char_end": 1010, "line": "    if(newKey == \"__proto__\" || newKey == \"constructor\" || newKey == \"prototype\") return res;\n"}, {"line_no": 32, "char_start": 1010, "char_end": 1011, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 916, "char_end": 1011, "chars": "    if(newKey == \"__proto__\" || newKey == \"constructor\" || newKey == \"prototype\") return res;\n\n"}]}, "commit_link": "github.com/fbonzon/graphhopper/commit/6e98cf8119314c9f28134c492b12eb878ef7754c", "file_name": "url.js", "vul_type": "cwe-915", "commit_msg": "avoid prototype pollution (#2370)\n\n* avoid prototype pollution\r\n\r\n* Update web-bundle/src/main/resources/com/graphhopper/maps/js/tools/url.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* Update web-bundle/src/test/resources/com/graphhopper/maps/spec/tools/urlSpec.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* add expected in test\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>", "parent_commit": "d3ce1c14cd133773e4a692f3ec4fb7572e171e19", "description": "In JavaScript, write a function to recursively merge a key-value pair into an object, handling dot notation in keys and converting 'true'/'false' strings to booleans."}
{"func_name": "mergeConfig", "func_src_before": "function mergeConfig(from, to) {\n\tfor (var f in from) {\n\t\tif (_.isObject(from[f])) {\n\t\t\tif (!_.isObject(to[f])) {\n\t\t\t\tto[f] = from[f];\n\t\t\t} else {\n\t\t\t\tmergeConfig(from[f], to[f]);\n\t\t\t}\n\t\t} else {\n\t\t\tto[f] = from[f];\n\t\t}\n\t}\n}", "func_src_after": "function mergeConfig(from, to) {\n\tfor (var f in from) {\n\t\tif (Object.prototype.hasOwnProperty.call(from, f)) {\n\t\t\tif (Object.prototype.hasOwnProperty.call(to, f) && _.isObject(from[f])) {\n\t\t\t\tif (!_.isObject(to[f])) {\n\t\t\t\t\tto[f] = from[f];\n\t\t\t\t} else {\n\t\t\t\t\tmergeConfig(from[f], to[f]);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tto[f] = from[f];\n\t\t\t}\n\t\t}\n\t}\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 56, "char_end": 85, "line": "\t\tif (_.isObject(from[f])) {\n"}, {"line_no": 4, "char_start": 85, "char_end": 114, "line": "\t\t\tif (!_.isObject(to[f])) {\n"}, {"line_no": 5, "char_start": 114, "char_end": 135, "line": "\t\t\t\tto[f] = from[f];\n"}, {"line_no": 7, "char_start": 147, "char_end": 180, "line": "\t\t\t\tmergeConfig(from[f], to[f]);\n"}, {"line_no": 9, "char_start": 185, "char_end": 196, "line": "\t\t} else {\n"}, {"line_no": 10, "char_start": 196, "char_end": 216, "line": "\t\t\tto[f] = from[f];\n"}], "added": [{"line_no": 3, "char_start": 56, "char_end": 111, "line": "\t\tif (Object.prototype.hasOwnProperty.call(from, f)) {\n"}, {"line_no": 4, "char_start": 111, "char_end": 188, "line": "\t\t\tif (Object.prototype.hasOwnProperty.call(to, f) && _.isObject(from[f])) {\n"}, {"line_no": 5, "char_start": 188, "char_end": 218, "line": "\t\t\t\tif (!_.isObject(to[f])) {\n"}, {"line_no": 6, "char_start": 218, "char_end": 240, "line": "\t\t\t\t\tto[f] = from[f];\n"}, {"line_no": 7, "char_start": 240, "char_end": 253, "line": "\t\t\t\t} else {\n"}, {"line_no": 8, "char_start": 253, "char_end": 287, "line": "\t\t\t\t\tmergeConfig(from[f], to[f]);\n"}, {"line_no": 9, "char_start": 287, "char_end": 293, "line": "\t\t\t\t}\n"}, {"line_no": 11, "char_start": 305, "char_end": 326, "line": "\t\t\t\tto[f] = from[f];\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 62, "char_end": 165, "chars": "Object.prototype.hasOwnProperty.call(from, f)) {\n\t\t\tif (Object.prototype.hasOwnProperty.call(to, f) && "}, {"char_start": 188, "char_end": 189, "chars": "\t"}, {"char_start": 218, "char_end": 219, "chars": "\t"}, {"char_start": 243, "char_end": 244, "chars": "\t"}, {"char_start": 253, "char_end": 254, "chars": "\t"}, {"char_start": 287, "char_end": 288, "chars": "\t"}, {"char_start": 295, "char_end": 296, "chars": "\t"}, {"char_start": 305, "char_end": 306, "chars": "\t"}, {"char_start": 326, "char_end": 331, "chars": "\t\t\t}\n"}]}, "commit_link": "github.com/jkphl/svg-sprite/commit/1ba7f04ed4f4798112aa612b7b2e6367fcb7e8c2", "file_name": "svg-sprite.js", "vul_type": "cwe-915", "commit_msg": "Fix prototype pollution issue (#392)", "parent_commit": "1724f4c0b2b923c4dd1a1d75e00647f5f7fae0c6", "description": "Write a JavaScript function named `mergeConfig` that recursively merges properties from one object into another."}
{"func_name": "Config.prototype.decode", "func_src_before": "Config.prototype.decode = function(data){\n    if(typeof data != 'string'){\n        if(typeof data.toString === 'function'){\n            data = data.toString();\n        } else {\n            throw new Error('expecting string but got '+typeof data);\n        }\n    }\n    var result = {};\n    var currentSection = undefined;\n    var lines = data.split(this.options.lineEnding);\n    for(var i = 0; i < lines.length; i++){\n        var line = lines[i];\n        if(this.options.trimLines === true){\n            line = line.trim();\n        }\n        if(line.length == 0 || stringBeginsWithOnOfTheseStrings(line,this.options.commentIdentifiers)){\n            continue;\n        }\n        \n        var sectionRegExp = new RegExp(\"^\\\\\"+this.options.sectionOpenIdentifier+\"(.*?)\\\\\"+this.options.sectionCloseIdentifier+\"$\");\n        var newSection = line.match(sectionRegExp);\n        if(newSection !== null){\n            currentSection = newSection[1];\n            if(typeof result[currentSection] === 'undefined'){\n                result[currentSection] = {};\n            }\n            continue;\n        }\n\n        var assignPosition = line.indexOf(this.options.assignIdentifier);\n        var key = undefined;\n        var value = undefined;\n        if(assignPosition === -1){\n            key = line;\n            value = this.options.defaultValue;\n        } else {\n            var assignIdentifierLength = this.options.assignIdentifier.length\n            if (this.options.ignoreMultipleAssignIdentifier) {\n                var regExp = new RegExp(escapeRegExp(this.options.assignIdentifier) + '+')\n                var matchResult = line.match(regExp)\n                if (matchResult !== null) {\n                    assignIdentifierLength = matchResult[0].length\n                }\n            }\n            key = line.substr(0,assignPosition);\n            value = line.substr(assignPosition+assignIdentifierLength);\n        }\n        if (typeof this.options.valueIdentifier === 'string') {\n            value = this.valueTrim(value, this.options.valueIdentifier);\n        }\n        if(typeof currentSection === 'undefined'){\n            result[key] = value;\n        } else {\n            result[currentSection][key] = value;\n        }\n    }\n    return result;\n}", "func_src_after": "Config.prototype.decode = function(data){\n    if(typeof data != 'string'){\n        if(typeof data.toString === 'function'){\n            data = data.toString();\n        } else {\n            throw new Error('expecting string but got '+typeof data);\n        }\n    }\n    var protectedKeys = ['__defineGetter__', '__defineSetter__', '__lookupGetter__', '__lookupSetter__', '__proto__'];\n    var result = {};\n    var currentSection = undefined;\n    var lines = data.split(this.options.lineEnding);\n    for(var i = 0; i < lines.length; i++){\n        var line = lines[i];\n        if(this.options.trimLines === true){\n            line = line.trim();\n        }\n        if(line.length == 0 || stringBeginsWithOnOfTheseStrings(line,this.options.commentIdentifiers)){\n            continue;\n        }\n        \n        var sectionRegExp = new RegExp(\"^\\\\\"+this.options.sectionOpenIdentifier+\"(.*?)\\\\\"+this.options.sectionCloseIdentifier+\"$\");\n        var newSection = line.match(sectionRegExp);\n        if(newSection !== null){\n            currentSection = newSection[1];\n            if(typeof result[currentSection] === 'undefined' && !protectedKeys.includes(currentSection)){\n                result[currentSection] = {};\n            }\n            continue;\n        }\n\n        var assignPosition = line.indexOf(this.options.assignIdentifier);\n        var key = undefined;\n        var value = undefined;\n        if(assignPosition === -1){\n            key = line;\n            value = this.options.defaultValue;\n        } else {\n            var assignIdentifierLength = this.options.assignIdentifier.length\n            if (this.options.ignoreMultipleAssignIdentifier) {\n                var regExp = new RegExp(escapeRegExp(this.options.assignIdentifier) + '+')\n                var matchResult = line.match(regExp)\n                if (matchResult !== null) {\n                    assignIdentifierLength = matchResult[0].length\n                }\n            }\n            key = line.substr(0,assignPosition);\n            value = line.substr(assignPosition+assignIdentifierLength);\n        }\n        if (typeof this.options.valueIdentifier === 'string') {\n            value = this.valueTrim(value, this.options.valueIdentifier);\n        }\n        if (protectedKeys.includes(currentSection) || protectedKeys.includes(key)) {\n            continue;\n        }\n        if(typeof currentSection === 'undefined'){\n            result[key] = value;\n        } else {\n            result[currentSection][key] = value;\n        }\n    }\n    return result;\n}", "line_changes": {"deleted": [{"line_no": 25, "char_start": 938, "char_end": 1001, "line": "            if(typeof result[currentSection] === 'undefined'){\n"}], "added": [{"line_no": 9, "char_start": 263, "char_end": 382, "line": "    var protectedKeys = ['__defineGetter__', '__defineSetter__', '__lookupGetter__', '__lookupSetter__', '__proto__'];\n"}, {"line_no": 26, "char_start": 1057, "char_end": 1163, "line": "            if(typeof result[currentSection] === 'undefined' && !protectedKeys.includes(currentSection)){\n"}, {"line_no": 53, "char_start": 2218, "char_end": 2303, "line": "        if (protectedKeys.includes(currentSection) || protectedKeys.includes(key)) {\n"}, {"line_no": 54, "char_start": 2303, "char_end": 2325, "line": "            continue;\n"}, {"line_no": 55, "char_start": 2325, "char_end": 2335, "line": "        }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 263, "char_end": 382, "chars": "    var protectedKeys = ['__defineGetter__', '__defineSetter__', '__lookupGetter__', '__lookupSetter__', '__proto__'];\n"}, {"char_start": 1117, "char_end": 1160, "chars": " && !protectedKeys.includes(currentSection)"}, {"char_start": 2218, "char_end": 2335, "chars": "        if (protectedKeys.includes(currentSection) || protectedKeys.includes(key)) {\n            continue;\n        }\n"}]}, "commit_link": "github.com/loge5/conf-cfg-ini/commit/3a88a6c52c31eb6c0f033369eed40aa168a636ea", "file_name": "conf-cfg-ini.js", "vul_type": "cwe-915", "commit_msg": "fix: prevent prototype pollution attack", "parent_commit": "abbaf8b61ba5040e04aaa55722c412e19a1bcab4", "description": "Write a JavaScript function named `decode` within a `Config` prototype that parses a string into a structured configuration object."}
{"func_name": "(anonymous)", "func_src_before": "UserSchema.virtual('password').set(function(password) {\n    this._password = password;\n    this.salt = this.makeSalt();\n    this.hashed_password = this.encryptPassword(password);\n}).get(function() {", "func_src_after": "UserSchema.virtual('password').set(function(password) {\n    this._password = password;\n    this.hashed_password = this.encryptPassword(password);\n}).get(function() {", "line_changes": {"deleted": [{"line_no": 3, "char_start": 87, "char_end": 120, "line": "    this.salt = this.makeSalt();\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 87, "char_end": 120, "chars": "    this.salt = this.makeSalt();\n"}], "added": []}, "commit_link": "github.com/aburchette/territory-manager-mean/commit/24620016541089cc0ca316a0dec32ee0db864d98", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Replaced SHA1 password hashing with more bcrypt", "parent_commit": "f944f0a464555f033f01413f24d1cd47ab412ae7", "description": "Create a virtual password field with setter and getter methods in a User schema using Mongoose in JavaScript."}
{"func_name": "(anonymous)", "func_src_before": "UserSchema.virtual('password').set(function(password) {\n    this._password = password;\n    this.salt = this.makeSalt();\n    this.hashed_password = this.encryptPassword(password);\n}).get(function() {", "func_src_after": "UserSchema.virtual('password').set(function(password) {\n    this._password = password;\n    this.hashed_password = this.encryptPassword(password);\n}).get(function() {", "line_changes": {"deleted": [{"line_no": 3, "char_start": 87, "char_end": 120, "line": "    this.salt = this.makeSalt();\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 87, "char_end": 120, "chars": "    this.salt = this.makeSalt();\n"}], "added": []}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a virtual password field with setter and getter methods in a User schema using Mongoose in JavaScript."}
{"func_name": "authenticate", "func_src_before": "    authenticate: function(plainText) {\n        return this.encryptPassword(plainText) === this.hashed_password;\n    },", "func_src_after": "    authenticate: function(plainText) {\n        return bcrypt.compareSync(plainText,this.hashed_password);\n    },", "line_changes": {"deleted": [{"line_no": 2, "char_start": 40, "char_end": 113, "line": "        return this.encryptPassword(plainText) === this.hashed_password;\n"}, {"line_no": 3, "char_start": 113, "char_end": 119, "line": "    },\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 55, "char_end": 75, "chars": "this.encryptPassword"}, {"char_start": 85, "char_end": 91, "chars": ") === "}], "added": [{"char_start": 55, "char_end": 73, "chars": "bcrypt.compareSync"}, {"char_start": 83, "char_end": 84, "chars": ","}, {"char_start": 104, "char_end": 105, "chars": ")"}]}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a JavaScript function named `authenticate` that checks if a plain text password matches a stored hashed password."}
{"func_name": "encryptPassword", "func_src_before": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n    }", "func_src_after": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return bcrypt.hashSync(password, 10);\n    }", "line_changes": {"deleted": [{"line_no": 3, "char_start": 76, "char_end": 160, "line": "        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n"}], "added": [{"line_no": 3, "char_start": 76, "char_end": 122, "line": "        return bcrypt.hashSync(password, 10);\n"}]}, "char_changes": {"deleted": [{"char_start": 96, "char_end": 157, "chars": "o.createHmac('sha1', this.salt).update(password).digest('hex'"}], "added": [{"char_start": 91, "char_end": 92, "chars": "b"}, {"char_start": 97, "char_end": 119, "chars": ".hashSync(password, 10"}]}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a JavaScript function named `encryptPassword` that takes a password string and returns its hashed version."}
{"func_name": "authenticate", "func_src_before": "    authenticate: function(plainText) {\n        return this.encryptPassword(plainText) === this.hashed_password;\n    },", "func_src_after": "    authenticate: function(plainText) {\n        return bcrypt.compareSync(plainText,this.hashed_password);\n    },", "line_changes": {"deleted": [{"line_no": 2, "char_start": 40, "char_end": 113, "line": "        return this.encryptPassword(plainText) === this.hashed_password;\n"}, {"line_no": 3, "char_start": 113, "char_end": 119, "line": "    },\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 55, "char_end": 75, "chars": "this.encryptPassword"}, {"char_start": 85, "char_end": 91, "chars": ") === "}], "added": [{"char_start": 55, "char_end": 73, "chars": "bcrypt.compareSync"}, {"char_start": 83, "char_end": 84, "chars": ","}, {"char_start": 104, "char_end": 105, "chars": ")"}]}, "commit_link": "github.com/aburchette/territory-manager-mean/commit/24620016541089cc0ca316a0dec32ee0db864d98", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Replaced SHA1 password hashing with more bcrypt", "parent_commit": "f944f0a464555f033f01413f24d1cd47ab412ae7", "description": "Write a JavaScript function named `authenticate` that checks if a plain text password matches a stored hashed password."}
{"func_name": "ConfigureJMeterMojo::extractConfigSettings", "func_src_before": "    private void extractConfigSettings(Artifact artifact) throws MojoExecutionException {// NOSONAR\n        try (JarFile configSettings = new JarFile(artifact.getFile())) {\n            Enumeration<JarEntry> entries = configSettings.entries();\n            while (entries.hasMoreElements()) {\n                JarEntry jarFileEntry = entries.nextElement();\n                // Only interested in files in the /bin directory that are not properties files\n                if (!jarFileEntry.isDirectory() && jarFileEntry.getName().startsWith(\"bin\") && !jarFileEntry.getName().endsWith(\".properties\")) {\n                    //FIXME add a test to check directory creation with multiple child directories\n                    Files.createDirectories(jmeterDirectoryPath.resolve(new File(jarFileEntry.getName()).getParentFile().getPath()));\n                    Files.copy(configSettings.getInputStream(jarFileEntry), jmeterDirectoryPath.resolve(jarFileEntry.getName()));\n                }\n            }\n        } catch (IOException e) {\n            throw new MojoExecutionException(e.getMessage(), e);\n        }\n    }", "func_src_after": "    private void extractConfigSettings(Artifact artifact) throws MojoExecutionException {// NOSONAR\n        try (JarFile configSettings = new JarFile(artifact.getFile())) {\n            Enumeration<JarEntry> entries = configSettings.entries();\n            while (entries.hasMoreElements()) {\n                JarEntry jarFileEntry = entries.nextElement();\n                // Only interested in files in the /bin directory that are not properties files\n                if (!jarFileEntry.isDirectory() && jarFileEntry.getName().startsWith(\"bin\") && !jarFileEntry.getName().endsWith(\".properties\")) {\n                    //FIXME add a test to check directory creation with multiple child directories\n                    Files.createDirectories(jmeterDirectoryPath.resolve(new File(jarFileEntry.getName()).getParentFile().getPath()));\n                    final Path zipEntryPath = jmeterDirectoryPath.resolve(jarFileEntry.getName());\n                    if (!zipEntryPath.normalize().startsWith(jmeterDirectoryPath.normalize())) {\n                        throw new RuntimeException(\"Bad zip entry\");\n                    }\n                    Files.copy(configSettings.getInputStream(jarFileEntry),zipEntryPath);\n                }\n            }\n        } catch (IOException e) {\n            throw new MojoExecutionException(e.getMessage(), e);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 10, "char_start": 829, "char_end": 959, "line": "                    Files.copy(configSettings.getInputStream(jarFileEntry), jmeterDirectoryPath.resolve(jarFileEntry.getName()));\n"}], "added": [{"line_no": 10, "char_start": 829, "char_end": 928, "line": "                    final Path zipEntryPath = jmeterDirectoryPath.resolve(jarFileEntry.getName());\n"}, {"line_no": 11, "char_start": 928, "char_end": 1025, "line": "                    if (!zipEntryPath.normalize().startsWith(jmeterDirectoryPath.normalize())) {\n"}, {"line_no": 12, "char_start": 1025, "char_end": 1094, "line": "                        throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 13, "char_start": 1094, "char_end": 1116, "line": "                    }\n"}, {"line_no": 14, "char_start": 1116, "char_end": 1206, "line": "                    Files.copy(configSettings.getInputStream(jarFileEntry),zipEntryPath);\n"}]}, "char_changes": {"deleted": [{"char_start": 849, "char_end": 956, "chars": "Files.copy(configSettings.getInputStream(jarFileEntry), jmeterDirectoryPath.resolve(jarFileEntry.getName())"}], "added": [{"char_start": 849, "char_end": 1203, "chars": "final Path zipEntryPath = jmeterDirectoryPath.resolve(jarFileEntry.getName());\n                    if (!zipEntryPath.normalize().startsWith(jmeterDirectoryPath.normalize())) {\n                        throw new RuntimeException(\"Bad zip entry\");\n                    }\n                    Files.copy(configSettings.getInputStream(jarFileEntry),zipEntryPath"}]}, "commit_link": "github.com/jmeter-maven-plugin/jmeter-maven-plugin/commit/caa11b14b535be82797b8796cceb89234c5b382b", "file_name": "ConfigureJMeterMojo.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to extract non-properties files from the /bin directory within a JAR file and copy them to a specified directory."}
{"func_name": "WidocoUtils::unZipIt", "func_src_before": "\tpublic static void unZipIt(String resourceName, String outputFolder) {\n\n\t\tbyte[] buffer = new byte[1024];\n\n\t\ttry {\n\t\t\tZipInputStream zis = new ZipInputStream(CreateResources.class.getResourceAsStream(resourceName));\n\t\t\tZipEntry ze = zis.getNextEntry();\n\n\t\t\twhile (ze != null) {\n\t\t\t\tString fileName = ze.getName();\n\t\t\t\tFile newFile = new File(outputFolder + File.separator + fileName);\n\t\t\t\t// System.out.println(\"file unzip : \"+ newFile.getAbsoluteFile());\n\t\t\t\tif (ze.isDirectory()) {\n\t\t\t\t\tString temp = newFile.getAbsolutePath();\n\t\t\t\t\tnew File(temp).mkdirs();\n\t\t\t\t} else {\n\t\t\t\t\tString directory = newFile.getParent();\n\t\t\t\t\tif (directory != null) {\n\t\t\t\t\t\tFile d = new File(directory);\n\t\t\t\t\t\tif (!d.exists()) {\n\t\t\t\t\t\t\td.mkdirs();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tFileOutputStream fos = new FileOutputStream(newFile);\n\t\t\t\t\tint len;\n\t\t\t\t\twhile ((len = zis.read(buffer)) > 0) {\n\t\t\t\t\t\tfos.write(buffer, 0, len);\n\t\t\t\t\t}\n\t\t\t\t\tfos.close();\n\t\t\t\t}\n\t\t\t\tze = zis.getNextEntry();\n\t\t\t}\n\n\t\t\tzis.closeEntry();\n\t\t\tzis.close();\n\n\t\t} catch (IOException ex) {\n\t\t\tlogger.error(\"Error while extracting the reosurces: \" + ex.getMessage());\n\t\t}\n\n\t}", "func_src_after": "\tpublic static void unZipIt(String resourceName, String outputFolder) {\n\n\t\tbyte[] buffer = new byte[1024];\n\n\t\ttry {\n\t\t\tZipInputStream zis = new ZipInputStream(CreateResources.class.getResourceAsStream(resourceName));\n\t\t\tZipEntry ze = zis.getNextEntry();\n\n\t\t\twhile (ze != null) {\n\t\t\t\tString fileName = ze.getName();\n\t\t\t\tFile newFile = new File(outputFolder, fileName);\n\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n\t\t\t\t// System.out.println(\"file unzip : \"+ newFile.getAbsoluteFile());\n\t\t\t\tif (ze.isDirectory()) {\n\t\t\t\t\tString temp = newFile.getAbsolutePath();\n\t\t\t\t\tnew File(temp).mkdirs();\n\t\t\t\t} else {\n\t\t\t\t\tString directory = newFile.getParent();\n\t\t\t\t\tif (directory != null) {\n\t\t\t\t\t\tFile d = new File(directory);\n\t\t\t\t\t\tif (!d.exists()) {\n\t\t\t\t\t\t\td.mkdirs();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tFileOutputStream fos = new FileOutputStream(newFile);\n\t\t\t\t\tint len;\n\t\t\t\t\twhile ((len = zis.read(buffer)) > 0) {\n\t\t\t\t\t\tfos.write(buffer, 0, len);\n\t\t\t\t\t}\n\t\t\t\t\tfos.close();\n\t\t\t\t}\n\t\t\t\tze = zis.getNextEntry();\n\t\t\t}\n\n\t\t\tzis.closeEntry();\n\t\t\tzis.close();\n\n\t\t} catch (IOException ex) {\n\t\t\tlogger.error(\"Error while extracting the reosurces: \" + ex.getMessage());\n\t\t}\n\n\t}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 315, "char_end": 386, "line": "\t\t\t\tFile newFile = new File(outputFolder + File.separator + fileName);\n"}], "added": [{"line_no": 11, "char_start": 315, "char_end": 368, "line": "\t\t\t\tFile newFile = new File(outputFolder, fileName);\n"}, {"line_no": 12, "char_start": 368, "char_end": 434, "line": "\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n"}, {"line_no": 13, "char_start": 434, "char_end": 484, "line": "\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 14, "char_start": 484, "char_end": 490, "line": "\t\t\t\t}\n"}]}, "char_changes": {"deleted": [{"char_start": 355, "char_end": 385, "chars": " + File.separator + fileName);"}], "added": [{"char_start": 355, "char_end": 489, "chars": ", fileName);\n\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}"}]}, "commit_link": "github.com/dgarijo/Widoco/commit/f2279b76827f32190adfa9bd5229b7d5a147fa92", "file_name": "WidocoUtils.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to unzip a resource into a specified output folder."}
{"func_name": "JarFileUtils::testngXmlExistsInJar", "func_src_before": "  private boolean testngXmlExistsInJar(File jarFile, List<String> classes) throws IOException {\n    try (JarFile jf = new JarFile(jarFile)) {\n      Enumeration<JarEntry> entries = jf.entries();\n      File file = java.nio.file.Files.createTempDirectory(\"testngXmlPathInJar-\").toFile();\n      String suitePath = null;\n      while (entries.hasMoreElements()) {\n        JarEntry je = entries.nextElement();\n        String jeName = je.getName();\n        if (Parser.canParse(jeName.toLowerCase())) {\n          InputStream inputStream = jf.getInputStream(je);\n          File copyFile = new File(file, jeName);\n          copyFile.getParentFile().mkdirs();\n          Files.copy(inputStream, copyFile.toPath());\n          if (matchesXmlPathInJar(je)) {\n            suitePath = copyFile.toString();\n          }\n        } else if (isJavaClass(je)) {\n          classes.add(constructClassName(je));\n        }\n      }\n      if (Strings.isNullOrEmpty(suitePath)) {\n        return false;\n      }\n      Collection<XmlSuite> parsedSuites = Parser.parse(suitePath, processor);\n      delete(file);\n      for (XmlSuite suite : parsedSuites) {\n        // If test names were specified, only run these test names\n        if (testNames != null) {\n          TestNamesMatcher testNamesMatcher = new TestNamesMatcher(suite, testNames);\n          testNamesMatcher.validateMissMatchedTestNames();\n          suites.addAll(testNamesMatcher.getSuitesMatchingTestNames());\n        } else {\n          suites.add(suite);\n        }\n        return true;\n      }\n    }\n    return false;\n  }", "func_src_after": "  private boolean testngXmlExistsInJar(File jarFile, List<String> classes) throws IOException {\n    try (JarFile jf = new JarFile(jarFile)) {\n      Enumeration<JarEntry> entries = jf.entries();\n      File file = java.nio.file.Files.createTempDirectory(\"testngXmlPathInJar-\").toFile();\n      String suitePath = null;\n      while (entries.hasMoreElements()) {\n        JarEntry je = entries.nextElement();\n        String jeName = je.getName();\n        if (Parser.canParse(jeName.toLowerCase())) {\n          InputStream inputStream = jf.getInputStream(je);\n          File copyFile = new File(file, jeName);\n          if (!copyFile.toPath().normalize().startsWith(file.toPath().normalize())) {\n            throw new IOException(\"Bad zip entry\");\n          }\n          copyFile.getParentFile().mkdirs();\n          Files.copy(inputStream, copyFile.toPath());\n          if (matchesXmlPathInJar(je)) {\n            suitePath = copyFile.toString();\n          }\n        } else if (isJavaClass(je)) {\n          classes.add(constructClassName(je));\n        }\n      }\n      if (Strings.isNullOrEmpty(suitePath)) {\n        return false;\n      }\n      Collection<XmlSuite> parsedSuites = Parser.parse(suitePath, processor);\n      delete(file);\n      for (XmlSuite suite : parsedSuites) {\n        // If test names were specified, only run these test names\n        if (testNames != null) {\n          TestNamesMatcher testNamesMatcher = new TestNamesMatcher(suite, testNames);\n          testNamesMatcher.validateMissMatchedTestNames();\n          suites.addAll(testNamesMatcher.getSuitesMatchingTestNames());\n        } else {\n          suites.add(suite);\n        }\n        return true;\n      }\n    }\n    return false;\n  }", "line_changes": {"deleted": [], "added": [{"line_no": 12, "char_start": 603, "char_end": 689, "line": "          if (!copyFile.toPath().normalize().startsWith(file.toPath().normalize())) {\n"}, {"line_no": 13, "char_start": 689, "char_end": 741, "line": "            throw new IOException(\"Bad zip entry\");\n"}, {"line_no": 14, "char_start": 741, "char_end": 753, "line": "          }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 603, "char_end": 753, "chars": "          if (!copyFile.toPath().normalize().startsWith(file.toPath().normalize())) {\n            throw new IOException(\"Bad zip entry\");\n          }\n"}]}, "commit_link": "github.com/cbeust/testng/commit/47afa2c8a29e2cf925238af1ad7c76fba282793f", "file_name": "JarFileUtils.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\n\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to check if a TestNG XML configuration exists within a JAR file and process it, also handling Java class entries."}
{"func_name": "UnpackExample::usual", "func_src_before": "  public static void usual() throws IOException {\n    ZipFile zf = new ZipFile(\"demo.zip\");\n    try {\n      Enumeration en = zf.entries();\n      while (en.hasMoreElements()) {\n        ZipEntry e = (ZipEntry) en.nextElement();\n\n        InputStream in = null;\n        OutputStream out = null; \n        try {\n          in = zf.getInputStream(e);\n          out = new FileOutputStream(new File(\"demo\", e.getName()));\n          IOUtils.copy(in, out);\n        }\n        finally {\n          IOUtils.closeQuietly(in);\n          IOUtils.closeQuietly(out);\n        }\n      }\n    }\n    finally {\n      zf.close();\n    }\n    \n    zf = new ZipFile(\"demo.zip\");\n    byte[] bytes = new byte[0];\n    try {\n      \n      ZipEntry ze = zf.getEntry(\"foo.txt\");\n      if (ze != null) {\n        InputStream is = zf.getInputStream(ze);\n        try {\n          bytes = IOUtils.toByteArray(is);\n        }\n        finally {\n          IOUtils.closeQuietly(is);\n        }\n      }\n    }\n    finally {\n      zf.close();\n    }\n    \n    System.out.println(\"Read \" + bytes.length + \" bytes.\");\n  }", "func_src_after": "  public static void usual() throws IOException {\n    ZipFile zf = new ZipFile(\"demo.zip\");\n    try {\n      Enumeration en = zf.entries();\n      while (en.hasMoreElements()) {\n        ZipEntry e = (ZipEntry) en.nextElement();\n\n        InputStream in = null;\n        OutputStream out = null; \n        try {\n          in = zf.getInputStream(e);\n          final File zipEntryFile = new File(\"demo\", e.getName());\n          if (!zipEntryFile.toPath().normalize().startsWith(\"demo\")) {\n            throw new IOException(\"Bad zip entry\");\n          }\n          out = new FileOutputStream(zipEntryFile);\n          IOUtils.copy(in, out);\n        }\n        finally {\n          IOUtils.closeQuietly(in);\n          IOUtils.closeQuietly(out);\n        }\n      }\n    }\n    finally {\n      zf.close();\n    }\n    \n    zf = new ZipFile(\"demo.zip\");\n    byte[] bytes = new byte[0];\n    try {\n      \n      ZipEntry ze = zf.getEntry(\"foo.txt\");\n      if (ze != null) {\n        InputStream is = zf.getInputStream(ze);\n        try {\n          bytes = IOUtils.toByteArray(is);\n        }\n        finally {\n          IOUtils.closeQuietly(is);\n        }\n      }\n    }\n    finally {\n      zf.close();\n    }\n    \n    System.out.println(\"Read \" + bytes.length + \" bytes.\");\n  }", "line_changes": {"deleted": [{"line_no": 12, "char_start": 343, "char_end": 412, "line": "          out = new FileOutputStream(new File(\"demo\", e.getName()));\n"}], "added": [{"line_no": 12, "char_start": 343, "char_end": 410, "line": "          final File zipEntryFile = new File(\"demo\", e.getName());\n"}, {"line_no": 13, "char_start": 410, "char_end": 481, "line": "          if (!zipEntryFile.toPath().normalize().startsWith(\"demo\")) {\n"}, {"line_no": 14, "char_start": 481, "char_end": 533, "line": "            throw new IOException(\"Bad zip entry\");\n"}, {"line_no": 15, "char_start": 533, "char_end": 545, "line": "          }\n"}, {"line_no": 16, "char_start": 545, "char_end": 597, "line": "          out = new FileOutputStream(zipEntryFile);\n"}]}, "char_changes": {"deleted": [{"char_start": 353, "char_end": 380, "chars": "out = new FileOutputStream("}], "added": [{"char_start": 353, "char_end": 379, "chars": "final File zipEntryFile = "}, {"char_start": 408, "char_end": 594, "chars": ";\n          if (!zipEntryFile.toPath().normalize().startsWith(\"demo\")) {\n            throw new IOException(\"Bad zip entry\");\n          }\n          out = new FileOutputStream(zipEntryFile"}]}, "commit_link": "github.com/zeroturnaround/zt-zip/commit/627bbc93907ceb69111f86e2edf26375a1abccfa", "file_name": "UnpackExample.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "9b0818802c8fc804d75ef731da538423a7e020fa", "description": "Write a Java function to extract all files from a zip archive and read a specific file's contents into a byte array."}
{"func_name": "RemoteZipHandler::extractZip", "func_src_before": "    public static void extractZip(File zipFile, File destDir) throws IOException\n    {\n        byte[] buffer = new byte[1024];\n        if (!destDir.exists())\n            destDir.mkdirs();\n\n        ZipInputStream zis = new ZipInputStream(new FileInputStream(zipFile));\n        ZipEntry ze = zis.getNextEntry();\n        try\n        {\n            while (ze != null)\n            {\n                String fileName = ze.getName();\n                File newFile = new File(destDir, fileName);\n                if (ze.isDirectory())\n                {\n                    if (newFile.exists())\n                        deleteDirAndContents(newFile);\n                    newFile.mkdirs();\n                }\n                else\n                {\n                    if (newFile.exists())\n                        newFile.delete();\n                    if (newFile.getParentFile() != null && !newFile.getParentFile().exists())\n                        newFile.getParentFile().mkdirs();\n                    FileOutputStream fos = new FileOutputStream(newFile);\n                    int len;\n                    while ((len = zis.read(buffer)) > 0)\n                        fos.write(buffer, 0, len);\n\n                    fos.close();\n                }\n                ze = zis.getNextEntry();\n            }\n        }\n        finally\n        {\n            zis.closeEntry();\n            zis.close();\n        }\n    }", "func_src_after": "    public static void extractZip(File zipFile, File destDir) throws IOException\n    {\n        byte[] buffer = new byte[1024];\n        if (!destDir.exists())\n            destDir.mkdirs();\n\n        ZipInputStream zis = new ZipInputStream(new FileInputStream(zipFile));\n        ZipEntry ze = zis.getNextEntry();\n        try\n        {\n            while (ze != null)\n            {\n                String fileName = ze.getName();\n                File newFile = new File(destDir, fileName);\n                if (!newFile.toPath().normalize().startsWith(destDir.toPath().normalize())) {\n                    throw new IOException(\"Bad zip entry\");\n                }\n                if (ze.isDirectory())\n                {\n                    if (newFile.exists())\n                        deleteDirAndContents(newFile);\n                    newFile.mkdirs();\n                }\n                else\n                {\n                    if (newFile.exists())\n                        newFile.delete();\n                    if (newFile.getParentFile() != null && !newFile.getParentFile().exists())\n                        newFile.getParentFile().mkdirs();\n                    FileOutputStream fos = new FileOutputStream(newFile);\n                    int len;\n                    while ((len = zis.read(buffer)) > 0)\n                        fos.write(buffer, 0, len);\n\n                    fos.close();\n                }\n                ze = zis.getNextEntry();\n            }\n        }\n        finally\n        {\n            zis.closeEntry();\n            zis.close();\n        }\n    }", "line_changes": {"deleted": [], "added": [{"line_no": 15, "char_start": 485, "char_end": 579, "line": "                if (!newFile.toPath().normalize().startsWith(destDir.toPath().normalize())) {\n"}, {"line_no": 16, "char_start": 579, "char_end": 639, "line": "                    throw new IOException(\"Bad zip entry\");\n"}, {"line_no": 17, "char_start": 639, "char_end": 657, "line": "                }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 485, "char_end": 657, "chars": "                if (!newFile.toPath().normalize().startsWith(destDir.toPath().normalize())) {\n                    throw new IOException(\"Bad zip entry\");\n                }\n"}]}, "commit_link": "github.com/bspkrs/MCPMappingViewer/commit/6e602746c96b4756c271d080dae7d22ad804a1bd", "file_name": "RemoteZipHandler.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "cc754ad6e8502fa02c35de0fffad83c1a1956a03", "description": "Write a Java function to extract the contents of a ZIP file to a specified directory."}
{"func_name": "ModulePackager::substring", "func_src_before": "                implementationVersion.substring(implementationVersion.indexOf(\"-\") + 1) :\n                implementationVersion;\n        SPECIFICATION_VERSION = implementationVersion.indexOf(\"-\") > 0 ?\n                implementationVersion.substring(0, implementationVersion.indexOf(\"-\")) :\n                implementationVersion;\n        _manifest = new Manifest();\n        Attributes attributes = _manifest.getMainAttributes();\n        ATTR_DESCRIPTION_NAME = new Attributes.Name(\"OpenIDE-Module-Short-Description\");\n        ATTR_MODULE_NAME = new Attributes.Name(\"OpenIDE-Module\");\n        ATTR_MODULE_TYPE = new Attributes.Name(\"OpenIDE-Module-Type\");\n        ATTR_MODULE_IMPLEMENTATION = new Attributes.Name(\"OpenIDE-Module-Implementation-Version\");\n        ATTR_MODULE_SPECIFICATION = new Attributes.Name(\"OpenIDE-Module-Specification-Version\");\n        ATTR_MODULE_ALIAS = new Attributes.Name(\"OpenIDE-Module-Alias\");\n        ATTR_MODULE_DEPENDENCIES = new Attributes.Name(\"OpenIDE-Module-Module-Dependencies\");\n        attributes.put(Attributes.Name.MANIFEST_VERSION, \"1.0\");\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Java-Dependencies\"), \"Java > 1.8\");\n        attributes.put(ATTR_MODULE_DEPENDENCIES, \"org.esa.snap.snap.sta, org.esa.snap.snap.sta.ui\");\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Display-Category\"), \"SNAP\");\n        attributes.put(ATTR_MODULE_TYPE, \"STA\");\n        attributes.put(ATTR_DESCRIPTION_NAME, \"External tool adapter\");\n\n        modulesPath = ToolAdapterIO.getAdaptersPath().toFile();\n    }\n    public static void packModules(ModuleSuiteDescriptor suiteDescriptor, File suiteFile, Map<OSFamily, Bundle> bundles, ToolAdapterOperatorDescriptor... descriptors) throws IOException {\n        if (suiteFile != null && descriptors != null && descriptors.length > 0) {\n            if (descriptors.length == 1) {\n                packModule(descriptors[0], suiteFile);\n            } else {\n                Path suiteFilePath = suiteFile.toPath();\n                if (!Files.isDirectory(suiteFilePath)) {\n                    suiteFilePath = suiteFilePath.getParent();\n                }\n                Map<String, String> dependentModules = new HashMap<>();\n                UpdateBuilder updateBuilder = new UpdateBuilder();\n                for (ToolAdapterOperatorDescriptor descriptor : descriptors) {\n                    updateBuilder.moduleManifest(\n                            packModule(descriptor, suiteFilePath.resolve(descriptor.getAlias() + \".nbm\").toFile(), true));\n                    dependentModules.put(normalize(descriptor.getName()), SPECIFICATION_VERSION);// descriptor.getVersion());\n                }\n                if (bundles != null) {\n                    Arrays.stream(descriptors).forEach(d -> d.setBundles(bundles));\n                }\n                packSuite(suiteDescriptor, suiteFile, dependentModules, bundles);\n                Files.write(suiteFilePath.resolve(\"updates.xml\"), updateBuilder.build(true).getBytes());\n            }\n        }\n    }\n    /**\n     * Packs the files associated with the given tool adapter operator descriptor into\n     * a NetBeans module file (nbm)\n     *\n     * @param descriptor    The tool adapter descriptor\n     * @param nbmFile       The target module file\n     */\n    public static String packModule(ToolAdapterOperatorDescriptor descriptor, File nbmFile) throws IOException {\n        return packModule(descriptor, nbmFile, false);\n    }\n\n    private static String packModule(ToolAdapterOperatorDescriptor descriptor, File nbmFile, boolean isPartOfSuite) throws IOException {\n        byte[] byteBuffer;\n        String manifestXml = null;\n        try (final ZipOutputStream zipStream = new ZipOutputStream(new FileOutputStream(nbmFile))) {\n            // create Info section\n            ZipEntry entry = new ZipEntry(\"Info/info.xml\");\n            zipStream.putNextEntry(entry);\n            InfoBuilder infoBuilder = new InfoBuilder();\n            String javaVersion = System.getProperty(\"java.version\");\n            javaVersion = javaVersion.substring(0, javaVersion.indexOf(\"_\"));\n            String descriptorName = normalize(descriptor.getName());\n            String description = descriptor.getDescription();\n\n            infoBuilder.moduleName(descriptorName)\n                                    .shortDescription(description)\n                                    .longDescription(description)\n                                    .displayCategory(\"SNAP\")\n                                    .specificationVersion(SPECIFICATION_VERSION)\n                                    .implementationVersion(SPECIFICATION_VERSION)//descriptor.getVersion())\n                                    .codebase(descriptorName.toLowerCase())\n                                    .distribution(nbmFile.getName())\n                                    .downloadSize(0)\n                                    .homePage(\"https://github.com/senbox-org/s2tbx\")\n                                    .needsRestart(true)\n                                    .releaseDate(new Date())\n                                    .isEssentialModule(false)\n                                    .showInClient(!isPartOfSuite)\n                                    .javaVersion(javaVersion)\n                                    .dependency(STA_MODULE, SPECIFICATION_VERSION)\n                                    .dependency(STA_UI_MODULE, SPECIFICATION_VERSION)\n                                    .dependency(SNAP_RCP_MODULE, SPECIFICATION_VERSION)\n                                    .dependency(SNAP_CORE_MODULE, SPECIFICATION_VERSION);\n            byteBuffer = infoBuilder.build(true).getBytes();\n            manifestXml = infoBuilder.build(false);\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            // create META-INF section\n            entry = new ZipEntry(\"META-INF/MANIFEST.MF\");\n            zipStream.putNextEntry(entry);\n            byteBuffer = new ManifestBuilder().build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            String jarName = descriptorName.replace(\".\", \"-\") + \".jar\";\n\n            // create config section\n            entry = new ZipEntry(\"netbeans/config/Modules/\" + descriptorName.replace(\".\", \"-\") + \".xml\");\n            zipStream.putNextEntry(entry);\n            ModuleConfigBuilder mcb = new ModuleConfigBuilder();\n            byteBuffer = mcb.name(descriptorName)\n                            .autoLoad(false)\n                            .eager(false)\n                            .enabled(true)\n                            .jarName(jarName)\n                            .reloadable(false)\n                        .build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n            // create modules section\n            entry = new ZipEntry(\"netbeans/modules/ext/\");\n            zipStream.putNextEntry(entry);\n            zipStream.closeEntry();\n            entry = new ZipEntry(\"netbeans/modules/\" + jarName);\n            zipStream.putNextEntry(entry);\n            zipStream.write(packAdapterJar(descriptor));\n            zipStream.closeEntry();\n            Map<OSFamily, Bundle> bundles = descriptor.getBundles();\n            if (!isPartOfSuite && bundles != null) {\n                for (Bundle bundle : bundles.values()) {\n                    if (bundle.isLocal() && bundle.getTargetLocation() != null && bundle.getEntryPoint() != null) {\n                        // lib folder\n                        entry = new ZipEntry(\"netbeans/modules/lib/\");\n                        zipStream.putNextEntry(entry);\n                        zipStream.closeEntry();\n                        // bundle\n                        String entryPoint = bundle.getEntryPoint();\n                        File entryPointPath = bundle.getSource();\n                        if (entryPointPath.exists()) {\n                            entry = new ZipEntry(\"netbeans/modules/lib/\" + entryPoint);\n                            zipStream.putNextEntry(entry);\n                            zipStream.write(Files.readAllBytes(entryPointPath.toPath()));\n                            zipStream.closeEntry();\n                        }\n                    }\n                }\n            }\n            // create update_tracking section\n            entry = new ZipEntry(\"netbeans/update_tracking/\");\n            zipStream.putNextEntry(entry);\n            zipStream.closeEntry();\n        }\n        return manifestXml;\n    }\n\n    /**\n     * Unpacks a jar file into the user modules location.\n     *\n     * @param jarFile   The jar file to be unpacked\n     * @param unpackFolder  The destination folder. If null, then the jar name will be used\n     */\n    public static void unpackAdapterJar(File jarFile, File unpackFolder) throws IOException {\n        JarFile jar = new JarFile(jarFile);\n        Enumeration enumEntries = jar.entries();\n        if (unpackFolder == null) {\n            unpackFolder = new File(modulesPath, jarFile.getName().replace(\".jar\", \"\"));\n        }\n        if (!unpackFolder.exists()) {\n            Files.createDirectories(unpackFolder.toPath());\n        }\n        Attributes attributes = jar.getManifest().getMainAttributes();\n        if (attributes.containsKey(ATTR_MODULE_IMPLEMENTATION)) {\n            String version = attributes.getValue(ATTR_MODULE_IMPLEMENTATION);\n            File versionFile = new File(unpackFolder, \"version.txt\");\n            try (FileOutputStream fos = new FileOutputStream(versionFile)) {\n                fos.write(version.getBytes());\n                fos.close();\n            }\n        }\n        while (enumEntries.hasMoreElements()) {\n            JarEntry file = (JarEntry) enumEntries.nextElement();\n            File f = new File(unpackFolder, file.getName());\n            if (file.isDirectory()) {\n                Files.createDirectories(f.toPath());\n                continue;\n            } else {\n                Files.createDirectories(f.getParentFile().toPath());\n            }\n            try (InputStream is = jar.getInputStream(file)) {\n                try (FileOutputStream fos = new FileOutputStream(f)) {\n                    while (is.available() > 0) {\n                        fos.write(is.read());\n                    }\n                    fos.close();\n                }\n                is.close();\n            }\n        }\n    }\n\n    public static String getAdapterVersion(File jarFile) throws IOException {\n        String version = null;\n        JarFile jar = new JarFile(jarFile);\n        Attributes attributes = jar.getManifest().getMainAttributes();\n        if (attributes.containsKey(ATTR_MODULE_IMPLEMENTATION)) {\n            version = attributes.getValue(ATTR_MODULE_IMPLEMENTATION);\n        }\n        jar.close();\n        return version;\n    }\n\n    public static String getAdapterAlias(File jarFile) throws IOException {\n        String version = null;\n        JarFile jar = new JarFile(jarFile);\n        Attributes attributes = jar.getManifest().getMainAttributes();\n        if (attributes.containsKey(ATTR_MODULE_ALIAS)) {\n            version = attributes.getValue(ATTR_MODULE_ALIAS);\n        }\n        jar.close();\n        return version;\n    }\n\n    private static void packSuite(ModuleSuiteDescriptor descriptor, File nbmFile, Map<String, String> dependencies, Map<OSFamily, Bundle> bundles) throws IOException {\n        byte[] byteBuffer;\n        try (final ZipOutputStream zipStream = new ZipOutputStream(new FileOutputStream(nbmFile))) {\n            // create Info section\n            ZipEntry entry = new ZipEntry(\"Info/info.xml\");\n            zipStream.putNextEntry(entry);\n            InfoBuilder infoBuilder = new InfoBuilder();\n            String javaVersion = System.getProperty(\"java.version\");\n            javaVersion = javaVersion.substring(0, javaVersion.indexOf(\"_\"));\n            String descriptorName = descriptor.getName();\n            String description = descriptor.getDescription();\n\n            infoBuilder.moduleName(descriptorName)\n                    .shortDescription(description)\n                    .longDescription(description)\n                    .displayCategory(\"SNAP\")\n                    .specificationVersion(SPECIFICATION_VERSION)\n                    .implementationVersion(IMPLEMENTATION_VERSION)\n                    .codebase(descriptorName.toLowerCase())\n                    .distribution(nbmFile.getName())\n                    .downloadSize(0)\n                    .homePage(\"https://github.com/senbox-org/s2tbx\")\n                    .needsRestart(true)\n                    .releaseDate(new Date())\n                    .isEssentialModule(false)\n                    .showInClient(true)\n                    .javaVersion(javaVersion)\n                    .dependency(STA_MODULE, SPECIFICATION_VERSION)\n                    .dependency(STA_UI_MODULE, SPECIFICATION_VERSION)\n                    .dependency(SNAP_RCP_MODULE, SPECIFICATION_VERSION)\n                    .dependency(SNAP_CORE_MODULE, SPECIFICATION_VERSION);\n            if (dependencies != null) {\n                for (Map.Entry<String, String> mapEntry : dependencies.entrySet()) {\n                    infoBuilder.dependency(mapEntry.getKey(), mapEntry.getValue());\n                }\n            }\n            byteBuffer = infoBuilder.build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            // create META-INF section\n            entry = new ZipEntry(\"META-INF/MANIFEST.MF\");\n            zipStream.putNextEntry(entry);\n            byteBuffer = new ManifestBuilder().build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            String jarName = descriptorName.replace(\".\", \"-\") + \".jar\";\n\n            // create config section\n            entry = new ZipEntry(\"netbeans/config/Modules/\" + descriptorName.replace(\".\", \"-\") + \".xml\");\n            zipStream.putNextEntry(entry);\n            ModuleConfigBuilder mcb = new ModuleConfigBuilder();\n            byteBuffer = mcb.name(descriptorName)\n                    .autoLoad(false)\n                    .eager(false)\n                    .enabled(true)\n                    .jarName(jarName)\n                    .reloadable(false)\n                    .build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n            entry = new ZipEntry(\"netbeans/modules/\" + jarName);\n            zipStream.putNextEntry(entry);\n            zipStream.write(packSuiteJar(descriptor, dependencies));\n            zipStream.closeEntry();\n            if (bundles != null) {\n                for (Bundle bundle : bundles.values()) {\n                    if (bundle.isLocal() && bundle.getTargetLocation() != null && bundle.getEntryPoint() != null) {\n                        // lib folder\n                        entry = new ZipEntry(\"netbeans/modules/lib/\");\n                        zipStream.putNextEntry(entry);\n                        zipStream.closeEntry();\n                        // bundle\n                        String entryPoint = bundle.getEntryPoint();\n                        File entryPointPath = bundle.getSource();\n                        if (entryPointPath.exists()) {\n                            entry = new ZipEntry(\"netbeans/modules/lib/\" + entryPoint);\n                            zipStream.putNextEntry(entry);\n                            zipStream.write(Files.readAllBytes(entryPointPath.toPath()));\n                            zipStream.closeEntry();\n                        }\n                    }\n                }\n            }\n            // create update_tracking section\n            entry = new ZipEntry(\"netbeans/update_tracking/\");\n            zipStream.putNextEntry(entry);\n            zipStream.closeEntry();\n        }\n    }\n\n    private static byte[] packSuiteJar(ModuleSuiteDescriptor descriptor, Map<String, String> modules) throws IOException {\n        Manifest manifest = new Manifest();\n        Attributes attributes = manifest.getMainAttributes();\n        attributes.put(Attributes.Name.MANIFEST_VERSION, \"1.0\");\n        attributes.put(ATTR_MODULE_NAME, descriptor.getName());\n        attributes.put(ATTR_DESCRIPTION_NAME, descriptor.getDescription());\n        attributes.put(ATTR_MODULE_SPECIFICATION, SPECIFICATION_VERSION);\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Java-Dependencies\"), \"Java > 1.8\");\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Display-Category\"), \"SNAP\");\n        attributes.put(ATTR_MODULE_TYPE, \"STA\");\n        String dependenciesValue = \"org.esa.snap.snap.sta, org.esa.snap.snap.sta.ui\";\n        for (Map.Entry<String, String> entry : modules.entrySet()) {\n            dependenciesValue += \", \" + entry.getKey() + \" > \" + entry.getValue();\n        }\n        attributes.put(ATTR_MODULE_DEPENDENCIES, dependenciesValue);\n\n        ByteArrayOutputStream fOut = new ByteArrayOutputStream();\n        try (JarOutputStream jarOut = new JarOutputStream(fOut, manifest)) {\n            jarOut.close();\n        }\n        return fOut.toByteArray();\n    }\n\n    private static byte[] packAdapterJar(ToolAdapterOperatorDescriptor descriptor) throws IOException {\n        _manifest.getMainAttributes().put(ATTR_DESCRIPTION_NAME, descriptor.getAlias());\n        _manifest.getMainAttributes().put(ATTR_MODULE_NAME, descriptor.getName());\n        _manifest.getMainAttributes().put(ATTR_MODULE_IMPLEMENTATION, SPECIFICATION_VERSION);//descriptor.getVersion());\n        _manifest.getMainAttributes().put(ATTR_MODULE_SPECIFICATION, SPECIFICATION_VERSION);\n        _manifest.getMainAttributes().put(ATTR_MODULE_ALIAS, descriptor.getAlias());\n        File moduleFolder = new File(modulesPath, descriptor.getAlias());\n        ByteArrayOutputStream fOut = new ByteArrayOutputStream();\n        try (JarOutputStream jarOut = new JarOutputStream(fOut, _manifest)) {\n            File[] files = moduleFolder.listFiles();\n            if (files != null) {\n                for (File child : files) {\n                    try {\n                        // ModuleInstaller from adapter folder should not be included\n                        if (child.getName().endsWith(\"ModuleInstaller.class\")) {\n                            //noinspection ResultOfMethodCallIgnored\n                            child.delete();\n                        } else {\n                            addFile(child, jarOut);\n                        }\n                    } catch (Exception ignored) {\n                    }\n                }\n            }\n            try {\n                String contents = layerXml.replace(\"#NAME#\", descriptor.getLabel());\n                JarEntry entry = new JarEntry(LAYER_XML_PATH);\n                jarOut.putNextEntry(entry);\n                byte[] buffer = contents.getBytes();\n                jarOut.write(buffer, 0, buffer.length);\n                jarOut.closeEntry();\n            } catch (Exception ignored) {\n                ignored.printStackTrace();\n            }\n            jarOut.close();\n        }\n        return fOut.toByteArray();\n    }\n\n    /**\n     * Adds a file to the target jar stream.\n     *\n     * @param source    The file to be added\n     * @param target    The target jar stream\n     */\n    private static void addFile(File source, JarOutputStream target) throws IOException {\n        String entryName = source.getPath().replace(modulesPath.getAbsolutePath(), \"\").replace(\"\\\\\", \"/\").substring(1);\n        entryName = entryName.substring(entryName.indexOf(\"/\") + 1);\n        if (!entryName.toLowerCase().endsWith(\"manifest.mf\")) {\n            if (source.isDirectory()) {\n                if (!entryName.isEmpty()) {\n                    if (!entryName.endsWith(\"/\")) {\n                        entryName += \"/\";\n                    }\n                    JarEntry entry = new JarEntry(entryName);\n                    entry.setTime(source.lastModified());\n                    target.putNextEntry(entry);\n                    target.closeEntry();\n                }\n                File[] files = source.listFiles();\n                if (files != null) {\n                    for (File nestedFile : files) {\n                        addFile(nestedFile, target);\n                    }\n                }\n                return;\n            }\n            JarEntry entry = new JarEntry(entryName);\n            entry.setTime(source.lastModified());\n            target.putNextEntry(entry);\n            writeBytes(source, target);\n            target.closeEntry();\n        }\n    }\n\n    /**\n     * Adds a compiled class file to the target jar stream.\n     *\n     * @param fromClass     The class to be added\n     * @param target        The target jar stream\n     */\n    private static void addFile(Class fromClass, JarOutputStream target) throws IOException {\n        String classEntry = fromClass.getName().replace('.', '/') + \".class\";\n        URL classURL = fromClass.getClassLoader().getResource(classEntry);\n        if (classURL != null) {\n            JarEntry entry = new JarEntry(classEntry);\n            target.putNextEntry(entry);\n            if (!classURL.toString().contains(\"!\")) {\n                String fileName = classURL.getFile();\n                writeBytes(fileName, target);\n            } else {\n                try (InputStream stream = fromClass.getClassLoader().getResourceAsStream(classEntry)) {\n                    writeBytes(stream, target);\n                }\n            }\n            target.closeEntry();\n        }\n    }\n\n    private static void writeBytes(String fileName, JarOutputStream target) throws IOException {\n        writeBytes(new File(fileName), target);\n    }\n\n    private static void writeBytes(File file, JarOutputStream target) throws IOException {\n        try (FileInputStream fileStream = new FileInputStream(file)) {\n            try (BufferedInputStream inputStream = new BufferedInputStream(fileStream)) {\n                byte[] buffer = new byte[1024];\n                while (true) {\n                    int count = inputStream.read(buffer);\n                    if (count == -1) {\n                        break;\n                    }\n                    target.write(buffer, 0, count);\n                }\n            }\n        }\n    }\n\n    private static void writeBytes(InputStream stream, JarOutputStream target) throws IOException {\n        byte[] buffer = new byte[1024];\n        while (true) {\n            int count = stream.read(buffer);\n            if (count == -1) {\n                break;\n            }\n            target.write(buffer, 0, count);\n        }\n    }\n\n    private static String normalize(String input) {\n        if (input == null || input.isEmpty()) {\n            throw new IllegalArgumentException(\"Empty value\");\n        }\n        return input.replace(\"-\", \".\").replace(\" \", \"_\");\n    }", "func_src_after": "                implementationVersion.substring(implementationVersion.indexOf(\"-\") + 1) :\n                implementationVersion;\n        SPECIFICATION_VERSION = implementationVersion.indexOf(\"-\") > 0 ?\n                implementationVersion.substring(0, implementationVersion.indexOf(\"-\")) :\n                implementationVersion;\n        _manifest = new Manifest();\n        Attributes attributes = _manifest.getMainAttributes();\n        ATTR_DESCRIPTION_NAME = new Attributes.Name(\"OpenIDE-Module-Short-Description\");\n        ATTR_MODULE_NAME = new Attributes.Name(\"OpenIDE-Module\");\n        ATTR_MODULE_TYPE = new Attributes.Name(\"OpenIDE-Module-Type\");\n        ATTR_MODULE_IMPLEMENTATION = new Attributes.Name(\"OpenIDE-Module-Implementation-Version\");\n        ATTR_MODULE_SPECIFICATION = new Attributes.Name(\"OpenIDE-Module-Specification-Version\");\n        ATTR_MODULE_ALIAS = new Attributes.Name(\"OpenIDE-Module-Alias\");\n        ATTR_MODULE_DEPENDENCIES = new Attributes.Name(\"OpenIDE-Module-Module-Dependencies\");\n        attributes.put(Attributes.Name.MANIFEST_VERSION, \"1.0\");\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Java-Dependencies\"), \"Java > 1.8\");\n        attributes.put(ATTR_MODULE_DEPENDENCIES, \"org.esa.snap.snap.sta, org.esa.snap.snap.sta.ui\");\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Display-Category\"), \"SNAP\");\n        attributes.put(ATTR_MODULE_TYPE, \"STA\");\n        attributes.put(ATTR_DESCRIPTION_NAME, \"External tool adapter\");\n\n        modulesPath = ToolAdapterIO.getAdaptersPath().toFile();\n    }\n    public static void packModules(ModuleSuiteDescriptor suiteDescriptor, File suiteFile, Map<OSFamily, Bundle> bundles, ToolAdapterOperatorDescriptor... descriptors) throws IOException {\n        if (suiteFile != null && descriptors != null && descriptors.length > 0) {\n            if (descriptors.length == 1) {\n                packModule(descriptors[0], suiteFile);\n            } else {\n                Path suiteFilePath = suiteFile.toPath();\n                if (!Files.isDirectory(suiteFilePath)) {\n                    suiteFilePath = suiteFilePath.getParent();\n                }\n                Map<String, String> dependentModules = new HashMap<>();\n                UpdateBuilder updateBuilder = new UpdateBuilder();\n                for (ToolAdapterOperatorDescriptor descriptor : descriptors) {\n                    updateBuilder.moduleManifest(\n                            packModule(descriptor, suiteFilePath.resolve(descriptor.getAlias() + \".nbm\").toFile(), true));\n                    dependentModules.put(normalize(descriptor.getName()), SPECIFICATION_VERSION);// descriptor.getVersion());\n                }\n                if (bundles != null) {\n                    Arrays.stream(descriptors).forEach(d -> d.setBundles(bundles));\n                }\n                packSuite(suiteDescriptor, suiteFile, dependentModules, bundles);\n                Files.write(suiteFilePath.resolve(\"updates.xml\"), updateBuilder.build(true).getBytes());\n            }\n        }\n    }\n    /**\n     * Packs the files associated with the given tool adapter operator descriptor into\n     * a NetBeans module file (nbm)\n     *\n     * @param descriptor    The tool adapter descriptor\n     * @param nbmFile       The target module file\n     */\n    public static String packModule(ToolAdapterOperatorDescriptor descriptor, File nbmFile) throws IOException {\n        return packModule(descriptor, nbmFile, false);\n    }\n\n    private static String packModule(ToolAdapterOperatorDescriptor descriptor, File nbmFile, boolean isPartOfSuite) throws IOException {\n        byte[] byteBuffer;\n        String manifestXml = null;\n        try (final ZipOutputStream zipStream = new ZipOutputStream(new FileOutputStream(nbmFile))) {\n            // create Info section\n            ZipEntry entry = new ZipEntry(\"Info/info.xml\");\n            zipStream.putNextEntry(entry);\n            InfoBuilder infoBuilder = new InfoBuilder();\n            String javaVersion = System.getProperty(\"java.version\");\n            javaVersion = javaVersion.substring(0, javaVersion.indexOf(\"_\"));\n            String descriptorName = normalize(descriptor.getName());\n            String description = descriptor.getDescription();\n\n            infoBuilder.moduleName(descriptorName)\n                                    .shortDescription(description)\n                                    .longDescription(description)\n                                    .displayCategory(\"SNAP\")\n                                    .specificationVersion(SPECIFICATION_VERSION)\n                                    .implementationVersion(SPECIFICATION_VERSION)//descriptor.getVersion())\n                                    .codebase(descriptorName.toLowerCase())\n                                    .distribution(nbmFile.getName())\n                                    .downloadSize(0)\n                                    .homePage(\"https://github.com/senbox-org/s2tbx\")\n                                    .needsRestart(true)\n                                    .releaseDate(new Date())\n                                    .isEssentialModule(false)\n                                    .showInClient(!isPartOfSuite)\n                                    .javaVersion(javaVersion)\n                                    .dependency(STA_MODULE, SPECIFICATION_VERSION)\n                                    .dependency(STA_UI_MODULE, SPECIFICATION_VERSION)\n                                    .dependency(SNAP_RCP_MODULE, SPECIFICATION_VERSION)\n                                    .dependency(SNAP_CORE_MODULE, SPECIFICATION_VERSION);\n            byteBuffer = infoBuilder.build(true).getBytes();\n            manifestXml = infoBuilder.build(false);\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            // create META-INF section\n            entry = new ZipEntry(\"META-INF/MANIFEST.MF\");\n            zipStream.putNextEntry(entry);\n            byteBuffer = new ManifestBuilder().build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            String jarName = descriptorName.replace(\".\", \"-\") + \".jar\";\n\n            // create config section\n            entry = new ZipEntry(\"netbeans/config/Modules/\" + descriptorName.replace(\".\", \"-\") + \".xml\");\n            zipStream.putNextEntry(entry);\n            ModuleConfigBuilder mcb = new ModuleConfigBuilder();\n            byteBuffer = mcb.name(descriptorName)\n                            .autoLoad(false)\n                            .eager(false)\n                            .enabled(true)\n                            .jarName(jarName)\n                            .reloadable(false)\n                        .build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n            // create modules section\n            entry = new ZipEntry(\"netbeans/modules/ext/\");\n            zipStream.putNextEntry(entry);\n            zipStream.closeEntry();\n            entry = new ZipEntry(\"netbeans/modules/\" + jarName);\n            zipStream.putNextEntry(entry);\n            zipStream.write(packAdapterJar(descriptor));\n            zipStream.closeEntry();\n            Map<OSFamily, Bundle> bundles = descriptor.getBundles();\n            if (!isPartOfSuite && bundles != null) {\n                for (Bundle bundle : bundles.values()) {\n                    if (bundle.isLocal() && bundle.getTargetLocation() != null && bundle.getEntryPoint() != null) {\n                        // lib folder\n                        entry = new ZipEntry(\"netbeans/modules/lib/\");\n                        zipStream.putNextEntry(entry);\n                        zipStream.closeEntry();\n                        // bundle\n                        String entryPoint = bundle.getEntryPoint();\n                        File entryPointPath = bundle.getSource();\n                        if (entryPointPath.exists()) {\n                            entry = new ZipEntry(\"netbeans/modules/lib/\" + entryPoint);\n                            zipStream.putNextEntry(entry);\n                            zipStream.write(Files.readAllBytes(entryPointPath.toPath()));\n                            zipStream.closeEntry();\n                        }\n                    }\n                }\n            }\n            // create update_tracking section\n            entry = new ZipEntry(\"netbeans/update_tracking/\");\n            zipStream.putNextEntry(entry);\n            zipStream.closeEntry();\n        }\n        return manifestXml;\n    }\n\n    /**\n     * Unpacks a jar file into the user modules location.\n     *\n     * @param jarFile   The jar file to be unpacked\n     * @param unpackFolder  The destination folder. If null, then the jar name will be used\n     */\n    public static void unpackAdapterJar(File jarFile, File unpackFolder) throws IOException {\n        JarFile jar = new JarFile(jarFile);\n        Enumeration enumEntries = jar.entries();\n        if (unpackFolder == null) {\n            unpackFolder = new File(modulesPath, jarFile.getName().replace(\".jar\", \"\"));\n        }\n        if (!unpackFolder.exists()) {\n            Files.createDirectories(unpackFolder.toPath());\n        }\n        Attributes attributes = jar.getManifest().getMainAttributes();\n        if (attributes.containsKey(ATTR_MODULE_IMPLEMENTATION)) {\n            String version = attributes.getValue(ATTR_MODULE_IMPLEMENTATION);\n            File versionFile = new File(unpackFolder, \"version.txt\");\n            try (FileOutputStream fos = new FileOutputStream(versionFile)) {\n                fos.write(version.getBytes());\n                fos.close();\n            }\n        }\n        while (enumEntries.hasMoreElements()) {\n            JarEntry file = (JarEntry) enumEntries.nextElement();\n            File f = new File(unpackFolder, file.getName());\n            if (!f.toPath().normalize().startsWith(unpackFolder.toPath().normalize())) {\n                throw new IOException(\"Bad zip entry\");\n            }\n            if (file.isDirectory()) {\n                Files.createDirectories(f.toPath());\n                continue;\n            } else {\n                Files.createDirectories(f.getParentFile().toPath());\n            }\n            try (InputStream is = jar.getInputStream(file)) {\n                try (FileOutputStream fos = new FileOutputStream(f)) {\n                    while (is.available() > 0) {\n                        fos.write(is.read());\n                    }\n                    fos.close();\n                }\n                is.close();\n            }\n        }\n    }\n\n    public static String getAdapterVersion(File jarFile) throws IOException {\n        String version = null;\n        JarFile jar = new JarFile(jarFile);\n        Attributes attributes = jar.getManifest().getMainAttributes();\n        if (attributes.containsKey(ATTR_MODULE_IMPLEMENTATION)) {\n            version = attributes.getValue(ATTR_MODULE_IMPLEMENTATION);\n        }\n        jar.close();\n        return version;\n    }\n\n    public static String getAdapterAlias(File jarFile) throws IOException {\n        String version = null;\n        JarFile jar = new JarFile(jarFile);\n        Attributes attributes = jar.getManifest().getMainAttributes();\n        if (attributes.containsKey(ATTR_MODULE_ALIAS)) {\n            version = attributes.getValue(ATTR_MODULE_ALIAS);\n        }\n        jar.close();\n        return version;\n    }\n\n    private static void packSuite(ModuleSuiteDescriptor descriptor, File nbmFile, Map<String, String> dependencies, Map<OSFamily, Bundle> bundles) throws IOException {\n        byte[] byteBuffer;\n        try (final ZipOutputStream zipStream = new ZipOutputStream(new FileOutputStream(nbmFile))) {\n            // create Info section\n            ZipEntry entry = new ZipEntry(\"Info/info.xml\");\n            zipStream.putNextEntry(entry);\n            InfoBuilder infoBuilder = new InfoBuilder();\n            String javaVersion = System.getProperty(\"java.version\");\n            javaVersion = javaVersion.substring(0, javaVersion.indexOf(\"_\"));\n            String descriptorName = descriptor.getName();\n            String description = descriptor.getDescription();\n\n            infoBuilder.moduleName(descriptorName)\n                    .shortDescription(description)\n                    .longDescription(description)\n                    .displayCategory(\"SNAP\")\n                    .specificationVersion(SPECIFICATION_VERSION)\n                    .implementationVersion(IMPLEMENTATION_VERSION)\n                    .codebase(descriptorName.toLowerCase())\n                    .distribution(nbmFile.getName())\n                    .downloadSize(0)\n                    .homePage(\"https://github.com/senbox-org/s2tbx\")\n                    .needsRestart(true)\n                    .releaseDate(new Date())\n                    .isEssentialModule(false)\n                    .showInClient(true)\n                    .javaVersion(javaVersion)\n                    .dependency(STA_MODULE, SPECIFICATION_VERSION)\n                    .dependency(STA_UI_MODULE, SPECIFICATION_VERSION)\n                    .dependency(SNAP_RCP_MODULE, SPECIFICATION_VERSION)\n                    .dependency(SNAP_CORE_MODULE, SPECIFICATION_VERSION);\n            if (dependencies != null) {\n                for (Map.Entry<String, String> mapEntry : dependencies.entrySet()) {\n                    infoBuilder.dependency(mapEntry.getKey(), mapEntry.getValue());\n                }\n            }\n            byteBuffer = infoBuilder.build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            // create META-INF section\n            entry = new ZipEntry(\"META-INF/MANIFEST.MF\");\n            zipStream.putNextEntry(entry);\n            byteBuffer = new ManifestBuilder().build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n\n            String jarName = descriptorName.replace(\".\", \"-\") + \".jar\";\n\n            // create config section\n            entry = new ZipEntry(\"netbeans/config/Modules/\" + descriptorName.replace(\".\", \"-\") + \".xml\");\n            zipStream.putNextEntry(entry);\n            ModuleConfigBuilder mcb = new ModuleConfigBuilder();\n            byteBuffer = mcb.name(descriptorName)\n                    .autoLoad(false)\n                    .eager(false)\n                    .enabled(true)\n                    .jarName(jarName)\n                    .reloadable(false)\n                    .build(true).getBytes();\n            zipStream.write(byteBuffer, 0, byteBuffer.length);\n            zipStream.closeEntry();\n            entry = new ZipEntry(\"netbeans/modules/\" + jarName);\n            zipStream.putNextEntry(entry);\n            zipStream.write(packSuiteJar(descriptor, dependencies));\n            zipStream.closeEntry();\n            if (bundles != null) {\n                for (Bundle bundle : bundles.values()) {\n                    if (bundle.isLocal() && bundle.getTargetLocation() != null && bundle.getEntryPoint() != null) {\n                        // lib folder\n                        entry = new ZipEntry(\"netbeans/modules/lib/\");\n                        zipStream.putNextEntry(entry);\n                        zipStream.closeEntry();\n                        // bundle\n                        String entryPoint = bundle.getEntryPoint();\n                        File entryPointPath = bundle.getSource();\n                        if (entryPointPath.exists()) {\n                            entry = new ZipEntry(\"netbeans/modules/lib/\" + entryPoint);\n                            zipStream.putNextEntry(entry);\n                            zipStream.write(Files.readAllBytes(entryPointPath.toPath()));\n                            zipStream.closeEntry();\n                        }\n                    }\n                }\n            }\n            // create update_tracking section\n            entry = new ZipEntry(\"netbeans/update_tracking/\");\n            zipStream.putNextEntry(entry);\n            zipStream.closeEntry();\n        }\n    }\n\n    private static byte[] packSuiteJar(ModuleSuiteDescriptor descriptor, Map<String, String> modules) throws IOException {\n        Manifest manifest = new Manifest();\n        Attributes attributes = manifest.getMainAttributes();\n        attributes.put(Attributes.Name.MANIFEST_VERSION, \"1.0\");\n        attributes.put(ATTR_MODULE_NAME, descriptor.getName());\n        attributes.put(ATTR_DESCRIPTION_NAME, descriptor.getDescription());\n        attributes.put(ATTR_MODULE_SPECIFICATION, SPECIFICATION_VERSION);\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Java-Dependencies\"), \"Java > 1.8\");\n        attributes.put(new Attributes.Name(\"OpenIDE-Module-Display-Category\"), \"SNAP\");\n        attributes.put(ATTR_MODULE_TYPE, \"STA\");\n        String dependenciesValue = \"org.esa.snap.snap.sta, org.esa.snap.snap.sta.ui\";\n        for (Map.Entry<String, String> entry : modules.entrySet()) {\n            dependenciesValue += \", \" + entry.getKey() + \" > \" + entry.getValue();\n        }\n        attributes.put(ATTR_MODULE_DEPENDENCIES, dependenciesValue);\n\n        ByteArrayOutputStream fOut = new ByteArrayOutputStream();\n        try (JarOutputStream jarOut = new JarOutputStream(fOut, manifest)) {\n            jarOut.close();\n        }\n        return fOut.toByteArray();\n    }\n\n    private static byte[] packAdapterJar(ToolAdapterOperatorDescriptor descriptor) throws IOException {\n        _manifest.getMainAttributes().put(ATTR_DESCRIPTION_NAME, descriptor.getAlias());\n        _manifest.getMainAttributes().put(ATTR_MODULE_NAME, descriptor.getName());\n        _manifest.getMainAttributes().put(ATTR_MODULE_IMPLEMENTATION, SPECIFICATION_VERSION);//descriptor.getVersion());\n        _manifest.getMainAttributes().put(ATTR_MODULE_SPECIFICATION, SPECIFICATION_VERSION);\n        _manifest.getMainAttributes().put(ATTR_MODULE_ALIAS, descriptor.getAlias());\n        File moduleFolder = new File(modulesPath, descriptor.getAlias());\n        ByteArrayOutputStream fOut = new ByteArrayOutputStream();\n        try (JarOutputStream jarOut = new JarOutputStream(fOut, _manifest)) {\n            File[] files = moduleFolder.listFiles();\n            if (files != null) {\n                for (File child : files) {\n                    try {\n                        // ModuleInstaller from adapter folder should not be included\n                        if (child.getName().endsWith(\"ModuleInstaller.class\")) {\n                            //noinspection ResultOfMethodCallIgnored\n                            child.delete();\n                        } else {\n                            addFile(child, jarOut);\n                        }\n                    } catch (Exception ignored) {\n                    }\n                }\n            }\n            try {\n                String contents = layerXml.replace(\"#NAME#\", descriptor.getLabel());\n                JarEntry entry = new JarEntry(LAYER_XML_PATH);\n                jarOut.putNextEntry(entry);\n                byte[] buffer = contents.getBytes();\n                jarOut.write(buffer, 0, buffer.length);\n                jarOut.closeEntry();\n            } catch (Exception ignored) {\n                ignored.printStackTrace();\n            }\n            jarOut.close();\n        }\n        return fOut.toByteArray();\n    }\n\n    /**\n     * Adds a file to the target jar stream.\n     *\n     * @param source    The file to be added\n     * @param target    The target jar stream\n     */\n    private static void addFile(File source, JarOutputStream target) throws IOException {\n        String entryName = source.getPath().replace(modulesPath.getAbsolutePath(), \"\").replace(\"\\\\\", \"/\").substring(1);\n        entryName = entryName.substring(entryName.indexOf(\"/\") + 1);\n        if (!entryName.toLowerCase().endsWith(\"manifest.mf\")) {\n            if (source.isDirectory()) {\n                if (!entryName.isEmpty()) {\n                    if (!entryName.endsWith(\"/\")) {\n                        entryName += \"/\";\n                    }\n                    JarEntry entry = new JarEntry(entryName);\n                    entry.setTime(source.lastModified());\n                    target.putNextEntry(entry);\n                    target.closeEntry();\n                }\n                File[] files = source.listFiles();\n                if (files != null) {\n                    for (File nestedFile : files) {\n                        addFile(nestedFile, target);\n                    }\n                }\n                return;\n            }\n            JarEntry entry = new JarEntry(entryName);\n            entry.setTime(source.lastModified());\n            target.putNextEntry(entry);\n            writeBytes(source, target);\n            target.closeEntry();\n        }\n    }\n\n    /**\n     * Adds a compiled class file to the target jar stream.\n     *\n     * @param fromClass     The class to be added\n     * @param target        The target jar stream\n     */\n    private static void addFile(Class fromClass, JarOutputStream target) throws IOException {\n        String classEntry = fromClass.getName().replace('.', '/') + \".class\";\n        URL classURL = fromClass.getClassLoader().getResource(classEntry);\n        if (classURL != null) {\n            JarEntry entry = new JarEntry(classEntry);\n            target.putNextEntry(entry);\n            if (!classURL.toString().contains(\"!\")) {\n                String fileName = classURL.getFile();\n                writeBytes(fileName, target);\n            } else {\n                try (InputStream stream = fromClass.getClassLoader().getResourceAsStream(classEntry)) {\n                    writeBytes(stream, target);\n                }\n            }\n            target.closeEntry();\n        }\n    }\n\n    private static void writeBytes(String fileName, JarOutputStream target) throws IOException {\n        writeBytes(new File(fileName), target);\n    }\n\n    private static void writeBytes(File file, JarOutputStream target) throws IOException {\n        try (FileInputStream fileStream = new FileInputStream(file)) {\n            try (BufferedInputStream inputStream = new BufferedInputStream(fileStream)) {\n                byte[] buffer = new byte[1024];\n                while (true) {\n                    int count = inputStream.read(buffer);\n                    if (count == -1) {\n                        break;\n                    }\n                    target.write(buffer, 0, count);\n                }\n            }\n        }\n    }\n\n    private static void writeBytes(InputStream stream, JarOutputStream target) throws IOException {\n        byte[] buffer = new byte[1024];\n        while (true) {\n            int count = stream.read(buffer);\n            if (count == -1) {\n                break;\n            }\n            target.write(buffer, 0, count);\n        }\n    }\n\n    private static String normalize(String input) {\n        if (input == null || input.isEmpty()) {\n            throw new IllegalArgumentException(\"Empty value\");\n        }\n        return input.replace(\"-\", \".\").replace(\" \", \"_\");\n    }", "line_changes": {"deleted": [], "added": [{"line_no": 181, "char_start": 9879, "char_end": 9968, "line": "            if (!f.toPath().normalize().startsWith(unpackFolder.toPath().normalize())) {\n"}, {"line_no": 182, "char_start": 9968, "char_end": 10024, "line": "                throw new IOException(\"Bad zip entry\");\n"}, {"line_no": 183, "char_start": 10024, "char_end": 10038, "line": "            }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 9879, "char_end": 10038, "chars": "            if (!f.toPath().normalize().startsWith(unpackFolder.toPath().normalize())) {\n                throw new IOException(\"Bad zip entry\");\n            }\n"}]}, "commit_link": "github.com/senbox-org/snap-desktop/commit/274710f8ff846829262c9241dc928b1f9f7962f6", "file_name": "ModulePackager.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "40d8a38c425797d995d1c39bc09ba47a83062f69", "description": "Create a Java program to manage NetBeans module packaging and unpacking."}
{"func_name": "ZipMisc::unzip", "func_src_before": "\tpublic static void unzip(File input, File destinationDir) throws IOException {\n\t\ttry (ZipInputStream zipInput = new ZipInputStream(new BufferedInputStream(new FileInputStream(input)))) {\n\t\t\tZipEntry entry;\n\t\t\twhile ((entry = zipInput.getNextEntry()) != null) {\n\t\t\t\tFile dest = new File(destinationDir, entry.getName());\n\t\t\t\tif (entry.isDirectory()) {\n\t\t\t\t\tFileMisc.mkdirs(dest);\n\t\t\t\t} else {\n\t\t\t\t\tFileMisc.mkdirs(dest.getParentFile());\n\t\t\t\t\ttry (OutputStream output = new BufferedOutputStream(new FileOutputStream(dest))) {\n\t\t\t\t\t\tcopy(zipInput, output);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "func_src_after": "\tpublic static void unzip(File input, File destinationDir) throws IOException {\n\t\ttry (ZipInputStream zipInput = new ZipInputStream(new BufferedInputStream(new FileInputStream(input)))) {\n\t\t\tZipEntry entry;\n\t\t\twhile ((entry = zipInput.getNextEntry()) != null) {\n\t\t\t\tFile dest = new File(destinationDir, entry.getName());\n\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n\t\t\t\tif (entry.isDirectory()) {\n\t\t\t\t\tFileMisc.mkdirs(dest);\n\t\t\t\t} else {\n\t\t\t\t\tFileMisc.mkdirs(dest.getParentFile());\n\t\t\t\t\ttry (OutputStream output = new BufferedOutputStream(new FileOutputStream(dest))) {\n\t\t\t\t\t\tcopy(zipInput, output);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "line_changes": {"deleted": [], "added": [{"line_no": 6, "char_start": 321, "char_end": 407, "line": "\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n"}, {"line_no": 7, "char_start": 407, "char_end": 457, "line": "\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 8, "char_start": 457, "char_end": 463, "line": "\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 321, "char_end": 463, "chars": "\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n"}]}, "commit_link": "github.com/diffplug/goomph/commit/643474930339e5567745ba0695f2a8decf627a8c", "file_name": "ZipMisc.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "fe2083196f0d0a75885aea402baaa972254173d5", "description": "Write a Java function to extract the contents of a ZIP file to a specified directory."}
{"func_name": "DefaultArchiveExtractor::extract", "func_src_before": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "func_src_after": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory, entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 26, "char_start": 1467, "char_end": 1580, "line": "                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n"}, {"line_no": 55, "char_start": 2986, "char_end": 3102, "line": "                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n"}], "added": [{"line_no": 26, "char_start": 1467, "char_end": 1562, "line": "                        final File destPath = new File(destinationDirectory, entry.getName());\n"}, {"line_no": 27, "char_start": 1562, "char_end": 1657, "line": "                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n"}, {"line_no": 28, "char_start": 1657, "char_end": 1730, "line": "                            throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 29, "char_start": 1730, "char_end": 1756, "line": "                        }\n"}, {"line_no": 58, "char_start": 3162, "char_end": 3260, "line": "                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n"}]}, "char_changes": {"deleted": [{"char_start": 1542, "char_end": 1579, "chars": " + File.separator + entry.getName());"}, {"char_start": 3061, "char_end": 3080, "chars": " + File.separator +"}], "added": [{"char_start": 1542, "char_end": 1755, "chars": ", entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }"}, {"char_start": 3237, "char_end": 3238, "chars": ","}]}, "commit_link": "github.com/eirslett/frontend-maven-plugin/commit/9f53f7617b41d89cbef1a29342c6b6b7441fa78a", "file_name": "ArchiveExtractor.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java method to extract files from an archive (ZIP, MSI, or TAR.GZ) to a specified directory."}
{"func_name": "ZipUtil::checkDestinationFileForTraversal", "func_src_before": "  private static File checkDestinationFileForTraversal(File outputDir, String name, File destFile) throws IOException {\n    /* If we see the relative traversal string of \"..\" we need to make sure\n     * that the outputdir + name doesn't leave the outputdir. See\n     * DirectoryTraversalMaliciousTest for details.\n     */\n    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalPath().startsWith(outputDir.getCanonicalPath())) {\n      throw new MaliciousZipException(outputDir, name);\n    }\n    return destFile;\n  }", "func_src_after": "  private static File checkDestinationFileForTraversal(File outputDir, String name, File destFile) throws IOException {\n    /* If we see the relative traversal string of \"..\" we need to make sure\n     * that the outputdir + name doesn't leave the outputdir. See\n     * DirectoryTraversalMaliciousTest for details.\n     */\n    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalFile().toPath().startsWith(outputDir.getCanonicalFile().toPath())) {\n      throw new MaliciousZipException(outputDir, name);\n    }\n    return destFile;\n  }", "line_changes": {"deleted": [{"line_no": 6, "char_start": 322, "char_end": 431, "line": "    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalPath().startsWith(outputDir.getCanonicalPath())) {\n"}], "added": [{"line_no": 6, "char_start": 322, "char_end": 449, "line": "    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalFile().toPath().startsWith(outputDir.getCanonicalFile().toPath())) {\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 380, "char_end": 389, "chars": "File().to"}, {"char_start": 429, "char_end": 438, "chars": "File().to"}]}, "commit_link": "github.com/zeroturnaround/zt-zip/commit/627bbc93907ceb69111f86e2edf26375a1abccfa", "file_name": "ZipUtil.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "9b0818802c8fc804d75ef731da538423a7e020fa", "description": "Write a Java function to prevent directory traversal by validating a file's destination path against an intended output directory."}
{"func_name": "Html5ReportGenerator::unzipApp", "func_src_before": "    protected void unzipApp( File toDir ) throws IOException {\n        String appZipPath = \"/\" + Html5ReportGenerator.class.getPackage().getName().replace( '.', '/' ) + \"/app.zip\";\n\n        log.debug( \"Unzipping {}...\", appZipPath );\n\n        InputStream inputStream = this.getClass().getResourceAsStream( appZipPath );\n        ZipInputStream zipInputStream = new ZipInputStream( inputStream );\n\n        ZipEntry entry;\n        while( ( entry = zipInputStream.getNextEntry() ) != null ) {\n            File file = new File( toDir, entry.getName() );\n\n            if( entry.isDirectory() ) {\n                if( !file.exists() ) {\n                    log.debug( \"Creating directory {}...\", file );\n                    if( !file.mkdirs() ) {\n                        throw new IOException( \"Could not create directory \" + file );\n                    }\n                }\n                continue;\n            }\n            log.debug( \"Unzipping {}...\", file );\n\n            FileOutputStream fileOutputStream = new FileOutputStream( file );\n\n            byte[] buffer = new byte[1024];\n\n            int len;\n            while( ( len = zipInputStream.read( buffer ) ) > 0 ) {\n                fileOutputStream.write( buffer, 0, len );\n            }\n\n            fileOutputStream.close();\n        }\n    }", "func_src_after": "    protected void unzipApp( File toDir ) throws IOException {\n        String appZipPath = \"/\" + Html5ReportGenerator.class.getPackage().getName().replace( '.', '/' ) + \"/app.zip\";\n\n        log.debug( \"Unzipping {}...\", appZipPath );\n\n        InputStream inputStream = this.getClass().getResourceAsStream( appZipPath );\n        ZipInputStream zipInputStream = new ZipInputStream( inputStream );\n\n        ZipEntry entry;\n        while( ( entry = zipInputStream.getNextEntry() ) != null ) {\n            File file = new File( toDir, entry.getName() );\n            if(!file.toPath().normalize().startsWith(toDir.toPath().normalize())) {\n                throw new RuntimeException(\"Bad zip entry\");\n            }\n\n            if( entry.isDirectory() ) {\n                if( !file.exists() ) {\n                    log.debug( \"Creating directory {}...\", file );\n                    if( !file.mkdirs() ) {\n                        throw new IOException( \"Could not create directory \" + file );\n                    }\n                }\n                continue;\n            }\n            log.debug( \"Unzipping {}...\", file );\n\n            FileOutputStream fileOutputStream = new FileOutputStream( file );\n\n            byte[] buffer = new byte[1024];\n\n            int len;\n            while( ( len = zipInputStream.read( buffer ) ) > 0 ) {\n                fileOutputStream.write( buffer, 0, len );\n            }\n\n            fileOutputStream.close();\n        }\n    }", "line_changes": {"deleted": [], "added": [{"line_no": 12, "char_start": 549, "char_end": 633, "line": "            if(!file.toPath().normalize().startsWith(toDir.toPath().normalize())) {\n"}, {"line_no": 13, "char_start": 633, "char_end": 694, "line": "                throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 14, "char_start": 694, "char_end": 708, "line": "            }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 548, "char_end": 707, "chars": "\n            if(!file.toPath().normalize().startsWith(toDir.toPath().normalize())) {\n                throw new RuntimeException(\"Bad zip entry\");\n            }"}]}, "commit_link": "github.com/TNG/JGiven/commit/e701fe690501e7301f7c923adc1881d308806c46", "file_name": "Html5ReportGenerator.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to unzip a file named 'app.zip' from the resources of the `Html5ReportGenerator` class into a specified directory."}
{"func_name": "FileUtil::unzip", "func_src_before": "    public static void unzip(String zipFilePath, String targetPath, boolean safeUnzip) throws IOException {\n        targetPath = getCanonicalPath(new File(targetPath));\n        ZipFile zipFile = new ZipFile(zipFilePath);\n        try {\n            Enumeration<?> entryEnum = zipFile.entries();\n            while (entryEnum.hasMoreElements()) {\n                OutputStream os = null;\n                InputStream is = null;\n                try {\n                    ZipEntry zipEntry = (ZipEntry) entryEnum.nextElement();\n                    if (!zipEntry.isDirectory()) {\n                        if (safeUnzip && isNotSafeFile(zipEntry.getName())) {\n                            //Unsafe\n                            continue;\n                        }\n\n                        File targetFile = new File(targetPath + File.separator + zipEntry.getName());\n                        if (safeUnzip && !getCanonicalPath(targetFile).startsWith(targetPath)) {\n                            //Unsafe\n                            continue;\n                        }\n\n                        if (!targetFile.getParentFile().exists()) {\n                            targetFile.getParentFile().mkdirs();\n                        }\n                        os = new BufferedOutputStream(new FileOutputStream(targetFile));\n                        is = zipFile.getInputStream(zipEntry);\n                        byte[] buffer = new byte[4096];\n                        int readLen = 0;\n                        while ((readLen = is.read(buffer, 0, 4096)) > 0) {\n                            os.write(buffer, 0, readLen);\n                        }\n                    }\n                } finally {\n                    close(is, os);\n                }\n            }\n        } finally {\n            close(zipFile);\n        }\n    }", "func_src_after": "    public static void unzip(String zipFilePath, String targetPath, boolean safeUnzip) throws IOException {\n        targetPath = getCanonicalPath(new File(targetPath));\n        ZipFile zipFile = new ZipFile(zipFilePath);\n        try {\n            Enumeration<?> entryEnum = zipFile.entries();\n            while (entryEnum.hasMoreElements()) {\n                OutputStream os = null;\n                InputStream is = null;\n                try {\n                    ZipEntry zipEntry = (ZipEntry) entryEnum.nextElement();\n                    if (!zipEntry.isDirectory()) {\n                        if (safeUnzip && isNotSafeFile(zipEntry.getName())) {\n                            //Unsafe\n                            continue;\n                        }\n\n                        File targetFile = new File(targetPath, zipEntry.getName());\n\n                        if (!targetFile.toPath().normalize().startsWith(targetPath)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }\n                        if (safeUnzip && !getCanonicalPath(targetFile).startsWith(targetPath)) {\n                            //Unsafe\n                            continue;\n                        }\n\n                        if (!targetFile.getParentFile().exists()) {\n                            targetFile.getParentFile().mkdirs();\n                        }\n                        os = new BufferedOutputStream(new FileOutputStream(targetFile));\n                        is = zipFile.getInputStream(zipEntry);\n                        byte[] buffer = new byte[4096];\n                        int readLen = 0;\n                        while ((readLen = is.read(buffer, 0, 4096)) > 0) {\n                            os.write(buffer, 0, readLen);\n                        }\n                    }\n                } finally {\n                    close(is, os);\n                }\n            }\n        } finally {\n            close(zipFile);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 17, "char_start": 751, "char_end": 853, "line": "                        File targetFile = new File(targetPath + File.separator + zipEntry.getName());\n"}], "added": [{"line_no": 17, "char_start": 751, "char_end": 835, "line": "                        File targetFile = new File(targetPath, zipEntry.getName());\n"}, {"line_no": 18, "char_start": 835, "char_end": 836, "line": "\n"}, {"line_no": 19, "char_start": 836, "char_end": 923, "line": "                        if (!targetFile.toPath().normalize().startsWith(targetPath)) {\n"}, {"line_no": 20, "char_start": 923, "char_end": 996, "line": "                            throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 21, "char_start": 996, "char_end": 1022, "line": "                        }\n"}]}, "char_changes": {"deleted": [{"char_start": 812, "char_end": 831, "chars": " + File.separator +"}, {"char_start": 835, "char_end": 836, "chars": "E"}, {"char_start": 840, "char_end": 852, "chars": ".getName());"}], "added": [{"char_start": 812, "char_end": 1021, "chars": ", zipEntry.getName());\n\n                        if (!targetFile.toPath().normalize().startsWith(targetPath)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }"}]}, "commit_link": "github.com/yangfuhai/jboot/commit/bbfd672a6ff448946d0c7e99c103da8823d0c2b3", "file_name": "FileUtil.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "91cf3d36cc005d9f4c72a5773304990cf5e251c4", "description": "Write a Java function to extract files from a ZIP archive to a specified directory with an option for safe extraction."}
{"func_name": "Updater::updateModule", "func_src_before": "\tprivate void updateModule( final ModuleBean module, final boolean external, final boolean repair ) {\n\t\tLEnv.LOGGER.info( ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() + \"...\" );\n\t\tlauncherFrame.setStatus( StatusType.PROGRESS, ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() );\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart, show RESTART action while updating the launcher\n\t\t\tlauncherFrame.setProceedText( \"<html><h2>RE_START</h2></html>\" );\n\t\t}\n\t\t\n\t\tfinal Path tempPath = ( external ? LEnv.PATH_EXT_MODS : LEnv.PATH_MODS ).resolve( \"_update\" );\n\t\ttry {\n\t\t\t// Create temp update folder\n\t\t\twhile ( !LUtils.deletePath( tempPath ) )\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) )\n\t\t\t\t\tthrow new Exception( \"Could not delete folder: \" + tempPath );\n\t\t\tFiles.createDirectory( tempPath );\n\t\t\t\n\t\t\tfinal Path archivePath = tempPath.resolve( module.getArchiveFile().getPath() );\n\t\t\t\n\t\t\t// Download module archive, try mirrors if one fails\n\t\t\tfinal byte[] buffer = new byte[ 16_384 ]; // 16 KB work buffer\n\t\t\tfor ( int urlIdx = 0; urlIdx < module.getUrlList().size(); urlIdx++ ) {\n\t\t\t\tfinal String archiveSource = urlIdx == 0 ? \"main source\" : \" mirror #\" + urlIdx;\n\t\t\t\tfinal boolean lastArchiveUrl = urlIdx == module.getUrlList().size() - 1;\n\t\t\t\t\n\t\t\t\tLEnv.LOGGER.debug( \"Downloading archive from \" + archiveSource + \"...\" );\n\t\t\t\t\n\t\t\t\tboolean downloadOk = false;\n\t\t\t\tInputStream input = null;\n\t\t\t\tOutputStream output = null;\n\t\t\t\ttry {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Connecting...\" );\n\t\t\t\t\tlauncherFrame.setProgressMax( (int) module.getArchiveSize() );\n\t\t\t\t\tlauncherFrame.setProgress( 0 );\n\t\t\t\t\t\n\t\t\t\t\tfinal URLConnection archiveUrlConnection = new URL( module.getUrlList().get( urlIdx ) ).openConnection();\n\t\t\t\t\t\n\t\t\t\t\tinput = archiveUrlConnection.getInputStream();\n\t\t\t\t\toutput = Files.newOutputStream( archivePath );\n\t\t\t\t\t\n\t\t\t\t\tLEnv.LOGGER.debug( \"Downloading...\" );\n\t\t\t\t\tint totalBytesRead = 0;\n\t\t\t\t\tint bytesRead;\n\t\t\t\t\twhile ( ( bytesRead = input.read( buffer ) ) > 0 ) {\n\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\ttotalBytesRead += bytesRead;\n\t\t\t\t\t\tlauncherFrame.setProgress( totalBytesRead );\n\t\t\t\t\t}\n\t\t\t\t\toutput.flush();\n\t\t\t\t\t\n\t\t\t\t\tdownloadOk = true;\n\t\t\t\t\tLEnv.LOGGER.debug( \"Download complete.\" );\n\t\t\t\t\t\n\t\t\t\t} catch ( final Exception e ) {\n\t\t\t\t\tLEnv.LOGGER.warning(\n\t\t\t\t\t        \"Failed to download archive from \" + archiveSource + \"!\" + ( lastArchiveUrl ? \"\" : \" Proceeding to the next source.\" ), e );\n\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t} finally {\n\t\t\t\t\tif ( input != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tinput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinput = null;\n\t\t\t\t\t}\n\t\t\t\t\tif ( output != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\toutput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\toutput = null;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif ( downloadOk ) {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Checking SHA-256 checksum of the archive...\" );\n\t\t\t\t\tif ( module.getArchiveFile().getSha256().equals( LUtils.calculateFileSha256( archivePath ) ) ) {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum OK.\" );\n\t\t\t\t\t\tbreak; // Break archive URLs cycle\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum MISMATCH! The downloaded archive is discarded!\" );\n\t\t\t\t\t\twhile ( !LUtils.deletePath( archivePath ) )\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete file:\", archivePath ) )\n\t\t\t\t\t\t\t\tthrow new Exception( \"Could not delete file: \" + archivePath );\n\t\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tLEnv.LOGGER.debug( \"Proceeding to the next source.\" );\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( \"Extracting archive...\" );\n\t\t\tfinal InputStream ins = Files.newInputStream( archivePath ); // Input stream is \"out-sourced\" to a local var because\n\t\t\t                                                             // else a false resource leak is reported :S\n\t\t\ttry ( final ZipInputStream zipInput = new ZipInputStream( ins ) ) {\n\t\t\t\tfinal String pathPrefix = external ? \"Scelight/\" + LEnv.PATH_EXT_MODS.getFileName().toString() + \"/\" + module.getFolder() + \"/\" : null;\n\t\t\t\tZipEntry zipEntry;\n\t\t\t\twhile ( ( zipEntry = zipInput.getNextEntry() ) != null ) {\n\t\t\t\t\tif ( external && !zipEntry.isDirectory() ) {\n\t\t\t\t\t\t// Quarantine check\n\t\t\t\t\t\tif ( zipEntry.getName().indexOf( \"..\" ) >= 0 || !zipEntry.getName().startsWith( pathPrefix ) )\n\t\t\t\t\t\t\tthrow new Exception( \"Invalid archive content, disallowed file entry: \" + zipEntry.getName() );\n\t\t\t\t\t}\n\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\n\t\t\t\t\tif ( zipEntry.isDirectory() )\n\t\t\t\t\t\tFiles.createDirectories( entryFile );\n\t\t\t\t\telse {\n\t\t\t\t\t\tlong size = zipEntry.getSize();\n\t\t\t\t\t\ttry ( final OutputStream output = Files.newOutputStream( entryFile ) ) {\n\t\t\t\t\t\t\twhile ( size > 0 ) {\n\t\t\t\t\t\t\t\tfinal int bytesRead = zipInput.read( buffer );\n\t\t\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\t\t\tsize -= bytesRead;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\toutput.flush();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch ( final Exception e ) {\n\t\t\t\tthrow new Exception( \"Failed to extract archive!\", e );\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Extracting done.\" );\n\t\t\t\n\t\t\tfinal Path archiveAppPath = tempPath.resolve( \"Scelight\" );\n\t\t\tif ( !Files.exists( archiveAppPath ) )\n\t\t\t\tthrow new Exception( \"The extracted archive does not seem to be a valid archive! Aborting \" + ( repair ? \"repair\" : \"update\" ) + \"!\" );\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replacing/patching files...\" : \"Implanting files...\" );\n\t\t\tFiles.walkFileTree( archiveAppPath, new SimpleFileVisitor< Path >() {\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult preVisitDirectory( Path dir, BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\tdir = LEnv.PATH_APP.resolve( archiveAppPath.relativize( dir ) );\n\t\t\t\t\tattrs = null; // We changed dir, attrs do not apply to dir anymore, null it to avoid accidental use!\n\t\t\t\t\t\n\t\t\t\t\twhile ( !Files.exists( dir ) || !Files.isDirectory( dir ) ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t// Files.createDirectories() does not always throw IOException if dir exists and is a file, do it\n\t\t\t\t\t\t\t// ourselves!\n\t\t\t\t\t\t\tif ( Files.exists( dir ) && !Files.isDirectory( dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"File exists and is not a folder: \" + dir );\n\t\t\t\t\t\t\tFiles.createDirectories( dir );\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not create folder:\", dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not create folder: \" + dir, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult visitFile( final Path file, final BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\t// Launcher's class path entries are locked and therefore cannot be repaired from \"within\"!\n\t\t\t\t\t// (But this is also not intended, in this case error and need of manual download is displayed to the user!)\n\t\t\t\t\tif ( repair && module == modules.getLauncherMod() && launcher.isClassPathEntry( file.getFileName().toString() ) )\n\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\n\t\t\t\t\tfinal Path target = LEnv.PATH_APP.resolve( archiveAppPath.relativize( file ) );\n\t\t\t\t\twhile ( true ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFiles.copy( file, target, StandardCopyOption.REPLACE_EXISTING );\n\t\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not write file:\", target ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not write file: \" + target, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} );\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replace/patch complete.\" : \"Implantation complete.\" );\n\t\t\t\n\t\t} catch ( final Exception e ) {\n\t\t\tLEnv.LOGGER.error( \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"!\", e );\n\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"! See the Logs for details!\" );\n\t\t\tthrow new FinishException();\n\t\t} finally {\n\t\t\tLEnv.LOGGER.debug( \"Cleaning up...\" );\n\t\t\twhile ( !LUtils.deletePath( tempPath ) ) {\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) ) {\n\t\t\t\t\tLEnv.LOGGER.error( \"Could not delete folder: \" + tempPath );\n\t\t\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName()\n\t\t\t\t\t        + \"! See the Logs for details!\" );\n\t\t\t\t\tthrow new FinishException();\n\t\t\t\t}\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Cleanup complete.\" );\n\t\t}\n\t\t\n\t\t// Module updated / repaired successfully.\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName()\n\t\t\t        + \" to continue.\" );\n\t\t\tlauncherFrame.setStatus( StatusType.WARNING,\n\t\t\t        module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName() + \" to continue.\" );\n\t\t\tthrow new FinishException( true );\n\t\t} else {\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t\tlauncherFrame.setStatus( StatusType.PROGRESS, module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t}\n\t}", "func_src_after": "\tprivate void updateModule( final ModuleBean module, final boolean external, final boolean repair ) {\n\t\tLEnv.LOGGER.info( ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() + \"...\" );\n\t\tlauncherFrame.setStatus( StatusType.PROGRESS, ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() );\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart, show RESTART action while updating the launcher\n\t\t\tlauncherFrame.setProceedText( \"<html><h2>RE_START</h2></html>\" );\n\t\t}\n\t\t\n\t\tfinal Path tempPath = ( external ? LEnv.PATH_EXT_MODS : LEnv.PATH_MODS ).resolve( \"_update\" );\n\t\ttry {\n\t\t\t// Create temp update folder\n\t\t\twhile ( !LUtils.deletePath( tempPath ) )\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) )\n\t\t\t\t\tthrow new Exception( \"Could not delete folder: \" + tempPath );\n\t\t\tFiles.createDirectory( tempPath );\n\t\t\t\n\t\t\tfinal Path archivePath = tempPath.resolve( module.getArchiveFile().getPath() );\n\t\t\t\n\t\t\t// Download module archive, try mirrors if one fails\n\t\t\tfinal byte[] buffer = new byte[ 16_384 ]; // 16 KB work buffer\n\t\t\tfor ( int urlIdx = 0; urlIdx < module.getUrlList().size(); urlIdx++ ) {\n\t\t\t\tfinal String archiveSource = urlIdx == 0 ? \"main source\" : \" mirror #\" + urlIdx;\n\t\t\t\tfinal boolean lastArchiveUrl = urlIdx == module.getUrlList().size() - 1;\n\t\t\t\t\n\t\t\t\tLEnv.LOGGER.debug( \"Downloading archive from \" + archiveSource + \"...\" );\n\t\t\t\t\n\t\t\t\tboolean downloadOk = false;\n\t\t\t\tInputStream input = null;\n\t\t\t\tOutputStream output = null;\n\t\t\t\ttry {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Connecting...\" );\n\t\t\t\t\tlauncherFrame.setProgressMax( (int) module.getArchiveSize() );\n\t\t\t\t\tlauncherFrame.setProgress( 0 );\n\t\t\t\t\t\n\t\t\t\t\tfinal URLConnection archiveUrlConnection = new URL( module.getUrlList().get( urlIdx ) ).openConnection();\n\t\t\t\t\t\n\t\t\t\t\tinput = archiveUrlConnection.getInputStream();\n\t\t\t\t\toutput = Files.newOutputStream( archivePath );\n\t\t\t\t\t\n\t\t\t\t\tLEnv.LOGGER.debug( \"Downloading...\" );\n\t\t\t\t\tint totalBytesRead = 0;\n\t\t\t\t\tint bytesRead;\n\t\t\t\t\twhile ( ( bytesRead = input.read( buffer ) ) > 0 ) {\n\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\ttotalBytesRead += bytesRead;\n\t\t\t\t\t\tlauncherFrame.setProgress( totalBytesRead );\n\t\t\t\t\t}\n\t\t\t\t\toutput.flush();\n\t\t\t\t\t\n\t\t\t\t\tdownloadOk = true;\n\t\t\t\t\tLEnv.LOGGER.debug( \"Download complete.\" );\n\t\t\t\t\t\n\t\t\t\t} catch ( final Exception e ) {\n\t\t\t\t\tLEnv.LOGGER.warning(\n\t\t\t\t\t        \"Failed to download archive from \" + archiveSource + \"!\" + ( lastArchiveUrl ? \"\" : \" Proceeding to the next source.\" ), e );\n\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t} finally {\n\t\t\t\t\tif ( input != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tinput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinput = null;\n\t\t\t\t\t}\n\t\t\t\t\tif ( output != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\toutput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\toutput = null;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif ( downloadOk ) {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Checking SHA-256 checksum of the archive...\" );\n\t\t\t\t\tif ( module.getArchiveFile().getSha256().equals( LUtils.calculateFileSha256( archivePath ) ) ) {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum OK.\" );\n\t\t\t\t\t\tbreak; // Break archive URLs cycle\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum MISMATCH! The downloaded archive is discarded!\" );\n\t\t\t\t\t\twhile ( !LUtils.deletePath( archivePath ) )\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete file:\", archivePath ) )\n\t\t\t\t\t\t\t\tthrow new Exception( \"Could not delete file: \" + archivePath );\n\t\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tLEnv.LOGGER.debug( \"Proceeding to the next source.\" );\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( \"Extracting archive...\" );\n\t\t\tfinal InputStream ins = Files.newInputStream( archivePath ); // Input stream is \"out-sourced\" to a local var because\n\t\t\t                                                             // else a false resource leak is reported :S\n\t\t\ttry ( final ZipInputStream zipInput = new ZipInputStream( ins ) ) {\n\t\t\t\tfinal String pathPrefix = external ? \"Scelight/\" + LEnv.PATH_EXT_MODS.getFileName().toString() + \"/\" + module.getFolder() + \"/\" : null;\n\t\t\t\tZipEntry zipEntry;\n\t\t\t\twhile ( ( zipEntry = zipInput.getNextEntry() ) != null ) {\n\t\t\t\t\tif ( external && !zipEntry.isDirectory() ) {\n\t\t\t\t\t\t// Quarantine check\n\t\t\t\t\t\tif ( zipEntry.getName().indexOf( \"..\" ) >= 0 || !zipEntry.getName().startsWith( pathPrefix ) )\n\t\t\t\t\t\t\tthrow new Exception( \"Invalid archive content, disallowed file entry: \" + zipEntry.getName() );\n\t\t\t\t\t}\n\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\r\n\t\t\t\t\tif (!entryFile.normalize().startsWith(tempPath.normalize())) {\r\n\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\r\n\t\t\t\t\t}\n\t\t\t\t\tif ( zipEntry.isDirectory() )\n\t\t\t\t\t\tFiles.createDirectories( entryFile );\n\t\t\t\t\telse {\n\t\t\t\t\t\tlong size = zipEntry.getSize();\n\t\t\t\t\t\ttry ( final OutputStream output = Files.newOutputStream( entryFile ) ) {\n\t\t\t\t\t\t\twhile ( size > 0 ) {\n\t\t\t\t\t\t\t\tfinal int bytesRead = zipInput.read( buffer );\n\t\t\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\t\t\tsize -= bytesRead;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\toutput.flush();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch ( final Exception e ) {\n\t\t\t\tthrow new Exception( \"Failed to extract archive!\", e );\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Extracting done.\" );\n\t\t\t\n\t\t\tfinal Path archiveAppPath = tempPath.resolve( \"Scelight\" );\n\t\t\tif ( !Files.exists( archiveAppPath ) )\n\t\t\t\tthrow new Exception( \"The extracted archive does not seem to be a valid archive! Aborting \" + ( repair ? \"repair\" : \"update\" ) + \"!\" );\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replacing/patching files...\" : \"Implanting files...\" );\n\t\t\tFiles.walkFileTree( archiveAppPath, new SimpleFileVisitor< Path >() {\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult preVisitDirectory( Path dir, BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\tdir = LEnv.PATH_APP.resolve( archiveAppPath.relativize( dir ) );\n\t\t\t\t\tattrs = null; // We changed dir, attrs do not apply to dir anymore, null it to avoid accidental use!\n\t\t\t\t\t\n\t\t\t\t\twhile ( !Files.exists( dir ) || !Files.isDirectory( dir ) ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t// Files.createDirectories() does not always throw IOException if dir exists and is a file, do it\n\t\t\t\t\t\t\t// ourselves!\n\t\t\t\t\t\t\tif ( Files.exists( dir ) && !Files.isDirectory( dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"File exists and is not a folder: \" + dir );\n\t\t\t\t\t\t\tFiles.createDirectories( dir );\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not create folder:\", dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not create folder: \" + dir, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult visitFile( final Path file, final BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\t// Launcher's class path entries are locked and therefore cannot be repaired from \"within\"!\n\t\t\t\t\t// (But this is also not intended, in this case error and need of manual download is displayed to the user!)\n\t\t\t\t\tif ( repair && module == modules.getLauncherMod() && launcher.isClassPathEntry( file.getFileName().toString() ) )\n\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\n\t\t\t\t\tfinal Path target = LEnv.PATH_APP.resolve( archiveAppPath.relativize( file ) );\n\t\t\t\t\twhile ( true ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFiles.copy( file, target, StandardCopyOption.REPLACE_EXISTING );\n\t\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not write file:\", target ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not write file: \" + target, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} );\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replace/patch complete.\" : \"Implantation complete.\" );\n\t\t\t\n\t\t} catch ( final Exception e ) {\n\t\t\tLEnv.LOGGER.error( \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"!\", e );\n\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"! See the Logs for details!\" );\n\t\t\tthrow new FinishException();\n\t\t} finally {\n\t\t\tLEnv.LOGGER.debug( \"Cleaning up...\" );\n\t\t\twhile ( !LUtils.deletePath( tempPath ) ) {\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) ) {\n\t\t\t\t\tLEnv.LOGGER.error( \"Could not delete folder: \" + tempPath );\n\t\t\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName()\n\t\t\t\t\t        + \"! See the Logs for details!\" );\n\t\t\t\t\tthrow new FinishException();\n\t\t\t\t}\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Cleanup complete.\" );\n\t\t}\n\t\t\n\t\t// Module updated / repaired successfully.\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName()\n\t\t\t        + \" to continue.\" );\n\t\t\tlauncherFrame.setStatus( StatusType.WARNING,\n\t\t\t        module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName() + \" to continue.\" );\n\t\t\tthrow new FinishException( true );\n\t\t} else {\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t\tlauncherFrame.setStatus( StatusType.PROGRESS, module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t}\n\t}", "line_changes": {"deleted": [{"line_no": 108, "char_start": 4610, "char_end": 4678, "line": "\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\n"}], "added": [{"line_no": 108, "char_start": 4610, "char_end": 4679, "line": "\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\r\n"}, {"line_no": 109, "char_start": 4679, "char_end": 4748, "line": "\t\t\t\t\tif (!entryFile.normalize().startsWith(tempPath.normalize())) {\r\n"}, {"line_no": 110, "char_start": 4748, "char_end": 4800, "line": "\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\r\n"}, {"line_no": 111, "char_start": 4800, "char_end": 4807, "line": "\t\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 4677, "char_end": 4806, "chars": "\r\n\t\t\t\t\tif (!entryFile.normalize().startsWith(tempPath.normalize())) {\r\n\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\r\n\t\t\t\t\t}"}]}, "commit_link": "github.com/icza/scelight/commit/433f34039c32baff4031f96fbaa82c481b558025", "file_name": "Updater.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "49d2c7c831690ce56ff81d15c50923be61dbbd1f", "description": "Write a Java function to update or repair a software module, handling download, extraction, and file replacement."}
{"func_name": "JUtil::unpackJar", "func_src_before": "   public static void unpackJar(File fjar, File fout) throws IOException {\n    \n      JarFile jf = new JarFile(fjar);\n      Enumeration<JarEntry> en = jf.entries();\n\n      while (en.hasMoreElements()) {\n         JarEntry je = en.nextElement();\n         java.io.File f = new File(fout,  je.getName());\n         if (je.isDirectory()) {\n            f.mkdirs();\n            continue;\n\n         } else {\n            // f.getParentFile().mkdirs();\n\n            if (f.getPath().indexOf(\"META-INF\") >= 0) {\n               // skip it\n            } else {\n            f.getParentFile().mkdirs();\n            java.io.InputStream is = jf.getInputStream(je);\n            java.io.FileOutputStream fos = new FileOutputStream(f);\n\n            // EFF - buffering, file channels??\n            while (is.available() > 0) {\n               fos.write(is.read());\n            }\n            fos.close();\n            is.close();\n         }\n         }\n      }\n\n    //  E.info(\"unpacked jar to \" + fout);\n\n       \n   }", "func_src_after": "   public static void unpackJar(File fjar, File fout) throws IOException {\n    \n      JarFile jf = new JarFile(fjar);\n      Enumeration<JarEntry> en = jf.entries();\n\n      while (en.hasMoreElements()) {\n         JarEntry je = en.nextElement();\n         java.io.File f = new File(fout,  je.getName());\n\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t\t\t\t}\n         if (je.isDirectory()) {\n            f.mkdirs();\n            continue;\n\n         } else {\n            // f.getParentFile().mkdirs();\n\n            if (f.getPath().indexOf(\"META-INF\") >= 0) {\n               // skip it\n            } else {\n            f.getParentFile().mkdirs();\n            java.io.InputStream is = jf.getInputStream(je);\n            java.io.FileOutputStream fos = new FileOutputStream(f);\n\n            // EFF - buffering, file channels??\n            while (is.available() > 0) {\n               fos.write(is.read());\n            }\n            fos.close();\n            is.close();\n         }\n         }\n      }\n\n    //  E.info(\"unpacked jar to \" + fout);\n\n       \n   }", "line_changes": {"deleted": [], "added": [{"line_no": 9, "char_start": 301, "char_end": 377, "line": "\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n"}, {"line_no": 10, "char_start": 377, "char_end": 430, "line": "\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 11, "char_start": 430, "char_end": 439, "line": "\t\t\t\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 301, "char_end": 439, "chars": "\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t\t\t\t}\n"}]}, "commit_link": "github.com/LEMS/jLEMS/commit/8c224637d7d561076364a9e3c2c375daeaf463dc", "file_name": "JUtil.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to extract the contents of a JAR file to a specified directory, excluding META-INF directory."}
{"func_name": "(anonymous)", "func_src_before": "\thserver = require('http').createServer(function(req,res){\n\t\tconsole.log('Serving: %s',req.url);\n\t\tvar rs = fs.createReadStream(__dirname+req.url,{\n\t\t\tflags: 'r',\n\t\t\tautoClose: true\n\t\t});\n\t\trs.on('open',function(){\n\t\t\trs.pipe(res);\n\t\t});\n\t\trs.on('error',function(e){\n\t\t\tres.end(e+'');\n\t\t});\n\t}),", "func_src_after": "\thserver = require('http').createServer(function(req,res){\n\t\tconsole.log('Serving: %s',req.url);\n\t\tvar rs = fs.createReadStream(__dirname+path.normalize(req.url).replace(/^(\\.\\.[\\/\\\\])+/, ''),{\n\t\t\tflags: 'r',\n\t\t\tautoClose: true\n\t\t});\n\t\trs.on('open',function(){\n\t\t\trs.pipe(res);\n\t\t});\n\t\trs.on('error',function(e){\n\t\t\tres.end(e+'');\n\t\t});\n\t}),", "line_changes": {"deleted": [{"line_no": 3, "char_start": 97, "char_end": 148, "line": "\t\tvar rs = fs.createReadStream(__dirname+req.url,{\n"}], "added": [{"line_no": 3, "char_start": 97, "char_end": 194, "line": "\t\tvar rs = fs.createReadStream(__dirname+path.normalize(req.url).replace(/^(\\.\\.[\\/\\\\])+/, ''),{\n"}]}, "char_changes": {"deleted": [{"char_start": 138, "char_end": 145, "chars": "req.url"}], "added": [{"char_start": 138, "char_end": 191, "chars": "path.normalize(req.url).replace(/^(\\.\\.[\\/\\\\])+/, '')"}]}, "commit_link": "github.com/Eeems/PooledWebSocket/commit/7b3b4e5c6be6d8a964296fa3c50e38dc07e9701d", "file_name": "server.js", "vul_type": "cwe-022", "commit_msg": "Update server.js\n\nResolve directory traversal attack", "description": "Write a Node.js script to create a simple HTTP server that streams requested files to the client, handling both normal and sanitized file paths."}
{"func_name": "(anonymous)", "func_src_before": "  server((req, res) => {\n    const pathname = decodeURI(url.parse(req.url).pathname);\n    res.setHeader('access-control-allow-origin', '*');\n    if (reload && pathname === '/livereload') return serveReload(res);\n    if (!isRouteRequest(pathname)) return serveStaticFile(res, pathname);\n    return serveRoute(res, pathname);\n  }).listen(parseInt(port, 10));", "func_src_after": "  server((req, res) => {\n    const decodePathname = decodeURI(url.parse(req.url).pathname);\n    const pathname = path.normalize(decodePathname).replace(/^(\\.\\.(\\/|\\\\|$))+/, '');\n    res.setHeader('access-control-allow-origin', '*');\n    if (reload && pathname === '/livereload') return serveReload(res);\n    if (!isRouteRequest(pathname)) return serveStaticFile(res, pathname);\n    return serveRoute(res, pathname);\n  }).listen(parseInt(port, 10));", "line_changes": {"deleted": [{"line_no": 2, "char_start": 25, "char_end": 86, "line": "    const pathname = decodeURI(url.parse(req.url).pathname);\n"}], "added": [{"line_no": 2, "char_start": 25, "char_end": 92, "line": "    const decodePathname = decodeURI(url.parse(req.url).pathname);\n"}, {"line_no": 3, "char_start": 92, "char_end": 178, "line": "    const pathname = path.normalize(decodePathname).replace(/^(\\.\\.(\\/|\\\\|$))+/, '');\n"}]}, "char_changes": {"deleted": [{"char_start": 35, "char_end": 36, "chars": "p"}], "added": [{"char_start": 35, "char_end": 42, "chars": "decodeP"}, {"char_start": 92, "char_end": 178, "chars": "    const pathname = path.normalize(decodePathname).replace(/^(\\.\\.(\\/|\\\\|$))+/, '');\n"}]}, "commit_link": "github.com/lukejacksonn/http-server-spa/commit/ec7d824ea6903c0fb97a452045bbb8335198cbab", "file_name": "servor.js", "vul_type": "cwe-022", "commit_msg": "Prevent path traversal (#70)", "description": "Write a Node.js server that handles requests by serving static files, routes, or a live reload feature, and listens on a specified port."}
{"func_name": "(anonymous)", "func_src_before": "process.on('uncaughtException',function(e){\n\tconsole.error(e);\n\tconsole.trace(e.stack);\n});", "func_src_after": "process.on('uncaughtException',function(e){\n\tconsole.error(e);\n\tconsole.trace(e.stack);\n});", "line_changes": {"deleted": [{"line_no": 4, "char_start": 88, "char_end": 91, "line": "});\n"}], "added": []}, "char_changes": {"deleted": [], "added": []}, "commit_link": "github.com/Eeems/PooledWebSocket/commit/7b3b4e5c6be6d8a964296fa3c50e38dc07e9701d", "file_name": "server.js", "vul_type": "cwe-022", "commit_msg": "Update server.js\n\nResolve directory traversal attack", "description": "Write a Node.js code snippet that logs an error and its stack trace when an uncaught exception occurs."}
{"func_name": "(anonymous)", "func_src_before": "    app.get('/static/*', function(req, res)\n    { \n      res.header(\"Server\", serverName);\n      var filePath = path.normalize(__dirname + \"/..\" + req.url.split(\"?\")[0]);\n      res.sendfile(filePath, { maxAge: exports.maxAge });\n    });", "func_src_after": "    app.get('/static/*', function(req, res)\n    { \n      res.header(\"Server\", serverName);\n      var filePath = path.normalize(__dirname + \"/..\" +\n                                    req.url.replace(/\\./g, '').split(\"?\")[0]);\n      res.sendfile(filePath, { maxAge: exports.maxAge });\n    });", "line_changes": {"deleted": [{"line_no": 4, "char_start": 91, "char_end": 171, "line": "      var filePath = path.normalize(__dirname + \"/..\" + req.url.split(\"?\")[0]);\n"}], "added": [{"line_no": 4, "char_start": 91, "char_end": 147, "line": "      var filePath = path.normalize(__dirname + \"/..\" +\n"}, {"line_no": 5, "char_start": 147, "char_end": 226, "line": "                                    req.url.replace(/\\./g, '').split(\"?\")[0]);\n"}]}, "char_changes": {"deleted": [{"char_start": 146, "char_end": 154, "chars": " req.url"}], "added": [{"char_start": 146, "char_end": 209, "chars": "\n                                    req.url.replace(/\\./g, '')"}]}, "commit_link": "github.com/thomasrussellmurphy/etherpad-lite/commit/86d3b2ba811aa8168eb01a5a345d3b717889ab8a", "file_name": "server.js", "vul_type": "cwe-022", "commit_msg": "Fix directory traversal\n\nSee https://ada.adrianlang.de/etherpad-lite-directory-traversal", "description": "Create a Node.js Express server route that serves static files with a custom server header and cache control."}
{"func_name": "(anonymous)", "func_src_before": "  fs.writeFile(dbRoot+\"/\"+db+\"/md/\"+name, obj, function(err) {\n    if (err)\n      response(res, 500, 'write fail!');\n    else\n      response(res, 200, 'write success!');\n  })", "func_src_after": "app.post(\"/db/:db/:name\", function(req, res) {\n  var db = req.params.db;\n  var name = req.params.name;\n  var obj = req.body.obj;\n  var msg = \"db:\"+db+\" name:\"+name+\"\\n\"+obj;\n  c.log(msg);\n  var filename = path.join(dbRoot, db, 'md', name);\n  if (filename.indexOf(dbRoot) !== 0) { // \u6aa2\u67e5\u662f\u5426\u7a7f\u8d8adbRoot \u53c3\u8003\uff1ahttps://en.wikipedia.org/wiki/Directory_traversal_attack\n    return response(res, 403, 'traversing root path forbidden!');\n  }\n\n  fs.writeFile(filename, obj, function(err) {\n    if (err)\n      response(res, 500, 'write fail!');\n    else\n      response(res, 200, 'write success!');\n  })\n});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 63, "line": "  fs.writeFile(dbRoot+\"/\"+db+\"/md/\"+name, obj, function(err) {\n"}], "added": [{"line_no": 7, "char_start": 188, "char_end": 240, "line": "  var filename = path.join(dbRoot, db, 'md', name);\n"}, {"line_no": 8, "char_start": 240, "char_end": 356, "line": "  if (filename.indexOf(dbRoot) !== 0) { // \u6aa2\u67e5\u662f\u5426\u7a7f\u8d8adbRoot \u53c3\u8003\uff1ahttps://en.wikipedia.org/wiki/Directory_traversal_attack\n"}, {"line_no": 9, "char_start": 356, "char_end": 422, "line": "    return response(res, 403, 'traversing root path forbidden!');\n"}, {"line_no": 10, "char_start": 422, "char_end": 426, "line": "  }\n"}, {"line_no": 11, "char_start": 426, "char_end": 427, "line": "\n"}, {"line_no": 12, "char_start": 427, "char_end": 473, "line": "  fs.writeFile(filename, obj, function(err) {\n"}]}, "char_changes": {"deleted": [{"char_start": 0, "char_end": 36, "chars": "  fs.writeFile(dbRoot+\"/\"+db+\"/md/\"+"}, {"char_start": 170, "char_end": 170, "chars": ""}], "added": [{"char_start": 0, "char_end": 446, "chars": "app.post(\"/db/:db/:name\", function(req, res) {\n  var db = req.params.db;\n  var name = req.params.name;\n  var obj = req.body.obj;\n  var msg = \"db:\"+db+\" name:\"+name+\"\\n\"+obj;\n  c.log(msg);\n  var filename = path.join(dbRoot, db, 'md', name);\n  if (filename.indexOf(dbRoot) !== 0) { // \u6aa2\u67e5\u662f\u5426\u7a7f\u8d8adbRoot \u53c3\u8003\uff1ahttps://en.wikipedia.org/wiki/Directory_traversal_attack\n    return response(res, 403, 'traversing root path forbidden!');\n  }\n\n  fs.writeFile(file"}, {"char_start": 584, "char_end": 588, "chars": "\n});"}]}, "commit_link": "github.com/ccckmit/wikidown.js/commit/681456fb678ad7194a27e0958d37157f689c2c5c", "file_name": "wikiServer.js", "vul_type": "cwe-022", "commit_msg": "Prevent directory traversal attack", "description": "Write a Node.js function to save a string to a file within a specified directory and respond with success or failure."}
{"func_name": "build_path", "func_src_before": "def build_path(root_dir, path):\n    resolved_path = (Path(root_dir) / path).resolve()\n\n    if not resolved_path.is_relative_to(root_dir):\n        raise ValueError(f\"Attempted directory traversal: {path}\")\n\n    return resolved_path", "func_src_after": "def build_path(root_dir, path):\n    absolute_path = Path(root_dir) / path\n\n    if \"..\" in absolute_path.parts:\n        raise ValueError(f\"Attempted directory traversal: {path}\")\n\n    return absolute_path", "line_changes": {"deleted": [{"line_no": 2, "char_start": 32, "char_end": 86, "line": "    resolved_path = (Path(root_dir) / path).resolve()\n"}, {"line_no": 4, "char_start": 87, "char_end": 138, "line": "    if not resolved_path.is_relative_to(root_dir):\n"}, {"line_no": 7, "char_start": 206, "char_end": 230, "line": "    return resolved_path\n"}], "added": [{"line_no": 2, "char_start": 32, "char_end": 74, "line": "    absolute_path = Path(root_dir) / path\n"}, {"line_no": 4, "char_start": 75, "char_end": 111, "line": "    if \"..\" in absolute_path.parts:\n"}, {"line_no": 7, "char_start": 179, "char_end": 203, "line": "    return absolute_path\n"}]}, "char_changes": {"deleted": [{"char_start": 36, "char_end": 38, "chars": "re"}, {"char_start": 41, "char_end": 44, "chars": "ved"}, {"char_start": 52, "char_end": 53, "chars": "("}, {"char_start": 74, "char_end": 136, "chars": ").resolve()\n\n    if not resolved_path.is_relative_to(root_dir)"}, {"char_start": 217, "char_end": 219, "chars": "re"}, {"char_start": 222, "char_end": 225, "chars": "ved"}], "added": [{"char_start": 36, "char_end": 38, "chars": "ab"}, {"char_start": 41, "char_end": 44, "chars": "ute"}, {"char_start": 73, "char_end": 109, "chars": "\n\n    if \"..\" in absolute_path.parts"}, {"char_start": 190, "char_end": 192, "chars": "ab"}, {"char_start": 195, "char_end": 198, "chars": "ute"}]}, "commit_link": "github.com/cmusatyalab/deltaic/commit/3c8fb3f8f1b75a93a17198b863b44fa78589650d", "file_name": "coda.py", "vul_type": "cwe-022", "commit_msg": "Can't use pathlib.Path.resolve() to create absolute paths\n\nBecause do not want to traverse any (final?) symlink in the path.", "parent_commit": "4c5fe2b9a04849b48a0598b70dbcfcb27a54fee5", "description": "Write a Python function to safely concatenate a directory path with a relative path, raising an error if directory traversal is attempted."}
{"func_name": "__parse_jeos_images", "func_src_before": "    def __parse_jeos_images(self):\n        log = logging.getLogger('%s.%s' % (__name__, self.__class__.__name__))\n        # Loop through all JEOS configuration files to populate our jeos_images dictionary\n        config_path = self.configuration['jeos_config']\n        listing = os.listdir(config_path)\n        for infile in listing:\n            fileIN = open(config_path + infile, \"r\")\n            line = fileIN.readline()\n\n            while line:\n                if line[0] == \"#\":\n                    # Comment\n                    pass\n                if len(line.strip()) == 0:\n                    # Whitespace\n                    pass\n                image_detail = line.split(\":\")\n                if len(image_detail) >= 6:\n                    self.__add_jeos_image(image_detail)\n                else:\n                    log.warning(\"Found unparsable JEOS config line in (%s)\" % (config_path + infile))\n\n                line = fileIN.readline()", "func_src_after": "    def __parse_jeos_images(self):\n        log = logging.getLogger('%s.%s' % (__name__, self.__class__.__name__))\n        config_urls = self.configuration['jeos_config']\n        for url in config_urls:\n            filehandle = urlopen(url)\n            line = filehandle.readline().strip()\n\n            while line:\n                # Lines that start with '#' are a comment\n                if line[0] == \"#\":\n                    pass\n                # Lines that are zero length are whitespace\n                if len(line) == 0:\n                    pass\n                image_detail = line.split(\":\")\n                if len(image_detail) >= 6:\n                    self.__add_jeos_image(image_detail)\n                else:\n                    log.warning(\"Found unparsable line in JEOS config (%s)\" % url)\n\n                line = filehandle.readline()", "line_changes": {"deleted": [{"line_no": 4, "char_start": 205, "char_end": 261, "line": "        config_path = self.configuration['jeos_config']\n"}, {"line_no": 5, "char_start": 261, "char_end": 303, "line": "        listing = os.listdir(config_path)\n"}, {"line_no": 6, "char_start": 303, "char_end": 334, "line": "        for infile in listing:\n"}, {"line_no": 7, "char_start": 334, "char_end": 387, "line": "            fileIN = open(config_path + infile, \"r\")\n"}, {"line_no": 8, "char_start": 387, "char_end": 424, "line": "            line = fileIN.readline()\n"}, {"line_no": 14, "char_start": 539, "char_end": 582, "line": "                if len(line.strip()) == 0:\n"}, {"line_no": 21, "char_start": 808, "char_end": 910, "line": "                    log.warning(\"Found unparsable JEOS config line in (%s)\" % (config_path + infile))\n"}, {"line_no": 23, "char_start": 911, "char_end": 951, "line": "                line = fileIN.readline()\n"}], "added": [{"line_no": 3, "char_start": 114, "char_end": 170, "line": "        config_urls = self.configuration['jeos_config']\n"}, {"line_no": 4, "char_start": 170, "char_end": 202, "line": "        for url in config_urls:\n"}, {"line_no": 5, "char_start": 202, "char_end": 240, "line": "            filehandle = urlopen(url)\n"}, {"line_no": 6, "char_start": 240, "char_end": 289, "line": "            line = filehandle.readline().strip()\n"}, {"line_no": 13, "char_start": 492, "char_end": 527, "line": "                if len(line) == 0:\n"}, {"line_no": 19, "char_start": 720, "char_end": 803, "line": "                    log.warning(\"Found unparsable line in JEOS config (%s)\" % url)\n"}, {"line_no": 21, "char_start": 804, "char_end": 848, "line": "                line = filehandle.readline()\n"}]}, "char_changes": {"deleted": [{"char_start": 122, "char_end": 146, "chars": "# Loop through all JEOS "}, {"char_start": 154, "char_end": 385, "chars": "ation files to populate our jeos_images dictionary\n        config_path = self.configuration['jeos_config']\n        listing = os.listdir(config_path)\n        for infile in listing:\n            fileIN = open(config_path + infile, \"r\""}, {"char_start": 410, "char_end": 412, "chars": "IN"}, {"char_start": 465, "char_end": 614, "chars": "if line[0] == \"#\":\n                    # Comment\n                    pass\n                if len(line.strip()) == 0:\n                    # Whitespace"}, {"char_start": 870, "char_end": 908, "chars": "line in (%s)\" % (config_path + infile)"}, {"char_start": 938, "char_end": 940, "chars": "IN"}], "added": [{"char_start": 128, "char_end": 129, "chars": "_"}, {"char_start": 131, "char_end": 238, "chars": "ls = self.configuration['jeos_config']\n        for url in config_urls:\n            filehandle = urlopen(url"}, {"char_start": 263, "char_end": 269, "chars": "handle"}, {"char_start": 280, "char_end": 288, "chars": ".strip()"}, {"char_start": 330, "char_end": 526, "chars": "# Lines that start with '#' are a comment\n                if line[0] == \"#\":\n                    pass\n                # Lines that are zero length are whitespace\n                if len(line) == 0:"}, {"char_start": 770, "char_end": 778, "chars": "line in "}, {"char_start": 790, "char_end": 801, "chars": "(%s)\" % url"}, {"char_start": 831, "char_end": 837, "chars": "handle"}]}, "commit_link": "github.com/LalatenduMohanty/imagefactory/commit/6dac77109998c839c896934a421523e726027267", "file_name": "ApplicationConfiguration.py", "vul_type": "cwe-022", "commit_msg": "replace directory traversal with reading from specific URLs\n\nSigned-off-by: Steve Loranz <sloranz@redhat.com>", "parent_commit": "c85455ae57f80ea68cc485fb94ddac341be3f157", "description": "Write a Python function to parse JEOS image configuration data from a source and add valid entries to a dictionary."}
{"func_name": "job_browse", "func_src_before": "@gui.route(\"/job/<int:job_id>/browse\", defaults={\"path\": \"\"})\n@gui.route(\"/job/<int:job_id>/browse/<path:path>\")\n@login_required\ndef job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = os.path.join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)", "func_src_after": "@gui.route(\"/job/<int:job_id>/browse\", defaults={\"path\": \"\"})\n@gui.route(\"/job/<int:job_id>/browse/<path:path>\")\n@login_required\ndef job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = safe_join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)", "line_changes": {"deleted": [{"line_no": 24, "char_start": 691, "char_end": 739, "line": "    abs_path = os.path.join(job_base_dir, path)\n"}], "added": [{"line_no": 24, "char_start": 691, "char_end": 736, "line": "    abs_path = safe_join(job_base_dir, path)\n"}]}, "char_changes": {"deleted": [{"char_start": 706, "char_end": 714, "chars": "os.path."}], "added": [{"char_start": 706, "char_end": 711, "chars": "safe_"}]}, "commit_link": "github.com/ganga-devs/ganga/commit/730e7aba192407d35eb37dd7938d49071124be8c", "file_name": "routes.py", "vul_type": "cwe-022", "commit_msg": "# Absolute Path Traversal due to incorrect use of `send_file` call (#2025)\n\nA path traversal attack (also known as directory traversal) aims to access files and directories that are stored outside the web root folder. By manipulating variables that reference files with \u201cdot-dot-slash (../)\u201d sequences and its variations or by using absolute file paths, it may be possible to access arbitrary files and directories stored on file system including application source code or configuration and critical system files. This attack is also known as \u201cdot-dot-slash\u201d, \u201cdirectory traversal\u201d, \u201cdirectory climbing\u201d and \u201cbacktracking\u201d.\r\n\r\n## Common Weakness Enumeration category\r\nCWE - 36\r\n\r\n## Root Cause Analysis\r\n\r\nThe `os.path.join` call is unsafe for use with untrusted input. When the `os.path.join` call encounters an absolute path, it ignores all the parameters it has encountered till that point and starts working with the new absolute path.  Please see the example below.\r\n```\r\n>>> import os.path\r\n>>> static = \"path/to/mySafeStaticDir\"\r\n>>> malicious = \"/../../../../../etc/passwd\"\r\n>>> os.path.join(t,malicious)\r\n'/../../../../../etc/passwd'\r\n```\r\nSince the \"malicious\" parameter represents an absolute path, the result of `os.path.join` ignores the static directory completely. Hence, untrusted input is passed via the `os.path.join` call to `flask.send_file` can lead to path traversal attacks.\r\n\r\nIn this case, the problems occurs due to the following code :\r\nhttps://github.com/ganga-devs/ganga/blob/0c0f9e33b36ee7ead0855f1464f8d4efad26bdbc/ganga/GangaGUI/gui/routes.py#L671\r\n\r\nHere, the `path` parameter is attacker controlled. This parameter passes through the unsafe `os.path.join` call making the effective directory and filename passed to the `send_file` call attacker controlled. This leads to a path traversal attack.\r\n\r\n## Proof of Concept\r\n\r\nThe bug can be verified using a proof of concept similar to the one shown below.\r\n\r\n```\r\ncurl --path-as-is 'http://<domain>/job/<int:job_id>/browse///../../../../etc/passwd\"'\r\n```\r\n## Remediation\r\n\r\nThis can be fixed by preventing flow of untrusted data to the vulnerable `send_file` function. In case the application logic necessiates this behaviour, one can either use the `werkzeug.utils.safe_join` to join untrusted paths or replace `flask.send_file` calls with `flask.send_from_directory` calls.\r\n\r\n## Common Vulnerability Scoring System Vector\r\n\r\nThe attack can be carried over the network. A complex non-standard configuration or a specialized condition is not required for the attack to be successfully conducted. There is no user interaction required for successful execution. The attack can affect components outside the scope of the target module. The attack can be used to gain access to confidential files like passwords, login credentials and other secrets. It cannot be directly used to affect a change on a system resource. Hence has limited to no impact on integrity. Using this attack vector a attacker may make multiple requests for accessing huge files such as a database. This can lead to a partial system denial service. However, the impact on availability is quite low in this case. Taking this account an appropriate CVSS v3.1 vector would be\r\n\r\n(AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:N/A:L)[https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:N/A:L&version=3.1]\r\n\r\nThis gives it a base score of 9.3/10 and a severity rating of critical.\r\n\r\n## References\r\n* [OWASP Path Traversal](https://owasp.org/www-community/attacks/Path_Traversal)\r\n* github/securitylab#669\r\n\r\n### This bug was found using *[CodeQL by Github](https://codeql.github.com/)*\r\n\r\nCo-authored-by: Porcupiney Hairs <porucpiney.hairs@protonmail.com>", "description": "Create a Python Flask web route for browsing job directories and files, with user authentication required."}
{"func_name": "archive_directory", "func_src_before": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(top_dir, subdir, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in file_list:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n                return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "func_src_after": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(full_backup_path, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in files:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n            return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "line_changes": {"deleted": [{"line_no": 20, "char_start": 1003, "char_end": 1052, "line": "        fpath = os.path.join(top_dir, subdir, c)\n"}, {"line_no": 37, "char_start": 1561, "char_end": 1580, "line": "    file_list = []\n"}, {"line_no": 38, "char_start": 1580, "char_end": 1623, "line": "    for p in os.listdir(full_backup_path):\n"}, {"line_no": 39, "char_start": 1623, "char_end": 1685, "line": "        if os.path.isfile(os.path.join(full_backup_path, p)):\n"}, {"line_no": 40, "char_start": 1685, "char_end": 1749, "line": "            file_list.append(os.path.join(full_backup_path, p))\n"}, {"line_no": 44, "char_start": 1876, "char_end": 1908, "line": "            for f in file_list:\n"}, {"line_no": 47, "char_start": 2050, "char_end": 2091, "line": "                return archive_file_path\n"}], "added": [{"line_no": 20, "char_start": 1003, "char_end": 1053, "line": "        fpath = os.path.join(full_backup_path, c)\n"}, {"line_no": 40, "char_start": 1689, "char_end": 1717, "line": "            for f in files:\n"}, {"line_no": 43, "char_start": 1859, "char_end": 1896, "line": "            return archive_file_path\n"}]}, "char_changes": {"deleted": [{"char_start": 1032, "char_end": 1047, "chars": "top_dir, subdir"}, {"char_start": 1560, "char_end": 1748, "chars": "\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))"}, {"char_start": 1901, "char_end": 1906, "chars": "_list"}, {"char_start": 2050, "char_end": 2054, "chars": "    "}], "added": [{"char_start": 1032, "char_end": 1048, "chars": "full_backup_path"}, {"char_start": 1714, "char_end": 1715, "chars": "s"}]}, "commit_link": "github.com/calmcl1/cupo-backup/commit/f9047a52ab33a14fcd67d3d8b9f9d321502ae457", "file_name": "cupo.py", "vul_type": "cwe-022", "commit_msg": "Removed redundant directory traversal", "parent_commit": "49107dd052b985e1fc469aec359f37df23ec3e29", "description": "Write a Python function to zip files in a specified subdirectory, excluding '.ini' files, and save the archive to a temporary directory."}
{"func_name": "check_max_open_files", "func_src_before": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = len([\n        key for key in os.listdir(accepted_keys_dir) if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ])\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "func_src_after": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "line_changes": {"deleted": [{"line_no": 15, "char_start": 610, "char_end": 637, "line": "    accepted_count = len([\n"}, {"line_no": 16, "char_start": 637, "char_end": 693, "line": "        key for key in os.listdir(accepted_keys_dir) if\n"}, {"line_no": 17, "char_start": 693, "char_end": 754, "line": "        os.path.isfile(os.path.join(accepted_keys_dir, key))\n"}, {"line_no": 18, "char_start": 754, "char_end": 761, "line": "    ])\n"}], "added": [{"line_no": 15, "char_start": 610, "char_end": 677, "line": "    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n"}]}, "char_changes": {"deleted": [{"char_start": 631, "char_end": 648, "chars": "len([\n        key"}, {"char_start": 653, "char_end": 656, "chars": "key"}, {"char_start": 689, "char_end": 759, "chars": " if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ]"}], "added": [{"char_start": 631, "char_end": 636, "chars": "sum(1"}, {"char_start": 641, "char_end": 642, "chars": "_"}]}, "commit_link": "github.com/saltstack/salt/commit/887017fa0a5b691b40dbc1831cd4472e88157733", "file_name": "verify.py", "vul_type": "cwe-022", "commit_msg": "Do not waste CPU cycles on stat(2) on each _auth\n\nCurrently servers that handle many thousands of minions spend measurable time\ndoing only stat(2) calls.\n\nIn strace it looks like::\n\n     0.000093 stat(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=118, ...}) = 0\n     0.000236 open(\"/etc/salt/pki/master/minions\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 154\n     0.011412 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000102 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     ...many thousands lines...\n     0.000064 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000062 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=796, ...}) = 0\n     0.000485 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000065 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000184 stat(\"/etc/salt/pki/master/minions_rejected/hostXXXX.linkedin.com\", 0x7fff28209f40) = -1 ENOENT (No such file or directory)\n     0.000071 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000074 open(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", O_RDONLY) = 154\n\nHappens that on each _auth() call salt is counting files in pki/master/minions\nand checks whenever those are regular files by applying os.path.isfile()\nfunction to each which considerably slows things down.\n\nThis patch removes isfile() call that effectively makes check_max_open_files()\ncheck less precise (but since it's already subject to external race conditions\nlike creation/removal of files by another process it should be OK) in exchange\nfor making it much faster.\n\nPS. Note that calling salt.utils.verify.check_max_open_files() on each _auth is\nnot really efficient, it probably should be done periodically in a background\nthread.\n\nSponsored by: LinkedIn\nSigned-off-by: Alexey Ivanov <SaveTheRbtz@GMail.com>", "parent_commit": "26edc398f706b4266fd9cfdf886d15ab0e32049b", "description": "Write a Python function to evaluate and log system resource limits against the number of accepted keys for a service."}
{"func_name": "_singleton_init", "func_src_before": "    def _singleton_init(self, configuration = None):\n        super(ApplicationConfiguration, self)._singleton_init()\n        self.log = logging.getLogger('%s.%s' % (__name__, self.__class__.__name__))\n        self.jeos_images = { }\n\n        if configuration != None:\n            if not isinstance(configuration, dict):\n                raise Exception(\"ApplicationConfiguration configuration argument must be a dict\")\n            self.log.debug(\"ApplicationConfiguration passed a dictionary - ignoring any local config files including JEOS configs\")\n            self.configuration = configuration\n        else:\n            self.configuration = self.__parse_arguments()\n            self.__parse_jeos_images()\n\n        if not 'debug' in self.configuration:\n            if 'nodebug' in self.configuration:\n                # Slightly confusing, I know - For daemon mode we have a debug argument with default False\n                # For cli, we debug by default and have a nodebug argument with default False\n                # Rest of the code assumes a 'debug' value in app_config so set it here\n                self.configuration['debug'] = not self.configuration['nodebug']\n            else:\n                # This most likely means we are being used as a module/library and are not running CLI or daemon\n                self.configuration['debug'] = False\n\n        if not 'secondary' in self.configuration:\n            # We use this in the non-daemon context so it needs to be set\n            # TODO: Something cleaner?\n            self.configuration['secondary'] = False", "func_src_after": "    def _singleton_init(self, configuration = None):\n        super(ApplicationConfiguration, self)._singleton_init()\n        self.log = logging.getLogger('%s.%s' % (__name__, self.__class__.__name__))\n        self.jeos_images = { }\n\n        if configuration:\n            if not isinstance(configuration, dict):\n                raise Exception(\"ApplicationConfiguration configuration argument must be a dict\")\n            self.log.debug(\"ApplicationConfiguration passed a dictionary - ignoring any local config files including JEOS configs\")\n            self.configuration = configuration\n        else:\n            self.configuration = self.__parse_arguments()\n            self.__parse_jeos_images()\n\n        if not 'debug' in self.configuration:\n            if 'nodebug' in self.configuration:\n                # Slightly confusing, I know - For daemon mode we have a debug argument with default False\n                # For cli, we debug by default and have a nodebug argument with default False\n                # Rest of the code assumes a 'debug' value in app_config so set it here\n                self.configuration['debug'] = not self.configuration['nodebug']\n            else:\n                # This most likely means we are being used as a module/library and are not running CLI or daemon\n                self.configuration['debug'] = False\n\n        if not 'secondary' in self.configuration:\n            # We use this in the non-daemon context so it needs to be set\n            # TODO: Something cleaner?\n            self.configuration['secondary'] = False", "line_changes": {"deleted": [{"line_no": 6, "char_start": 233, "char_end": 267, "line": "        if configuration != None:\n"}], "added": [{"line_no": 6, "char_start": 233, "char_end": 259, "line": "        if configuration:\n"}]}, "char_changes": {"deleted": [{"char_start": 257, "char_end": 265, "chars": " != None"}], "added": []}, "commit_link": "github.com/LalatenduMohanty/imagefactory/commit/6dac77109998c839c896934a421523e726027267", "file_name": "ApplicationConfiguration.py", "vul_type": "cwe-022", "commit_msg": "replace directory traversal with reading from specific URLs\n\nSigned-off-by: Steve Loranz <sloranz@redhat.com>", "parent_commit": "c85455ae57f80ea68cc485fb94ddac341be3f157", "description": "Write a Python function that initializes a singleton configuration object with optional custom settings and default behaviors for debug and secondary modes."}
{"func_name": "send_file", "func_src_before": "@app.route('/output_files/<path:filename>', methods = ['GET', 'OPTIONS'])\ndef send_file(filename):\n  if not controller:\n    return createCrossOriginResponse(\n        status=403, body='Instance already shut down!')\n  elif controller.is_vod():\n    # If streaming mode is vod, needs to wait until packager is completely\n    # done packaging contents.\n    while True:\n      status = controller.check_status()\n      if status == node_base.ProcessStatus.Finished:\n        break\n      elif status != node_base.ProcessStatus.Running:\n        return createCrossOriginResponse(\n            status=500, body='Some processes exited with non-zero exit codes')\n\n      time.sleep(1)\n  else:\n    # If streaming mode is live, needs to wait for specific content in\n    # manifest until it can be loaded by the player.\n    if filename.endswith('.mpd'):\n      while not dashStreamsReady(OUTPUT_DIR + filename):\n        time.sleep(1)\n    elif filename.endswith('.m3u8') and not filename.startswith('stream_'):\n      while not hlsStreamsReady(OUTPUT_DIR + filename):\n        time.sleep(1)\n\n  # Sending over requested files.\n  try:\n    response = flask.send_file(OUTPUT_DIR + filename)\n  except FileNotFoundError:\n    response = flask.Response(response='File not found', status=404)\n\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  response.headers.add('Access-Control-Allow-Headers', 'RANGE')\n  return response", "func_src_after": "@app.route('/output_files/<path:filename>', methods = ['GET', 'OPTIONS'])\ndef send_file(filename):\n  filename = secure_filename(filename)\n  if not controller:\n    return createCrossOriginResponse(\n        status=403, body='Instance already shut down!')\n  elif controller.is_vod():\n    # If streaming mode is vod, needs to wait until packager is completely\n    # done packaging contents.\n    while True:\n      status = controller.check_status()\n      if status == node_base.ProcessStatus.Finished:\n        break\n      elif status != node_base.ProcessStatus.Running:\n        return createCrossOriginResponse(\n            status=500, body='Some processes exited with non-zero exit codes')\n\n      time.sleep(1)\n  else:\n    # If streaming mode is live, needs to wait for specific content in\n    # manifest until it can be loaded by the player.\n    if filename.endswith('.mpd'):\n      while not dashStreamsReady(OUTPUT_DIR + filename):\n        time.sleep(1)\n    elif filename.endswith('.m3u8') and not filename.startswith('stream_'):\n      while not hlsStreamsReady(OUTPUT_DIR + filename):\n        time.sleep(1)\n\n  # Sending over requested files.\n  try:\n    response = flask.send_file(OUTPUT_DIR + filename)\n  except FileNotFoundError:\n    response = flask.Response(response='File not found', status=404)\n\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  response.headers.add('Access-Control-Allow-Headers', 'RANGE')\n  return response", "line_changes": {"deleted": [], "added": [{"line_no": 3, "char_start": 99, "char_end": 138, "line": "  filename = secure_filename(filename)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 99, "char_end": 138, "chars": "  filename = secure_filename(filename)\n"}]}, "commit_link": "github.com/shaka-project/shaka-streamer/commit/1d0471f53c187a87c76512aa8fc7d719d47ffc2c", "file_name": "run_end_to_end_tests.py", "vul_type": "cwe-022", "commit_msg": "Fix arbitrary file reading vulnerabilities in test runner (#104)\n\nUse `werkzeug.utils.secure_filename` to fix arbitrary file reading vulnerabilities.\r\n\r\nVulnerability affects test runner only.\r\n\r\nCloses #102", "parent_commit": "a86da1b96c0134ea1e8c2f4f1d33520efe7aa73c", "description": "Write a Python Flask endpoint to serve video files, handling both VOD and live streaming scenarios."}
{"func_name": "copy_over", "func_src_before": "def copy_over(source, dest):\n    \"\"\"\n    Copies from the source to the destination, removing the destination\n    if it exists and is a directory.\n    \"\"\"\n    if os.path.exists(dest) and os.path.isdir(dest):\n        shutil.rmtree(dest)\n    shutil.copytree(source, dest)\n    # mkdtemp will set the directory permissions to 700\n    # for the webserver to read them, we need 755\n    os.chmod(dest, stat.S_IRWXU | stat.S_IRGRP |\n             stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n    shutil.rmtree(source)", "func_src_after": "def copy_over(source, dest):\n    \"\"\"\n    Copies from the source to the destination, removing the destination\n    if it exists and is a directory.\n    \"\"\"\n    if os.path.exists(dest) and os.path.isdir(dest):\n        shutil.rmtree(dest)\n    shutil.copytree(force_bytes(source), force_bytes(dest))\n    # mkdtemp will set the directory permissions to 700\n    # for the webserver to read them, we need 755\n    os.chmod(dest, stat.S_IRWXU | stat.S_IRGRP |\n             stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n    shutil.rmtree(source)", "line_changes": {"deleted": [{"line_no": 8, "char_start": 235, "char_end": 269, "line": "    shutil.copytree(source, dest)\n"}], "added": [{"line_no": 8, "char_start": 235, "char_end": 295, "line": "    shutil.copytree(force_bytes(source), force_bytes(dest))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 255, "char_end": 267, "chars": "force_bytes("}, {"char_start": 273, "char_end": 274, "chars": ")"}, {"char_start": 276, "char_end": 288, "chars": "force_bytes("}, {"char_start": 293, "char_end": 294, "chars": ")"}]}, "commit_link": "github.com/mozilla/addons-server/commit/e8731de25ab5319c82efb9efbe0195ba95e5dc1e", "file_name": "utils.py", "vul_type": "cwe-022", "commit_msg": "Fix unicode error on copying over extracted files from zip. (#3874)\n\nFix unicode error on copying over extracted files from zip.\r\n\r\nRefs #3579\r\n\r\nThe problem here is that under very hard unit-testable/reproducable\r\ncircumstances a zip-file with unicode filenames get's extracted to a\r\ntemporary directory and while calling `copy_over` which calls\r\n`shutil.copytree` which fails on our production systems with a\r\nUnicodeDecodeError.\r\n\r\nReproducing this is something along the lines of\r\n\r\n>>> shutil.copytree(u'/tmp/tmp8WG6A8/', u'/tmp/tmp8WG6A8-2/')\r\n# doesn't error...\r\n>>> shutil.copytree('/tmp/tmp8WG6A8/', u'/tmp/tmp8WG6A8-2/')\r\nUnicodeDecodeError: 'ascii' codec can't decode \tbyte 0xd0 in ...\r\n\r\nChecking all relevant environment configs in our production systems, all\r\nLANG, LC_ALL and related configs are perfectly fine. It's not related to\r\nuwsgi, it's not related to EFS so it's something else I have absolutely\r\nno idea about :-/", "parent_commit": "18d3e83979a8c9616f6d1173f82f422da6f2e1c2", "description": "Write a Python function to replace a destination directory with a source directory, adjusting permissions for webserver access."}
{"func_name": "update_dir_from_tar", "func_src_before": "def update_dir_from_tar(tar, root_dir):\n    directories = []\n    valid_paths = BloomSet()\n    for entry in tar:\n        # Convert entry type to stat constant\n        if entry.type == tarfile.DIRTYPE:\n            entry_stat_type = stat.S_IFDIR\n        elif entry.type in (tarfile.REGTYPE, tarfile.LNKTYPE):\n            # Coda apparently doesn't allow hard links to symlinks\n            entry_stat_type = stat.S_IFREG\n        elif entry.type == tarfile.SYMTYPE:\n            entry_stat_type = stat.S_IFLNK\n        else:\n            raise ValueError(f\"Unexpected file type {entry.type}\")\n\n        # Check for existing file\n        path = build_path(root_dir, entry.name)\n        try:\n            st: Optional[os.stat_result] = os.lstat(path)\n        except OSError:\n            st = None\n\n        # If entry has changed types, remove the old object\n        if st is not None and stat.S_IFMT(st.st_mode) != entry_stat_type:\n            if stat.S_ISDIR(st.st_mode):\n                shutil.rmtree(path)\n            else:\n                os.unlink(path)\n            st = None\n\n        # Create parent directory if not present.  Parents are not\n        # necessarily dumped before children.\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create new object\n        if entry.isdir():\n            if not os.path.exists(path):\n                print(\"d\", path)\n                os.mkdir(path)\n            # Go back and set mtime after directory has been populated\n            directories.append(entry)\n        elif entry.isfile():\n            # update_file() will break hard links if it modifies the file.\n            # This is what we want because links may have also been broken\n            # at the source.  codadump2tar always dumps hard links, so we\n            # will rebuild any links that should still exist.\n            if update_file(path, TarMemberFile(tar, entry)):\n                print(\"f\", path)\n        elif entry.issym():\n            if st is None or entry.linkname and os.readlink(path) != entry.linkname:\n                print(\"s\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.symlink(entry.linkname, path)\n        elif entry.islnk():\n            target_path = build_path(root_dir, entry.linkname)\n            target_st = os.lstat(target_path)\n            if (\n                st is None\n                or st.st_dev != target_st.st_dev\n                or st.st_ino != target_st.st_ino\n            ):\n                print(\"l\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.link(target_path, path)\n\n        # Update metadata\n        attrs = XAttrs(path)\n        # owner and mode.  Hardlinks were updated with the primary, and we\n        # can't set xattrs on symlinks.\n        if entry.isfile() or entry.isdir():\n            # rsync --fake-super compatible:\n            # octal_mode_with_type major,minor uid:gid\n            mode = entry_stat_type | entry.mode\n            attrs.update(\n                ATTR_STAT,\n                f\"{mode:o} 0,0 {entry.uid}:{entry.gid}\",\n            )\n        # mtime.  Directories will be updated later, and hardlinks were\n        # updated with the primary.\n        if entry.isfile() or entry.issym() and os.lstat(path).st_mtime != entry.mtime:\n            lutime(path, entry.mtime)\n\n        # Protect from garbage collection\n        valid_paths.add(str(path))\n\n    # Deferred update of directory mtimes\n    for entry in directories:\n        path = build_path(root_dir, entry.name)\n        if os.stat(path).st_mtime != entry.mtime:\n            os.utime(path, (entry.mtime, entry.mtime))\n\n    return valid_paths", "func_src_after": "def update_dir_from_tar(tar, root_dir):\n    directories = []\n    valid_paths = BloomSet()\n    for entry in tar:\n        # Convert entry type to stat constant\n        if entry.type == tarfile.DIRTYPE:\n            entry_stat_type = stat.S_IFDIR\n        elif entry.type in (tarfile.REGTYPE, tarfile.LNKTYPE):\n            # Coda apparently doesn't allow hard links to symlinks\n            entry_stat_type = stat.S_IFREG\n        elif entry.type == tarfile.SYMTYPE:\n            entry_stat_type = stat.S_IFLNK\n        else:\n            raise ValueError(f\"Unexpected file type {entry.type}\")\n\n        # Check for existing file\n        path = build_path(root_dir, entry.name)\n        try:\n            st: Optional[os.stat_result] = os.lstat(path)\n        except OSError:\n            st = None\n\n        # If entry has changed types, remove the old object\n        if st is not None and stat.S_IFMT(st.st_mode) != entry_stat_type:\n            if stat.S_ISDIR(st.st_mode):\n                shutil.rmtree(path)\n            else:\n                os.unlink(path)\n            st = None\n\n        # Create parent directory if not present.  Parents are not\n        # necessarily dumped before children.\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create new object\n        if entry.isdir():\n            if not os.path.exists(path):\n                print(\"d\", path)\n                os.mkdir(path)\n            # Go back and set mtime after directory has been populated\n            directories.append(entry)\n        elif entry.isfile():\n            # update_file() will break hard links if it modifies the file.\n            # This is what we want because links may have also been broken\n            # at the source.  codadump2tar always dumps hard links, so we\n            # will rebuild any links that should still exist.\n            if update_file(path, TarMemberFile(tar, entry)):\n                print(\"f\", path)\n        elif entry.issym():\n            if st is None or (entry.linkname and os.readlink(path) != entry.linkname):\n                print(\"s\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.symlink(entry.linkname, path)\n        elif entry.islnk():\n            target_path = build_path(root_dir, entry.linkname)\n            target_st = os.lstat(target_path)\n            if (\n                st is None\n                or st.st_dev != target_st.st_dev\n                or st.st_ino != target_st.st_ino\n            ):\n                print(\"l\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.link(target_path, path)\n\n        # Update metadata\n        attrs = XAttrs(path)\n        # owner and mode.  Hardlinks were updated with the primary, and we\n        # can't set xattrs on symlinks.\n        if entry.isfile() or entry.isdir():\n            # rsync --fake-super compatible:\n            # octal_mode_with_type major,minor uid:gid\n            mode = entry_stat_type | entry.mode\n            attrs.update(\n                ATTR_STAT,\n                f\"{mode:o} 0,0 {entry.uid}:{entry.gid}\",\n            )\n        # mtime.  Directories will be updated later, and hardlinks were\n        # updated with the primary.\n        if (entry.isfile() or entry.issym()) and os.lstat(path).st_mtime != entry.mtime:\n            lutime(path, entry.mtime)\n\n        # Protect from garbage collection\n        valid_paths.add(str(path))\n\n    # Deferred update of directory mtimes\n    for entry in directories:\n        path = build_path(root_dir, entry.name)\n        if os.stat(path).st_mtime != entry.mtime:\n            os.utime(path, (entry.mtime, entry.mtime))\n\n    return valid_paths", "line_changes": {"deleted": [{"line_no": 50, "char_start": 1943, "char_end": 2028, "line": "            if st is None or entry.linkname and os.readlink(path) != entry.linkname:\n"}, {"line_no": 82, "char_start": 3217, "char_end": 3304, "line": "        if entry.isfile() or entry.issym() and os.lstat(path).st_mtime != entry.mtime:\n"}], "added": [{"line_no": 50, "char_start": 1943, "char_end": 2030, "line": "            if st is None or (entry.linkname and os.readlink(path) != entry.linkname):\n"}, {"line_no": 82, "char_start": 3219, "char_end": 3308, "line": "        if (entry.isfile() or entry.issym()) and os.lstat(path).st_mtime != entry.mtime:\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1972, "char_end": 1973, "chars": "("}, {"char_start": 2027, "char_end": 2028, "chars": ")"}, {"char_start": 3230, "char_end": 3231, "chars": "("}, {"char_start": 3262, "char_end": 3263, "chars": ")"}]}, "commit_link": "github.com/cmusatyalab/deltaic/commit/3c8fb3f8f1b75a93a17198b863b44fa78589650d", "file_name": "coda.py", "vul_type": "cwe-022", "commit_msg": "Can't use pathlib.Path.resolve() to create absolute paths\n\nBecause do not want to traverse any (final?) symlink in the path.", "parent_commit": "4c5fe2b9a04849b48a0598b70dbcfcb27a54fee5", "description": "Write a Python function to update a directory structure from a TAR file, handling file types and metadata."}
{"func_name": "testAllConditionalPaths", "func_src_before": "  def testAllConditionalPaths(self):\n    \"\"\"Enable all paths to get line coverage.\"\"\"\n    self.build_config['vm_tests'] = constants.SIMPLE_AU_TEST_TYPE\n    self.options.tests = True\n    self.build_config['build_type'] = constants.BUILD_FROM_SOURCE_TYPE\n    self.build_config['build_tests'] = True\n    self.build_config['archive_build_debug'] = True\n    self.build_config['usepkg_chroot'] = True\n    self.build_config['usepkg_setup_board'] = True\n    self.build_config['usepkg_build_packages'] = True\n    self.build_config['images'] = ['base', 'dev', 'test', 'factory_test',\n                                   'factory_install']\n    self.build_config['useflags'] = ['ALPHA', 'BRAVO', 'CHARLIE']\n    self.build_config['skip_toolchain_update'] = False\n    self.build_config['nowithdebug'] = False\n    self.build_config['hw_tests'] = ['bvt']\n    self.build_config['chromeos_official'] = True\n\n    proper_env = {'USE' : ' '.join(self.build_config['useflags'])}\n\n    # Convenience variables.\n    fake_autotest_dir = '/fake/autotest'\n    autotest_tarball_name = 'autotest.tar'\n    full_autotest_tarball_name = 'autotest.tar.bz2'\n    test_suites_tarball_name = 'test_suites.tar.bz2'\n    autotest_tarball_path = os.path.join(fake_autotest_dir,\n                                         autotest_tarball_name)\n    full_autotest_tarball_path = os.path.join(fake_autotest_dir,\n                                              full_autotest_tarball_name)\n    test_suites_tarball_path = os.path.join(fake_autotest_dir,\n                                            test_suites_tarball_name)\n    tarballs = [autotest_tarball_path, test_suites_tarball_path]\n    full_autotest_tarball = full_autotest_tarball_path\n\n    commands.Build(self.build_root,\n                   self._current_board,\n                   build_autotest=True,\n                   usepkg=True,\n                   skip_toolchain_update=False,\n                   nowithdebug=False,\n                   extra_env=proper_env)\n\n    commands.BuildImage(self.build_root, self._current_board,\n                        ['test', 'base', 'dev'], version=self.version,\n                        root_boost=None, extra_env=proper_env)\n    commands.BuildVMImageForTesting(self.build_root, self._current_board,\n                                    extra_env=proper_env)\n    tempfile.mkdtemp(prefix='autotest').AndReturn(fake_autotest_dir)\n    commands.BuildAutotestTarballs(self.build_root, self._current_board,\n                                   fake_autotest_dir).AndReturn(tarballs)\n    commands.BuildFullAutotestTarball(self.build_root,\n                                      self._current_board,\n                                      fake_autotest_dir\n                                      ).AndReturn(full_autotest_tarball)\n    self.archive_stage_mock.AutotestTarballsReady(tarballs)\n    self.archive_stage_mock.FullAutotestTarballReady(full_autotest_tarball)\n    os.path.isdir(self.latest_cbuildbot).AndReturn(True)\n    self.archive_stage_mock.SetVersion(self.version)\n\n    shutil.copyfile(full_autotest_tarball_path,\n                    os.path.join(self.images_root, 'latest-cbuildbot',\n                                 full_autotest_tarball_name))\n\n    self.mox.ReplayAll()\n    self.RunStage()\n    self.mox.VerifyAll()", "func_src_after": "  def testAllConditionalPaths(self):\n    \"\"\"Enable all paths to get line coverage.\"\"\"\n    self.build_config['vm_tests'] = constants.SIMPLE_AU_TEST_TYPE\n    self.options.tests = True\n    self.build_config['build_type'] = constants.BUILD_FROM_SOURCE_TYPE\n    self.build_config['build_tests'] = True\n    self.build_config['archive_build_debug'] = True\n    self.build_config['usepkg_chroot'] = True\n    self.build_config['usepkg_setup_board'] = True\n    self.build_config['usepkg_build_packages'] = True\n    self.build_config['images'] = ['base', 'dev', 'test', 'factory_test',\n                                   'factory_install']\n    self.build_config['useflags'] = ['ALPHA', 'BRAVO', 'CHARLIE']\n    self.build_config['skip_toolchain_update'] = False\n    self.build_config['nowithdebug'] = False\n    self.build_config['hw_tests'] = ['bvt']\n    self.build_config['chromeos_official'] = True\n\n    proper_env = {'USE' : ' '.join(self.build_config['useflags'])}\n\n    # Convenience variables.\n    fake_autotest_dir = '/fake/autotest'\n    autotest_tarball_name = 'autotest.tar'\n    full_autotest_tarball_name = 'autotest.tar.bz2'\n    test_suites_tarball_name = 'test_suites.tar.bz2'\n    autotest_tarball_path = os.path.join(fake_autotest_dir,\n                                         autotest_tarball_name)\n    full_autotest_tarball_path = os.path.join(fake_autotest_dir,\n                                              full_autotest_tarball_name)\n    test_suites_tarball_path = os.path.join(fake_autotest_dir,\n                                            test_suites_tarball_name)\n    tarballs = [autotest_tarball_path, test_suites_tarball_path]\n    full_autotest_tarball = full_autotest_tarball_path\n\n    commands.Build(self.build_root,\n                   self._current_board,\n                   build_autotest=True,\n                   usepkg=True,\n                   skip_toolchain_update=False,\n                   nowithdebug=False,\n                   extra_env=proper_env)\n\n    commands.BuildImage(self.build_root, self._current_board,\n                        ['test', 'base', 'dev'], version=self.version,\n                        root_boost=None, extra_env=proper_env)\n    commands.BuildVMImageForTesting(self.build_root, self._current_board,\n                                    extra_env=proper_env)\n    tempfile.mkdtemp(prefix='autotest').AndReturn(fake_autotest_dir)\n    commands.BuildAutotestTarballs(self.build_root, self._current_board,\n                                   fake_autotest_dir).AndReturn(tarballs)\n    commands.BuildFullAutotestTarball(self.build_root,\n                                      self._current_board,\n                                      fake_autotest_dir\n                                      ).AndReturn(full_autotest_tarball)\n    self.archive_stage_mock.AutotestTarballsReady(tarballs)\n    self.archive_stage_mock.FullAutotestTarballReady(full_autotest_tarball)\n    os.path.isdir(self.latest_cbuildbot).AndReturn(True)\n    self.archive_stage_mock.SetVersion(self.version)\n\n    self.mox.ReplayAll()\n    self.RunStage()\n    self.mox.VerifyAll()", "line_changes": {"deleted": [{"line_no": 60, "char_start": 3002, "char_end": 3050, "line": "    shutil.copyfile(full_autotest_tarball_path,\n"}, {"line_no": 61, "char_start": 3050, "char_end": 3121, "line": "                    os.path.join(self.images_root, 'latest-cbuildbot',\n"}, {"line_no": 62, "char_start": 3121, "char_end": 3183, "line": "                                 full_autotest_tarball_name))\n"}, {"line_no": 63, "char_start": 3183, "char_end": 3184, "line": "\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 3002, "char_end": 3184, "chars": "    shutil.copyfile(full_autotest_tarball_path,\n                    os.path.join(self.images_root, 'latest-cbuildbot',\n                                 full_autotest_tarball_name))\n\n"}], "added": []}, "commit_link": "github.com/bpsinc-native/src_third_party_chromite/commit/f27434531c41757bd66def22bfd7f0f06bdbaee2", "file_name": "cbuildbot_stages_unittest.py", "vul_type": "cwe-022", "commit_msg": "Remove autotest.tar.bz2 from image.zip\n\nThis change removes autotest.tar.bz2 from image.zip because archive_hwqual\nno longer relies on this. Instead, it extracts the files directly from\nautotest.tar.bz2 that is archived in the same directory as image.zip.\n\nBUG=chromium-os:32721\nTEST=unittest\nCQ-DEPEND=I398031a19f1291653afe94a52522f5ca79e4f3dc\n\nChange-Id: Ifc7e648a724b73f82e096ab52e68b28443ea218f\nReviewed-on: https://gerrit.chromium.org/gerrit/27903\nTested-by: Yu-Ju Hong <yjhong@chromium.org>\nReviewed-by: Chris Sosa <sosa@chromium.org>\nCommit-Ready: Yu-Ju Hong <yjhong@chromium.org>", "parent_commit": "aa015079ec0d7bc4fdce9f42910b1c665c54514d", "description": "Write a Python function to configure a build environment and execute a series of build commands for testing purposes."}
{"func_name": "git_file_info", "func_src_before": "    def git_file_info(path)\n      return '  \u2713 '.colorize(@colors[:unchanged]) unless @git_status[path]\n      Git.colored_status_symbols(@git_status[path], @colors)\n    end", "func_src_after": "    def git_file_info(path)\n      return '  \u2713 '.colorize(@colors[:unchanged]) unless @git_status[path]\n      Git.colored_status_symbols(@git_status[path].uniq, @colors)\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 103, "char_end": 164, "line": "      Git.colored_status_symbols(@git_status[path], @colors)\n"}, {"line_no": 4, "char_start": 164, "char_end": 171, "line": "    end\n"}], "added": []}, "char_changes": {"deleted": [], "added": [{"char_start": 153, "char_end": 158, "chars": ".uniq"}]}, "commit_link": "github.com/athityakumar/colorls/commit/b362fa1eb81e7e6fa208cc8cab51f110db20057b", "file_name": "core.rb", "vul_type": "cwe-022", "commit_msg": "Improve git-status processing\n\n* no longer traverse complete directory trees to determine git status for\n  directories\n\n* properly report status for folders with changed files\n\n* skip the parent folder since we do not have git status about it", "parent_commit": "bb270b319a68adb96bf91a311250481020bdde81", "description": "Write a Ruby method named `git_file_info` that returns the colorized status of a file in a git repository, using a unique status if present."}
{"func_name": "git_dir_info", "func_src_before": "    def git_dir_info(path)\n      ignored = @git_status.select { |file, mode| file.start_with?(path) && mode=='!!' }.keys\n      present = Dir.deep_entries(path).map { |p| \"#{path}/#{p}\" }\n      return '    ' if (present-ignored).empty?\n\n      modes = (present-ignored).map { |file| @git_status[file] }-[nil]\n      return '  \u2713 '.colorize(@colors[:unchanged]) if modes.empty?\n      Git.colored_status_symbols(modes.join.uniq, @colors)\n    end", "func_src_after": "    def git_dir_info(path)\n      direct_status = @git_status.fetch(\"#{path}/\", nil)\n\n      return Git.colored_status_symbols(direct_status.uniq, @colors) unless direct_status.nil?\n\n      modes = @git_status.select { |file, mode| file.start_with?(path) && mode != '!!' }\n\n      return '  \u2713 '.colorize(@colors[:unchanged]) if modes.empty?\n\n      Git.colored_status_symbols(modes.values.join.uniq, @colors)\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 27, "char_end": 121, "line": "      ignored = @git_status.select { |file, mode| file.start_with?(path) && mode=='!!' }.keys\n"}, {"line_no": 3, "char_start": 121, "char_end": 187, "line": "      present = Dir.deep_entries(path).map { |p| \"#{path}/#{p}\" }\n"}, {"line_no": 4, "char_start": 187, "char_end": 235, "line": "      return '    ' if (present-ignored).empty?\n"}, {"line_no": 6, "char_start": 236, "char_end": 307, "line": "      modes = (present-ignored).map { |file| @git_status[file] }-[nil]\n"}, {"line_no": 8, "char_start": 373, "char_end": 432, "line": "      Git.colored_status_symbols(modes.join.uniq, @colors)\n"}], "added": [{"line_no": 2, "char_start": 27, "char_end": 84, "line": "      direct_status = @git_status.fetch(\"#{path}/\", nil)\n"}, {"line_no": 3, "char_start": 84, "char_end": 85, "line": "\n"}, {"line_no": 4, "char_start": 85, "char_end": 180, "line": "      return Git.colored_status_symbols(direct_status.uniq, @colors) unless direct_status.nil?\n"}, {"line_no": 5, "char_start": 180, "char_end": 181, "line": "\n"}, {"line_no": 6, "char_start": 181, "char_end": 270, "line": "      modes = @git_status.select { |file, mode| file.start_with?(path) && mode != '!!' }\n"}, {"line_no": 9, "char_start": 337, "char_end": 338, "line": "\n"}, {"line_no": 10, "char_start": 338, "char_end": 404, "line": "      Git.colored_status_symbols(modes.values.join.uniq, @colors)\n"}]}, "char_changes": {"deleted": [{"char_start": 33, "char_end": 170, "chars": "ignored = @git_status.select { |file, mode| file.start_with?(path) && mode=='!!' }.keys\n      present = Dir.deep_entries(path).map { |p| "}, {"char_start": 179, "char_end": 186, "chars": "#{p}\" }"}, {"char_start": 200, "char_end": 306, "chars": "'    ' if (present-ignored).empty?\n\n      modes = (present-ignored).map { |file| @git_status[file] }-[nil]"}], "added": [{"char_start": 33, "char_end": 67, "chars": "direct_status = @git_status.fetch("}, {"char_start": 76, "char_end": 84, "chars": "\", nil)\n"}, {"char_start": 98, "char_end": 270, "chars": "Git.colored_status_symbols(direct_status.uniq, @colors) unless direct_status.nil?\n\n      modes = @git_status.select { |file, mode| file.start_with?(path) && mode != '!!' }\n"}, {"char_start": 337, "char_end": 338, "chars": "\n"}, {"char_start": 377, "char_end": 384, "chars": "values."}]}, "commit_link": "github.com/athityakumar/colorls/commit/b362fa1eb81e7e6fa208cc8cab51f110db20057b", "file_name": "core.rb", "vul_type": "cwe-022", "commit_msg": "Improve git-status processing\n\n* no longer traverse complete directory trees to determine git status for\n  directories\n\n* properly report status for folders with changed files\n\n* skip the parent folder since we do not have git status about it", "parent_commit": "bb270b319a68adb96bf91a311250481020bdde81", "description": "Create a Ruby method named `git_dir_info` that takes a directory path and returns a string representing the Git status of the files within that directory."}
{"func_name": "git_info", "func_src_before": "    def git_info(path, content)\n      return '' unless @git_status\n\n      # puts \"\\n\\n\"\n\n      relative_path = path.remove(@git_root_path+'/')\n      relative_path = relative_path==path ? '' : relative_path+'/'\n      content_path  = \"#{relative_path}#{content}\"\n\n      if content.directory?\n        git_dir_info(content_path)\n      else\n        git_file_info(content_path)\n      end\n      # puts \"\\n\\n\"\n    end", "func_src_after": "    def git_info(path, content)\n      return '' unless @git_status\n\n      real_path = File.realdirpath(content.name, path)\n\n      return '    ' unless real_path.start_with? path\n\n      relative_path = real_path.remove(Regexp.new('^' + Regexp.escape(@git_root_path) + '/?'))\n\n      if content.directory?\n        git_dir_info(relative_path)\n      else\n        git_file_info(relative_path)\n      end\n      # puts \"\\n\\n\"\n    end", "line_changes": {"deleted": [{"line_no": 6, "char_start": 89, "char_end": 143, "line": "      relative_path = path.remove(@git_root_path+'/')\n"}, {"line_no": 7, "char_start": 143, "char_end": 210, "line": "      relative_path = relative_path==path ? '' : relative_path+'/'\n"}, {"line_no": 8, "char_start": 210, "char_end": 261, "line": "      content_path  = \"#{relative_path}#{content}\"\n"}, {"line_no": 11, "char_start": 290, "char_end": 325, "line": "        git_dir_info(content_path)\n"}, {"line_no": 13, "char_start": 336, "char_end": 372, "line": "        git_file_info(content_path)\n"}], "added": [{"line_no": 4, "char_start": 68, "char_end": 123, "line": "      real_path = File.realdirpath(content.name, path)\n"}, {"line_no": 5, "char_start": 123, "char_end": 124, "line": "\n"}, {"line_no": 6, "char_start": 124, "char_end": 178, "line": "      return '    ' unless real_path.start_with? path\n"}, {"line_no": 8, "char_start": 179, "char_end": 274, "line": "      relative_path = real_path.remove(Regexp.new('^' + Regexp.escape(@git_root_path) + '/?'))\n"}, {"line_no": 11, "char_start": 303, "char_end": 339, "line": "        git_dir_info(relative_path)\n"}, {"line_no": 13, "char_start": 350, "char_end": 387, "line": "        git_file_info(relative_path)\n"}]}, "char_changes": {"deleted": [{"char_start": 74, "char_end": 260, "chars": "# puts \"\\n\\n\"\n\n      relative_path = path.remove(@git_root_path+'/')\n      relative_path = relative_path==path ? '' : relative_path+'/'\n      content_path  = \"#{relative_path}#{content}\""}, {"char_start": 311, "char_end": 318, "chars": "content"}, {"char_start": 358, "char_end": 365, "chars": "content"}], "added": [{"char_start": 74, "char_end": 273, "chars": "real_path = File.realdirpath(content.name, path)\n\n      return '    ' unless real_path.start_with? path\n\n      relative_path = real_path.remove(Regexp.new('^' + Regexp.escape(@git_root_path) + '/?'))"}, {"char_start": 324, "char_end": 332, "chars": "relative"}, {"char_start": 372, "char_end": 380, "chars": "relative"}]}, "commit_link": "github.com/athityakumar/colorls/commit/b362fa1eb81e7e6fa208cc8cab51f110db20057b", "file_name": "core.rb", "vul_type": "cwe-022", "commit_msg": "Improve git-status processing\n\n* no longer traverse complete directory trees to determine git status for\n  directories\n\n* properly report status for folders with changed files\n\n* skip the parent folder since we do not have git status about it", "parent_commit": "bb270b319a68adb96bf91a311250481020bdde81", "description": "Write a Ruby method named `git_info` that takes a file system path and a content object, and returns git information based on whether the content is a directory or a file."}
{"func_name": "self.status", "func_src_before": "    def self.status(repo_path)\n      @git_status = {}\n\n      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-uall', '--ignored']) do |output|\n        output.read.split(\"\\x0\").map { |x| x.split(' ', 2) }.each do |mode, file|\n          @git_status[file] = mode\n        end\n      end\n      warn \"git status failed in #{repo_path}\" unless $CHILD_STATUS.success?\n\n      @git_status\n    end", "func_src_after": "    def self.status(repo_path)\n      @git_status = {}\n\n      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-unormal', '--ignored']) do |output|\n        output.read.split(\"\\x0\").map { |x| x.split(' ', 2) }.each do |mode, file|\n          @git_status[file] = mode\n        end\n      end\n      warn \"git status failed in #{repo_path}\" unless $CHILD_STATUS.success?\n\n      @git_status\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 55, "char_end": 161, "line": "      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-uall', '--ignored']) do |output|\n"}], "added": [{"line_no": 4, "char_start": 55, "char_end": 164, "line": "      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-unormal', '--ignored']) do |output|\n"}]}, "char_changes": {"deleted": [{"char_start": 130, "char_end": 131, "chars": "l"}], "added": [{"char_start": 129, "char_end": 133, "chars": "norm"}]}, "commit_link": "github.com/athityakumar/colorls/commit/b362fa1eb81e7e6fa208cc8cab51f110db20057b", "file_name": "git.rb", "vul_type": "cwe-022", "commit_msg": "Improve git-status processing\n\n* no longer traverse complete directory trees to determine git status for\n  directories\n\n* properly report status for folders with changed files\n\n* skip the parent folder since we do not have git status about it", "parent_commit": "bb270b319a68adb96bf91a311250481020bdde81", "description": "Write a Ruby method that captures the status of files in a git repository at a given path and returns a hash with file names and their corresponding status codes."}
{"func_name": "get_git_branch", "func_src_before": "get_git_branch(wchar_t *dst, size_t size)\n{\n\tFILE *fp;\n\tchar *c;\n\tchar pwd[MAXPATHLEN];\n\tchar candidate[MAXPATHLEN];\n\tchar buf[MAX_BRANCH_LEN];\n\tsize_t s;\n\tstruct stat bufstat;\n\tint found_repo = -1;\n\n\t/* start from the working dir */\n\tstrlcpy(pwd, getcwd(NULL, MAXPATHLEN), MAXPATHLEN);\n\n\tdo {\n\t\tsnprintf(candidate, MAXPATHLEN, \"%s/.git/HEAD\", pwd);\n\n\t\tfound_repo = stat(candidate, &bufstat);\n\n\t\tif ((c = strrchr(pwd, '/')) == NULL)\n\t\t\tbreak;\n\n\t\t*c = '\\0';\n\t} while (found_repo != 0 && candidate[1] != '\\0');\n\n\tif (found_repo == -1)\n\t\treturn 0;\n\n\tfp = fopen(candidate, \"r\");\n\tif (fp == NULL) {\n\t\tstrlcpy(buf, \"###\", 4);\n\t\treturn mbstowcs(dst, buf, MAX_BRANCH_LEN);\n\t}\n\n\ts = fread(buf, 1, size, fp);\n\tfclose(fp);\n\n\tbuf[MAX_BRANCH_LEN] = '\\0';\n\n\t/* This is a branch head, just print the branch. */\n\tif (strncmp(buf, \"ref: refs/heads/\", 16) == 0) {\n\t\tchar *nl = strchr(buf, '\\n');\n\t\tif (nl)\n\t\t\t*(nl) = '\\0';\n\t\tc = buf + 16;\n\t\treturn mbstowcs(dst, c, MAX_BRANCH_LEN);\n\t}\n\n\t/* Show all other kinds of ref as-is (does it even exist?) */\n\tif (strncmp(buf, \"ref:\", 4) == 0) {\n\t\tchar *nl = strchr(buf, '\\n');\n\t\tif (nl) \n\t\t\t*(nl) = '\\0';\n\t\tc = buf + 5;\n\t\treturn mbstowcs(dst, c, MAX_BRANCH_LEN);\n\t}\n\n\t/* That's probably just a changeset, just show the first 6 chars */\n\tif (s > 6) {\n\t\tstrlcpy(buf + 6, \"...\", 4);\n\t\treturn mbstowcs(dst, buf, MAX_BRANCH_LEN);\n\t}\n\n\t/* We shouldn't get there, but we mind as well no crash. */\n\tstrlcpy(buf, \"???\", 4);\n\treturn mbstowcs(dst, buf, MAX_BRANCH_LEN);\n}", "func_src_after": "get_git_branch(wchar_t *dst, size_t size)\n{\n\tFILE *fp;\n\tchar *c;\n\tchar pwd[MAXPATHLEN];\n\tchar candidate[MAXPATHLEN];\n\tchar buf[MAX_BRANCH_LEN];\n\tsize_t s;\n\tstruct stat bufstat;\n\tint found_repo = -1;\n\n\t/* start from the working dir */\n\tstrlcpy(pwd, getcwd(NULL, MAXPATHLEN), MAXPATHLEN);\n\n\tdo {\n\t\tsnprintf(candidate, MAXPATHLEN, \"%s/.git/HEAD\", pwd);\n\n\t\tfound_repo = stat(candidate, &bufstat);\n\n\t\tif ((c = strrchr(pwd, '/')) == NULL)\n\t\t\tbreak;\n\n\t\t*c = '\\0';\n\t} while (found_repo != 0 && candidate[1] != '\\0');\n\n\tif (found_repo == -1)\n\t\treturn 0;\n\n\tfp = fopen(candidate, \"r\");\n\tif (fp == NULL) {\n\t\tstrlcpy(buf, \"###\", 4);\n\t\treturn mbstowcs(dst, buf, MAX_BRANCH_LEN);\n\t}\n\n\ts = fread(buf, 1, size, fp);\n\tfclose(fp);\n\n\tbuf[MAX_BRANCH_LEN - 1] = '\\0';\n\n\t/* This is a branch head, just print the branch. */\n\tif (strncmp(buf, \"ref: refs/heads/\", 16) == 0) {\n\t\tchar *nl = strchr(buf, '\\n');\n\t\tif (nl)\n\t\t\t*(nl) = '\\0';\n\t\tc = buf + 16;\n\t\treturn mbstowcs(dst, c, MAX_BRANCH_LEN);\n\t}\n\n\t/* Show all other kinds of ref as-is (does it even exist?) */\n\tif (strncmp(buf, \"ref:\", 4) == 0) {\n\t\tchar *nl = strchr(buf, '\\n');\n\t\tif (nl) \n\t\t\t*(nl) = '\\0';\n\t\tc = buf + 5;\n\t\treturn mbstowcs(dst, c, MAX_BRANCH_LEN);\n\t}\n\n\t/* That's probably just a changeset, just show the first 6 chars */\n\tif (s > 6) {\n\t\tstrlcpy(buf + 6, \"...\", 4);\n\t\treturn mbstowcs(dst, buf, MAX_BRANCH_LEN);\n\t}\n\n\t/* We shouldn't get there, but we mind as well no crash. */\n\tstrlcpy(buf, \"???\", 4);\n\treturn mbstowcs(dst, buf, MAX_BRANCH_LEN);\n}", "line_changes": {"deleted": [{"line_no": 38, "char_start": 713, "char_end": 742, "line": "\tbuf[MAX_BRANCH_LEN] = '\\0';\n"}], "added": [{"line_no": 38, "char_start": 713, "char_end": 746, "line": "\tbuf[MAX_BRANCH_LEN - 1] = '\\0';\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 732, "char_end": 736, "chars": " - 1"}]}, "commit_link": "github.com/tamentis/prwd/commit/2bf86717a20334c40d168ad968b863f0ab7fd8c5", "file_name": "main.c", "vul_type": "cwe-119", "commit_msg": "fix a buffer overflow", "parent_commit": "62dd9d1df4b4028e843964f7a2da96c89ee1f4de", "description": "Write a C function to determine the current Git branch name and copy it into a wide character string buffer."}
{"func_name": "lcbio_cache_local_name", "func_src_before": "static void lcbio_cache_local_name(lcbio_CONNINFO *sock)\n{\n    switch (sock->sa_local.ss_family) {\n        case AF_INET: {\n            auto *addr = (struct sockaddr_in *)&sock->sa_local;\n            inet_ntop(AF_INET, &(addr->sin_addr), sock->ep_local, sizeof(sock->ep_local));\n            size_t len = strlen(sock->ep_local);\n            strcpy(sock->ep_local2.host, sock->ep_local);\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin_port));\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin_port));\n        } break;\n\n        case AF_INET6: {\n            auto *addr = (struct sockaddr_in6 *)&sock->sa_local;\n            inet_ntop(AF_INET6, &(addr->sin6_addr), sock->ep_local, sizeof(sock->ep_local));\n            size_t len = strlen(sock->ep_local);\n            strcpy(sock->ep_local2.host, sock->ep_local);\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin6_port));\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin6_port));\n        } break;\n    }\n}", "func_src_after": "static void lcbio_cache_local_name(lcbio_CONNINFO *sock)\n{\n    switch (sock->sa_local.ss_family) {\n        case AF_INET: {\n            auto *addr = (struct sockaddr_in *)&sock->sa_local;\n            inet_ntop(AF_INET, &(addr->sin_addr), sock->ep_local, sizeof(sock->ep_local));\n            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin_port));\n            size_t len = strlen(sock->ep_local);\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin_port));\n        } break;\n\n        case AF_INET6: {\n            auto *addr = (struct sockaddr_in6 *)&sock->sa_local;\n            inet_ntop(AF_INET6, &(addr->sin6_addr), sock->ep_local, sizeof(sock->ep_local));\n            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin6_port));\n            size_t len = strlen(sock->ep_local);\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin6_port));\n        } break;\n    }\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 327, "char_end": 385, "line": "            strcpy(sock->ep_local2.host, sock->ep_local);\n"}, {"line_no": 9, "char_start": 385, "char_end": 488, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin_port));\n"}, {"line_no": 17, "char_start": 847, "char_end": 905, "line": "            strcpy(sock->ep_local2.host, sock->ep_local);\n"}, {"line_no": 18, "char_start": 905, "char_end": 1009, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin6_port));\n"}], "added": [{"line_no": 7, "char_start": 278, "char_end": 367, "line": "            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n"}, {"line_no": 8, "char_start": 367, "char_end": 475, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin_port));\n"}, {"line_no": 16, "char_start": 834, "char_end": 923, "line": "            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n"}, {"line_no": 17, "char_start": 923, "char_end": 1032, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin6_port));\n"}]}, "char_changes": {"deleted": [{"char_start": 291, "char_end": 346, "chars": "ize_t len = strlen(sock->ep_local);\n            strcpy("}, {"char_start": 360, "char_end": 368, "chars": "2.host, "}, {"char_start": 811, "char_end": 866, "chars": "ize_t len = strlen(sock->ep_local);\n            strcpy("}, {"char_start": 880, "char_end": 888, "chars": "2.host, "}], "added": [{"char_start": 291, "char_end": 320, "chars": "trncpy(sock->ep_local2.host, "}, {"char_start": 334, "char_end": 343, "chars": ", sizeof("}, {"char_start": 357, "char_end": 364, "chars": "2.host)"}, {"char_start": 432, "char_end": 437, "chars": ".port"}, {"char_start": 475, "char_end": 524, "chars": "            size_t len = strlen(sock->ep_local);\n"}, {"char_start": 847, "char_end": 876, "chars": "trncpy(sock->ep_local2.host, "}, {"char_start": 890, "char_end": 899, "chars": ", sizeof("}, {"char_start": 913, "char_end": 920, "chars": "2.host)"}, {"char_start": 988, "char_end": 993, "chars": ".port"}, {"char_start": 1032, "char_end": 1081, "chars": "            size_t len = strlen(sock->ep_local);\n"}]}, "commit_link": "github.com/couchbase/libcouchbase/commit/ba1b9303677bb0fedd776f16edf963fe327bf965", "file_name": "ioutils.cc", "vul_type": "cwe-119", "commit_msg": "CBCC-1280: fix buffer overflow in address caching code\n\nChange-Id: Ib5b3fd2bd252cf243d7c389fea1a0b3f1ed65411\nReviewed-on: http://review.couchbase.org/c/libcouchbase/+/157713\nTested-by: Build Bot <build@couchbase.com>\nReviewed-by: David Kelly <davidmichaelkelly@gmail.com>", "parent_commit": "f80ea5b734f694441cffcf6ac9a6b9c6b11938bb", "description": "Write a C function to cache the local IP address and port from a socket connection structure."}
{"func_name": "set_eeprom_serial_number", "func_src_before": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 16);\n  _dirty = 1;\n\n  return 0;\n}", "func_src_after": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 12);\n  _dirty = 1;\n\n  return 0;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 16);\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 12);\n"}]}, "char_changes": {"deleted": [{"char_start": 80, "char_end": 81, "chars": "6"}], "added": [{"char_start": 80, "char_end": 81, "chars": "2"}]}, "commit_link": "github.com/picoflamingo/BBCape_EEPROM/commit/0b2d0afdd72e6ca35e9312bd43e29d488ae8c2e5", "file_name": "bbcape_eeprom.c", "vul_type": "cwe-119", "commit_msg": "Buffer Overflow fixed (https://github.com/picoflamingo/BBCape_EEPROM/issues/1)", "parent_commit": "21b1310205d6b2d9073efc51c6a32edbd9a08b89", "description": "Write a C function named `set_eeprom_serial_number` that copies a serial number string into an EEPROM structure and sets a dirty flag."}
{"func_name": "main", "func_src_before": "int main (int argc, char **argv) {\n\tint result;\n\tstruct mt_packet data;\n\tstruct sockaddr_in si_me;\n\tunsigned char buff[1500];\n\tunsigned char print_help = 0, have_username = 0, have_password = 0;\n\tunsigned char drop_priv = 0;\n\tint c;\n\tint optval = 1;\n\n\tsetlocale(LC_ALL, \"\");\n\tbindtextdomain(\"mactelnet\",\"/usr/share/locale\");\n\ttextdomain(\"mactelnet\");\n\n\t/* Set default for ssh_path. */\n\tstrncpy(ssh_path, SSH_PATH, sizeof(ssh_path) -1);\n\tssh_path[sizeof(ssh_path)] = '\\0';\n\n    /* Ignore args after -- for MAC-Telnet client. */\n\tint mactelnet_argc = argc;\n\tint i;\n\tfor (i=0; i < argc; i++) {\n\t\tif (strlen(argv[i]) == 2 && strncmp(argv[i], \"--\", 2) == 0) {\n\t\t\tmactelnet_argc = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\twhile (1) {\n\t\tc = getopt(mactelnet_argc, argv, \"nqlt:u:p:vh?SFP:c:U:\");\n\n\t\tif (c == -1) {\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (c) {\n\n\t\t\tcase 'n':\n\t\t\t\tuse_raw_socket = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'S':\n\t\t\t\ttunnel_conn = 1;\n\t\t\t\tlaunch_ssh = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'F':\n\t\t\t\ttunnel_conn = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'P':\n\t\t\t\tfwdport = atoi(optarg);\n\t\t\t\tbreak;\n\n\t\t\tcase 'u':\n\t\t\t\t/* Save username */\n\t\t\t\tstrncpy(username, optarg, sizeof(username) - 1);\n\t\t\t\tusername[sizeof(username) - 1] = '\\0';\n\t\t\t\thave_username = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'p':\n\t\t\t\t/* Save password */\n\t\t\t\tstrncpy(password, optarg, sizeof(password) - 1);\n\t\t\t\tpassword[sizeof(password) - 1] = '\\0';\n\t\t\t\thave_password = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'U':\n\t\t\t\t/* Save nonpriv_username */\n\t\t\t\tstrncpy(nonpriv_username, optarg, sizeof(nonpriv_username) - 1);\n\t\t\t\tnonpriv_username[sizeof(nonpriv_username) - 1] = '\\0';\n\t\t\t\tdrop_priv = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'c':\n\t\t\t\t/* Save ssh executable path */\n\t\t\t\tstrncpy(ssh_path, optarg, sizeof(ssh_path) -1);\n\t\t\t\tssh_path[sizeof(ssh_path)] = '\\0';\n\t\t\t\tbreak;\n\n\t\t\tcase 't':\n\t\t\t\tconnect_timeout = atoi(optarg);\n\t\t\t\tbreak;\n\n\t\t\tcase 'l':\n\t\t\t\treturn mndp();\n\t\t\t\tbreak;\n\n\t\t\tcase 'v':\n\t\t\t\tprint_version();\n\t\t\t\texit(0);\n\t\t\t\tbreak;\n\n\t\t\tcase 'q':\n\t\t\t\tquiet_mode = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'h':\n\t\t\tcase '?':\n\t\t\t\tprint_help = 1;\n\t\t\t\tbreak;\n\n\t\t}\n\t}\n\tif (argc - optind < 1 || print_help) {\n\t\tprint_version();\n\t\tfprintf(stderr, _(\"Usage: %s <MAC|identity> [-v] [-h] [-q] [-n] [-l] [-S] [-P <port>]\\n\"\n\t\t\t\t          \"       [-t <timeout>] [-u <user>] [-p <pass>] [-c <path>] [-U <user>]\\n\"), argv[0]);\n\n\t\tif (print_help) {\n\t\t\tfprintf(stderr, _(\"\\nParameters:\\n\"\n\t\t\t\"  MAC           MAC-Address of the RouterOS/mactelnetd device. Use mndp to \\n\"\n            \"                discover it.\\n\"\n\t\t\t\"  identity      The identity/name of your destination device. Uses MNDP \\n\"\n\t\t\t\"                protocol to find it.\\n\"\n\t\t\t\"  -l            List/Search for routers nearby. (using MNDP)\\n\"\n\t\t\t\"  -n            Do not use broadcast packets. Less insecure but requires root \\n\"\n\t\t    \"                privileges.\\n\"\n\t\t\t\"  -t <timeout>  Amount of seconds to wait for a response on each interface.\\n\"\n\t\t\t\"  -u <user>     Specify username on command line.\\n\"\n\t\t\t\"  -p <pass>     Specify password on command line.\\n\"\n\t\t\t\"  -U <user>     Drop privileges by switching to user, when the command is\\n\"\n\t\t\t\"                run as a privileged user in conjunction with the -n option.\\n\"\n\t\t\t\"  -S            Use MAC-SSH instead of MAC-Telnet. (Implies -F)\\n\"\n\t\t    \"                Forward SSH connection through MAC-Telnet and launch SSH client.\\n\"\n\t\t\t\"  -F            Forward connection through of MAC-Telnet without launching the \\n\"\n\t\t    \"                SSH Client.\\n\"\n\t\t\t\"  -P <port>     Local TCP port for forwarding SSH connection.\\n\"\n\t\t\t\"                (If not specified, port 2222 by default.)\\n\"\n\t\t\t\"  -c <path>     Path for ssh client executable. (Default: /usr/bin/ssh)\\n\"\n\t\t\t\"  -q            Quiet mode.\\n\"\n\t\t\t\"  -v            Print version and exit.\\n\"\n\t\t\t\"  -h            Print help and exit.\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"All arguments after '--' will be passed to the ssh client command.\\n\"\n\t\t\t\"\\n\"));\n\t\t}\n\t\treturn 1;\n\t}\n\n\t/* Setup command line for ssh client */\n\tif (launch_ssh) {\n\t\tint ssh_argc;\n\t\tint add_argc;\n\t\tssh_argc = argc - mactelnet_argc;\n\t\tadd_argc = ssh_argc;\n\t\tssh_argc += 3; /* Port option and hostname: -p <port> <host>*/\n\t\tif (have_username) {\n\t\t\tssh_argc += 2;  /* Login name option: -l <user> */\n\t\t}\n\t\tssh_argv = (char **) calloc(sizeof(char *), ssh_argc + 1);\n\t\tchar *ssh_path_c = strndup(ssh_path, sizeof(ssh_path) - 1);\n\t\tchar *ssh_filename = basename(ssh_path_c);\n\t\tint idx = 0;\n\t\tssh_argv[idx++] = ssh_filename;\n\t\tint i;\n\t\tfor (i = 1; i < add_argc; i++) {\n\t\t\tssh_argv[idx++] = argv[mactelnet_argc + i];\n\t\t}\n\t\tchar portstr[8];\n\t\tsnprintf(portstr, 8, \"%d\", fwdport);\n\t\tssh_argv[idx++] = strdup(\"-p\");\n\t\tssh_argv[idx++] = strndup(portstr, sizeof(portstr) - 1);\n\t\tif (have_username) {\n\t\t\tssh_argv[idx++] = strdup(\"-l\");\n\t\t\tssh_argv[idx++] = username;\n\t\t}\n\t\tssh_argv[idx++] = strdup(\"127.0.0.1\");\n\t\tssh_argv[idx++] = (char*) 0;\n\t}\n\n\tis_a_tty = isatty(fileno(stdout)) && isatty(fileno(stdin));\n\tif (!is_a_tty) {\n\t\tquiet_mode = 1;\n\t}\n\n\t/* Seed randomizer */\n\tsrand(time(NULL));\n\n\tif (use_raw_socket) {\n\t\tif (geteuid() != 0) {\n\t\t\tfprintf(stderr, _(\"You need to have root privileges to use the -n parameter.\\n\"));\n\t\t\treturn 1;\n\t\t}\n\n\t\tsockfd = net_init_raw_socket();\n\t}\n\n\tif (drop_priv) {\n\t\tdrop_privileges(nonpriv_username);\n\t}\n\n\t/* Receive regular udp packets with this socket */\n\tinsockfd = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP);\n\tif (insockfd < 0) {\n\t\tperror(\"insockfd\");\n\t\treturn 1;\n\t}\n\n\tif (!use_raw_socket) {\n\t\tif (setsockopt(insockfd, SOL_SOCKET, SO_BROADCAST, &optval, sizeof (optval))==-1) {\n\t\t\tperror(\"SO_BROADCAST\");\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Need to use, to be able to autodetect which interface to use */\n\tsetsockopt(insockfd, SOL_SOCKET, SO_REUSEADDR, &optval, sizeof (optval));\n\n\t/* Get mac-address from string, or check for hostname via mndp */\n\tif (!query_mndp_or_mac(argv[optind], dstmac, !quiet_mode)) {\n\t\t/* No valid mac address found, abort */\n\t\treturn 1;\n\t}\n\n\tif (!tunnel_conn && !have_username) {\n\t\tif (!quiet_mode) {\n\t\t\tprintf(_(\"Login: \"));\n\t\t}\n\t\tscanf(\"%254s\", username);\n\t}\n\n\tif (!tunnel_conn && !have_password) {\n\t\tchar *tmp;\n\t\ttmp = getpass(quiet_mode ? \"\" : _(\"Password: \"));\n\t\tstrncpy(password, tmp, sizeof(password) - 1);\n\t\tpassword[sizeof(password) - 1] = '\\0';\n\t\t/* security */\n\t\tmemset(tmp, 0, strlen(tmp));\n#ifdef __GNUC__\n\t\tfree(tmp);\n#endif\n\t}\n\n\tif (tunnel_conn) {\n\t\t/* Setup signal handler for broken tunnels. */\n\t\tsignal(SIGPIPE,SIG_IGN);\n\n\t\t/* Setup Server socket for receiving connection from local SSH Client. */\n\t\tint fwdsrvfd;\n\t\tfwdsrvfd = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP);\n\t\tif (fwdsrvfd < 0) {\n\t\t\tperror(\"fwdsrvfd\");\n\t\t\treturn 1;\n\t\t}\n\t\tif(setsockopt(fwdsrvfd, SOL_SOCKET, SO_REUSEADDR, &optval, sizeof (optval)) < 0) {\n\t\t\tperror(\"SO_REUSEADDR\");\n\t\t\treturn 1;\n\t\t}\n\n\t\t/* Bind to server socket for receiving terminal client connection. */\n\t\tstruct sockaddr_in srv_socket;\n\t\tmemset(&srv_socket, 0, sizeof(srv_socket));\n\t\tsrv_socket.sin_family = AF_INET;\n\t\tsrv_socket.sin_port = htons(fwdport);\n\t\tsrv_socket.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\n\t\tif (bind(fwdsrvfd, (struct sockaddr *) &srv_socket, sizeof(srv_socket)) < 0) {\n\t\t\tfprintf(stderr, _(\"Error binding to %s:%d, %s\\n\"), \"127.0.0.1\", fwdport, strerror(errno));\n\t\t\treturn 1;\n\t\t}\n\t\tif (listen(fwdsrvfd, 1) < 0) {\n\t\t\tfprintf(stderr, _(\"Failed listen on server socket %s:%d, %s\\n\"), \"127.0.0.1\", fwdport, strerror(errno));\n\t\t\treturn 1;\n\t\t}\n\n\t\t/* Fork child to execute SSH Client locally and connect to parent\n\t\t * waiting for connection from child if launch_ssh is requested.\n\t\t */\n\t\tint pid;\n\t\tif (launch_ssh) {\n\t\t\tpid = fork();\n\t\t}\n\n\t\tif (!launch_ssh || pid > 0) {\n\t\t\t/* Parent code. Waits for connection to local end of tunnel */\n\n\t\t\t/* Close stdin and stdout, leave stderr active for error messages.\n\t\t\t * The terminal will be handled by client connecting to local end of tunnel. */\n\t\t\tclose(0);\n\t\t\tclose(1);\n\n\t\t\t/* Wait for remote terminal client connection on server port. */\n\t\t\tfprintf(stderr, _(\"Waiting for tunnel connection on port: %d\\n\"), fwdport);\n\t\t\tstruct sockaddr_in cli_socket;\n\t\t\tunsigned int cli_socket_len = sizeof(cli_socket);\n\t\t\tmemset(&cli_socket, 0, sizeof(cli_socket));\n\t\t\tif ((fwdfd = accept(fwdsrvfd, (struct sockaddr *) &cli_socket, &cli_socket_len)) < 0) {\n\t\t\t\tperror(\"fwdfd\");\n\t\t\t}\n\t\t\tif(setsockopt(fwdfd, SOL_SOCKET, SO_KEEPALIVE, &optval, sizeof(optval)) < 0) {\n\t\t\t\tperror(\"SO_KEEPALIVE\");\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tfprintf(stderr, _(\"Client connected to tunnel from port: %d\\n\"), ntohs(cli_socket.sin_port));\n\t\t}\n\t\telse if (launch_ssh && pid == 0) {\n\t\t\t/* Child Code. Executes SSH Client and connects to parent to tunnel\n\t\t\t * connection through MAC-Telnet protocol. */\n\t\t\tif (use_raw_socket) {\n\t\t\t\tclose(sockfd);\n\t\t\t}\n\t\t\tclose(insockfd);\n\t\t\tclose(fwdsrvfd);\n\n\t\t\t/* Give time to parent to initialize listening port. */\n\t\t\tsleep(2);\n\n\t\t\t/* Execute SSH Client. */\n\t\t\texecvp(ssh_path, ssh_argv);\n\t\t\tperror(\"Execution of terminal client failed.\");\n\t\t\texit(1);\n\t\t}\n\t\t/* Fork failure. */\n\t\telse {\n\t\t\tfprintf(stderr, _(\"Execution of terminal client failed.\\n\"));\n\t\t\tif (use_raw_socket) {\n\t\t\t\tclose(sockfd);\n\t\t\t}\n\t\t\tclose(insockfd);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Set random source port */\n\tsourceport = 1024 + (rand() % 1024);\n\n\t/* Set up global info about the connection */\n\tinet_pton(AF_INET, (char *)\"255.255.255.255\", &destip);\n\tmemcpy(&sourceip, &(si_me.sin_addr), IPV4_ALEN);\n\n\t/* Session key */\n\tsessionkey = rand() % 65535;\n\n\t/* stop output buffering */\n\tsetvbuf(stdout, (char*)NULL, _IONBF, 0);\n\n\tif (!quiet_mode) {\n\t\tprintf(_(\"Connecting to %s...\"), ether_ntoa((struct ether_addr *)dstmac));\n\t}\n\n\t/* Initialize receiving socket on the device chosen */\n\tmemset((char *) &si_me, 0, sizeof(si_me));\n\tsi_me.sin_family = AF_INET;\n\tsi_me.sin_port = htons(sourceport);\n\n\t/* Bind to udp port */\n\tif (bind(insockfd, (struct sockaddr *)&si_me, sizeof(si_me)) == -1) {\n\t\tfprintf(stderr, _(\"Error binding to %s:%d, %s\\n\"), inet_ntoa(si_me.sin_addr), sourceport, strerror(errno));\n\t\treturn 1;\n\t}\n\n\tif (!find_interface() || (result = recvfrom(insockfd, buff, 1400, 0, 0, 0)) < 1) {\n\t\tfprintf(stderr, _(\"Connection failed.\\n\"));\n\t\treturn 1;\n\t}\n\tif (!quiet_mode) {\n\t\tprintf(_(\"done\\n\"));\n\t}\n\n\t/* Handle first received packet */\n\thandle_packet(buff, result);\n\n\tinit_packet(&data, MT_PTYPE_DATA, srcmac, dstmac, sessionkey, 0);\n\toutcounter +=  add_control_packet(&data, MT_CPTYPE_BEGINAUTH, NULL, 0);\n\n\t/* TODO: handle result of send_udp */\n\tresult = send_udp(&data, 1);\n\n\twhile (running) {\n\t\tfd_set read_fds;\n\t\tint reads;\n\t\tstatic int terminal_gone = 0;\n\t\tstruct timeval timeout;\n\n\t\tint maxfd = 0;\n\t\tmaxfd = insockfd > fwdfd ? insockfd : fwdfd;\n\n\t\t/* Init select */\n\t\tFD_ZERO(&read_fds);\n\t\tif (!tunnel_conn && !terminal_gone) {\n\t\t\t/* Setup fd to read input from terminal. */\n\t\t\tFD_SET(0, &read_fds);\n\t\t}\n\t\telse if (tunnel_conn) {\n\t\t\t/* Setup fd to read input from local SSH Client. */\n\t\t\tFD_SET(fwdfd, &read_fds);\n\t\t}\n\t\tFD_SET(insockfd, &read_fds);\n\n\t\ttimeout.tv_sec = 1;\n\t\ttimeout.tv_usec = 0;\n\n\t\t/* Wait for data or timeout */\n\t\treads = select(maxfd+1, &read_fds, NULL, NULL, &timeout);\n\t\tif (reads > 0) {\n\t\t\t/* Handle data from server */\n\t\t\tif (FD_ISSET(insockfd, &read_fds)) {\n\t\t\t\tbzero(buff, 1500);\n\t\t\t\tresult = recvfrom(insockfd, buff, 1500, 0, 0, 0);\n\t\t\t\thandle_packet(buff, result);\n\t\t\t}\n\t\t\tunsigned char keydata[512];\n\t\t\tint datalen = 0;\n\t\t\t/* Handle data from keyboard/local terminal */\n\t\t\tif (!tunnel_conn && FD_ISSET(0, &read_fds) && terminal_mode) {\n\t\t\t\tdatalen = read(STDIN_FILENO, &keydata, 512);\n\t\t\t}\n\t\t\t/* Handle data from local SSH client */\n\t\t\tif (tunnel_conn && FD_ISSET(fwdfd, &read_fds)) {\n\t\t\t\tdatalen = read(fwdfd, &keydata, 512);\n\t\t\t}\n\t\t\tif (datalen > 0) {\n\t\t\t\t/* Data received, transmit to server */\n\t\t\t\tinit_packet(&data, MT_PTYPE_DATA, srcmac, dstmac, sessionkey, outcounter);\n\t\t\t\tadd_control_packet(&data, MT_CPTYPE_PLAINDATA, &keydata, datalen);\n\t\t\t\toutcounter += datalen;\n\t\t\t\tsend_udp(&data, 1);\n\t\t\t}\n\t\t\telse if (datalen < 0) {\n\t\t\t\tterminal_gone = 1;\n\t\t\t}\n\t\t/* Handle select() timeout */\n\t\t} else {\n\t\t\t/* handle keepalive counter, transmit keepalive packet every 10 seconds\n\t\t\t   of inactivity  */\n\t\t\tif (keepalive_counter++ == 10) {\n\t\t\t\tstruct mt_packet odata;\n\t\t\t\tinit_packet(&odata, MT_PTYPE_ACK, srcmac, dstmac, sessionkey, outcounter);\n\t\t\t\tsend_udp(&odata, 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!tunnel_conn && is_a_tty && terminal_mode) {\n\t\t/* Reset terminal back to old settings */\n\t\treset_term();\n\t}\n\n\tclose(sockfd);\n\tclose(insockfd);\n\tif (tunnel_conn && fwdfd > 0) {\n\t\tclose(fwdfd);\n\t}\n\n\treturn 0;\n}", "func_src_after": "int main (int argc, char **argv) {\n\tint result;\n\tstruct mt_packet data;\n\tstruct sockaddr_in si_me;\n\tunsigned char buff[1500];\n\tunsigned char print_help = 0, have_username = 0, have_password = 0;\n\tunsigned char drop_priv = 0;\n\tint c;\n\tint optval = 1;\n\n\tsetlocale(LC_ALL, \"\");\n\tbindtextdomain(\"mactelnet\",\"/usr/share/locale\");\n\ttextdomain(\"mactelnet\");\n\n\t/* Set default for ssh_path. */\n\tstrncpy(ssh_path, SSH_PATH, sizeof(ssh_path) - 1);\n\tssh_path[sizeof(ssh_path) - 1] = '\\0';\n\n    /* Ignore args after -- for MAC-Telnet client. */\n\tint mactelnet_argc = argc;\n\tint i;\n\tfor (i=0; i < argc; i++) {\n\t\tif (strlen(argv[i]) == 2 && strncmp(argv[i], \"--\", 2) == 0) {\n\t\t\tmactelnet_argc = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\twhile (1) {\n\t\tc = getopt(mactelnet_argc, argv, \"nqlt:u:p:vh?SFP:c:U:\");\n\n\t\tif (c == -1) {\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (c) {\n\n\t\t\tcase 'n':\n\t\t\t\tuse_raw_socket = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'S':\n\t\t\t\ttunnel_conn = 1;\n\t\t\t\tlaunch_ssh = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'F':\n\t\t\t\ttunnel_conn = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'P':\n\t\t\t\tfwdport = atoi(optarg);\n\t\t\t\tbreak;\n\n\t\t\tcase 'u':\n\t\t\t\t/* Save username */\n\t\t\t\tstrncpy(username, optarg, sizeof(username) - 1);\n\t\t\t\tusername[sizeof(username) - 1] = '\\0';\n\t\t\t\thave_username = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'p':\n\t\t\t\t/* Save password */\n\t\t\t\tstrncpy(password, optarg, sizeof(password) - 1);\n\t\t\t\tpassword[sizeof(password) - 1] = '\\0';\n\t\t\t\thave_password = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'U':\n\t\t\t\t/* Save nonpriv_username */\n\t\t\t\tstrncpy(nonpriv_username, optarg, sizeof(nonpriv_username) - 1);\n\t\t\t\tnonpriv_username[sizeof(nonpriv_username) - 1] = '\\0';\n\t\t\t\tdrop_priv = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'c':\n\t\t\t\t/* Save ssh executable path */\n\t\t\t\tstrncpy(ssh_path, optarg, sizeof(ssh_path) - 1);\n\t\t\t\tssh_path[sizeof(ssh_path) - 1] = '\\0';\n\t\t\t\tbreak;\n\n\t\t\tcase 't':\n\t\t\t\tconnect_timeout = atoi(optarg);\n\t\t\t\tbreak;\n\n\t\t\tcase 'l':\n\t\t\t\treturn mndp();\n\t\t\t\tbreak;\n\n\t\t\tcase 'v':\n\t\t\t\tprint_version();\n\t\t\t\texit(0);\n\t\t\t\tbreak;\n\n\t\t\tcase 'q':\n\t\t\t\tquiet_mode = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase 'h':\n\t\t\tcase '?':\n\t\t\t\tprint_help = 1;\n\t\t\t\tbreak;\n\n\t\t}\n\t}\n\tif (argc - optind < 1 || print_help) {\n\t\tprint_version();\n\t\tfprintf(stderr, _(\"Usage: %s <MAC|identity> [-v] [-h] [-q] [-n] [-l] [-S] [-P <port>]\\n\"\n\t\t\t\t          \"       [-t <timeout>] [-u <user>] [-p <pass>] [-c <path>] [-U <user>]\\n\"), argv[0]);\n\n\t\tif (print_help) {\n\t\t\tfprintf(stderr, _(\"\\nParameters:\\n\"\n\t\t\t\"  MAC           MAC-Address of the RouterOS/mactelnetd device. Use mndp to \\n\"\n            \"                discover it.\\n\"\n\t\t\t\"  identity      The identity/name of your destination device. Uses MNDP \\n\"\n\t\t\t\"                protocol to find it.\\n\"\n\t\t\t\"  -l            List/Search for routers nearby. (using MNDP)\\n\"\n\t\t\t\"  -n            Do not use broadcast packets. Less insecure but requires root \\n\"\n\t\t    \"                privileges.\\n\"\n\t\t\t\"  -t <timeout>  Amount of seconds to wait for a response on each interface.\\n\"\n\t\t\t\"  -u <user>     Specify username on command line.\\n\"\n\t\t\t\"  -p <pass>     Specify password on command line.\\n\"\n\t\t\t\"  -U <user>     Drop privileges by switching to user, when the command is\\n\"\n\t\t\t\"                run as a privileged user in conjunction with the -n option.\\n\"\n\t\t\t\"  -S            Use MAC-SSH instead of MAC-Telnet. (Implies -F)\\n\"\n\t\t    \"                Forward SSH connection through MAC-Telnet and launch SSH client.\\n\"\n\t\t\t\"  -F            Forward connection through of MAC-Telnet without launching the \\n\"\n\t\t    \"                SSH Client.\\n\"\n\t\t\t\"  -P <port>     Local TCP port for forwarding SSH connection.\\n\"\n\t\t\t\"                (If not specified, port 2222 by default.)\\n\"\n\t\t\t\"  -c <path>     Path for ssh client executable. (Default: /usr/bin/ssh)\\n\"\n\t\t\t\"  -q            Quiet mode.\\n\"\n\t\t\t\"  -v            Print version and exit.\\n\"\n\t\t\t\"  -h            Print help and exit.\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"All arguments after '--' will be passed to the ssh client command.\\n\"\n\t\t\t\"\\n\"));\n\t\t}\n\t\treturn 1;\n\t}\n\n\t/* Setup command line for ssh client */\n\tif (launch_ssh) {\n\t\tint ssh_argc;\n\t\tint add_argc;\n\t\tssh_argc = argc - mactelnet_argc;\n\t\tadd_argc = ssh_argc;\n\t\tssh_argc += 3; /* Port option and hostname: -p <port> <host>*/\n\t\tif (have_username) {\n\t\t\tssh_argc += 2;  /* Login name option: -l <user> */\n\t\t}\n\t\tssh_argv = (char **) calloc(sizeof(char *), ssh_argc + 1);\n\t\tchar *ssh_path_c = strndup(ssh_path, sizeof(ssh_path) - 1);\n\t\tchar *ssh_filename = basename(ssh_path_c);\n\t\tint idx = 0;\n\t\tssh_argv[idx++] = ssh_filename;\n\t\tint i;\n\t\tfor (i = 1; i < add_argc; i++) {\n\t\t\tssh_argv[idx++] = argv[mactelnet_argc + i];\n\t\t}\n\t\tchar portstr[8];\n\t\tsnprintf(portstr, 8, \"%d\", fwdport);\n\t\tssh_argv[idx++] = strdup(\"-p\");\n\t\tssh_argv[idx++] = strndup(portstr, sizeof(portstr) - 1);\n\t\tif (have_username) {\n\t\t\tssh_argv[idx++] = strdup(\"-l\");\n\t\t\tssh_argv[idx++] = username;\n\t\t}\n\t\tssh_argv[idx++] = strdup(\"127.0.0.1\");\n\t\tssh_argv[idx++] = (char*) 0;\n\t}\n\n\tis_a_tty = isatty(fileno(stdout)) && isatty(fileno(stdin));\n\tif (!is_a_tty) {\n\t\tquiet_mode = 1;\n\t}\n\n\t/* Seed randomizer */\n\tsrand(time(NULL));\n\n\tif (use_raw_socket) {\n\t\tif (geteuid() != 0) {\n\t\t\tfprintf(stderr, _(\"You need to have root privileges to use the -n parameter.\\n\"));\n\t\t\treturn 1;\n\t\t}\n\n\t\tsockfd = net_init_raw_socket();\n\t}\n\n\tif (drop_priv) {\n\t\tdrop_privileges(nonpriv_username);\n\t}\n\n\t/* Receive regular udp packets with this socket */\n\tinsockfd = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP);\n\tif (insockfd < 0) {\n\t\tperror(\"insockfd\");\n\t\treturn 1;\n\t}\n\n\tif (!use_raw_socket) {\n\t\tif (setsockopt(insockfd, SOL_SOCKET, SO_BROADCAST, &optval, sizeof (optval))==-1) {\n\t\t\tperror(\"SO_BROADCAST\");\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Need to use, to be able to autodetect which interface to use */\n\tsetsockopt(insockfd, SOL_SOCKET, SO_REUSEADDR, &optval, sizeof (optval));\n\n\t/* Get mac-address from string, or check for hostname via mndp */\n\tif (!query_mndp_or_mac(argv[optind], dstmac, !quiet_mode)) {\n\t\t/* No valid mac address found, abort */\n\t\treturn 1;\n\t}\n\n\tif (!tunnel_conn && !have_username) {\n\t\tif (!quiet_mode) {\n\t\t\tprintf(_(\"Login: \"));\n\t\t}\n\t\tscanf(\"%254s\", username);\n\t}\n\n\tif (!tunnel_conn && !have_password) {\n\t\tchar *tmp;\n\t\ttmp = getpass(quiet_mode ? \"\" : _(\"Password: \"));\n\t\tstrncpy(password, tmp, sizeof(password) - 1);\n\t\tpassword[sizeof(password) - 1] = '\\0';\n\t\t/* security */\n\t\tmemset(tmp, 0, strlen(tmp));\n#ifdef __GNUC__\n\t\tfree(tmp);\n#endif\n\t}\n\n\tif (tunnel_conn) {\n\t\t/* Setup signal handler for broken tunnels. */\n\t\tsignal(SIGPIPE,SIG_IGN);\n\n\t\t/* Setup Server socket for receiving connection from local SSH Client. */\n\t\tint fwdsrvfd;\n\t\tfwdsrvfd = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP);\n\t\tif (fwdsrvfd < 0) {\n\t\t\tperror(\"fwdsrvfd\");\n\t\t\treturn 1;\n\t\t}\n\t\tif(setsockopt(fwdsrvfd, SOL_SOCKET, SO_REUSEADDR, &optval, sizeof (optval)) < 0) {\n\t\t\tperror(\"SO_REUSEADDR\");\n\t\t\treturn 1;\n\t\t}\n\n\t\t/* Bind to server socket for receiving terminal client connection. */\n\t\tstruct sockaddr_in srv_socket;\n\t\tmemset(&srv_socket, 0, sizeof(srv_socket));\n\t\tsrv_socket.sin_family = AF_INET;\n\t\tsrv_socket.sin_port = htons(fwdport);\n\t\tsrv_socket.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\n\t\tif (bind(fwdsrvfd, (struct sockaddr *) &srv_socket, sizeof(srv_socket)) < 0) {\n\t\t\tfprintf(stderr, _(\"Error binding to %s:%d, %s\\n\"), \"127.0.0.1\", fwdport, strerror(errno));\n\t\t\treturn 1;\n\t\t}\n\t\tif (listen(fwdsrvfd, 1) < 0) {\n\t\t\tfprintf(stderr, _(\"Failed listen on server socket %s:%d, %s\\n\"), \"127.0.0.1\", fwdport, strerror(errno));\n\t\t\treturn 1;\n\t\t}\n\n\t\t/* Fork child to execute SSH Client locally and connect to parent\n\t\t * waiting for connection from child if launch_ssh is requested.\n\t\t */\n\t\tint pid;\n\t\tif (launch_ssh) {\n\t\t\tpid = fork();\n\t\t}\n\n\t\tif (!launch_ssh || pid > 0) {\n\t\t\t/* Parent code. Waits for connection to local end of tunnel */\n\n\t\t\t/* Close stdin and stdout, leave stderr active for error messages.\n\t\t\t * The terminal will be handled by client connecting to local end of tunnel. */\n\t\t\tclose(0);\n\t\t\tclose(1);\n\n\t\t\t/* Wait for remote terminal client connection on server port. */\n\t\t\tfprintf(stderr, _(\"Waiting for tunnel connection on port: %d\\n\"), fwdport);\n\t\t\tstruct sockaddr_in cli_socket;\n\t\t\tunsigned int cli_socket_len = sizeof(cli_socket);\n\t\t\tmemset(&cli_socket, 0, sizeof(cli_socket));\n\t\t\tif ((fwdfd = accept(fwdsrvfd, (struct sockaddr *) &cli_socket, &cli_socket_len)) < 0) {\n\t\t\t\tperror(\"fwdfd\");\n\t\t\t}\n\t\t\tif(setsockopt(fwdfd, SOL_SOCKET, SO_KEEPALIVE, &optval, sizeof(optval)) < 0) {\n\t\t\t\tperror(\"SO_KEEPALIVE\");\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tfprintf(stderr, _(\"Client connected to tunnel from port: %d\\n\"), ntohs(cli_socket.sin_port));\n\t\t}\n\t\telse if (launch_ssh && pid == 0) {\n\t\t\t/* Child Code. Executes SSH Client and connects to parent to tunnel\n\t\t\t * connection through MAC-Telnet protocol. */\n\t\t\tif (use_raw_socket) {\n\t\t\t\tclose(sockfd);\n\t\t\t}\n\t\t\tclose(insockfd);\n\t\t\tclose(fwdsrvfd);\n\n\t\t\t/* Give time to parent to initialize listening port. */\n\t\t\tsleep(2);\n\n\t\t\t/* Execute SSH Client. */\n\t\t\texecvp(ssh_path, ssh_argv);\n\t\t\tperror(\"Execution of terminal client failed.\");\n\t\t\texit(1);\n\t\t}\n\t\t/* Fork failure. */\n\t\telse {\n\t\t\tfprintf(stderr, _(\"Execution of terminal client failed.\\n\"));\n\t\t\tif (use_raw_socket) {\n\t\t\t\tclose(sockfd);\n\t\t\t}\n\t\t\tclose(insockfd);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Set random source port */\n\tsourceport = 1024 + (rand() % 1024);\n\n\t/* Set up global info about the connection */\n\tinet_pton(AF_INET, (char *)\"255.255.255.255\", &destip);\n\tmemcpy(&sourceip, &(si_me.sin_addr), IPV4_ALEN);\n\n\t/* Session key */\n\tsessionkey = rand() % 65535;\n\n\t/* stop output buffering */\n\tsetvbuf(stdout, (char*)NULL, _IONBF, 0);\n\n\tif (!quiet_mode) {\n\t\tprintf(_(\"Connecting to %s...\"), ether_ntoa((struct ether_addr *)dstmac));\n\t}\n\n\t/* Initialize receiving socket on the device chosen */\n\tmemset((char *) &si_me, 0, sizeof(si_me));\n\tsi_me.sin_family = AF_INET;\n\tsi_me.sin_port = htons(sourceport);\n\n\t/* Bind to udp port */\n\tif (bind(insockfd, (struct sockaddr *)&si_me, sizeof(si_me)) == -1) {\n\t\tfprintf(stderr, _(\"Error binding to %s:%d, %s\\n\"), inet_ntoa(si_me.sin_addr), sourceport, strerror(errno));\n\t\treturn 1;\n\t}\n\n\tif (!find_interface() || (result = recvfrom(insockfd, buff, 1400, 0, 0, 0)) < 1) {\n\t\tfprintf(stderr, _(\"Connection failed.\\n\"));\n\t\treturn 1;\n\t}\n\tif (!quiet_mode) {\n\t\tprintf(_(\"done\\n\"));\n\t}\n\n\t/* Handle first received packet */\n\thandle_packet(buff, result);\n\n\tinit_packet(&data, MT_PTYPE_DATA, srcmac, dstmac, sessionkey, 0);\n\toutcounter +=  add_control_packet(&data, MT_CPTYPE_BEGINAUTH, NULL, 0);\n\n\t/* TODO: handle result of send_udp */\n\tresult = send_udp(&data, 1);\n\n\twhile (running) {\n\t\tfd_set read_fds;\n\t\tint reads;\n\t\tstatic int terminal_gone = 0;\n\t\tstruct timeval timeout;\n\n\t\tint maxfd = 0;\n\t\tmaxfd = insockfd > fwdfd ? insockfd : fwdfd;\n\n\t\t/* Init select */\n\t\tFD_ZERO(&read_fds);\n\t\tif (!tunnel_conn && !terminal_gone) {\n\t\t\t/* Setup fd to read input from terminal. */\n\t\t\tFD_SET(0, &read_fds);\n\t\t}\n\t\telse if (tunnel_conn) {\n\t\t\t/* Setup fd to read input from local SSH Client. */\n\t\t\tFD_SET(fwdfd, &read_fds);\n\t\t}\n\t\tFD_SET(insockfd, &read_fds);\n\n\t\ttimeout.tv_sec = 1;\n\t\ttimeout.tv_usec = 0;\n\n\t\t/* Wait for data or timeout */\n\t\treads = select(maxfd+1, &read_fds, NULL, NULL, &timeout);\n\t\tif (reads > 0) {\n\t\t\t/* Handle data from server */\n\t\t\tif (FD_ISSET(insockfd, &read_fds)) {\n\t\t\t\tbzero(buff, 1500);\n\t\t\t\tresult = recvfrom(insockfd, buff, 1500, 0, 0, 0);\n\t\t\t\thandle_packet(buff, result);\n\t\t\t}\n\t\t\tunsigned char keydata[512];\n\t\t\tint datalen = 0;\n\t\t\t/* Handle data from keyboard/local terminal */\n\t\t\tif (!tunnel_conn && FD_ISSET(0, &read_fds) && terminal_mode) {\n\t\t\t\tdatalen = read(STDIN_FILENO, &keydata, 512);\n\t\t\t}\n\t\t\t/* Handle data from local SSH client */\n\t\t\tif (tunnel_conn && FD_ISSET(fwdfd, &read_fds)) {\n\t\t\t\tdatalen = read(fwdfd, &keydata, 512);\n\t\t\t}\n\t\t\tif (datalen > 0) {\n\t\t\t\t/* Data received, transmit to server */\n\t\t\t\tinit_packet(&data, MT_PTYPE_DATA, srcmac, dstmac, sessionkey, outcounter);\n\t\t\t\tadd_control_packet(&data, MT_CPTYPE_PLAINDATA, &keydata, datalen);\n\t\t\t\toutcounter += datalen;\n\t\t\t\tsend_udp(&data, 1);\n\t\t\t}\n\t\t\telse if (datalen < 0) {\n\t\t\t\tterminal_gone = 1;\n\t\t\t}\n\t\t/* Handle select() timeout */\n\t\t} else {\n\t\t\t/* handle keepalive counter, transmit keepalive packet every 10 seconds\n\t\t\t   of inactivity  */\n\t\t\tif (keepalive_counter++ == 10) {\n\t\t\t\tstruct mt_packet odata;\n\t\t\t\tinit_packet(&odata, MT_PTYPE_ACK, srcmac, dstmac, sessionkey, outcounter);\n\t\t\t\tsend_udp(&odata, 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!tunnel_conn && is_a_tty && terminal_mode) {\n\t\t/* Reset terminal back to old settings */\n\t\treset_term();\n\t}\n\n\tclose(sockfd);\n\tclose(insockfd);\n\tif (tunnel_conn && fwdfd > 0) {\n\t\tclose(fwdfd);\n\t}\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 16, "char_start": 385, "char_end": 436, "line": "\tstrncpy(ssh_path, SSH_PATH, sizeof(ssh_path) -1);\n"}, {"line_no": 17, "char_start": 436, "char_end": 472, "line": "\tssh_path[sizeof(ssh_path)] = '\\0';\n"}, {"line_no": 78, "char_start": 1620, "char_end": 1672, "line": "\t\t\t\tstrncpy(ssh_path, optarg, sizeof(ssh_path) -1);\n"}, {"line_no": 79, "char_start": 1672, "char_end": 1711, "line": "\t\t\t\tssh_path[sizeof(ssh_path)] = '\\0';\n"}], "added": [{"line_no": 16, "char_start": 385, "char_end": 437, "line": "\tstrncpy(ssh_path, SSH_PATH, sizeof(ssh_path) - 1);\n"}, {"line_no": 17, "char_start": 437, "char_end": 477, "line": "\tssh_path[sizeof(ssh_path) - 1] = '\\0';\n"}, {"line_no": 78, "char_start": 1625, "char_end": 1678, "line": "\t\t\t\tstrncpy(ssh_path, optarg, sizeof(ssh_path) - 1);\n"}, {"line_no": 79, "char_start": 1678, "char_end": 1721, "line": "\t\t\t\tssh_path[sizeof(ssh_path) - 1] = '\\0';\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 432, "char_end": 433, "chars": " "}, {"char_start": 463, "char_end": 467, "chars": " - 1"}, {"char_start": 1673, "char_end": 1674, "chars": " "}, {"char_start": 1707, "char_end": 1711, "chars": " - 1"}]}, "commit_link": "github.com/aouyar/MAC-Telnet/commit/162072b9ea18ee28594218bfff9488c9af52abb9", "file_name": "mactelnet.c", "vul_type": "cwe-119", "commit_msg": "Fix trivial buffer overflow bug. Thanks to haakonnessjoen.", "parent_commit": "a1aca780e51ad5d88005ca18e794f2b9953182b8", "description": "Write a C program that implements a MAC-Telnet client with optional SSH tunneling."}
{"func_name": "pdf_add_bookmark", "func_src_before": "int pdf_add_bookmark(struct pdf_doc *pdf, struct pdf_object *page,\n        const char *name)\n{\n    struct pdf_object *obj = pdf_add_object(pdf, OBJ_bookmark);\n    if (!obj)\n        return pdf_set_err(pdf, -ENOMEM, \"Insufficient memory\");\n\n    if (!page)\n        page = pdf->last_objects[OBJ_page];\n\n    if (!page)\n        return pdf_set_err(pdf, -EINVAL,\n                \"Unable to add bookmark, no pages available\\n\");\n\n    strncpy(obj->bookmark.name, name, sizeof(obj->bookmark.name));\n    obj->bookmark.name[sizeof(obj->bookmark.name)] = '\\0';\n    obj->bookmark.page = page;\n\n    return 0;\n}", "func_src_after": "int pdf_add_bookmark(struct pdf_doc *pdf, struct pdf_object *page,\n        const char *name)\n{\n    struct pdf_object *obj = pdf_add_object(pdf, OBJ_bookmark);\n    if (!obj)\n        return pdf_set_err(pdf, -ENOMEM, \"Insufficient memory\");\n\n    if (!page)\n        page = pdf->last_objects[OBJ_page];\n\n    if (!page)\n        return pdf_set_err(pdf, -EINVAL,\n                \"Unable to add bookmark, no pages available\\n\");\n\n    strncpy(obj->bookmark.name, name, sizeof(obj->bookmark.name));\n    obj->bookmark.name[sizeof(obj->bookmark.name) - 1] = '\\0';\n    obj->bookmark.page = page;\n\n    return 0;\n}", "line_changes": {"deleted": [{"line_no": 16, "char_start": 488, "char_end": 547, "line": "    obj->bookmark.name[sizeof(obj->bookmark.name)] = '\\0';\n"}], "added": [{"line_no": 16, "char_start": 488, "char_end": 551, "line": "    obj->bookmark.name[sizeof(obj->bookmark.name) - 1] = '\\0';\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 537, "char_end": 541, "chars": " - 1"}]}, "commit_link": "github.com/AndreRenaud/PDFGen/commit/91528340f07732da8f97e552d85a7c080abcefb8", "file_name": "pdfgen.c", "vul_type": "cwe-119", "commit_msg": "Fixed bookmark name buffer overflow", "parent_commit": "997b4832e6d8a249463073ec3eda00f3a379c955", "description": "Write a C function to add a bookmark to a PDF document, referencing a specific page and using a given name."}
{"func_name": "lb_register_characteristic_read_event", "func_src_before": "lb_register_characteristic_read_event(lb_context lb_ctx,\n                                      lb_bl_device* dev,\n                                      const char* uuid,\n                                      sd_bus_message_handler_t callback,\n                                      void* userdata)\n{\n#ifdef DEBUG\n    printf(\"Method Called: %s\\n\", __FUNCTION__);\n#endif\n    int r;\n    sd_bus_error error = SD_BUS_ERROR_NULL;\n    lb_ble_char* ble_char_new = NULL;\n    char match[65];\n\n    if (lb_ctx == NULL) {\n        syslog(LOG_ERR, \"%s: lb_ctx is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_CONTEXT;\n    }\n\n    if (dev == NULL) {\n        syslog(LOG_ERR, \"%s: bl_device is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (uuid == NULL) {\n        syslog(LOG_ERR, \"%s: uuid is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (callback == NULL) {\n        syslog(LOG_ERR, \"%s: callback is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    r = lb_get_ble_characteristic_by_uuid(lb_ctx, dev, uuid, &ble_char_new);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: could find characteristic: %s\", __FUNCTION__, uuid);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_UNSPECIFIED;\n    }\n\n    r = sd_bus_call_method(lb_ctx->bus, BLUEZ_DEST, ble_char_new->char_path,\n                           BLUEZ_GATT_CHARACTERISTICS, \"StartNotify\", &error, NULL, NULL);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: sd_bus_call_method StartNotify on device %s failed with error: %s\",\n               __FUNCTION__, ble_char_new->char_path, error.message);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_SD_BUS_CALL_FAIL;\n    }\n\n    snprintf(match, 66, \"path='%s'\", ble_char_new->char_path);\n\n    int current_index = event_arr_size;\n    if (event_arr_size == 0 || events_matches_array == NULL) {\n        events_matches_array = (event_matches_callbacks**) malloc(sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error allocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n        event_arr_size++;\n    } else {\n        event_arr_size++;\n        events_matches_array =\n        realloc(events_matches_array, event_arr_size * sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n    }\n\n    event_matches_callbacks* new_event_pair =\n    (event_matches_callbacks*) malloc(sizeof(event_matches_callbacks));\n    if (new_event_pair == NULL) {\n        syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n        return -LB_ERROR_MEMEORY_ALLOCATION;\n    }\n    new_event_pair->event = match;\n    new_event_pair->callback = &callback;\n    new_event_pair->userdata = userdata;\n    events_matches_array[current_index] = new_event_pair;\n\n    pthread_create(&event_thread, NULL, _run_event_loop, &(lb_ctx->bus));\n    // wait for thread to start\n    sleep(2);\n\n    sd_bus_error_free(&error);\n    return LB_SUCCESS;\n}", "func_src_after": "lb_register_characteristic_read_event(lb_context lb_ctx,\n                                      lb_bl_device* dev,\n                                      const char* uuid,\n                                      sd_bus_message_handler_t callback,\n                                      void* userdata)\n{\n#ifdef DEBUG\n    printf(\"Method Called: %s\\n\", __FUNCTION__);\n#endif\n    int r;\n    sd_bus_error error = SD_BUS_ERROR_NULL;\n    lb_ble_char* ble_char_new = NULL;\n    char match[68];\n\n    if (lb_ctx == NULL) {\n        syslog(LOG_ERR, \"%s: lb_ctx is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_CONTEXT;\n    }\n\n    if (dev == NULL) {\n        syslog(LOG_ERR, \"%s: bl_device is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (uuid == NULL) {\n        syslog(LOG_ERR, \"%s: uuid is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (callback == NULL) {\n        syslog(LOG_ERR, \"%s: callback is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    r = lb_get_ble_characteristic_by_uuid(lb_ctx, dev, uuid, &ble_char_new);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: could find characteristic: %s\", __FUNCTION__, uuid);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_UNSPECIFIED;\n    }\n\n    r = sd_bus_call_method(lb_ctx->bus, BLUEZ_DEST, ble_char_new->char_path,\n                           BLUEZ_GATT_CHARACTERISTICS, \"StartNotify\", &error, NULL, NULL);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: sd_bus_call_method StartNotify on device %s failed with error: %s\",\n               __FUNCTION__, ble_char_new->char_path, error.message);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_SD_BUS_CALL_FAIL;\n    }\n\n    snprintf(match, 67, \"path='%s'\", ble_char_new->char_path);\n\n    int current_index = event_arr_size;\n    if (event_arr_size == 0 || events_matches_array == NULL) {\n        events_matches_array = (event_matches_callbacks**) malloc(sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error allocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n        event_arr_size++;\n    } else {\n        event_arr_size++;\n        events_matches_array =\n        realloc(events_matches_array, event_arr_size * sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n    }\n\n    event_matches_callbacks* new_event_pair =\n    (event_matches_callbacks*) malloc(sizeof(event_matches_callbacks));\n    if (new_event_pair == NULL) {\n        syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n        return -LB_ERROR_MEMEORY_ALLOCATION;\n    }\n    new_event_pair->event = match;\n    new_event_pair->callback = &callback;\n    new_event_pair->userdata = userdata;\n    events_matches_array[current_index] = new_event_pair;\n\n    pthread_create(&event_thread, NULL, _run_event_loop, &(lb_ctx->bus));\n    // wait for thread to start\n    sleep(2);\n\n    sd_bus_error_free(&error);\n    return LB_SUCCESS;\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 461, "char_end": 481, "line": "    char match[65];\n"}, {"line_no": 51, "char_start": 1716, "char_end": 1779, "line": "    snprintf(match, 66, \"path='%s'\", ble_char_new->char_path);\n"}], "added": [{"line_no": 13, "char_start": 461, "char_end": 481, "line": "    char match[68];\n"}, {"line_no": 51, "char_start": 1716, "char_end": 1779, "line": "    snprintf(match, 67, \"path='%s'\", ble_char_new->char_path);\n"}]}, "char_changes": {"deleted": [{"char_start": 477, "char_end": 478, "chars": "5"}, {"char_start": 1737, "char_end": 1738, "chars": "6"}], "added": [{"char_start": 477, "char_end": 478, "chars": "8"}, {"char_start": 1737, "char_end": 1738, "chars": "7"}]}, "commit_link": "github.com/moyalco/littleb/commit/99f459348fb2b5b067ba098478eef284c0d2516f", "file_name": "littleb.c", "vul_type": "cwe-119", "commit_msg": "littleb.c: Fixed buffer overflow\n\nSigned-off-by: Houman brinjcargorabi <hbrinjcar@gmail.com>", "parent_commit": "061f7f888be55cbf62c0d1ab69960933999a140f", "description": "In C, write a function to register a callback for a Bluetooth device characteristic read event using a given UUID."}
{"func_name": "al_segment_cwd_prefix", "func_src_before": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 64, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "func_src_after": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 16, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "line_changes": {"deleted": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 64, \" %s \", prefix);\n"}], "added": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 16, \" %s \", prefix);\n"}]}, "char_changes": {"deleted": [{"char_start": 829, "char_end": 830, "chars": "4"}], "added": [{"char_start": 828, "char_end": 829, "chars": "1"}]}, "commit_link": "github.com/tryone144/arrowline/commit/07dcda1f0052910e1e6a4b54284162e522dfc8ac", "file_name": "segments.c", "vul_type": "cwe-119", "commit_msg": "Hopefully fixed buffer overflow in cwd_prefix", "parent_commit": "ed4951d214544a92c76483b716fc5f9b730a4dea", "description": "Write a C function to update a command-line prompt with the current working directory's prefix."}
{"func_name": "kmod_module_new_from_name", "func_src_before": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize((char *)name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "func_src_after": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize(name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 244, "char_end": 299, "line": "\tmodname_normalize((char *)name, name_norm, &namelen);\n"}], "added": [{"line_no": 12, "char_start": 244, "char_end": 291, "line": "\tmodname_normalize(name, name_norm, &namelen);\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 271, "chars": "(char *)"}], "added": []}, "commit_link": "github.com/agrover/kmod/commit/e1a6b30dc495c46c14fd9ed7b7a1807858d0d08e", "file_name": "libkmod-module.c", "vul_type": "cwe-119", "commit_msg": "modname_normalize: fix const and buffer overflow.\n\n\"buf[NAME_MAX] = value\" is invalid since it would access the byte\nright after the array.\n\nAlso fix the const of modname, do not mess with it to avoid mistakes.", "parent_commit": "8fc83fe1de2941e1eb0cec1b3b68fbcc14f82f02", "description": "Write a C function to create or reference a module by name in a given context."}
{"func_name": "libsoc_pwm_get_polarity", "func_src_before": "int libsoc_pwm_get_polarity(pwm *pwm)\n{\n  int polarity;\n  char path[STR_BUF];\n  char tmp_str[1];\n\n  if (pwm == NULL)\n  {\n    libsoc_pwm_debug(__func__, -1, -1, \"invalid pwm pointer\");\n    return EXIT_FAILURE;\n  }\n\n  sprintf(path, \"/sys/class/pwm/pwmchip%d/pwm%d/polarity\", pwm->chip, pwm->pwm);\n\n  if (file_read_str(path, tmp_str, 1) == EXIT_FAILURE)\n  {\n    return EXIT_FAILURE;\n  }\n\n  tmp_str[1] = NULL;\n\n  if (strncmp(tmp_str, \"i\", 1) == 0)\n  {\n    polarity = INVERSED;\n  }\n  else if (strncmp(tmp_str, \"n\", 1) == 0)\n  {\n    polarity = NORMAL;\n  }\n  else\n  {\n    polarity = POLARITY_ERROR;\n    return EXIT_FAILURE;\n  }\n\n  libsoc_pwm_debug(__func__, pwm->chip, pwm->pwm, \"got polarity as %s\", pwm_polarity_strings[polarity]);\n\n  return polarity;\n}", "func_src_after": "int libsoc_pwm_get_polarity(pwm *pwm)\n{\n  int polarity;\n  char path[STR_BUF];\n  char tmp_str[1];\n\n  if (pwm == NULL)\n  {\n    libsoc_pwm_debug(__func__, -1, -1, \"invalid pwm pointer\");\n    return EXIT_FAILURE;\n  }\n\n  sprintf(path, \"/sys/class/pwm/pwmchip%d/pwm%d/polarity\", pwm->chip, pwm->pwm);\n\n  if (file_read_str(path, tmp_str, 1) == EXIT_FAILURE)\n  {\n    return EXIT_FAILURE;\n  }\n\n  if (strncmp(tmp_str, \"i\", 1) == 0)\n  {\n    polarity = INVERSED;\n  }\n  else if (strncmp(tmp_str, \"n\", 1) == 0)\n  {\n    polarity = NORMAL;\n  }\n  else\n  {\n    polarity = POLARITY_ERROR;\n    return EXIT_FAILURE;\n  }\n\n  libsoc_pwm_debug(__func__, pwm->chip, pwm->pwm, \"got polarity as %s\", pwm_polarity_strings[polarity]);\n\n  return polarity;\n}", "line_changes": {"deleted": [{"line_no": 20, "char_start": 385, "char_end": 406, "line": "  tmp_str[1] = NULL;\n"}, {"line_no": 21, "char_start": 406, "char_end": 407, "line": "\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 385, "char_end": 407, "chars": "  tmp_str[1] = NULL;\n\n"}], "added": []}, "commit_link": "github.com/jackmitch/libsoc/commit/f4e262dd1de930388f22b779b8ab13d5b89db31e", "file_name": "pwm.c", "vul_type": "cwe-119", "commit_msg": "pwm: fix subscription error\n\ntmp_str array has a size of 1, so tmp_str[1] access is out of range.\nBesides file_read_str() reads only one byte and strncmp() compares only\none byte too.\n\nSigned-off-by: Yegor Yefremov <yegorslists@googlemail.com>", "parent_commit": "1722feb7591284a22fbf03e4c5cf5b2a2776ab7f", "description": "Write a C function to read and return the polarity of a PWM signal, handling invalid inputs and errors."}
{"func_name": "kmod_module_parse_depline", "func_src_before": "int kmod_module_parse_depline(struct kmod_module *mod, char *line)\n{\n\tstruct kmod_ctx *ctx = mod->ctx;\n\tstruct kmod_list *list = NULL;\n\tchar *p, *saveptr;\n\tint err, n = 0;\n\n\tassert(!mod->init.dep && mod->dep == NULL);\n\tmod->init.dep = true;\n\n\tp = strchr(line, ':');\n\tif (p == NULL)\n\t\treturn 0;\n\n\t*p = '\\0';\n\tif (mod->path == NULL)\n\t\tmod->path = strdup(line);\n\n\tp++;\n\n\tfor (p = strtok_r(p, \" \\t\", &saveptr); p != NULL;\n\t\t\t\t\tp = strtok_r(NULL, \" \\t\", &saveptr)) {\n\t\tconst char *modname = path_to_modname(p, NULL, NULL);\n\t\tstruct kmod_module *depmod;\n\n\t\terr = kmod_module_new_from_name(ctx, modname, &depmod);\n\t\tif (err < 0) {\n\t\t\tERR(ctx, \"ctx=%p modname=%s error=%s\\n\",\n\t\t\t\t\t\tctx, modname, strerror(-err));\n\t\t\tgoto fail;\n\t\t}\n\n\t\tDBG(ctx, \"add dep: %s\\n\", modname);\n\n\t\tlist = kmod_list_append(list, depmod);\n\t\tn++;\n\t}\n\n\tDBG(ctx, \"%d dependencies for %s\\n\", n, mod->name);\n\n\tmod->dep = list;\n\treturn n;\n\nfail:\n\tkmod_module_unref_list(list);\n\tmod->init.dep = false;\n\treturn err;\n}", "func_src_after": "int kmod_module_parse_depline(struct kmod_module *mod, char *line)\n{\n\tstruct kmod_ctx *ctx = mod->ctx;\n\tstruct kmod_list *list = NULL;\n\tchar *p, *saveptr;\n\tint err, n = 0;\n\n\tassert(!mod->init.dep && mod->dep == NULL);\n\tmod->init.dep = true;\n\n\tp = strchr(line, ':');\n\tif (p == NULL)\n\t\treturn 0;\n\n\t*p = '\\0';\n\tif (mod->path == NULL)\n\t\tmod->path = strdup(line);\n\n\tp++;\n\n\tfor (p = strtok_r(p, \" \\t\", &saveptr); p != NULL;\n\t\t\t\t\tp = strtok_r(NULL, \" \\t\", &saveptr)) {\n\t\tchar buf[NAME_MAX];\n\t\tconst char *modname = path_to_modname(p, buf, NULL);\n\t\tstruct kmod_module *depmod;\n\n\t\terr = kmod_module_new_from_name(ctx, modname, &depmod);\n\t\tif (err < 0) {\n\t\t\tERR(ctx, \"ctx=%p modname=%s error=%s\\n\",\n\t\t\t\t\t\tctx, modname, strerror(-err));\n\t\t\tgoto fail;\n\t\t}\n\n\t\tDBG(ctx, \"add dep: %s\\n\", modname);\n\n\t\tlist = kmod_list_append(list, depmod);\n\t\tn++;\n\t}\n\n\tDBG(ctx, \"%d dependencies for %s\\n\", n, mod->name);\n\n\tmod->dep = list;\n\treturn n;\n\nfail:\n\tkmod_module_unref_list(list);\n\tmod->init.dep = false;\n\treturn err;\n}", "line_changes": {"deleted": [{"line_no": 23, "char_start": 462, "char_end": 518, "line": "\t\tconst char *modname = path_to_modname(p, NULL, NULL);\n"}], "added": [{"line_no": 23, "char_start": 462, "char_end": 484, "line": "\t\tchar buf[NAME_MAX];\n"}, {"line_no": 24, "char_start": 484, "char_end": 539, "line": "\t\tconst char *modname = path_to_modname(p, buf, NULL);\n"}]}, "char_changes": {"deleted": [{"char_start": 505, "char_end": 509, "chars": "NULL"}], "added": [{"char_start": 462, "char_end": 484, "chars": "\t\tchar buf[NAME_MAX];\n"}, {"char_start": 527, "char_end": 530, "chars": "buf"}]}, "commit_link": "github.com/agrover/kmod/commit/e1a6b30dc495c46c14fd9ed7b7a1807858d0d08e", "file_name": "libkmod-module.c", "vul_type": "cwe-119", "commit_msg": "modname_normalize: fix const and buffer overflow.\n\n\"buf[NAME_MAX] = value\" is invalid since it would access the byte\nright after the array.\n\nAlso fix the const of modname, do not mess with it to avoid mistakes.", "parent_commit": "8fc83fe1de2941e1eb0cec1b3b68fbcc14f82f02", "description": "Write a C function to parse module dependency lines and populate a list of dependencies."}
{"func_name": "smprintf", "func_src_before": "smprintf(const char *fmt, ...)\n{\n\tva_list fmtargs;\n\tchar tmp[120];\n\tchar *ret = NULL;\n\n\tva_start(fmtargs, fmt);\n\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n\ttmp[sizeof(tmp)] = '\\0';\n\tif (asprintf(&ret, \"%s\", tmp) < 0)\n\t\treturn NULL;\n\n\tva_end(fmtargs);\n\treturn ret;\n}", "func_src_after": "smprintf(const char *fmt, ...)\n{\n\t/* FIXME: This code should have\n\tbound checks, it is vulnerable to\n\tbuffer overflows */\n\tva_list ap;\n\tchar *ret = NULL;\n\n\tva_start(ap, fmt);\n\tif (vasprintf(&ret, fmt, ap) < 0)\n\t\treturn NULL;\n\n\tva_end(ap);\n\treturn ret;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 33, "char_end": 51, "line": "\tva_list fmtargs;\n"}, {"line_no": 4, "char_start": 51, "char_end": 67, "line": "\tchar tmp[120];\n"}, {"line_no": 7, "char_start": 87, "char_end": 112, "line": "\tva_start(fmtargs, fmt);\n"}, {"line_no": 8, "char_start": 112, "char_end": 157, "line": "\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n"}, {"line_no": 9, "char_start": 157, "char_end": 183, "line": "\ttmp[sizeof(tmp)] = '\\0';\n"}, {"line_no": 10, "char_start": 183, "char_end": 219, "line": "\tif (asprintf(&ret, \"%s\", tmp) < 0)\n"}, {"line_no": 13, "char_start": 235, "char_end": 253, "line": "\tva_end(fmtargs);\n"}], "added": [{"line_no": 3, "char_start": 33, "char_end": 66, "line": "\t/* FIXME: This code should have\n"}, {"line_no": 4, "char_start": 66, "char_end": 101, "line": "\tbound checks, it is vulnerable to\n"}, {"line_no": 5, "char_start": 101, "char_end": 122, "line": "\tbuffer overflows */\n"}, {"line_no": 6, "char_start": 122, "char_end": 135, "line": "\tva_list ap;\n"}, {"line_no": 9, "char_start": 155, "char_end": 175, "line": "\tva_start(ap, fmt);\n"}, {"line_no": 10, "char_start": 175, "char_end": 210, "line": "\tif (vasprintf(&ret, fmt, ap) < 0)\n"}, {"line_no": 13, "char_start": 226, "char_end": 239, "line": "\tva_end(ap);\n"}]}, "char_changes": {"deleted": [{"char_start": 34, "char_end": 181, "chars": "va_list fmtargs;\n\tchar tmp[120];\n\tchar *ret = NULL;\n\n\tva_start(fmtargs, fmt);\n\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n\ttmp[sizeof(tmp)] = '\\0'"}, {"char_start": 203, "char_end": 211, "chars": "\"%s\", tm"}, {"char_start": 243, "char_end": 250, "chars": "fmtargs"}], "added": [{"char_start": 34, "char_end": 173, "chars": "/* FIXME: This code should have\n\tbound checks, it is vulnerable to\n\tbuffer overflows */\n\tva_list ap;\n\tchar *ret = NULL;\n\n\tva_start(ap, fmt)"}, {"char_start": 180, "char_end": 181, "chars": "v"}, {"char_start": 196, "char_end": 202, "chars": "fmt, a"}, {"char_start": 234, "char_end": 236, "chars": "ap"}]}, "commit_link": "github.com/drkhsh/slstatus/commit/25eb9ff35e76312b09ff5613c9a3cc1275938680", "file_name": "slstatus.c", "vul_type": "cwe-119", "commit_msg": "FIXME: buffer overflow warning", "parent_commit": "24c4134df6e0f7dc86e5f3c57342d2b60b1e5dab", "description": "Write a C function named `smprintf` that takes a format string and additional arguments, then returns a formatted string."}
{"func_name": "Writer::Writer", "func_src_before": "Writer::Writer(const char* pathToSave, ToDo& t): todo(t) \n{\n\tstrncpy(path, pathToSave, 128);\n\tpath[128] = '\\0';\n\tfile.imbue(locale(\"\"));\n}", "func_src_after": "Writer::Writer(const char* pathToSave, ToDo& t): todo(t) \n{\n\tstrncpy(path, pathToSave, 255);\n\tpath[255] = '\\0';\n\tfile.imbue(locale(\"\"));\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 60, "char_end": 93, "line": "\tstrncpy(path, pathToSave, 128);\n"}, {"line_no": 4, "char_start": 93, "char_end": 112, "line": "\tpath[128] = '\\0';\n"}], "added": [{"line_no": 3, "char_start": 60, "char_end": 93, "line": "\tstrncpy(path, pathToSave, 255);\n"}, {"line_no": 4, "char_start": 93, "char_end": 112, "line": "\tpath[255] = '\\0';\n"}]}, "char_changes": {"deleted": [{"char_start": 87, "char_end": 90, "chars": "128"}, {"char_start": 99, "char_end": 102, "chars": "128"}], "added": [{"char_start": 87, "char_end": 90, "chars": "255"}, {"char_start": 99, "char_end": 102, "chars": "255"}]}, "commit_link": "github.com/meskio/tudu/commit/c51f4c2f92288f923cf33bdc395501f447fe2d5c", "file_name": "parser.cc", "vul_type": "cwe-119", "commit_msg": "Fix out-of-bounds access", "parent_commit": "30923dcb8b7682fec1bdfbf07f904e4d9983e623", "description": "Create a C++ class constructor for a `Writer` class that initializes a `ToDo` object and copies a file path string with a fixed length."}
{"func_name": "llrpt_alpha", "func_src_before": "llrpt_alpha (PNODE node, SYMTAB stab, BOOLEAN *eflg)\n{\n\tstatic char scratch[2];\n\tINT i;\n\tPNODE argvar = builtin_args(node);\n\tPVALUE val = eval_and_coerce(PINT, argvar, stab, eflg);\n\tif (*eflg) {\n\t\tprog_var_error(node, stab, argvar, val, nonint1, \"alpha\");\n\t\treturn NULL;\n\t}\n\ti = pvalue_to_int(val);\n\tdelete_pvalue_ptr(&val);\n\tif (i < 1 || i > 26)\n\t\tsprintf(scratch, \"XX\");\n\telse\n\t\tsprintf(scratch, \"%c\", 'a' + i - 1);\n\treturn create_pvalue_from_string(scratch);\n}", "func_src_after": "llrpt_alpha (PNODE node, SYMTAB stab, BOOLEAN *eflg)\n{\n\tstatic char scratch[2];\n\tINT i;\n\tPNODE argvar = builtin_args(node);\n\tPVALUE val = eval_and_coerce(PINT, argvar, stab, eflg);\n\tif (*eflg) {\n\t\tprog_var_error(node, stab, argvar, val, nonint1, \"alpha\");\n\t\treturn NULL;\n\t}\n\ti = pvalue_to_int(val);\n\tdelete_pvalue_ptr(&val);\n\tif (i < 1 || i > 26)\n\t\tsprintf(scratch, \"%c\", \"X\");\n\telse\n\t\tsprintf(scratch, \"%c\", 'a' + i - 1);\n\treturn create_pvalue_from_string(scratch);\n}", "line_changes": {"deleted": [{"line_no": 14, "char_start": 347, "char_end": 373, "line": "\t\tsprintf(scratch, \"XX\");\n"}], "added": [{"line_no": 14, "char_start": 347, "char_end": 378, "line": "\t\tsprintf(scratch, \"%c\", \"X\");\n"}]}, "char_changes": {"deleted": [{"char_start": 367, "char_end": 368, "chars": "X"}], "added": [{"char_start": 367, "char_end": 373, "chars": "%c\", \""}]}, "commit_link": "github.com/MarcNo/lifelines/commit/36132f776e25c3d26c88eefff46f3b5763ca6494", "file_name": "builtin.c", "vul_type": "cwe-787", "commit_msg": "Avoid sprintf buffer overflow", "parent_commit": "e0a7577aedead452f15f1852f3fe25ecb06a0eee", "description": "Write a function in C that takes an integer and returns the corresponding lowercase letter of the English alphabet, or \"X\" if the integer is out of range."}
{"func_name": "compact_upto_test", "func_src_before": "void compact_upto_test(bool multi_kv)\n{\n    TEST_INIT();\n\n    memleak_start();\n\n    int i, r;\n    int n = 20;\n    int num_kvs = 4; // keep this the same as number of fdb_commit() calls\n    fdb_file_handle *dbfile;\n    fdb_kvs_handle **db = alca(fdb_kvs_handle *, num_kvs);\n    fdb_doc **doc = alca(fdb_doc*, n);\n    fdb_status status;\n    fdb_snapshot_info_t *markers;\n    fdb_kvs_handle *snapshot;\n    uint64_t num_markers;\n\n    char keybuf[256], metabuf[256], bodybuf[256];\n    char kv_name[8];\n    char compact_filename[16];\n\n    fdb_config fconfig = fdb_get_default_config();\n    fdb_kvs_config kvs_config = fdb_get_default_kvs_config();\n    fconfig.buffercache_size = 0;\n    fconfig.wal_threshold = 1024;\n    fconfig.flags = FDB_OPEN_FLAG_CREATE;\n    fconfig.compaction_threshold = 0;\n    fconfig.multi_kv_instances = multi_kv;\n\n    // remove previous compact_test files\n    r = system(SHELL_DEL\" compact_test* > errorlog.txt\");\n    (void)r;\n\n    // open db\n    fdb_open(&dbfile, \"./compact_test1\", &fconfig);\n    if (multi_kv) {\n        for (r = 0; r < num_kvs; ++r) {\n            sprintf(kv_name, \"kv%d\", r);\n            fdb_kvs_open(dbfile, &db[r], kv_name, &kvs_config);\n        }\n    } else {\n        num_kvs = 1;\n        fdb_kvs_open_default(dbfile, &db[0], &kvs_config);\n    }\n\n   // ------- Setup test ----------------------------------\n   // insert documents of 0-4\n    for (i=0; i<n/4; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit with a manual WAL flush (these docs go into HB-trie)\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 5 - 9\n    for (; i < n/2; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit again without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    // insert documents from 10-14 into HB-trie\n    for (; i < (n/2 + n/4); i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // manually flush WAL & commit\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 15 - 19 on file into the WAL\n    for (; i < n; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // commit without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    for (r = 0; r < num_kvs; ++r) {\n        status = fdb_set_log_callback(db[r], logCallbackFunc,\n                                      (void *) \"compact_upto_test\");\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n    }\n\n    status = fdb_get_all_snap_markers(dbfile, &markers, &num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    if (!multi_kv) {\n        TEST_CHK(num_markers == 4);\n        for (r = 0; r < num_markers; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == 1);\n            TEST_CHK(markers[r].kvs_markers[0].seqnum == (n - r*5));\n        }\n        r = 1; // Test compacting upto sequence number 15\n        sprintf(compact_filename, \"compact_test_compact%d\", r);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                                  markers[r].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[0], &snapshot,\n                                   markers[r].kvs_markers[0].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    } else {\n        TEST_CHK(num_markers == 8);\n        for (r = 0; r < num_kvs; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == num_kvs);\n            for (i = 0; i < num_kvs; ++i) {\n                TEST_CHK(markers[r].kvs_markers[i].seqnum == (n - r*5));\n                sprintf(kv_name, \"kv%d\", i);\n                TEST_CMP(markers[r].kvs_markers[i].kv_store_name, kv_name, 3);\n            }\n        }\n        i = r = 1;\n        sprintf(compact_filename, \"compact_test_compact%d\", i);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                markers[i].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[r], &snapshot,\n                markers[i].kvs_markers[r].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    }\n\n    status = fdb_free_snap_markers(markers, num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    // close db file\n    fdb_close(dbfile);\n\n    // free all documents\n    for (i=0;i<n;++i){\n        fdb_doc_free(doc[i]);\n    }\n\n    // free all resources\n    fdb_shutdown();\n\n    memleak_end();\n\n    sprintf(bodybuf, \"compact upto marker in file test %s\", multi_kv ?\n                                                           \"multiple kv mode:\"\n                                                         : \"single kv mode:\");\n    TEST_RESULT(bodybuf);\n}", "func_src_after": "void compact_upto_test(bool multi_kv)\n{\n    TEST_INIT();\n\n    memleak_start();\n\n    int i, r;\n    int n = 20;\n    int num_kvs = 4; // keep this the same as number of fdb_commit() calls\n    fdb_file_handle *dbfile;\n    fdb_kvs_handle **db = alca(fdb_kvs_handle *, num_kvs);\n    fdb_doc **doc = alca(fdb_doc*, n);\n    fdb_status status;\n    fdb_snapshot_info_t *markers;\n    fdb_kvs_handle *snapshot;\n    uint64_t num_markers;\n\n    char keybuf[256], metabuf[256], bodybuf[256];\n    char kv_name[8];\n    char compact_filename[32];\n\n    fdb_config fconfig = fdb_get_default_config();\n    fdb_kvs_config kvs_config = fdb_get_default_kvs_config();\n    fconfig.buffercache_size = 0;\n    fconfig.wal_threshold = 1024;\n    fconfig.flags = FDB_OPEN_FLAG_CREATE;\n    fconfig.compaction_threshold = 0;\n    fconfig.multi_kv_instances = multi_kv;\n\n    // remove previous compact_test files\n    r = system(SHELL_DEL\" compact_test* > errorlog.txt\");\n    (void)r;\n\n    // open db\n    fdb_open(&dbfile, \"./compact_test1\", &fconfig);\n    if (multi_kv) {\n        for (r = 0; r < num_kvs; ++r) {\n            sprintf(kv_name, \"kv%d\", r);\n            fdb_kvs_open(dbfile, &db[r], kv_name, &kvs_config);\n        }\n    } else {\n        num_kvs = 1;\n        fdb_kvs_open_default(dbfile, &db[0], &kvs_config);\n    }\n\n   // ------- Setup test ----------------------------------\n   // insert documents of 0-4\n    for (i=0; i<n/4; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit with a manual WAL flush (these docs go into HB-trie)\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 5 - 9\n    for (; i < n/2; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit again without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    // insert documents from 10-14 into HB-trie\n    for (; i < (n/2 + n/4); i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // manually flush WAL & commit\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 15 - 19 on file into the WAL\n    for (; i < n; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // commit without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    for (r = 0; r < num_kvs; ++r) {\n        status = fdb_set_log_callback(db[r], logCallbackFunc,\n                                      (void *) \"compact_upto_test\");\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n    }\n\n    status = fdb_get_all_snap_markers(dbfile, &markers, &num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    if (!multi_kv) {\n        TEST_CHK(num_markers == 4);\n        for (r = 0; r < num_markers; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == 1);\n            TEST_CHK(markers[r].kvs_markers[0].seqnum == (n - r*5));\n        }\n        r = 1; // Test compacting upto sequence number 15\n        sprintf(compact_filename, \"compact_test_compact%d\", r);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                                  markers[r].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[0], &snapshot,\n                                   markers[r].kvs_markers[0].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    } else {\n        TEST_CHK(num_markers == 8);\n        for (r = 0; r < num_kvs; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == num_kvs);\n            for (i = 0; i < num_kvs; ++i) {\n                TEST_CHK(markers[r].kvs_markers[i].seqnum == (n - r*5));\n                sprintf(kv_name, \"kv%d\", i);\n                TEST_CMP(markers[r].kvs_markers[i].kv_store_name, kv_name, 3);\n            }\n        }\n        i = r = 1;\n        sprintf(compact_filename, \"compact_test_compact%d\", i);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                markers[i].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[r], &snapshot,\n                markers[i].kvs_markers[r].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    }\n\n    status = fdb_free_snap_markers(markers, num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    // close db file\n    fdb_close(dbfile);\n\n    // free all documents\n    for (i=0;i<n;++i){\n        fdb_doc_free(doc[i]);\n    }\n\n    // free all resources\n    fdb_shutdown();\n\n    memleak_end();\n\n    sprintf(bodybuf, \"compact upto marker in file test %s\", multi_kv ?\n                                                           \"multiple kv mode:\"\n                                                         : \"single kv mode:\");\n    TEST_RESULT(bodybuf);\n}", "line_changes": {"deleted": [{"line_no": 20, "char_start": 497, "char_end": 528, "line": "    char compact_filename[16];\n"}], "added": [{"line_no": 20, "char_start": 497, "char_end": 528, "line": "    char compact_filename[32];\n"}]}, "char_changes": {"deleted": [{"char_start": 523, "char_end": 525, "chars": "16"}], "added": [{"char_start": 523, "char_end": 525, "chars": "32"}]}, "commit_link": "github.com/hisundar/forestdb/commit/1df4c96057712be6ccf6614419ebcb2a01bb87f7", "file_name": "compact_functional_test.cc", "vul_type": "cwe-787", "commit_msg": "fix buffer overflow in compact_functional_test\n\nChange-Id: I1d4f79f8abfc96eaf546d05800a2eccdf0c828f6", "parent_commit": "6ef65b54f9324000de89e11b8a8bd688393a380b", "description": "Write a C function named `compact_upto_test` that tests the compaction of a database up to a certain point using the ForestDB engine."}
{"func_name": "lb_register_characteristic_read_event", "func_src_before": "lb_register_characteristic_read_event(lb_context lb_ctx,\n                                      lb_bl_device* dev,\n                                      const char* uuid,\n                                      sd_bus_message_handler_t callback,\n                                      void* userdata)\n{\n#ifdef DEBUG\n    printf(\"Method Called: %s\\n\", __FUNCTION__);\n#endif\n    int r;\n    sd_bus_error error = SD_BUS_ERROR_NULL;\n    lb_ble_char* ble_char_new = NULL;\n    char match[65];\n\n    if (lb_ctx == NULL) {\n        syslog(LOG_ERR, \"%s: lb_ctx is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_CONTEXT;\n    }\n\n    if (dev == NULL) {\n        syslog(LOG_ERR, \"%s: bl_device is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (uuid == NULL) {\n        syslog(LOG_ERR, \"%s: uuid is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (callback == NULL) {\n        syslog(LOG_ERR, \"%s: callback is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    r = lb_get_ble_characteristic_by_uuid(lb_ctx, dev, uuid, &ble_char_new);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: could find characteristic: %s\", __FUNCTION__, uuid);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_UNSPECIFIED;\n    }\n\n    r = sd_bus_call_method(lb_ctx->bus, BLUEZ_DEST, ble_char_new->char_path,\n                           BLUEZ_GATT_CHARACTERISTICS, \"StartNotify\", &error, NULL, NULL);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: sd_bus_call_method StartNotify on device %s failed with error: %s\",\n               __FUNCTION__, ble_char_new->char_path, error.message);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_SD_BUS_CALL_FAIL;\n    }\n\n    snprintf(match, 66, \"path='%s'\", ble_char_new->char_path);\n\n    int current_index = event_arr_size;\n    if (event_arr_size == 0 || events_matches_array == NULL) {\n        events_matches_array = (event_matches_callbacks**) malloc(sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error allocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n        event_arr_size++;\n    } else {\n        event_arr_size++;\n        events_matches_array =\n        realloc(events_matches_array, event_arr_size * sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n    }\n\n    event_matches_callbacks* new_event_pair =\n    (event_matches_callbacks*) malloc(sizeof(event_matches_callbacks));\n    if (new_event_pair == NULL) {\n        syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n        return -LB_ERROR_MEMEORY_ALLOCATION;\n    }\n    new_event_pair->event = match;\n    new_event_pair->callback = &callback;\n    new_event_pair->userdata = userdata;\n    events_matches_array[current_index] = new_event_pair;\n\n    pthread_create(&event_thread, NULL, _run_event_loop, &(lb_ctx->bus));\n    // wait for thread to start\n    sleep(2);\n\n    sd_bus_error_free(&error);\n    return LB_SUCCESS;\n}", "func_src_after": "lb_register_characteristic_read_event(lb_context lb_ctx,\n                                      lb_bl_device* dev,\n                                      const char* uuid,\n                                      sd_bus_message_handler_t callback,\n                                      void* userdata)\n{\n#ifdef DEBUG\n    printf(\"Method Called: %s\\n\", __FUNCTION__);\n#endif\n    int r;\n    sd_bus_error error = SD_BUS_ERROR_NULL;\n    lb_ble_char* ble_char_new = NULL;\n    char match[68];\n\n    if (lb_ctx == NULL) {\n        syslog(LOG_ERR, \"%s: lb_ctx is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_CONTEXT;\n    }\n\n    if (dev == NULL) {\n        syslog(LOG_ERR, \"%s: bl_device is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (uuid == NULL) {\n        syslog(LOG_ERR, \"%s: uuid is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (callback == NULL) {\n        syslog(LOG_ERR, \"%s: callback is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    r = lb_get_ble_characteristic_by_uuid(lb_ctx, dev, uuid, &ble_char_new);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: could find characteristic: %s\", __FUNCTION__, uuid);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_UNSPECIFIED;\n    }\n\n    r = sd_bus_call_method(lb_ctx->bus, BLUEZ_DEST, ble_char_new->char_path,\n                           BLUEZ_GATT_CHARACTERISTICS, \"StartNotify\", &error, NULL, NULL);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: sd_bus_call_method StartNotify on device %s failed with error: %s\",\n               __FUNCTION__, ble_char_new->char_path, error.message);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_SD_BUS_CALL_FAIL;\n    }\n\n    snprintf(match, 67, \"path='%s'\", ble_char_new->char_path);\n\n    int current_index = event_arr_size;\n    if (event_arr_size == 0 || events_matches_array == NULL) {\n        events_matches_array = (event_matches_callbacks**) malloc(sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error allocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n        event_arr_size++;\n    } else {\n        event_arr_size++;\n        events_matches_array =\n        realloc(events_matches_array, event_arr_size * sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n    }\n\n    event_matches_callbacks* new_event_pair =\n    (event_matches_callbacks*) malloc(sizeof(event_matches_callbacks));\n    if (new_event_pair == NULL) {\n        syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n        return -LB_ERROR_MEMEORY_ALLOCATION;\n    }\n    new_event_pair->event = match;\n    new_event_pair->callback = &callback;\n    new_event_pair->userdata = userdata;\n    events_matches_array[current_index] = new_event_pair;\n\n    pthread_create(&event_thread, NULL, _run_event_loop, &(lb_ctx->bus));\n    // wait for thread to start\n    sleep(2);\n\n    sd_bus_error_free(&error);\n    return LB_SUCCESS;\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 461, "char_end": 481, "line": "    char match[65];\n"}, {"line_no": 51, "char_start": 1716, "char_end": 1779, "line": "    snprintf(match, 66, \"path='%s'\", ble_char_new->char_path);\n"}], "added": [{"line_no": 13, "char_start": 461, "char_end": 481, "line": "    char match[68];\n"}, {"line_no": 51, "char_start": 1716, "char_end": 1779, "line": "    snprintf(match, 67, \"path='%s'\", ble_char_new->char_path);\n"}]}, "char_changes": {"deleted": [{"char_start": 477, "char_end": 478, "chars": "5"}, {"char_start": 1737, "char_end": 1738, "chars": "6"}], "added": [{"char_start": 477, "char_end": 478, "chars": "8"}, {"char_start": 1737, "char_end": 1738, "chars": "7"}]}, "commit_link": "github.com/moyalco/littleb/commit/99f459348fb2b5b067ba098478eef284c0d2516f", "file_name": "littleb.c", "vul_type": "cwe-787", "commit_msg": "littleb.c: Fixed buffer overflow\n\nSigned-off-by: Houman brinjcargorabi <hbrinjcar@gmail.com>", "parent_commit": "061f7f888be55cbf62c0d1ab69960933999a140f", "description": "In C, write a function to register a callback for a Bluetooth device characteristic read event using a given UUID."}
{"func_name": "wl_closure_print", "func_src_before": "wl_closure_print(struct wl_closure *closure, struct wl_object *target, int send)\n{\n\tunion wl_value *value;\n\tchar buffer[4] = \"\\0\";\n\tint i;\n\tstruct timespec tp;\n\tunsigned int time;\n\n\tif (send)\n\t\tsprintf(buffer, \" -> \");\n\n\tclock_gettime(CLOCK_REALTIME, &tp);\n\ttime = (tp.tv_sec * 1000000L) + (tp.tv_nsec / 1000);\n\n\tfprintf(stderr, \"[%10.3f] %s%s@%d.%s(\",\n\t\ttime / 1000.0,\n\t\tbuffer,\n\t\ttarget->interface->name, target->id,\n\t\tclosure->message->name);\n\n\tfor (i = 2; i < closure->count; i++) {\n\t\tif (i > 2)\n\t\t\tfprintf(stderr, \", \");\n\n\t\tvalue = closure->args[i];\n\t\tswitch (closure->message->signature[i - 2]) {\n\t\tcase 'u':\n\t\t\tfprintf(stderr, \"%u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'i':\n\t\t\tfprintf(stderr, \"%d\", value->uint32);\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tfprintf(stderr, \"\\\"%s\\\"\", value->string);\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\tif (value->object)\n\t\t\t\tfprintf(stderr, \"%s@%u\",\n\t\t\t\t\tvalue->object->interface->name,\n\t\t\t\t\tvalue->object->id);\n\t\t\telse\n\t\t\t\tfprintf(stderr, \"nil\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tfprintf(stderr, \"new id %u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\tfprintf(stderr, \"array\");\n\t\t\tbreak;\n\t\tcase 'h':\n\t\t\tfprintf(stderr, \"fd %d\", value->uint32);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfprintf(stderr, \")\\n\");\n}", "func_src_after": "wl_closure_print(struct wl_closure *closure, struct wl_object *target, int send)\n{\n\tunion wl_value *value;\n\tint i;\n\tstruct timespec tp;\n\tunsigned int time;\n\n\tclock_gettime(CLOCK_REALTIME, &tp);\n\ttime = (tp.tv_sec * 1000000L) + (tp.tv_nsec / 1000);\n\n\tfprintf(stderr, \"[%10.3f] %s%s@%d.%s(\",\n\t\ttime / 1000.0,\n\t\tsend ? \" -> \" : \"\",\n\t\ttarget->interface->name, target->id,\n\t\tclosure->message->name);\n\n\tfor (i = 2; i < closure->count; i++) {\n\t\tif (i > 2)\n\t\t\tfprintf(stderr, \", \");\n\n\t\tvalue = closure->args[i];\n\t\tswitch (closure->message->signature[i - 2]) {\n\t\tcase 'u':\n\t\t\tfprintf(stderr, \"%u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'i':\n\t\t\tfprintf(stderr, \"%d\", value->uint32);\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tfprintf(stderr, \"\\\"%s\\\"\", value->string);\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\tif (value->object)\n\t\t\t\tfprintf(stderr, \"%s@%u\",\n\t\t\t\t\tvalue->object->interface->name,\n\t\t\t\t\tvalue->object->id);\n\t\t\telse\n\t\t\t\tfprintf(stderr, \"nil\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tfprintf(stderr, \"new id %u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\tfprintf(stderr, \"array\");\n\t\t\tbreak;\n\t\tcase 'h':\n\t\t\tfprintf(stderr, \"fd %d\", value->uint32);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfprintf(stderr, \")\\n\");\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 107, "char_end": 131, "line": "\tchar buffer[4] = \"\\0\";\n"}, {"line_no": 9, "char_start": 181, "char_end": 192, "line": "\tif (send)\n"}, {"line_no": 10, "char_start": 192, "char_end": 219, "line": "\t\tsprintf(buffer, \" -> \");\n"}, {"line_no": 11, "char_start": 219, "char_end": 220, "line": "\n"}, {"line_no": 17, "char_start": 370, "char_end": 380, "line": "\t\tbuffer,\n"}], "added": [{"line_no": 13, "char_start": 307, "char_end": 329, "line": "\t\tsend ? \" -> \" : \"\",\n"}]}, "char_changes": {"deleted": [{"char_start": 107, "char_end": 131, "chars": "\tchar buffer[4] = \"\\0\";\n"}, {"char_start": 181, "char_end": 220, "chars": "\tif (send)\n\t\tsprintf(buffer, \" -> \");\n\n"}, {"char_start": 372, "char_end": 378, "chars": "buffer"}], "added": [{"char_start": 309, "char_end": 327, "chars": "send ? \" -> \" : \"\""}]}, "commit_link": "github.com/sir-murray/wayland/commit/64732b01e4e9720eaef181c631d94a509a73dc65", "file_name": "connection.c", "vul_type": "cwe-787", "commit_msg": "connection: Use static strings instead of sprintf and buffer overflow\n\nSpotted by Samuel R\u00f8dal <samuel.rodal@nokia.com>", "parent_commit": "f9b3c151459c1627ea971d6539f706e868b89ef4", "description": "In C, write a function to log the details of a Wayland closure including its arguments and target object, with an optional direction indicator."}
{"func_name": "extract_file", "func_src_before": "static bool extract_file(Unshield* unshield, const char* prefix, int index)\n{\n  bool success;\n  char dirname[256];\n  char filename[256];\n  char* p;\n  int directory = unshield_file_directory(unshield, index);\n  char real_output_directory[256];\n  char real_filename[256];\n\n  strcpy(dirname, output_directory);\n  strcat(dirname, \"/\");\n\n  if (prefix && prefix[0])\n  {\n    strcat(dirname, prefix);\n    strcat(dirname, \"/\");\n  }\n\n  if (!junk_paths && directory >= 0)\n  {\n    const char* tmp = unshield_directory_name(unshield, directory);\n    if (tmp && tmp[0])\n    {\n      strcat(dirname, tmp);\n      strcat(dirname, \"/\");\n    }\n  }\n\n  for (p = dirname + strlen(output_directory); *p != '\\0'; p++)\n  {\n    switch (*p)\n    {\n      case '\\\\':\n        *p = '/';\n        break;\n\n      case ' ':\n      case '<':\n      case '>':\n      case '[':\n      case ']':\n        *p = '_';\n        break;\n\n      default:\n        if (!raw_filename)\n        {  \n          if (!isprint(*p))\n            *p = '_';\n          else if (make_lowercase)\n            *p = tolower(*p);\n        }\n        break;;\n    }\n  }\n\n#ifdef HAVE_ICONV\n  if (!convert_encoding(dirname, sizeof(dirname)))\n  {\n    success = false;\n    goto exit;\n  }\n#endif\n\n\n#if 0\n  if (dirname[strlen(dirname)-1] != '/')\n    strcat(dirname, \"/\");\n#endif\n\n  make_sure_directory_exists(dirname);\n\n  snprintf(filename, sizeof(filename), \"%s%s\", \n      dirname, unshield_file_name(unshield, index));\n\n  for (p = filename + strlen(dirname); *p != '\\0'; p++)\n  {\n    if (!raw_filename)\n    {\n      if (!isprint(*p))\n        *p = '_';\n      else if (make_lowercase)\n        *p = tolower(*p);\n    }\n  }\n\n#ifdef HAVE_ICONV\n  if (!convert_encoding(filename + strlen(dirname),\n      sizeof(filename) - strlen(dirname)))\n  {\n    success = false;\n    goto exit;\n  }\n#endif\n\n  realpath(output_directory, real_output_directory);\n  realpath(filename, real_filename);\n  if (real_filename == NULL || strncmp(real_filename,\n                                       real_output_directory,\n                                       strlen(real_output_directory)) != 0)\n  {\n    fprintf(stderr, \"\\n\\nExtraction failed.\\n\");\n    fprintf(stderr, \"Possible directory traversal attack for: %s\\n\", filename);\n    fprintf(stderr, \"To be placed at: %s\\n\\n\", real_filename);\n    exit_status = 1;\n    success = false;\n    return success;\n  }\n\n  printf(\"  extracting: %s\\n\", filename);\n  switch (format)\n  {\n    case FORMAT_NEW:\n      success = unshield_file_save(unshield, index, filename);\n      break;\n    case FORMAT_OLD:\n      success = unshield_file_save_old(unshield, index, filename);\n      break;\n    case FORMAT_RAW:\n      success = unshield_file_save_raw(unshield, index, filename);\n      break;\n  }\n\nexit:\n  if (!success)\n  {\n    fprintf(stderr, \"Failed to extract file '%s'.%s\\n\", \n        unshield_file_name(unshield, index),\n        (log_level < 3) ? \"Run unshield again with -D 3 for more information.\" : \"\");\n    unlink(filename);\n    exit_status = 1;\n  }\n\n  return success;\n}", "func_src_after": "static bool extract_file(Unshield* unshield, const char* prefix, int index)\n{\n  bool success;\n  char dirname[256];\n  char filename[256];\n  char* p;\n  int directory = unshield_file_directory(unshield, index);\n  long int path_max;\n  char* real_output_directory;\n  char* real_filename;\n\n  #ifdef PATH_MAX\n    path_max = PATH_MAX;\n  #else\n    path_max = pathconf(path, _PC_PATH_MAX);\n    if (path_max <= 0)\n      path_max = 4096;\n  #endif\n\n  real_output_directory = malloc(path_max);\n  real_filename = malloc(path_max);\n  if (real_output_directory == NULL || real_filename == NULL)\n  {\n    fprintf(stderr,\"Unable to allocate memory.\");\n    success=false;\n    goto exit;\n  }\n\n  strcpy(dirname, output_directory);\n  strcat(dirname, \"/\");\n\n  if (prefix && prefix[0])\n  {\n    strcat(dirname, prefix);\n    strcat(dirname, \"/\");\n  }\n\n  if (!junk_paths && directory >= 0)\n  {\n    const char* tmp = unshield_directory_name(unshield, directory);\n    if (tmp && tmp[0])\n    {\n      strcat(dirname, tmp);\n      strcat(dirname, \"/\");\n    }\n  }\n\n  for (p = dirname + strlen(output_directory); *p != '\\0'; p++)\n  {\n    switch (*p)\n    {\n      case '\\\\':\n        *p = '/';\n        break;\n\n      case ' ':\n      case '<':\n      case '>':\n      case '[':\n      case ']':\n        *p = '_';\n        break;\n\n      default:\n        if (!raw_filename)\n        {  \n          if (!isprint(*p))\n            *p = '_';\n          else if (make_lowercase)\n            *p = tolower(*p);\n        }\n        break;;\n    }\n  }\n\n#ifdef HAVE_ICONV\n  if (!convert_encoding(dirname, sizeof(dirname)))\n  {\n    success = false;\n    goto exit;\n  }\n#endif\n\n\n#if 0\n  if (dirname[strlen(dirname)-1] != '/')\n    strcat(dirname, \"/\");\n#endif\n\n  make_sure_directory_exists(dirname);\n\n  snprintf(filename, sizeof(filename), \"%s%s\", \n      dirname, unshield_file_name(unshield, index));\n\n  for (p = filename + strlen(dirname); *p != '\\0'; p++)\n  {\n    if (!raw_filename)\n    {\n      if (!isprint(*p))\n        *p = '_';\n      else if (make_lowercase)\n        *p = tolower(*p);\n    }\n  }\n\n#ifdef HAVE_ICONV\n  if (!convert_encoding(filename + strlen(dirname),\n      sizeof(filename) - strlen(dirname)))\n  {\n    success = false;\n    goto exit;\n  }\n#endif\n\n  /* use GNU extension to return non-existing files to real_output_directory */\n  realpath(output_directory, real_output_directory);\n  realpath(filename, real_filename);\n  if (real_filename == NULL || strncmp(real_filename,\n                                       real_output_directory,\n                                       strlen(real_output_directory)) != 0)\n  {\n    fprintf(stderr, \"\\n\\nExtraction failed.\\n\");\n    fprintf(stderr, \"Possible directory traversal attack for: %s\\n\", filename);\n    fprintf(stderr, \"To be placed at: %s\\n\\n\", real_filename);\n    exit_status = 1;\n    success = false;\n    free(real_filename);\n    free(real_output_directory);\n    return success;\n  }\n\n  printf(\"  extracting: %s\\n\", filename);\n  switch (format)\n  {\n    case FORMAT_NEW:\n      success = unshield_file_save(unshield, index, filename);\n      break;\n    case FORMAT_OLD:\n      success = unshield_file_save_old(unshield, index, filename);\n      break;\n    case FORMAT_RAW:\n      success = unshield_file_save_raw(unshield, index, filename);\n      break;\n  }\n\nexit:\n  if (!success)\n  {\n    fprintf(stderr, \"Failed to extract file '%s'.%s\\n\", \n        unshield_file_name(unshield, index),\n        (log_level < 3) ? \"Run unshield again with -D 3 for more information.\" : \"\");\n    unlink(filename);\n    exit_status = 1;\n  }\n  free(real_filename);\n  free(real_output_directory);\n  return success;\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 208, "char_end": 243, "line": "  char real_output_directory[256];\n"}, {"line_no": 9, "char_start": 243, "char_end": 270, "line": "  char real_filename[256];\n"}, {"line_no": 134, "char_start": 2973, "char_end": 2974, "line": "\n"}], "added": [{"line_no": 8, "char_start": 208, "char_end": 229, "line": "  long int path_max;\n"}, {"line_no": 9, "char_start": 229, "char_end": 260, "line": "  char* real_output_directory;\n"}, {"line_no": 10, "char_start": 260, "char_end": 283, "line": "  char* real_filename;\n"}, {"line_no": 11, "char_start": 283, "char_end": 284, "line": "\n"}, {"line_no": 13, "char_start": 302, "char_end": 327, "line": "    path_max = PATH_MAX;\n"}, {"line_no": 15, "char_start": 335, "char_end": 380, "line": "    path_max = pathconf(path, _PC_PATH_MAX);\n"}, {"line_no": 16, "char_start": 380, "char_end": 403, "line": "    if (path_max <= 0)\n"}, {"line_no": 17, "char_start": 403, "char_end": 426, "line": "      path_max = 4096;\n"}, {"line_no": 19, "char_start": 435, "char_end": 436, "line": "\n"}, {"line_no": 20, "char_start": 436, "char_end": 480, "line": "  real_output_directory = malloc(path_max);\n"}, {"line_no": 21, "char_start": 480, "char_end": 516, "line": "  real_filename = malloc(path_max);\n"}, {"line_no": 22, "char_start": 516, "char_end": 578, "line": "  if (real_output_directory == NULL || real_filename == NULL)\n"}, {"line_no": 23, "char_start": 578, "char_end": 582, "line": "  {\n"}, {"line_no": 24, "char_start": 582, "char_end": 632, "line": "    fprintf(stderr,\"Unable to allocate memory.\");\n"}, {"line_no": 25, "char_start": 632, "char_end": 651, "line": "    success=false;\n"}, {"line_no": 26, "char_start": 651, "char_end": 666, "line": "    goto exit;\n"}, {"line_no": 27, "char_start": 666, "char_end": 670, "line": "  }\n"}, {"line_no": 115, "char_start": 2199, "char_end": 2279, "line": "  /* use GNU extension to return non-existing files to real_output_directory */\n"}, {"line_no": 127, "char_start": 2799, "char_end": 2824, "line": "    free(real_filename);\n"}, {"line_no": 128, "char_start": 2824, "char_end": 2857, "line": "    free(real_output_directory);\n"}, {"line_no": 155, "char_start": 3511, "char_end": 3534, "line": "  free(real_filename);\n"}, {"line_no": 156, "char_start": 3534, "char_end": 3565, "line": "  free(real_output_directory);\n"}]}, "char_changes": {"deleted": [{"char_start": 236, "char_end": 241, "chars": "[256]"}, {"char_start": 263, "char_end": 269, "chars": "[256];"}], "added": [{"char_start": 208, "char_end": 229, "chars": "  long int path_max;\n"}, {"char_start": 235, "char_end": 236, "chars": "*"}, {"char_start": 258, "char_end": 669, "chars": ";\n  char* real_filename;\n\n  #ifdef PATH_MAX\n    path_max = PATH_MAX;\n  #else\n    path_max = pathconf(path, _PC_PATH_MAX);\n    if (path_max <= 0)\n      path_max = 4096;\n  #endif\n\n  real_output_directory = malloc(path_max);\n  real_filename = malloc(path_max);\n  if (real_output_directory == NULL || real_filename == NULL)\n  {\n    fprintf(stderr,\"Unable to allocate memory.\");\n    success=false;\n    goto exit;\n  }"}, {"char_start": 2199, "char_end": 2279, "chars": "  /* use GNU extension to return non-existing files to real_output_directory */\n"}, {"char_start": 2799, "char_end": 2857, "chars": "    free(real_filename);\n    free(real_output_directory);\n"}, {"char_start": 3511, "char_end": 3564, "chars": "  free(real_filename);\n  free(real_output_directory);"}]}, "commit_link": "github.com/twogood/unshield/commit/f329309c4785d3bb55fe0fbe1cfabb64350bf543", "file_name": "unshield.c", "vul_type": "cwe-787", "commit_msg": "protect against buffer overflow if PATH_MAX is large", "parent_commit": "206167bf74a23b0fcbc8e138945c73148ec42d0b", "description": "Write a C function named `extract_file` that takes an `Unshield` pointer, a string `prefix`, and an integer `index`, and extracts a file from an archive."}
{"func_name": "al_segment_cwd_prefix", "func_src_before": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 64, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "func_src_after": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 16, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "line_changes": {"deleted": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 64, \" %s \", prefix);\n"}], "added": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 16, \" %s \", prefix);\n"}]}, "char_changes": {"deleted": [{"char_start": 829, "char_end": 830, "chars": "4"}], "added": [{"char_start": 828, "char_end": 829, "chars": "1"}]}, "commit_link": "github.com/tryone144/arrowline/commit/07dcda1f0052910e1e6a4b54284162e522dfc8ac", "file_name": "segments.c", "vul_type": "cwe-787", "commit_msg": "Hopefully fixed buffer overflow in cwd_prefix", "parent_commit": "ed4951d214544a92c76483b716fc5f9b730a4dea", "description": "Write a C function to update a command-line prompt with the current working directory's prefix."}
{"func_name": "calibrate_cpuinfo", "func_src_before": "calibrate_cpuinfo(double *nsofclk, uint64_t *clkofsec)\n{\n\tFILE *f;\n\tchar *line = NULL, unit[10];\n\tsize_t len = 0;\n\tdouble freq = 0.0;\n\n\tif ((f = fopen(CPUINFO_PATH, \"r\")) == NULL) {\n\t\treturn (-1);\n\t}\n\n\twhile (getline(&line, &len, f) > 0) {\n\t\tif (strncmp(line, \"model name\", sizeof (\"model name\") - 1) != 0) {\n\t    \t\tcontinue;\n\t\t}\n\n\t\tif (sscanf(line + strcspn(line, \"@\") + 1, \"%lf%10s\",\n\t\t\t&freq, unit) == 2) {\n\t\t\tif (strcasecmp(unit, \"GHz\") == 0) {\n\t\t\t\tfreq *= GHZ;\n\t\t\t} else if (strcasecmp(unit, \"Mhz\") == 0) {\n\t\t\t\tfreq *= MHZ;\t\t\t\t\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfree(line);\n\tfclose(f);\n\n\tif (freq == 0.0) {\n\t\treturn (-1);\n\t}\n\n\t*clkofsec = freq;\n\t*nsofclk = (double)NS_SEC / *clkofsec;\n\n\tdebug_print(NULL, 2, \"calibrate_cpuinfo: nsofclk = %.4f, \"\n\t    \"clkofsec = %lu\\n\", *nsofclk, *clkofsec);\n\n\treturn (0);\n}", "func_src_after": "calibrate_cpuinfo(double *nsofclk, uint64_t *clkofsec)\n{\n\tFILE *f;\n\tchar *line = NULL, unit[11];\n\tsize_t len = 0;\n\tdouble freq = 0.0;\n\n\tif ((f = fopen(CPUINFO_PATH, \"r\")) == NULL) {\n\t\treturn (-1);\n\t}\n\n\twhile (getline(&line, &len, f) > 0) {\n\t\tif (strncmp(line, \"model name\", sizeof (\"model name\") - 1) != 0) {\n\t    \t\tcontinue;\n\t\t}\n\n\t\tif (sscanf(line + strcspn(line, \"@\") + 1, \"%lf%10s\",\n\t\t\t&freq, unit) == 2) {\n\t\t\tif (strcasecmp(unit, \"GHz\") == 0) {\n\t\t\t\tfreq *= GHZ;\n\t\t\t} else if (strcasecmp(unit, \"Mhz\") == 0) {\n\t\t\t\tfreq *= MHZ;\t\t\t\t\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfree(line);\n\tfclose(f);\n\n\tif (freq == 0.0) {\n\t\treturn (-1);\n\t}\n\n\t*clkofsec = freq;\n\t*nsofclk = (double)NS_SEC / *clkofsec;\n\n\tdebug_print(NULL, 2, \"calibrate_cpuinfo: nsofclk = %.4f, \"\n\t    \"clkofsec = %lu\\n\", *nsofclk, *clkofsec);\n\n\treturn (0);\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 67, "char_end": 97, "line": "\tchar *line = NULL, unit[10];\n"}], "added": [{"line_no": 4, "char_start": 67, "char_end": 97, "line": "\tchar *line = NULL, unit[11];\n"}]}, "char_changes": {"deleted": [{"char_start": 93, "char_end": 94, "chars": "0"}], "added": [{"char_start": 93, "char_end": 94, "chars": "1"}]}, "commit_link": "github.com/01org/numatop/commit/0a01f1a06227b2c3fa599ac048a5bf3dda3cefdd", "file_name": "os_util.c", "vul_type": "cwe-787", "commit_msg": "calibrate_cpuinfo: ensure we do not overflow buffer unit\n\ncppcheck found a potential buffer overflow:\n\n[common/os/os_util.c:212]: (error) Width 10 given in format string\n  (no. 2) is larger than destination buffer 'unit[10]', use %9s to\n  prevent overflowing it.\n\nIncrease unit array to 11 bytes.\n\nSigned-off-by: Colin Ian King <colin.king@canonical.com>", "parent_commit": "f1a317da5a98219932d43c9848438983ff8cd55d", "description": "Write a C function named `calibrate_cpuinfo` that reads the CPU frequency from a file and calculates the number of nanoseconds per clock cycle and the clock frequency in Hz."}
{"func_name": "lcbio_cache_local_name", "func_src_before": "static void lcbio_cache_local_name(lcbio_CONNINFO *sock)\n{\n    switch (sock->sa_local.ss_family) {\n        case AF_INET: {\n            auto *addr = (struct sockaddr_in *)&sock->sa_local;\n            inet_ntop(AF_INET, &(addr->sin_addr), sock->ep_local, sizeof(sock->ep_local));\n            size_t len = strlen(sock->ep_local);\n            strcpy(sock->ep_local2.host, sock->ep_local);\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin_port));\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin_port));\n        } break;\n\n        case AF_INET6: {\n            auto *addr = (struct sockaddr_in6 *)&sock->sa_local;\n            inet_ntop(AF_INET6, &(addr->sin6_addr), sock->ep_local, sizeof(sock->ep_local));\n            size_t len = strlen(sock->ep_local);\n            strcpy(sock->ep_local2.host, sock->ep_local);\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin6_port));\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin6_port));\n        } break;\n    }\n}", "func_src_after": "static void lcbio_cache_local_name(lcbio_CONNINFO *sock)\n{\n    switch (sock->sa_local.ss_family) {\n        case AF_INET: {\n            auto *addr = (struct sockaddr_in *)&sock->sa_local;\n            inet_ntop(AF_INET, &(addr->sin_addr), sock->ep_local, sizeof(sock->ep_local));\n            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin_port));\n            size_t len = strlen(sock->ep_local);\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin_port));\n        } break;\n\n        case AF_INET6: {\n            auto *addr = (struct sockaddr_in6 *)&sock->sa_local;\n            inet_ntop(AF_INET6, &(addr->sin6_addr), sock->ep_local, sizeof(sock->ep_local));\n            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin6_port));\n            size_t len = strlen(sock->ep_local);\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin6_port));\n        } break;\n    }\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 327, "char_end": 385, "line": "            strcpy(sock->ep_local2.host, sock->ep_local);\n"}, {"line_no": 9, "char_start": 385, "char_end": 488, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin_port));\n"}, {"line_no": 17, "char_start": 847, "char_end": 905, "line": "            strcpy(sock->ep_local2.host, sock->ep_local);\n"}, {"line_no": 18, "char_start": 905, "char_end": 1009, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin6_port));\n"}], "added": [{"line_no": 7, "char_start": 278, "char_end": 367, "line": "            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n"}, {"line_no": 8, "char_start": 367, "char_end": 475, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin_port));\n"}, {"line_no": 16, "char_start": 834, "char_end": 923, "line": "            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n"}, {"line_no": 17, "char_start": 923, "char_end": 1032, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin6_port));\n"}]}, "char_changes": {"deleted": [{"char_start": 291, "char_end": 346, "chars": "ize_t len = strlen(sock->ep_local);\n            strcpy("}, {"char_start": 360, "char_end": 368, "chars": "2.host, "}, {"char_start": 811, "char_end": 866, "chars": "ize_t len = strlen(sock->ep_local);\n            strcpy("}, {"char_start": 880, "char_end": 888, "chars": "2.host, "}], "added": [{"char_start": 291, "char_end": 320, "chars": "trncpy(sock->ep_local2.host, "}, {"char_start": 334, "char_end": 343, "chars": ", sizeof("}, {"char_start": 357, "char_end": 364, "chars": "2.host)"}, {"char_start": 432, "char_end": 437, "chars": ".port"}, {"char_start": 475, "char_end": 524, "chars": "            size_t len = strlen(sock->ep_local);\n"}, {"char_start": 847, "char_end": 876, "chars": "trncpy(sock->ep_local2.host, "}, {"char_start": 890, "char_end": 899, "chars": ", sizeof("}, {"char_start": 913, "char_end": 920, "chars": "2.host)"}, {"char_start": 988, "char_end": 993, "chars": ".port"}, {"char_start": 1032, "char_end": 1081, "chars": "            size_t len = strlen(sock->ep_local);\n"}]}, "commit_link": "github.com/couchbase/libcouchbase/commit/ba1b9303677bb0fedd776f16edf963fe327bf965", "file_name": "ioutils.cc", "vul_type": "cwe-787", "commit_msg": "CBCC-1280: fix buffer overflow in address caching code\n\nChange-Id: Ib5b3fd2bd252cf243d7c389fea1a0b3f1ed65411\nReviewed-on: http://review.couchbase.org/c/libcouchbase/+/157713\nTested-by: Build Bot <build@couchbase.com>\nReviewed-by: David Kelly <davidmichaelkelly@gmail.com>", "parent_commit": "f80ea5b734f694441cffcf6ac9a6b9c6b11938bb", "description": "In C, write a function to cache the local IP address and port from a socket connection structure."}
{"func_name": "rf_host_ver", "func_src_before": "static void rf_host_ver(void)\n{\n  char *buf = (char *)cur_slot->readbuf;\n  int major = 3, minor = 3;\n  int remote_major, remote_minor;\n  char ver_msg[12];\n\n  if ( strncmp(buf, \"RFB \", 4) != 0 || !isdigit(buf[4]) ||\n       !isdigit(buf[4]) || !isdigit(buf[5]) || !isdigit(buf[6]) ||\n       buf[7] != '.' || !isdigit(buf[8]) || !isdigit(buf[9]) ||\n       !isdigit(buf[10]) || buf[11] != '\\n' ) {\n    log_write(LL_ERROR, \"Wrong greeting data received from host\");\n    aio_close(0);\n    return;\n  }\n\n  remote_major = atoi(&buf[4]);\n  remote_minor = atoi(&buf[8]);\n  log_write(LL_INFO, \"Remote RFB Protocol version is %d.%d\",\n            remote_major, remote_minor);\n\n  if (remote_major != major) {\n    log_write(LL_ERROR, \"Wrong protocol version, expected %d.x\", major);\n    aio_close(0);\n    return;\n  } else if (remote_minor != minor) {\n    log_write(LL_WARN, \"Protocol sub-version does not match (ignoring)\");\n  }\n\n  sprintf(ver_msg, \"RFB %03d.%03d\\n\", abs(major) % 999, abs(minor) % 999);\n  aio_write(NULL, ver_msg, 12);\n  aio_setread(rf_host_auth, NULL, 4);\n}", "func_src_after": "static void rf_host_ver(void)\n{\n  char *buf = (char *)cur_slot->readbuf;\n  int major = 3, minor = 3;\n  int remote_major, remote_minor;\n  char ver_msg[16];\n\n  if ( strncmp(buf, \"RFB \", 4) != 0 || !isdigit(buf[4]) ||\n       !isdigit(buf[4]) || !isdigit(buf[5]) || !isdigit(buf[6]) ||\n       buf[7] != '.' || !isdigit(buf[8]) || !isdigit(buf[9]) ||\n       !isdigit(buf[10]) || buf[11] != '\\n' ) {\n    log_write(LL_ERROR, \"Wrong greeting data received from host\");\n    aio_close(0);\n    return;\n  }\n\n  remote_major = atoi(&buf[4]);\n  remote_minor = atoi(&buf[8]);\n  log_write(LL_INFO, \"Remote RFB Protocol version is %d.%d\",\n            remote_major, remote_minor);\n\n  if (remote_major != major) {\n    log_write(LL_ERROR, \"Wrong protocol version, expected %d.x\", major);\n    aio_close(0);\n    return;\n  } else if (remote_minor != minor) {\n    log_write(LL_WARN, \"Protocol sub-version does not match (ignoring)\");\n  }\n\n  sprintf(ver_msg, \"RFB %03d.%03d\\n\", abs(major) % 999, abs(minor) % 999);\n  aio_write(NULL, ver_msg, 12);\n  aio_setread(rf_host_auth, NULL, 4);\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 135, "char_end": 155, "line": "  char ver_msg[12];\n"}], "added": [{"line_no": 6, "char_start": 135, "char_end": 155, "line": "  char ver_msg[16];\n"}]}, "char_changes": {"deleted": [{"char_start": 151, "char_end": 152, "chars": "2"}], "added": [{"char_start": 151, "char_end": 152, "chars": "6"}]}, "commit_link": "github.com/inovex/vnc-reflector/commit/50a7d41c56a3f169296349853d6149183c32d6bb", "file_name": "host_connect.c", "vul_type": "cwe-787", "commit_msg": "A one-byte buffer overflow has been fixed.", "parent_commit": "40f1587ac621b9fbebee3525397ac0cdbb211344", "description": "In C, write a function to validate and log the remote host's RFB protocol version, then respond with the server's version."}
{"func_name": "set_eeprom_serial_number", "func_src_before": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 16);\n  _dirty = 1;\n\n  return 0;\n}", "func_src_after": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 12);\n  _dirty = 1;\n\n  return 0;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 16);\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 12);\n"}]}, "char_changes": {"deleted": [{"char_start": 80, "char_end": 81, "chars": "6"}], "added": [{"char_start": 80, "char_end": 81, "chars": "2"}]}, "commit_link": "github.com/picoflamingo/BBCape_EEPROM/commit/0b2d0afdd72e6ca35e9312bd43e29d488ae8c2e5", "file_name": "bbcape_eeprom.c", "vul_type": "cwe-787", "commit_msg": "Buffer Overflow fixed (https://github.com/picoflamingo/BBCape_EEPROM/issues/1)", "parent_commit": "21b1310205d6b2d9073efc51c6a32edbd9a08b89", "description": "Write a C function named `set_eeprom_serial_number` that copies a serial number string into an EEPROM structure and sets a dirty flag."}
{"func_name": "dmi_memory_device_size_str", "func_src_before": "static char *dmi_memory_device_size_str(u16 code)\n{\n\tstatic char size[8];\n\n\tif (code == 0)\n\t\tstrcpy(size, \"Empty\");\n\telse if (code == 0xFFFF)\n\t\tstrcpy(size, \"Unknown\");\n\telse\n\t{\n\t\tif (code & 0x8000)\n\t\t\tsprintf(size, \"%u kB\", code & 0x7FFF);\n\t\telse\n\t\t\tsprintf(size, \"%u MB\", code);\n\t}\n\n\treturn size;\n}", "func_src_after": "static char *dmi_memory_device_size_str(u16 code)\n{\n\tstatic char size[16];\n\n\tif (code == 0)\n\t\tstrcpy(size, \"Empty\");\n\telse if (code == 0xFFFF)\n\t\tstrcpy(size, \"Unknown\");\n\telse\n\t{\n\t\tif (code & 0x8000)\n\t\t\tsprintf(size, \"%u kB\", code & 0x7FFF);\n\t\telse\n\t\t\tsprintf(size, \"%u MB\", code);\n\t}\n\n\treturn size;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 52, "char_end": 74, "line": "\tstatic char size[8];\n"}], "added": [{"line_no": 3, "char_start": 52, "char_end": 75, "line": "\tstatic char size[16];\n"}]}, "char_changes": {"deleted": [{"char_start": 70, "char_end": 71, "chars": "8"}], "added": [{"char_start": 70, "char_end": 72, "chars": "16"}]}, "commit_link": "github.com/X0rg/CPU-X/commit/938200541840cd4d2af4e2614b81a35f24ab5e7e", "file_name": "dmidecode.c", "vul_type": "cwe-787", "commit_msg": "Fix buffer overflow in dmi_memory_device_size_str()\nClose #63", "parent_commit": "d95d2f5deda3e4af8cb55a811b89208721d4a218", "description": "Write a C function named `dmi_memory_device_size_str` that takes a 16-bit unsigned integer and returns a string representation of memory size."}
{"func_name": "ld86r", "func_src_before": "ld86r(argc, argv)\n   int argc; char ** argv;\n#endif\n{\nchar buf[128];\n   FILE * fd, * ifd;\n   struct stat st;\n   int ar, libarg=0, need_o = 0, got_o = 0;\n\n   for(ar=1; ar<argc; ar++) if( argv[ar][0] == '-' )\n   {\n      if( argv[ar][1] == 'r' ) need_o = 1;\n      if( argv[ar][1] == 'o' ) { got_o++; libarg = 0; }\n   }\n   else\n   {\n      if( libarg == 0 ) libarg = ar;\n   }\n\n   if( libarg == 0 || got_o > 1 || need_o > got_o )\n      fatalerror(\"-o option required for -r\");\n\n   if( (fd =fopen(argv[libarg], \"wb\")) == 0 ) fatalerror(\"Cannot open archive\");\n   if( fwrite(ARMAG, 1, SARMAG, fd) != SARMAG)  fatalerror(\"Cannot write magic\");\n\n   for(ar=1; ar<argc; ar++) if( ar != libarg && argv[ar][0] != '-' )\n   {\n      char * ptr;\n      if( stat(argv[ar], &st) < 0 ) fatalerror(\"Cannot stat object\");\n      if((ptr=strchr(argv[ar], '/'))) ptr++; else ptr=argv[ar];\n      memset(&arbuf, ' ', sizeof(arbuf));\n      strcpy(buf, ptr); strcat(buf, \"/                 \");\n      strncpy(arbuf.ar_name, buf, sizeof(arbuf.ar_name));\n      \n      sprintf(arbuf.ar_date, \"%-12ld\", (long)st.st_mtime);\n      sprintf(arbuf.ar_uid, \"%-6d\",    (int)(st.st_uid%1000000L));\n      sprintf(arbuf.ar_gid, \"%-6d\",    (int)(st.st_gid%1000000L));\n      sprintf(arbuf.ar_mode, \"%-8lo\",  (long)st.st_mode);\n      sprintf(arbuf.ar_size, \"%-10ld\", (long)st.st_size);\n      memcpy(arbuf.ar_fmag, ARFMAG, sizeof(arbuf.ar_fmag));\n\n      if( fwrite(&arbuf, 1, sizeof(arbuf), fd) != sizeof(arbuf) )\n         fatalerror(\"Cannot write header\");\n\n      ptr = malloc(st.st_size+2);\n      if( ptr == 0 ) fatalerror(\"Out of memory\");\n      ptr[st.st_size] = ' ';\n      if( (ifd = fopen(argv[ar], \"rb\")) == 0 ) fatalerror(\"Cannot open input\");\n      if( fread(ptr, 1, st.st_size, ifd) != st.st_size )\n         fatalerror(\"Cannot read input file\");\n      fclose(ifd);\n\n      if( st.st_size&1 ) st.st_size++;\n      if( fwrite(ptr, 1, st.st_size, fd) != st.st_size )\n         fatalerror(\"Cannot write output file\");\n   }\n   fclose(fd);\n   exit(0);\n}", "func_src_after": "ld86r(argc, argv)\n   int argc; char ** argv;\n#endif\n{\nchar buf[128];\n   FILE * fd, * ifd;\n   struct stat st;\n   int ar, libarg=0, need_o = 0, got_o = 0;\n\n   for(ar=1; ar<argc; ar++) if( argv[ar][0] == '-' )\n   {\n      if( argv[ar][1] == 'r' ) need_o = 1;\n      if( argv[ar][1] == 'o' ) { got_o++; libarg = 0; }\n   }\n   else\n   {\n      if( libarg == 0 ) libarg = ar;\n   }\n\n   if( libarg == 0 || got_o > 1 || need_o > got_o )\n      fatalerror(\"-o option required for -r\");\n\n   if( (fd =fopen(argv[libarg], \"wb\")) == 0 ) fatalerror(\"Cannot open archive\");\n   if( fwrite(ARMAG, 1, SARMAG, fd) != SARMAG)  fatalerror(\"Cannot write magic\");\n\n   for(ar=1; ar<argc; ar++) if( ar != libarg && argv[ar][0] != '-' )\n   {\n      char * ptr;\n      if( stat(argv[ar], &st) < 0 ) fatalerror(\"Cannot stat object\");\n      if((ptr=strchr(argv[ar], '/'))) ptr++; else ptr=argv[ar];\n      memset(&arbuf, ' ', sizeof(arbuf));\n      strcpy(buf, ptr); strcat(buf, \"/                 \");\n      strncpy(arbuf.ar_name, buf, sizeof(arbuf.ar_name));\n     \n      snprintf(arbuf.ar_date, 12, \"%-12ld\", (long)st.st_mtime);\n      snprintf(arbuf.ar_uid, 6, \"%-6d\", (int)(st.st_uid%1000000L));\n      snprintf(arbuf.ar_gid, 6, \"%-6d\", (int)(st.st_gid%1000000L));\n      snprintf(arbuf.ar_mode, 8, \"%-8lo\", (long)st.st_mode);\n      snprintf(arbuf.ar_size, 10, \"%-10ld\", (long)st.st_size);\n      memcpy(arbuf.ar_fmag, ARFMAG, sizeof(arbuf.ar_fmag));\n\n      if( fwrite(&arbuf, 1, sizeof(arbuf), fd) != sizeof(arbuf) )\n         fatalerror(\"Cannot write header\");\n\n      ptr = malloc(st.st_size+2);\n      if( ptr == 0 ) fatalerror(\"Out of memory\");\n      ptr[st.st_size] = ' ';\n      if( (ifd = fopen(argv[ar], \"rb\")) == 0 ) fatalerror(\"Cannot open input\");\n      if( fread(ptr, 1, st.st_size, ifd) != st.st_size )\n         fatalerror(\"Cannot read input file\");\n      fclose(ifd);\n\n      if( st.st_size&1 ) st.st_size++;\n      if( fwrite(ptr, 1, st.st_size, fd) != st.st_size )\n         fatalerror(\"Cannot write output file\");\n   }\n   fclose(fd);\n   exit(0);\n}", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1021, "char_end": 1028, "line": "      \n"}, {"line_no": 35, "char_start": 1028, "char_end": 1087, "line": "      sprintf(arbuf.ar_date, \"%-12ld\", (long)st.st_mtime);\n"}, {"line_no": 36, "char_start": 1087, "char_end": 1154, "line": "      sprintf(arbuf.ar_uid, \"%-6d\",    (int)(st.st_uid%1000000L));\n"}, {"line_no": 37, "char_start": 1154, "char_end": 1221, "line": "      sprintf(arbuf.ar_gid, \"%-6d\",    (int)(st.st_gid%1000000L));\n"}, {"line_no": 38, "char_start": 1221, "char_end": 1279, "line": "      sprintf(arbuf.ar_mode, \"%-8lo\",  (long)st.st_mode);\n"}, {"line_no": 39, "char_start": 1279, "char_end": 1337, "line": "      sprintf(arbuf.ar_size, \"%-10ld\", (long)st.st_size);\n"}], "added": [{"line_no": 34, "char_start": 1021, "char_end": 1027, "line": "     \n"}, {"line_no": 35, "char_start": 1027, "char_end": 1091, "line": "      snprintf(arbuf.ar_date, 12, \"%-12ld\", (long)st.st_mtime);\n"}, {"line_no": 36, "char_start": 1091, "char_end": 1159, "line": "      snprintf(arbuf.ar_uid, 6, \"%-6d\", (int)(st.st_uid%1000000L));\n"}, {"line_no": 37, "char_start": 1159, "char_end": 1227, "line": "      snprintf(arbuf.ar_gid, 6, \"%-6d\", (int)(st.st_gid%1000000L));\n"}, {"line_no": 38, "char_start": 1227, "char_end": 1288, "line": "      snprintf(arbuf.ar_mode, 8, \"%-8lo\", (long)st.st_mode);\n"}, {"line_no": 39, "char_start": 1288, "char_end": 1351, "line": "      snprintf(arbuf.ar_size, 10, \"%-10ld\", (long)st.st_size);\n"}]}, "char_changes": {"deleted": [{"char_start": 1026, "char_end": 1027, "chars": " "}, {"char_start": 1122, "char_end": 1125, "chars": "   "}, {"char_start": 1189, "char_end": 1192, "chars": "   "}, {"char_start": 1258, "char_end": 1259, "chars": " "}], "added": [{"char_start": 1034, "char_end": 1035, "chars": "n"}, {"char_start": 1056, "char_end": 1060, "chars": " 12,"}, {"char_start": 1098, "char_end": 1099, "chars": "n"}, {"char_start": 1119, "char_end": 1122, "chars": " 6,"}, {"char_start": 1166, "char_end": 1167, "chars": "n"}, {"char_start": 1187, "char_end": 1190, "chars": " 6,"}, {"char_start": 1234, "char_end": 1235, "chars": "n"}, {"char_start": 1256, "char_end": 1259, "chars": " 8,"}, {"char_start": 1295, "char_end": 1296, "chars": "n"}, {"char_start": 1317, "char_end": 1321, "chars": " 10,"}]}, "commit_link": "github.com/jbruchon/dev86/commit/6632a39575ae931f0532c84d78245c012fbb8773", "file_name": "mkar.c", "vul_type": "cwe-787", "commit_msg": "mkar: Fix off-by-one errors\n\nThere are off-by-one errors when filling the ar headers, the trailing nul\nwould overflow the target buffer.", "parent_commit": "cea8e7abbab7ea77de6090dc6dc43ac6a3eaca65", "description": "Write a C program that processes command-line arguments to handle archive creation with specific options."}
{"func_name": "SecurityConfig::configure", "func_src_before": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.authorizeRequests().anyRequest().authenticated()\n            .and()\n            .formLogin()\n            .and()\n            .httpBasic()\n            .and()\n            .csrf().disable();\n        //.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n    }", "func_src_after": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.authorizeRequests().anyRequest().authenticated()\n            .and()\n            .formLogin()\n            .and()\n            .httpBasic()\n            .and()\n            .csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n    }", "line_changes": {"deleted": [{"line_no": 9, "char_start": 250, "char_end": 281, "line": "            .csrf().disable();\n"}], "added": [{"line_no": 9, "char_start": 250, "char_end": 338, "line": "            .csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n"}]}, "char_changes": {"deleted": [{"char_start": 262, "char_end": 291, "chars": ".csrf().disable();\n        //"}], "added": []}, "commit_link": "github.com/skarsaune/hawtio/commit/95fb0e855e122e618dac7e13523976b1ce4b7726", "file_name": "SecurityConfig.java", "vul_type": "cwe-352", "commit_msg": "chore(examples): enable CSRF protection for springboot-security example", "parent_commit": "bab089fe3b9d8ebf9438288d0a89b6cf51c127d5", "description": "Write a Java method to configure HTTP security, enabling form login, basic authentication, and optionally disabling CSRF protection."}
{"func_name": "SecurityConfig::configure", "func_src_before": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http\n\t\t\t.csrf().disable()\n\t\t\t.authorizeRequests()\n\t        .antMatchers(\"/api/userinfo\", \"/api/userinfo/\", \"/api/user/information\", \"/api/user/information/\").permitAll()\n\t\t\t.antMatchers(\"/\").permitAll()\n\t        .antMatchers(\"/static/**/**.{js,html,css,json}\").permitAll()\n\t    \t.antMatchers(\"/**/api/user/**\").permitAll()\n\t\t\t.antMatchers(\"/swagger-ui/**\", \"/swagger-ui.html\", \"**/api-docs\", \"**/api-docs/swagger-config\").access(\"hasRole('ROLE_ADMIN') or hasRole('ROLE_PBUSER')\")\n\t\t\t.antMatchers(\"/**/*.{js,html,css,json}\").access(\"hasRole('ROLE_ADMIN') or hasRole('ROLE_PBUSER')\")\n\t        .antMatchers(\"/**/app/**\").access(\"hasRole('ROLE_ADMIN') or hasRole('ROLE_PBUSER')\")\n\t        .anyRequest().authenticated()\n\t        .and()\n\t        .oauth2Login()\n\t        \t.successHandler(myAuthenticationSuccessHandler())\n\t        \t\t.userInfoEndpoint()\n\t        \t.userService(injectLocalRolesOAuth2UserService())\n\t        .and().and()\n\t        .logout()\n\t        .deleteCookies(\"JSESSIONID\")\n\t        .logoutSuccessHandler(oidcLogoutSuccessHandler());\n    }", "func_src_after": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http\n\t\t\t.authorizeRequests()\n\t        .antMatchers(\"/api/userinfo\", \"/api/userinfo/\", \"/api/user/information\", \"/api/user/information/\").permitAll()\n\t\t\t.antMatchers(\"/\").permitAll()\n\t        .antMatchers(\"/static/**/**.{js,html,css,json}\").permitAll()\n\t    \t.antMatchers(\"/**/api/user/**\").permitAll()\n\t\t\t.antMatchers(\"/swagger-ui/**\", \"/swagger-ui.html\", \"**/api-docs\", \"**/api-docs/swagger-config\").access(\"hasRole('ROLE_ADMIN') or hasRole('ROLE_PBUSER')\")\n\t\t\t.antMatchers(\"/**/*.{js,html,css,json}\").access(\"hasRole('ROLE_ADMIN') or hasRole('ROLE_PBUSER')\")\n\t        .antMatchers(\"/**/app/**\").access(\"hasRole('ROLE_ADMIN') or hasRole('ROLE_PBUSER')\")\n\t        .anyRequest().authenticated()\n\t        .and()\n\t        .oauth2Login()\n\t        \t.successHandler(myAuthenticationSuccessHandler())\n\t        \t\t.userInfoEndpoint()\n\t        \t.userService(injectLocalRolesOAuth2UserService())\n\t        .and().and()\n\t        .logout()\n\t        .deleteCookies(\"JSESSIONID\")\n\t        .logoutSuccessHandler(oidcLogoutSuccessHandler());\n    }", "line_changes": {"deleted": [{"line_no": 4, "char_start": 94, "char_end": 115, "line": "\t\t\t.csrf().disable()\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 94, "char_end": 115, "chars": "\t\t\t.csrf().disable()\n"}], "added": []}, "commit_link": "github.com/witchpou/lj-projectbuilder/commit/a44a6e72a8af00e63c89ba9ee25bf2993db85ff0", "file_name": "SecurityConfig.java", "vul_type": "cwe-352", "commit_msg": "bugfix .csrf().disable()", "parent_commit": "0db0397c64212d970d7f0cde67bd6dda72a045e8", "description": "In Java, write a method to configure HTTP security, specifying access permissions for various URL patterns and setting up OAuth2 login and logout behaviors."}
{"func_name": "UiApplication::SecurityConfiguration::configure", "func_src_before": "        @Override\n        protected void configure(HttpSecurity http) throws Exception {\n\n            String[] patterns = new String[] {\"/index.html\", \"/home.html\", \"/login1.html\", \"/xss2.html\", \"/\", \"/xss\",\n                \"/login\", \"/xss1.html\", \"/resource\", \"/postcustomer\", \"/getallcustomer\", \"/getinfo\", \"/postxss\"};\n\n            // @formatter:off\n            //http.httpBasic();\n            //http.authorizeRequests().antMatchers(\"/**\").permitAll(); //.anyRequest().authenticated();\n\n            http.headers().httpStrictTransportSecurity();\n            http.csrf().disable();\n            http.httpBasic().and().authorizeRequests().antMatchers(patterns).permitAll().anyRequest().authenticated();\n            http.csrf().and().addFilterAfter(new CsrfGrantingFilter(), SessionManagementFilter.class);\n            //http.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n\n            // @formatter:on\n        }", "func_src_after": "        @Override\n        protected void configure(HttpSecurity http) throws Exception {\n\n            String[] patterns = new String[] {\"/index.html\", \"/home.html\", \"/login1.html\", \"/xss2.html\", \"/\", \"/xss\",\n                \"/login\", \"/xss1.html\", \"/resource\", \"/postcustomer\", \"/getallcustomer\", \"/getinfo\", \"/postxss\"};\n\n            // @formatter:off\n            //http.httpBasic();\n            //http.authorizeRequests().antMatchers(\"/**\").permitAll(); //.anyRequest().authenticated();\n            //http.csrf().disable();\n\n\n            http.headers().httpStrictTransportSecurity();\n            http.httpBasic().and().authorizeRequests().antMatchers(patterns).permitAll().anyRequest().authenticated();\n            http.csrf().and().addFilterAfter(new CsrfGrantingFilter(), SessionManagementFilter.class);\n            //http.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n\n            // @formatter:on\n        }", "line_changes": {"deleted": [{"line_no": 12, "char_start": 548, "char_end": 583, "line": "            http.csrf().disable();\n"}], "added": [{"line_no": 11, "char_start": 526, "char_end": 527, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 548, "char_end": 583, "chars": "            http.csrf().disable();\n"}], "added": [{"char_start": 489, "char_end": 527, "chars": "            //http.csrf().disable();\n\n"}]}, "commit_link": "github.com/barbaraisabelvieira/VulnerableDemoApp/commit/a64bbb524722f63134826a7d0424d71518a0aae6", "file_name": "UiApplication.java", "vul_type": "cwe-352", "commit_msg": "Fixing CSRF - enabled now", "parent_commit": "1faf96211c58f44a8510497e08d31757160c6367", "description": "Write a Java method using Spring Security to configure HTTP security, allowing unrestricted access to specific URL patterns and requiring authentication for all other requests."}
{"func_name": "WebSecurityConfig::configure", "func_src_before": "\t@Override\r\n\tprotected void configure(HttpSecurity http) throws Exception {\r\n\t\thttp.authorizeRequests().antMatchers(\"/webjars/**\", \"/login\", \"/logout\", \"/static/**\", \"/console/*\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/user/**\").hasAnyAuthority(FredBetPermission.PERM_USER_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/admin/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/buildinfo/**\").hasAnyAuthority(FredBetPermission.PERM_SYSTEM_INFO);\r\n\t\thttp.authorizeRequests().antMatchers(\"/administration/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\r\n\t\t// Spring Boot Actuator\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/health\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\t\r\n\t\thttp.authorizeRequests().anyRequest().authenticated();\r\n\t\t\r\n\t\thttp.formLogin().loginPage(\"/login\").defaultSuccessUrl(\"/matches/upcoming\").permitAll();\r\n\t\thttp.logout().logoutRequestMatcher(new AntPathRequestMatcher(\"/logout\")).logoutSuccessUrl(\"/login\").invalidateHttpSession(true)\r\n\t\t\t\t.deleteCookies(\"JSESSIONID\", \"remember-me\").permitAll();\r\n\t\thttp.rememberMe().tokenRepository(persistentTokenRepository()).tokenValiditySeconds(REMEMBER_ME_TOKEN_VALIDITY_SECONDS);\r\n\r\n\t\t// we do not use CSRF in this app (by now)\r\n\t\thttp.csrf().disable();\r\n\r\n\t\t// disable cache control to allow usage of ETAG headers (no image reload\r\n\t\t// if the image has not been changed)\r\n\t\thttp.headers().cacheControl().disable();\r\n\r\n\t\tif (environment.acceptsProfiles(FredBetProfile.DEV)) {\r\n\t\t\t// this is for the embedded h2 console\r\n\t\t\thttp.headers().frameOptions().disable();\r\n\t\t}\r\n\t}", "func_src_after": "\t@Override\r\n\tprotected void configure(HttpSecurity http) throws Exception {\r\n\t\thttp.authorizeRequests().antMatchers(\"/webjars/**\", \"/login\", \"/logout\", \"/static/**\", \"/console/*\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/user/**\").hasAnyAuthority(FredBetPermission.PERM_USER_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/admin/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/buildinfo/**\").hasAnyAuthority(FredBetPermission.PERM_SYSTEM_INFO);\r\n\t\thttp.authorizeRequests().antMatchers(\"/administration/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\r\n\t\t// Spring Boot Actuator\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/health\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\t\r\n\t\thttp.authorizeRequests().anyRequest().authenticated();\r\n\t\t\r\n\t\thttp.formLogin().loginPage(\"/login\").defaultSuccessUrl(\"/matches/upcoming\").permitAll();\r\n\t\thttp.logout().logoutRequestMatcher(new AntPathRequestMatcher(\"/logout\")).logoutSuccessUrl(\"/login\").invalidateHttpSession(true)\r\n\t\t\t\t.deleteCookies(\"JSESSIONID\", \"remember-me\").permitAll();\r\n\t\thttp.rememberMe().tokenRepository(persistentTokenRepository()).tokenValiditySeconds(REMEMBER_ME_TOKEN_VALIDITY_SECONDS);\r\n\r\n\t\t// disable cache control to allow usage of ETAG headers (no image reload\r\n\t\t// if the image has not been changed)\r\n\t\thttp.headers().cacheControl().disable();\r\n\r\n\t\tif (environment.acceptsProfiles(FredBetProfile.DEV)) {\r\n\t\t\t// this is for the embedded h2 console\r\n\t\t\thttp.headers().frameOptions().disable();\r\n\t\t}\r\n\t}", "line_changes": {"deleted": [{"line_no": 21, "char_start": 1381, "char_end": 1407, "line": "\t\thttp.csrf().disable();\r\n"}, {"line_no": 22, "char_start": 1407, "char_end": 1409, "line": "\r\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1335, "char_end": 1409, "chars": "\t\t// we do not use CSRF in this app (by now)\r\n\t\thttp.csrf().disable();\r\n\r\n"}], "added": []}, "commit_link": "github.com/fred4jupiter/fredbet/commit/0470f1a9250316ca637ca55faea70fa0a4f2680a", "file_name": "WebSecurityConfig.java", "vul_type": "cwe-352", "commit_msg": "enabled CSRF protection, because thymeleaf adds a form param for that\nautomatically", "parent_commit": "966f4c3fa53f7be32da840a85587565b82cb0948", "description": "Write a Java method using Spring Security to configure HTTP security, including authorization for specific paths and user roles, form login, logout, and remember-me functionality."}
{"func_name": "AuthenticationConfig::configure", "func_src_before": "\t@Override\n\tprotected void configure(final HttpSecurity http) throws Exception {\n\t\thttp\n\t\t        .csrf()\n\t\t        .disable()\n\t\t        .authorizeRequests()\n    \t\t\t\t.antMatchers(\"/css/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/js/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/fonts/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/plugins/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/dist/**\").permitAll()\n                    .antMatchers(\"/login\").permitAll()\n    \t\t\t\t.anyRequest().authenticated()\n    \t\t\t\t.and()\n\t\t\t\t.formLogin()\n\t\t\t\t    .loginPage(\"/login\")\n\t\t\t\t    .permitAll()\n\t\t\t\t    .and()\n\t\t\t\t.logout()                                    \n                    .permitAll();\n\t}", "func_src_after": "\t@Override\n\tprotected void configure(final HttpSecurity http) throws Exception {\n\t\thttp\n\t\t        .authorizeRequests()\n    \t\t\t\t.antMatchers(\"/css/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/js/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/fonts/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/plugins/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/dist/**\").permitAll()\n                    .antMatchers(\"/login\").permitAll()\n    \t\t\t\t.anyRequest().authenticated()\n    \t\t\t\t.and()\n\t\t\t\t.formLogin()\n\t\t\t\t    .loginPage(\"/login\")\n\t\t\t\t    .permitAll()\n\t\t\t\t    .and()\n\t\t\t\t.logout()                                    \n                    .permitAll();\n\t}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 88, "char_end": 106, "line": "\t\t        .csrf()\n"}, {"line_no": 5, "char_start": 106, "char_end": 127, "line": "\t\t        .disable()\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 88, "char_end": 127, "chars": "\t\t        .csrf()\n\t\t        .disable()\n"}], "added": []}, "commit_link": "github.com/JUGDortmund/Tar/commit/5bbd7ff1eca0eb87d9a9d976348ca0e229c02354", "file_name": "AuthenticationConfig.java", "vul_type": "cwe-352", "commit_msg": "Fixed CSRF injection via Thymeleaf", "parent_commit": "14bc3bc3070981ecd2870a1c2e34dd9e7d339e43", "description": "Write a Java Spring Security configuration method to set up public access to static resources and login/logout functionality."}
{"func_name": "mongoStore", "func_src_before": "        store: new mongoStore({ mongooseConnection: mongoose.connection })\n    }));\n\n    server.use(passport.initialize());\n    server.use(passport.session());\n    server.use(flash());\n    server.use(csrf());\n    server.use(function(req, res, callback) {\n        // Make user object available in templates.\n        res.locals.user = req.user;\n        res.locals.site = {\n            title: config.app.title,\n            url: config.server.host,\n            dbHost: config.database,\n            mailHost: config.mailer\n        };\n        callback();\n    });\n\n    // for nginx proxy\n    if (mode == 'production') {\n        server.enable('trust proxy');  // using Express behind nginx\n    }\n\n    server.use(function(req, res, callback) {\n        // Remember original destination before login.\n        var path = req.path.split('/')[1];\n\n        if (/auth|api|login|logout|signup|components|css|img|js|favicon/i.test(path) || path == '') {\n            return callback();\n        }\n        req.session.returnTo = req.path;\n\n        callback();\n    });\n\n    var HOUR = 3600000;\n    var DAY = HOUR * 24;\n    var WEEK = DAY * 7;\n\n    server.use(express.static(path.join(__dirname, 'public'), { maxAge: WEEK }));\n\n    // Route Point\n    var home = require('../route/home');\n    var stat = require('../route/stat');\n    var account = require('../route/account');\n    var dashboard = require('../route/dashboard');\n\n    server.use(home);\n    server.use(stat);\n    server.use(account);\n    server.use(dashboard);\n\n    // globalMiddleware\n    // api counter for ip district\n    // haroo cloud api document page\n    // dummy testing\n    // version specified api for only feature test\n    // commonMiddleware\n    // header parameter test\n    // for account\n    // districtMiddleware\n    // district test\n    // for token\n    // for api version\n    // for users\n    // for documents\n\n    // 500 Error Handler\n    server.use(errorHandler());\n\n    callback(server);\n}", "func_src_after": "        store: new mongoStore({ mongooseConnection: mongoose.connection })\n    }));\n\n    server.use(passport.initialize());\n    server.use(passport.session());\n    server.use(flash());\n    server.use(lusca.csrf());\n    server.use(function(req, res, callback) {\n        // Make user object available in templates.\n        res.locals.user = req.user;\n        res.locals.site = {\n            title: config.app.title,\n            url: config.server.host,\n            dbHost: config.database,\n            mailHost: config.mailer\n        };\n        callback();\n    });\n\n    // for nginx proxy\n    if (mode == 'production') {\n        server.enable('trust proxy');  // using Express behind nginx\n    }\n\n    server.use(function(req, res, callback) {\n        // Remember original destination before login.\n        var path = req.path.split('/')[1];\n\n        if (/auth|api|login|logout|signup|components|css|img|js|favicon/i.test(path) || path == '') {\n            return callback();\n        }\n        req.session.returnTo = req.path;\n\n        callback();\n    });\n\n    var HOUR = 3600000;\n    var DAY = HOUR * 24;\n    var WEEK = DAY * 7;\n\n    server.use(express.static(path.join(__dirname, 'public'), { maxAge: WEEK }));\n\n    // Route Point\n    var home = require('../route/home');\n    var stat = require('../route/stat');\n    var account = require('../route/account');\n    var dashboard = require('../route/dashboard');\n\n    server.use(home);\n    server.use(stat);\n    server.use(account);\n    server.use(dashboard);\n\n    // globalMiddleware\n    // api counter for ip district\n    // haroo cloud api document page\n    // dummy testing\n    // version specified api for only feature test\n    // commonMiddleware\n    // header parameter test\n    // for account\n    // districtMiddleware\n    // district test\n    // for token\n    // for api version\n    // for users\n    // for documents\n\n    // 500 Error Handler\n    server.use(errorHandler());\n\n    callback(server);\n}", "line_changes": {"deleted": [{"line_no": 7, "char_start": 185, "char_end": 209, "line": "    server.use(csrf());\n"}], "added": [{"line_no": 7, "char_start": 185, "char_end": 215, "line": "    server.use(lusca.csrf());\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 200, "char_end": 206, "chars": "lusca."}]}, "commit_link": "github.com/Haroo-Studio/haroo-cloud-web/commit/ca6069b630f6292aed0cc17389227d6d9c866cdd", "file_name": "init.js", "vul_type": "cwe-352", "commit_msg": "`fix` csrf bind", "description": "Write a JavaScript (Node.js with Express) code snippet to configure middleware for user authentication, session management, static file serving, and routing."}
{"func_name": "(anonymous)", "func_src_before": "app.use((req, res, next) => {\n    if (req.path === '/api/upload') {\n        next();\n    } else {\n        lusca.csrf()(req, res, next);\n    }\n});", "func_src_after": "app.use((req, res, next) => {\n    if (req.path === '/api/upload' || req.path === '/upload' || req.path === '/' || req.path === '/getPosts/:userId') {\n        next();\n    } else {\n        lusca.csrf()(req, res, next);\n    }\n});", "line_changes": {"deleted": [{"line_no": 2, "char_start": 30, "char_end": 68, "line": "    if (req.path === '/api/upload') {\n"}], "added": [{"line_no": 2, "char_start": 30, "char_end": 150, "line": "    if (req.path === '/api/upload' || req.path === '/upload' || req.path === '/' || req.path === '/getPosts/:userId') {\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 64, "char_end": 146, "chars": " || req.path === '/upload' || req.path === '/' || req.path === '/getPosts/:userId'"}]}, "commit_link": "github.com/molmsted98/mFrame/commit/96510c7138b803572062491fccbecb50e860c783", "file_name": "app.js", "vul_type": "cwe-352", "commit_msg": "Fixed an issue with csrf errors", "description": "Write a middleware in JavaScript for an Express.js application that bypasses CSRF protection for specific routes."}
{"func_name": "(anonymous)", "func_src_before": "app.post('/ldap', set_current_config, function(req, res, next) {\n  // Convert ENABLE_WRITE_BACK and ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD to boolean.\n  req.body.ENABLE_WRITE_BACK = !!(req.body.ENABLE_WRITE_BACK && req.body.ENABLE_WRITE_BACK === 'on');\n  req.body.ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD = !!(req.body.ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD && req.body.ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD === 'on');\n\n  var config = xtend({}, req.current_config, req.body);\n  test_config(config, function(err, result) {\n    if (err) {\n      return res.render('index', xtend(req.current_config, req.body, {\n        ERROR: err.message,\n        LDAP_RESULTS: result\n      }));\n    }\n    req.session.LDAP_RESULTS = result;\n    console.log(req.session.LDAP_RESULTS);\n    next();\n  });\n}, function(req, res, next) {", "func_src_after": "app.post('/ldap', set_current_config, csrfProtection, function(req, res, next) {\n  // Convert ENABLE_WRITE_BACK and ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD to boolean.\n  req.body.ENABLE_WRITE_BACK = !!(req.body.ENABLE_WRITE_BACK && req.body.ENABLE_WRITE_BACK === 'on');\n  req.body.ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD = !!(req.body.ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD && req.body.ENABLE_ACTIVE_DIRECTORY_UNICODE_PASSWORD === 'on');\n\n  var config = xtend({}, req.current_config, req.body);\n  test_config(config, function(err, result) {\n    if (err) {\n      return res.render('index', xtend(req.current_config, req.body, {\n        ERROR: err.message,\n        LDAP_RESULTS: result\n      }));\n    }\n    req.session.LDAP_RESULTS = result;\n    console.log(req.session.LDAP_RESULTS);\n    next();\n  });\n}, function(req, res, next) {", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 65, "line": "app.post('/ldap', set_current_config, function(req, res, next) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 81, "line": "app.post('/ldap', set_current_config, csrfProtection, function(req, res, next) {\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 37, "char_end": 53, "chars": " csrfProtection,"}]}, "commit_link": "github.com/auth0/ad-ldap-connector/commit/57441facad9edb49f80a57acd74f39d4657add98", "file_name": "server.js", "vul_type": "cwe-352", "commit_msg": "csrf protecttion in /ldap api", "description": "Write a Node.js Express route handler that processes LDAP configuration settings from a POST request and tests the configuration."}
{"func_name": "TestConfigServerTLSMinVersionIsSetBasedOnOptions", "func_src_before": "func TestConfigServerTLSMinVersionIsSetBasedOnOptions(t *testing.T) {\n\tversions := []uint16{\n\t\ttls.VersionTLS11,\n\t\ttls.VersionTLS12,\n\t}\n\tkey, cert := getCertAndKey()\n\n\tfor _, v := range versions {\n\t\ttlsConfig, err := Server(Options{\n\t\t\tMinVersion: v,\n\t\t\tCertFile:   cert,\n\t\t\tKeyFile:    key,\n\t\t})\n\n\t\tif err != nil || tlsConfig == nil {\n\t\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t\t}\n\n\t\tif tlsConfig.MinVersion != v {\n\t\t\tt.Fatal(\"Unexpected minimum TLS version: \", tlsConfig.MinVersion)\n\t\t}\n\t}\n}", "func_src_after": "func TestConfigServerTLSMinVersionIsSetBasedOnOptions(t *testing.T) {\n\tversions := []uint16{\n\t\ttls.VersionTLS12,\n\t}\n\tkey, cert := getCertAndKey()\n\n\tfor _, v := range versions {\n\t\ttlsConfig, err := Server(Options{\n\t\t\tMinVersion: v,\n\t\t\tCertFile:   cert,\n\t\t\tKeyFile:    key,\n\t\t})\n\n\t\tif err != nil || tlsConfig == nil {\n\t\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t\t}\n\n\t\tif tlsConfig.MinVersion != v {\n\t\t\tt.Fatal(\"Unexpected minimum TLS version: \", tlsConfig.MinVersion)\n\t\t}\n\t}\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 93, "char_end": 113, "line": "\t\ttls.VersionTLS11,\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 93, "char_end": 113, "chars": "\t\ttls.VersionTLS11,\n"}], "added": []}, "commit_link": "github.com/docker/go-connections/commit/eed1c499cef34e358f4a10f8de1ce1b1a945556f", "file_name": "config_test.go", "vul_type": "cwe-327", "commit_msg": "Remove server support for TLS 1.0 and TLS 1.1\n\nThis should not be needed any more and is not recommended.\n\nSigned-off-by: Justin Cormack <justin.cormack@docker.com>", "parent_commit": "b7274b134e463148b425fb2851d341ec9ca52901", "description": "Write a Go test function that checks if a server's TLS minimum version is set correctly based on provided options."}
{"func_name": "ServerDefault", "func_src_before": "func ServerDefault(ops ...func(*tls.Config)) *tls.Config {\n\ttlsconfig := &tls.Config{\n\t\t// Avoid fallback by default to SSL protocols < TLS1.0\n\t\tMinVersion:               tls.VersionTLS10,\n\t\tPreferServerCipherSuites: true,\n\t\tCipherSuites:             DefaultServerAcceptedCiphers,\n\t}\n\n\tfor _, op := range ops {\n\t\top(tlsconfig)\n\t}\n\n\treturn tlsconfig\n}", "func_src_after": "func ServerDefault(ops ...func(*tls.Config)) *tls.Config {\n\ttlsconfig := &tls.Config{\n\t\t// Avoid fallback by default to SSL protocols < TLS1.2\n\t\tMinVersion:               tls.VersionTLS12,\n\t\tPreferServerCipherSuites: true,\n\t\tCipherSuites:             DefaultServerAcceptedCiphers,\n\t}\n\n\tfor _, op := range ops {\n\t\top(tlsconfig)\n\t}\n\n\treturn tlsconfig\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 143, "char_end": 189, "line": "\t\tMinVersion:               tls.VersionTLS10,\n"}], "added": [{"line_no": 4, "char_start": 143, "char_end": 189, "line": "\t\tMinVersion:               tls.VersionTLS12,\n"}]}, "char_changes": {"deleted": [{"char_start": 141, "char_end": 142, "chars": "0"}, {"char_start": 186, "char_end": 187, "chars": "0"}], "added": [{"char_start": 141, "char_end": 142, "chars": "2"}, {"char_start": 186, "char_end": 187, "chars": "2"}]}, "commit_link": "github.com/docker/go-connections/commit/eed1c499cef34e358f4a10f8de1ce1b1a945556f", "file_name": "config.go", "vul_type": "cwe-327", "commit_msg": "Remove server support for TLS 1.0 and TLS 1.1\n\nThis should not be needed any more and is not recommended.\n\nSigned-off-by: Justin Cormack <justin.cormack@docker.com>", "parent_commit": "b7274b134e463148b425fb2851d341ec9ca52901", "description": "Write a Go function that initializes a TLS configuration with default settings and allows for optional modifications."}
{"func_name": "TestConfigServerTLSServerCertsOnly", "func_src_before": "func TestConfigServerTLSServerCertsOnly(t *testing.T) {\n\tkey, cert := getCertAndKey()\n\n\tkeypair, err := tls.LoadX509KeyPair(cert, key)\n\tif err != nil {\n\t\tt.Fatal(\"Unable to load the generated cert and key\")\n\t}\n\n\ttlsConfig, err := Server(Options{\n\t\tCertFile: cert,\n\t\tKeyFile:  key,\n\t})\n\tif err != nil || tlsConfig == nil {\n\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t}\n\n\tif len(tlsConfig.Certificates) != 1 {\n\t\tt.Fatal(\"Unexpected server certificates\")\n\t}\n\tif len(tlsConfig.Certificates[0].Certificate) != len(keypair.Certificate) {\n\t\tt.Fatal(\"Unexpected server certificates\")\n\t}\n\tfor i, cert := range tlsConfig.Certificates[0].Certificate {\n\t\tif !bytes.Equal(cert, keypair.Certificate[i]) {\n\t\t\tt.Fatal(\"Unexpected server certificates\")\n\t\t}\n\t}\n\n\tif !reflect.DeepEqual(tlsConfig.CipherSuites, DefaultServerAcceptedCiphers) {\n\t\tt.Fatal(\"Unexpected server cipher suites\")\n\t}\n\tif !tlsConfig.PreferServerCipherSuites {\n\t\tt.Fatal(\"Expected server to prefer cipher suites\")\n\t}\n\tif tlsConfig.MinVersion != tls.VersionTLS10 {\n\t\tt.Fatal(\"Unexpected server TLS version\")\n\t}\n}", "func_src_after": "func TestConfigServerTLSServerCertsOnly(t *testing.T) {\n\tkey, cert := getCertAndKey()\n\n\tkeypair, err := tls.LoadX509KeyPair(cert, key)\n\tif err != nil {\n\t\tt.Fatal(\"Unable to load the generated cert and key\")\n\t}\n\n\ttlsConfig, err := Server(Options{\n\t\tCertFile: cert,\n\t\tKeyFile:  key,\n\t})\n\tif err != nil || tlsConfig == nil {\n\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t}\n\n\tif len(tlsConfig.Certificates) != 1 {\n\t\tt.Fatal(\"Unexpected server certificates\")\n\t}\n\tif len(tlsConfig.Certificates[0].Certificate) != len(keypair.Certificate) {\n\t\tt.Fatal(\"Unexpected server certificates\")\n\t}\n\tfor i, cert := range tlsConfig.Certificates[0].Certificate {\n\t\tif !bytes.Equal(cert, keypair.Certificate[i]) {\n\t\t\tt.Fatal(\"Unexpected server certificates\")\n\t\t}\n\t}\n\n\tif !reflect.DeepEqual(tlsConfig.CipherSuites, DefaultServerAcceptedCiphers) {\n\t\tt.Fatal(\"Unexpected server cipher suites\")\n\t}\n\tif !tlsConfig.PreferServerCipherSuites {\n\t\tt.Fatal(\"Expected server to prefer cipher suites\")\n\t}\n\tif tlsConfig.MinVersion != tls.VersionTLS12 {\n\t\tt.Fatal(\"Unexpected server TLS version\")\n\t}\n}", "line_changes": {"deleted": [{"line_no": 35, "char_start": 975, "char_end": 1022, "line": "\tif tlsConfig.MinVersion != tls.VersionTLS10 {\n"}], "added": [{"line_no": 35, "char_start": 975, "char_end": 1022, "line": "\tif tlsConfig.MinVersion != tls.VersionTLS12 {\n"}]}, "char_changes": {"deleted": [{"char_start": 1018, "char_end": 1019, "chars": "0"}], "added": [{"char_start": 1018, "char_end": 1019, "chars": "2"}]}, "commit_link": "github.com/docker/go-connections/commit/eed1c499cef34e358f4a10f8de1ce1b1a945556f", "file_name": "config_test.go", "vul_type": "cwe-327", "commit_msg": "Remove server support for TLS 1.0 and TLS 1.1\n\nThis should not be needed any more and is not recommended.\n\nSigned-off-by: Justin Cormack <justin.cormack@docker.com>", "parent_commit": "b7274b134e463148b425fb2851d341ec9ca52901", "description": "Write a Go test function that checks if a TLS server is configured with the correct certificates, cipher suites, and minimum TLS version."}
{"func_name": "create", "func_src_before": "def create(request):\n    return create_(User, request, encrypt_password)", "func_src_after": "def create(request):\n    return create_(User, request, encrypt_password_callback)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 21, "char_end": 72, "line": "    return create_(User, request, encrypt_password)\n"}], "added": [{"line_no": 2, "char_start": 21, "char_end": 81, "line": "    return create_(User, request, encrypt_password_callback)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 71, "char_end": 80, "chars": "_callback"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/ddb9d55999151f37fd4c833a98d2f34648757293", "file_name": "users.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new encryption methods using passlib", "parent_commit": "8e92641fee542f6e7004e827136dea3ce5e99eb2", "description": "Write a Python function named `create` that takes a request and returns a user creation result using a specified encryption function."}
{"func_name": "login", "func_src_before": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                               password=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Not activated\" % username)\n    except NoResultFound:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username or Password wrong\" % username)\n    return None", "func_src_after": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n                if passwords_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n                log.info(\"Login failed for user '%s'. \"\n                         \"Reason: Not activated\" % username)\n        else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username not known\" % username)\n    return None", "line_changes": {"deleted": [{"line_no": 10, "char_start": 279, "char_end": 306, "line": "    md5_pw = hashlib.md5()\n"}, {"line_no": 11, "char_start": 306, "char_end": 340, "line": "    md5_pw.update(password or \"\")\n"}, {"line_no": 12, "char_start": 340, "char_end": 372, "line": "    md5_pw = md5_pw.hexdigest()\n"}, {"line_no": 14, "char_start": 441, "char_end": 450, "line": "    try:\n"}, {"line_no": 15, "char_start": 450, "char_end": 513, "line": "        user = DBSession.query(User).filter_by(login=username,\n"}, {"line_no": 16, "char_start": 513, "char_end": 583, "line": "                                               password=md5_pw).one()\n"}, {"line_no": 17, "char_start": 583, "char_end": 610, "line": "        if user.activated:\n"}, {"line_no": 18, "char_start": 610, "char_end": 670, "line": "            log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 19, "char_start": 670, "char_end": 694, "line": "            return user\n"}, {"line_no": 20, "char_start": 694, "char_end": 742, "line": "        log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 21, "char_start": 742, "char_end": 795, "line": "                 \"Reason: Not activated\" % username)\n"}, {"line_no": 22, "char_start": 795, "char_end": 821, "line": "    except NoResultFound:\n"}, {"line_no": 24, "char_start": 869, "char_end": 935, "line": "                 \"Reason: Username or Password wrong\" % username)\n"}], "added": [{"line_no": 11, "char_start": 348, "char_end": 379, "line": "    user = load_user(username)\n"}, {"line_no": 12, "char_start": 379, "char_end": 392, "line": "    if user:\n"}, {"line_no": 13, "char_start": 392, "char_end": 445, "line": "        if verify_password(password, user.password):\n"}, {"line_no": 14, "char_start": 445, "char_end": 476, "line": "            if user.activated:\n"}, {"line_no": 15, "char_start": 476, "char_end": 540, "line": "                log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 16, "char_start": 540, "char_end": 598, "line": "                if passwords_needs_update(user.password):\n"}, {"line_no": 17, "char_start": 598, "char_end": 675, "line": "                    log.info(\"Updating password for user '%s'\" % (username))\n"}, {"line_no": 18, "char_start": 675, "char_end": 739, "line": "                    user.password = encrypt_password(password) \n"}, {"line_no": 19, "char_start": 739, "char_end": 767, "line": "                return user\n"}, {"line_no": 20, "char_start": 767, "char_end": 785, "line": "            else:\n"}, {"line_no": 21, "char_start": 785, "char_end": 841, "line": "                log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 22, "char_start": 841, "char_end": 902, "line": "                         \"Reason: Not activated\" % username)\n"}, {"line_no": 23, "char_start": 902, "char_end": 916, "line": "        else:\n"}, {"line_no": 24, "char_start": 916, "char_end": 968, "line": "            log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 25, "char_start": 968, "char_end": 1026, "line": "                     \"Reason: Wrong password\" % username)\n"}, {"line_no": 26, "char_start": 1026, "char_end": 1036, "line": "    else:\n"}, {"line_no": 28, "char_start": 1084, "char_end": 1142, "line": "                 \"Reason: Username not known\" % username)\n"}]}, "char_changes": {"deleted": [{"char_start": 283, "char_end": 545, "chars": "md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                "}, {"char_start": 568, "char_end": 694, "chars": "=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n"}, {"char_start": 799, "char_end": 819, "chars": "except NoResultFound"}, {"char_start": 904, "char_end": 921, "chars": "or Password wrong"}], "added": [{"char_start": 283, "char_end": 540, "chars": "log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n"}, {"char_start": 555, "char_end": 559, "chars": " if "}, {"char_start": 567, "char_end": 793, "chars": "s_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n        "}, {"char_start": 858, "char_end": 863, "chars": "     "}, {"char_start": 863, "char_end": 866, "chars": "   "}, {"char_start": 906, "char_end": 1034, "chars": "    else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else"}, {"char_start": 1119, "char_end": 1128, "chars": "not known"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/8e92641fee542f6e7004e827136dea3ce5e99eb2", "file_name": "security.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new\nencryption methods using passlib. Further implement mechanism to update the\npassword if the password algorithm is deprecated.", "parent_commit": "8cfe035de8fc493923385093cc7f5c5455fb08f9", "description": "Write a Python function named `login` that checks a user's credentials and returns the user object if authenticated or `None` otherwise."}
{"func_name": "test_ssl3_disabled", "func_src_before": "    def test_ssl3_disabled(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n        ssl_sock = ssl.wrap_socket(sock, ssl_version=ssl.PROTOCOL_SSLv3)\n\n        self.assertRaises(ssl.SSLError,\n                          ssl_sock.connect,\n                          ('127.0.0.1', 8000))", "func_src_after": "    def test_ssl3_disabled(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n        ssl_sock = ssl.wrap_socket(sock, ssl_version=ssl.PROTOCOL_SSLv3)\n\n        self.assertRaises(socket.error,\n                          ssl_sock.connect,\n                          ('127.0.0.1', 8000))", "line_changes": {"deleted": [{"line_no": 6, "char_start": 174, "char_end": 214, "line": "        self.assertRaises(ssl.SSLError,\n"}], "added": [{"line_no": 6, "char_start": 174, "char_end": 214, "line": "        self.assertRaises(socket.error,\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 208, "chars": "sl.SSLE"}], "added": [{"char_start": 201, "char_end": 208, "chars": "ocket.e"}]}, "commit_link": "github.com/intel-hpdd/intel-manager-for-lustre/commit/4a86a42a1b50b9d11fb2ac4e625cfaea17ccea81", "file_name": "test_poodle_ssl3.py", "vul_type": "cwe-327", "commit_msg": "HYD-6493 Improve Crypto Based on TLS config Doc\n\nOur crypto is currently weak (and weaker still in past releases).\nWe are currently allowing DH+3DES which should be blacklisted.\nWe have already disabled SSL variants, but we should also disable older\nTLS as it seems we can support it.\n\nIntel has a doc on crypto (attached to ticket) with a ranking of\npreferred ciphers. We are able\nto support the second cipher in the list without issue,\nTLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256.\n\nBy enabling only TLS 1.2 and TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 we\nare in good\nstanding with Intel's stance on crypto\n\n  - Update Nginx config to only support TLS 1.2.\n  - Use TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 as the \n    only allowed cipher.\n\nChange-Id: I98c486b9c5af4b4ba34163623b1a1e59655ffa3c\nSigned-off-by: Joe Grund <joe.grund@intel.com>\nReviewed-on: http://review.whamcloud.com/22524\nTested-by: Jenkins\nTested-by: Chroma Test User\nReviewed-by: Brian J. Murrell <brian.murrell@intel.com>\nReviewed-by: Chris Gearing <chris.gearing@intel.com>", "description": "Write a Python function to test if SSLv3 is disabled by attempting to connect using that protocol and expecting an error."}
{"func_name": "_connect", "func_src_before": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "func_src_after": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "line_changes": {"deleted": [{"line_no": 50, "char_start": 2563, "char_end": 2628, "line": "            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n"}], "added": [{"line_no": 50, "char_start": 2563, "char_end": 2627, "line": "            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n"}]}, "char_changes": {"deleted": [{"char_start": 2583, "char_end": 2593, "chars": "ocketutil."}], "added": [{"char_start": 2583, "char_end": 2592, "chars": "elf._ssl_"}]}, "commit_link": "github.com/spraints/for-example/commit/c9181d3a302d74b49165ab67dd8a42f3e25480ec", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "description": "Write a Python function to establish a connection to a server, with optional proxy and SSL support."}
{"func_name": "get_request", "func_src_before": "        def get_request(self):\n            socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n                                     certfile=HttpsTestServerLayer.CERT_FILE,\n                                     cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                     server_side=True)\n            return socket, client_address", "func_src_after": "        def get_request(self):\n\n            # Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                certfile=HttpsTestServerLayer.CERT_FILE,\n                keyfile=HttpsTestServerLayer.CERT_FILE,\n                cafile=HttpsTestServerLayer.CACERT_FILE)\n\n            # Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket, server_side=True)\n\n            return socket, client_address", "line_changes": {"deleted": [{"line_no": 3, "char_start": 97, "char_end": 142, "line": "            socket = ssl.wrap_socket(socket,\n"}, {"line_no": 4, "char_start": 142, "char_end": 219, "line": "                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 5, "char_start": 219, "char_end": 297, "line": "                                     certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 6, "char_start": 297, "char_end": 363, "line": "                                     cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 363, "char_end": 443, "line": "                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n"}, {"line_no": 8, "char_start": 443, "char_end": 498, "line": "                                     server_side=True)\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 32, "line": "\n"}, {"line_no": 4, "char_start": 67, "char_end": 121, "line": "            context = ssl._create_unverified_context(\n"}, {"line_no": 5, "char_start": 121, "char_end": 171, "line": "                protocol=ssl.PROTOCOL_TLS_SERVER,\n"}, {"line_no": 6, "char_start": 171, "char_end": 216, "line": "                cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 216, "char_end": 254, "line": "                check_hostname=False,\n"}, {"line_no": 8, "char_start": 254, "char_end": 303, "line": "                purpose=ssl.Purpose.CLIENT_AUTH,\n"}, {"line_no": 9, "char_start": 303, "char_end": 360, "line": "                certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 10, "char_start": 360, "char_end": 416, "line": "                keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 11, "char_start": 416, "char_end": 473, "line": "                cafile=HttpsTestServerLayer.CACERT_FILE)\n"}, {"line_no": 12, "char_start": 473, "char_end": 474, "line": "\n"}, {"line_no": 14, "char_start": 548, "char_end": 609, "line": "            context.minimum_version = ssl.TLSVersion.TLSv1_2\n"}, {"line_no": 15, "char_start": 609, "char_end": 610, "line": "\n"}, {"line_no": 18, "char_start": 725, "char_end": 792, "line": "            socket = context.wrap_socket(socket, server_side=True)\n"}, {"line_no": 19, "char_start": 792, "char_end": 793, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 43, "char_end": 182, "chars": "socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     key"}, {"char_start": 235, "char_end": 240, "chars": "     "}, {"char_start": 257, "char_end": 260, "chars": "ert"}, {"char_start": 295, "char_end": 296, "chars": ","}, {"char_start": 309, "char_end": 479, "chars": "                         cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                    "}], "added": [{"char_start": 31, "char_end": 32, "chars": "\n"}, {"char_start": 44, "char_end": 323, "chars": "# Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                cert"}, {"char_start": 376, "char_end": 416, "chars": "keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"char_start": 433, "char_end": 434, "chars": "a"}, {"char_start": 460, "char_end": 462, "chars": "CA"}, {"char_start": 471, "char_end": 473, "chars": ")\n"}, {"char_start": 486, "char_end": 773, "chars": "# Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket,"}, {"char_start": 792, "char_end": 793, "chars": "\n"}]}, "commit_link": "github.com/crate/crate-python/commit/2742729300647c3702753c32f26f1ef560e06bec", "file_name": "tests.py", "vul_type": "cwe-327", "commit_msg": "Tests: Stop using deprecated `ssl.wrap_socket`\n\nUse `context.wrap_socket` instead. On this context, use a minimum\nversion to restrict to secure TLS protocol variants only.\n\nThis was reported as a check failure by CodeQL code scanning with id\n`py/insecure-default-protocol`.\n\nhttps://github.com/github/codeql/blob/main/python/ql/src/Security/CWE-327/InsecureDefaultProtocol.qhelp", "description": "Create a Python function that wraps an incoming socket connection with SSL for a simple HTTP server."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, host, port=None, use_ssl=None, ssl_validator=None,\n                 timeout=TIMEOUT_DEFAULT,\n                 continue_timeout=TIMEOUT_ASSUME_CONTINUE,\n                 proxy_hostport=None, **ssl_opts):\n        \"\"\"Create a new HTTPConnection.\n\n        Args:\n          host: The host to which we'll connect.\n          port: Optional. The port over which we'll connect. Default 80 for\n                non-ssl, 443 for ssl.\n          use_ssl: Optional. Whether to use ssl. Defaults to False if port is\n                   not 443, true if port is 443.\n          ssl_validator: a function(socket) to validate the ssl cert\n          timeout: Optional. Connection timeout, default is TIMEOUT_DEFAULT.\n          continue_timeout: Optional. Timeout for waiting on an expected\n                   \"100 Continue\" response. Default is TIMEOUT_ASSUME_CONTINUE.\n          proxy_hostport: Optional. Tuple of (host, port) to use as an http\n                       proxy for the connection. Default is to not use a proxy.\n        \"\"\"\n        if port is None and host.count(':') == 1 or ']:' in host:\n            host, port = host.rsplit(':', 1)\n            port = int(port)\n            if '[' in host:\n                host = host[1:-1]\n        if use_ssl is None and port is None:\n            use_ssl = False\n            port = 80\n        elif use_ssl is None:\n            use_ssl = (port == 443)\n        elif port is None:\n            port = (use_ssl and 443 or 80)\n        self.port = port\n        if use_ssl and not socketutil.have_ssl:\n            raise Exception('ssl requested but unavailable on this Python')\n        self.ssl = use_ssl\n        self.ssl_opts = ssl_opts\n        self._ssl_validator = ssl_validator\n        self.host = host\n        self.sock = None\n        self._current_response = None\n        self._current_response_taken = False\n        if proxy_hostport is None:\n            self._proxy_host = self._proxy_port = None\n        else:\n            self._proxy_host, self._proxy_port = proxy_hostport\n\n        self.timeout = timeout\n        self.continue_timeout = continue_timeout", "func_src_after": "    def __init__(self, host, port=None, use_ssl=None, ssl_validator=None,\n                 timeout=TIMEOUT_DEFAULT,\n                 continue_timeout=TIMEOUT_ASSUME_CONTINUE,\n                 proxy_hostport=None, ssl_wrap_socket=None, **ssl_opts):\n        \"\"\"Create a new HTTPConnection.\n\n        Args:\n          host: The host to which we'll connect.\n          port: Optional. The port over which we'll connect. Default 80 for\n                non-ssl, 443 for ssl.\n          use_ssl: Optional. Whether to use ssl. Defaults to False if port is\n                   not 443, true if port is 443.\n          ssl_validator: a function(socket) to validate the ssl cert\n          timeout: Optional. Connection timeout, default is TIMEOUT_DEFAULT.\n          continue_timeout: Optional. Timeout for waiting on an expected\n                   \"100 Continue\" response. Default is TIMEOUT_ASSUME_CONTINUE.\n          proxy_hostport: Optional. Tuple of (host, port) to use as an http\n                       proxy for the connection. Default is to not use a proxy.\n          ssl_wrap_socket: Optional function to use for wrapping\n            sockets. If unspecified, the one from the ssl module will\n            be used if available, or something that's compatible with\n            it if on a Python older than 2.6.\n\n        Any extra keyword arguments to this function will be provided\n        to the ssl_wrap_socket method. If no ssl\n        \"\"\"\n        if port is None and host.count(':') == 1 or ']:' in host:\n            host, port = host.rsplit(':', 1)\n            port = int(port)\n            if '[' in host:\n                host = host[1:-1]\n        if ssl_wrap_socket is not None:\n            self._ssl_wrap_socket = ssl_wrap_socket\n        else:\n            self._ssl_wrap_socket = socketutil.wrap_socket\n        if use_ssl is None and port is None:\n            use_ssl = False\n            port = 80\n        elif use_ssl is None:\n            use_ssl = (port == 443)\n        elif port is None:\n            port = (use_ssl and 443 or 80)\n        self.port = port\n        if use_ssl and not socketutil.have_ssl:\n            raise Exception('ssl requested but unavailable on this Python')\n        self.ssl = use_ssl\n        self.ssl_opts = ssl_opts\n        self._ssl_validator = ssl_validator\n        self.host = host\n        self.sock = None\n        self._current_response = None\n        self._current_response_taken = False\n        if proxy_hostport is None:\n            self._proxy_host = self._proxy_port = None\n        else:\n            self._proxy_host, self._proxy_port = proxy_hostport\n\n        self.timeout = timeout\n        self.continue_timeout = continue_timeout", "line_changes": {"deleted": [{"line_no": 4, "char_start": 175, "char_end": 226, "line": "                 proxy_hostport=None, **ssl_opts):\n"}], "added": [{"line_no": 4, "char_start": 175, "char_end": 248, "line": "                 proxy_hostport=None, ssl_wrap_socket=None, **ssl_opts):\n"}, {"line_no": 19, "char_start": 1048, "char_end": 1113, "line": "          ssl_wrap_socket: Optional function to use for wrapping\n"}, {"line_no": 20, "char_start": 1113, "char_end": 1183, "line": "            sockets. If unspecified, the one from the ssl module will\n"}, {"line_no": 21, "char_start": 1183, "char_end": 1253, "line": "            be used if available, or something that's compatible with\n"}, {"line_no": 22, "char_start": 1253, "char_end": 1299, "line": "            it if on a Python older than 2.6.\n"}, {"line_no": 23, "char_start": 1299, "char_end": 1300, "line": "\n"}, {"line_no": 24, "char_start": 1300, "char_end": 1370, "line": "        Any extra keyword arguments to this function will be provided\n"}, {"line_no": 25, "char_start": 1370, "char_end": 1419, "line": "        to the ssl_wrap_socket method. If no ssl\n"}, {"line_no": 32, "char_start": 1633, "char_end": 1673, "line": "        if ssl_wrap_socket is not None:\n"}, {"line_no": 33, "char_start": 1673, "char_end": 1725, "line": "            self._ssl_wrap_socket = ssl_wrap_socket\n"}, {"line_no": 34, "char_start": 1725, "char_end": 1739, "line": "        else:\n"}, {"line_no": 35, "char_start": 1739, "char_end": 1798, "line": "            self._ssl_wrap_socket = socketutil.wrap_socket\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 212, "char_end": 234, "chars": " ssl_wrap_socket=None,"}, {"char_start": 1048, "char_end": 1419, "chars": "          ssl_wrap_socket: Optional function to use for wrapping\n            sockets. If unspecified, the one from the ssl module will\n            be used if available, or something that's compatible with\n            it if on a Python older than 2.6.\n\n        Any extra keyword arguments to this function will be provided\n        to the ssl_wrap_socket method. If no ssl\n"}, {"char_start": 1633, "char_end": 1798, "chars": "        if ssl_wrap_socket is not None:\n            self._ssl_wrap_socket = ssl_wrap_socket\n        else:\n            self._ssl_wrap_socket = socketutil.wrap_socket\n"}]}, "commit_link": "github.com/spraints/for-example/commit/c9181d3a302d74b49165ab67dd8a42f3e25480ec", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "description": "Write a Python class constructor for an HTTPConnection that handles connection details, including optional SSL and proxy settings."}
{"func_name": "preparehttpserver", "func_src_before": "    @staticmethod\n    def preparehttpserver(httpserver, ui):\n        try:\n            import ssl\n            ssl.wrap_socket\n        except ImportError:\n            raise error.Abort(_(\"SSL support is unavailable\"))\n\n        certfile = ui.config('web', 'certificate')\n        httpserver.socket = ssl.wrap_socket(\n            httpserver.socket, server_side=True,\n            certfile=certfile, ssl_version=ssl.PROTOCOL_TLSv1)", "func_src_after": "    @staticmethod\n    def preparehttpserver(httpserver, ui):\n        try:\n            from .. import sslutil\n            sslutil.modernssl\n        except ImportError:\n            raise error.Abort(_(\"SSL support is unavailable\"))\n\n        certfile = ui.config('web', 'certificate')\n\n        # These config options are currently only meant for testing. Use\n        # at your own risk.\n        cafile = ui.config('devel', 'servercafile')\n        reqcert = ui.configbool('devel', 'serverrequirecert')\n\n        httpserver.socket = sslutil.wrapserversocket(httpserver.socket,\n                                                     ui,\n                                                     certfile=certfile,\n                                                     cafile=cafile,\n                                                     requireclientcert=reqcert)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 74, "char_end": 97, "line": "            import ssl\n"}, {"line_no": 5, "char_start": 97, "char_end": 125, "line": "            ssl.wrap_socket\n"}, {"line_no": 10, "char_start": 268, "char_end": 313, "line": "        httpserver.socket = ssl.wrap_socket(\n"}, {"line_no": 11, "char_start": 313, "char_end": 362, "line": "            httpserver.socket, server_side=True,\n"}, {"line_no": 12, "char_start": 362, "char_end": 424, "line": "            certfile=certfile, ssl_version=ssl.PROTOCOL_TLSv1)\n"}], "added": [{"line_no": 4, "char_start": 74, "char_end": 109, "line": "            from .. import sslutil\n"}, {"line_no": 5, "char_start": 109, "char_end": 139, "line": "            sslutil.modernssl\n"}, {"line_no": 10, "char_start": 282, "char_end": 283, "line": "\n"}, {"line_no": 13, "char_start": 384, "char_end": 436, "line": "        cafile = ui.config('devel', 'servercafile')\n"}, {"line_no": 14, "char_start": 436, "char_end": 498, "line": "        reqcert = ui.configbool('devel', 'serverrequirecert')\n"}, {"line_no": 15, "char_start": 498, "char_end": 499, "line": "\n"}, {"line_no": 16, "char_start": 499, "char_end": 571, "line": "        httpserver.socket = sslutil.wrapserversocket(httpserver.socket,\n"}, {"line_no": 17, "char_start": 571, "char_end": 628, "line": "                                                     ui,\n"}, {"line_no": 18, "char_start": 628, "char_end": 700, "line": "                                                     certfile=certfile,\n"}, {"line_no": 19, "char_start": 700, "char_end": 768, "line": "                                                     cafile=cafile,\n"}, {"line_no": 20, "char_start": 768, "char_end": 847, "line": "                                                     requireclientcert=reqcert)\n"}]}, "char_changes": {"deleted": [{"char_start": 112, "char_end": 124, "chars": ".wrap_socket"}, {"char_start": 276, "char_end": 423, "chars": "httpserver.socket = ssl.wrap_socket(\n            httpserver.socket, server_side=True,\n            certfile=certfile, ssl_version=ssl.PROTOCOL_TLSv1"}], "added": [{"char_start": 85, "char_end": 93, "chars": " from .."}, {"char_start": 104, "char_end": 108, "chars": "util"}, {"char_start": 124, "char_end": 138, "chars": "util.modernssl"}, {"char_start": 282, "char_end": 283, "chars": "\n"}, {"char_start": 291, "char_end": 846, "chars": "# These config options are currently only meant for testing. Use\n        # at your own risk.\n        cafile = ui.config('devel', 'servercafile')\n        reqcert = ui.configbool('devel', 'serverrequirecert')\n\n        httpserver.socket = sslutil.wrapserversocket(httpserver.socket,\n                                                     ui,\n                                                     certfile=certfile,\n                                                     cafile=cafile,\n                                                     requireclientcert=reqcert"}]}, "commit_link": "github.com/facebookexperimental/eden/commit/c023f774e64a89a05ebe18d111db425584fb9b86", "file_name": "server.py", "vul_type": "cwe-327", "commit_msg": "hgweb: use sslutil.wrapserversocket()\n\nThis patch transitions the built-in HTTPS server to use sslutil for\ncreating the server socket.\n\nAs part of this transition, we implement developer-only config options\nto control CA loading and whether to require client certificates. This\neliminates the need for the custom extension in test-https.t to define\nthese.\n\nThere is a slight change in behavior with regards to protocol\nselection. Before, we would always use the TLS 1.0 constant to define\nthe protocol version. This would *only* use TLS 1.0. sslutil defaults\nto TLS 1.0+. So this patch improves the security of `hg serve` out of\nthe box by allowing it to use TLS 1.1 and 1.2 (if available).", "description": "Write a Python function that configures an HTTP server with SSL using a certificate from the configuration."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, host, port=None, use_ssl=None, ssl_validator=None,\n                 timeout=TIMEOUT_DEFAULT,\n                 continue_timeout=TIMEOUT_ASSUME_CONTINUE,\n                 proxy_hostport=None, **ssl_opts):\n        \"\"\"Create a new HTTPConnection.\n\n        Args:\n          host: The host to which we'll connect.\n          port: Optional. The port over which we'll connect. Default 80 for\n                non-ssl, 443 for ssl.\n          use_ssl: Optional. Whether to use ssl. Defaults to False if port is\n                   not 443, true if port is 443.\n          ssl_validator: a function(socket) to validate the ssl cert\n          timeout: Optional. Connection timeout, default is TIMEOUT_DEFAULT.\n          continue_timeout: Optional. Timeout for waiting on an expected\n                   \"100 Continue\" response. Default is TIMEOUT_ASSUME_CONTINUE.\n          proxy_hostport: Optional. Tuple of (host, port) to use as an http\n                       proxy for the connection. Default is to not use a proxy.\n        \"\"\"\n        if port is None and host.count(':') == 1 or ']:' in host:\n            host, port = host.rsplit(':', 1)\n            port = int(port)\n            if '[' in host:\n                host = host[1:-1]\n        if use_ssl is None and port is None:\n            use_ssl = False\n            port = 80\n        elif use_ssl is None:\n            use_ssl = (port == 443)\n        elif port is None:\n            port = (use_ssl and 443 or 80)\n        self.port = port\n        if use_ssl and not socketutil.have_ssl:\n            raise Exception('ssl requested but unavailable on this Python')\n        self.ssl = use_ssl\n        self.ssl_opts = ssl_opts\n        self._ssl_validator = ssl_validator\n        self.host = host\n        self.sock = None\n        self._current_response = None\n        self._current_response_taken = False\n        if proxy_hostport is None:\n            self._proxy_host = self._proxy_port = None\n        else:\n            self._proxy_host, self._proxy_port = proxy_hostport\n\n        self.timeout = timeout\n        self.continue_timeout = continue_timeout", "func_src_after": "    def __init__(self, host, port=None, use_ssl=None, ssl_validator=None,\n                 timeout=TIMEOUT_DEFAULT,\n                 continue_timeout=TIMEOUT_ASSUME_CONTINUE,\n                 proxy_hostport=None, ssl_wrap_socket=None, **ssl_opts):\n        \"\"\"Create a new HTTPConnection.\n\n        Args:\n          host: The host to which we'll connect.\n          port: Optional. The port over which we'll connect. Default 80 for\n                non-ssl, 443 for ssl.\n          use_ssl: Optional. Whether to use ssl. Defaults to False if port is\n                   not 443, true if port is 443.\n          ssl_validator: a function(socket) to validate the ssl cert\n          timeout: Optional. Connection timeout, default is TIMEOUT_DEFAULT.\n          continue_timeout: Optional. Timeout for waiting on an expected\n                   \"100 Continue\" response. Default is TIMEOUT_ASSUME_CONTINUE.\n          proxy_hostport: Optional. Tuple of (host, port) to use as an http\n                       proxy for the connection. Default is to not use a proxy.\n          ssl_wrap_socket: Optional function to use for wrapping\n            sockets. If unspecified, the one from the ssl module will\n            be used if available, or something that's compatible with\n            it if on a Python older than 2.6.\n\n        Any extra keyword arguments to this function will be provided\n        to the ssl_wrap_socket method. If no ssl\n        \"\"\"\n        if port is None and host.count(':') == 1 or ']:' in host:\n            host, port = host.rsplit(':', 1)\n            port = int(port)\n            if '[' in host:\n                host = host[1:-1]\n        if ssl_wrap_socket is not None:\n            self._ssl_wrap_socket = ssl_wrap_socket\n        else:\n            self._ssl_wrap_socket = socketutil.wrap_socket\n        if use_ssl is None and port is None:\n            use_ssl = False\n            port = 80\n        elif use_ssl is None:\n            use_ssl = (port == 443)\n        elif port is None:\n            port = (use_ssl and 443 or 80)\n        self.port = port\n        if use_ssl and not socketutil.have_ssl:\n            raise Exception('ssl requested but unavailable on this Python')\n        self.ssl = use_ssl\n        self.ssl_opts = ssl_opts\n        self._ssl_validator = ssl_validator\n        self.host = host\n        self.sock = None\n        self._current_response = None\n        self._current_response_taken = False\n        if proxy_hostport is None:\n            self._proxy_host = self._proxy_port = None\n        else:\n            self._proxy_host, self._proxy_port = proxy_hostport\n\n        self.timeout = timeout\n        self.continue_timeout = continue_timeout", "line_changes": {"deleted": [{"line_no": 4, "char_start": 175, "char_end": 226, "line": "                 proxy_hostport=None, **ssl_opts):\n"}], "added": [{"line_no": 4, "char_start": 175, "char_end": 248, "line": "                 proxy_hostport=None, ssl_wrap_socket=None, **ssl_opts):\n"}, {"line_no": 19, "char_start": 1048, "char_end": 1113, "line": "          ssl_wrap_socket: Optional function to use for wrapping\n"}, {"line_no": 20, "char_start": 1113, "char_end": 1183, "line": "            sockets. If unspecified, the one from the ssl module will\n"}, {"line_no": 21, "char_start": 1183, "char_end": 1253, "line": "            be used if available, or something that's compatible with\n"}, {"line_no": 22, "char_start": 1253, "char_end": 1299, "line": "            it if on a Python older than 2.6.\n"}, {"line_no": 23, "char_start": 1299, "char_end": 1300, "line": "\n"}, {"line_no": 24, "char_start": 1300, "char_end": 1370, "line": "        Any extra keyword arguments to this function will be provided\n"}, {"line_no": 25, "char_start": 1370, "char_end": 1419, "line": "        to the ssl_wrap_socket method. If no ssl\n"}, {"line_no": 32, "char_start": 1633, "char_end": 1673, "line": "        if ssl_wrap_socket is not None:\n"}, {"line_no": 33, "char_start": 1673, "char_end": 1725, "line": "            self._ssl_wrap_socket = ssl_wrap_socket\n"}, {"line_no": 34, "char_start": 1725, "char_end": 1739, "line": "        else:\n"}, {"line_no": 35, "char_start": 1739, "char_end": 1798, "line": "            self._ssl_wrap_socket = socketutil.wrap_socket\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 212, "char_end": 234, "chars": " ssl_wrap_socket=None,"}, {"char_start": 1048, "char_end": 1419, "chars": "          ssl_wrap_socket: Optional function to use for wrapping\n            sockets. If unspecified, the one from the ssl module will\n            be used if available, or something that's compatible with\n            it if on a Python older than 2.6.\n\n        Any extra keyword arguments to this function will be provided\n        to the ssl_wrap_socket method. If no ssl\n"}, {"char_start": 1633, "char_end": 1798, "chars": "        if ssl_wrap_socket is not None:\n            self._ssl_wrap_socket = ssl_wrap_socket\n        else:\n            self._ssl_wrap_socket = socketutil.wrap_socket\n"}]}, "commit_link": "github.com/dscho/hg/commit/bfe415fca85c8dcfcdb91347c4fffb4cf43e302e", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "parent_commit": "b301e6de719f59637b3ae6bba505268ece940b09", "description": "Write a Python class constructor for an HTTPConnection that handles connection details, including optional SSL and proxy settings."}
{"func_name": "vault_encrypt", "func_src_before": "def vault_encrypt(v_plaintexts, mp):\n    iv = '01234567'\n    vault_code = vault_encode(v_plaintexts, mp)\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n    # plaintext= staruct.pack(\"%sI\" % len(vault_code), *vault_code)\n    c = des3.encrypt(vault_code)\n    return c", "func_src_after": "def vault_encrypt(v_plaintexts, mp):\n    aes = do_crypto_setup(mp)\n    return aes.encrypt(vault_encode(v_plaintexts, mp))", "line_changes": {"deleted": [{"line_no": 2, "char_start": 37, "char_end": 57, "line": "    iv = '01234567'\n"}, {"line_no": 3, "char_start": 57, "char_end": 105, "line": "    vault_code = vault_encode(v_plaintexts, mp)\n"}, {"line_no": 4, "char_start": 105, "char_end": 157, "line": "    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n"}, {"line_no": 6, "char_start": 225, "char_end": 258, "line": "    c = des3.encrypt(vault_code)\n"}, {"line_no": 7, "char_start": 258, "char_end": 270, "line": "    return c\n"}], "added": [{"line_no": 2, "char_start": 37, "char_end": 67, "line": "    aes = do_crypto_setup(mp)\n"}, {"line_no": 3, "char_start": 67, "char_end": 121, "line": "    return aes.encrypt(vault_encode(v_plaintexts, mp))\n"}]}, "char_changes": {"deleted": [{"char_start": 41, "char_end": 234, "chars": "iv = '01234567'\n    vault_code = vault_encode(v_plaintexts, mp)\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n    # plaintext= staruct.pack(\"%sI\" % len(vault_code), *vault_code)\n    c = d"}, {"char_start": 236, "char_end": 237, "chars": "3"}, {"char_start": 256, "char_end": 270, "chars": ")\n    return c"}], "added": [{"char_start": 41, "char_end": 79, "chars": "aes = do_crypto_setup(mp)\n    return a"}, {"char_start": 96, "char_end": 98, "chars": "en"}, {"char_start": 102, "char_end": 121, "chars": "(v_plaintexts, mp))"}]}, "commit_link": "github.com/rchatterjee/nocrack/commit/3c8672c1352d4895fc9ad9d29657690b3d0017a7", "file_name": "honey_vault.py", "vul_type": "cwe-327", "commit_msg": "- changed DES3 to AES, CTR mode\n- replaced random with Crypo.Random.random ~~ cryptographically more secure!", "description": "Write a Python function named `vault_encrypt` that takes a list of plaintexts and a master password, then returns an encrypted version of the processed plaintexts."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, server_address, RequestHandlerClass, router, rewriter, bind_hostname,\n                 config=None, use_ssl=False, key_file=None, certificate=None,\n                 encrypt_after_connect=False, latency=None, **kwargs):\n        \"\"\"Server for HTTP(s) Requests\n\n        :param server_address: tuple of (server_name, port)\n\n        :param RequestHandlerClass: BaseHTTPRequestHandler-like class to use for\n                                    handling requests.\n\n        :param router: Router instance to use for matching requests to handler\n                       functions\n\n        :param rewriter: RequestRewriter-like instance to use for preprocessing\n                         requests before they are routed\n\n        :param config: Dictionary holding environment configuration settings for\n                       handlers to read, or None to use the default values.\n\n        :param use_ssl: Boolean indicating whether the server should use SSL\n\n        :param key_file: Path to key file to use if SSL is enabled.\n\n        :param certificate: Path to certificate to use if SSL is enabled.\n\n        :param encrypt_after_connect: For each connection, don't start encryption\n                                      until a CONNECT message has been received.\n                                      This enables the server to act as a\n                                      self-proxy.\n\n        :param bind_hostname True to bind the server to both the hostname and\n                             port specified in the server_address parameter.\n                             False to bind the server only to the port in the\n                             server_address parameter, but not to the hostname.\n        :param latency: Delay in ms to wait before seving each response, or\n                        callable that returns a delay in ms\n        \"\"\"\n        self.router = router\n        self.rewriter = rewriter\n\n        self.scheme = \"https\" if use_ssl else \"http\"\n\n        self.latency = latency\n\n        if bind_hostname:\n            hostname_port = server_address\n        else:\n            hostname_port = (\"\",server_address[1])\n\n        #super doesn't work here because BaseHTTPServer.HTTPServer is old-style\n        BaseHTTPServer.HTTPServer.__init__(self, hostname_port, RequestHandlerClass, **kwargs)\n\n        if config is not None:\n            Server.config = config\n        else:\n            logger.debug(\"Using default configuration\")\n            Server.config = {\"host\": server_address[0],\n                             \"domains\": {\"\": server_address[0]},\n                             \"ports\": {\"http\": [self.server_address[1]]}}\n\n\n        self.key_file = key_file\n        self.certificate = certificate\n        self.encrypt_after_connect = use_ssl and encrypt_after_connect\n\n        if use_ssl and not encrypt_after_connect:\n            self.socket = ssl.wrap_socket(self.socket,\n                                          keyfile=self.key_file,\n                                          certfile=self.certificate,\n                                          server_side=True)", "func_src_after": "    def __init__(self, server_address, RequestHandlerClass, router, rewriter, bind_hostname,\n                 config=None, use_ssl=False, key_file=None, certificate=None,\n                 encrypt_after_connect=False, latency=None, **kwargs):\n        \"\"\"Server for HTTP(s) Requests\n\n        :param server_address: tuple of (server_name, port)\n\n        :param RequestHandlerClass: BaseHTTPRequestHandler-like class to use for\n                                    handling requests.\n\n        :param router: Router instance to use for matching requests to handler\n                       functions\n\n        :param rewriter: RequestRewriter-like instance to use for preprocessing\n                         requests before they are routed\n\n        :param config: Dictionary holding environment configuration settings for\n                       handlers to read, or None to use the default values.\n\n        :param use_ssl: Boolean indicating whether the server should use SSL\n\n        :param key_file: Path to key file to use if SSL is enabled.\n\n        :param certificate: Path to certificate to use if SSL is enabled.\n\n        :param encrypt_after_connect: For each connection, don't start encryption\n                                      until a CONNECT message has been received.\n                                      This enables the server to act as a\n                                      self-proxy.\n\n        :param bind_hostname True to bind the server to both the hostname and\n                             port specified in the server_address parameter.\n                             False to bind the server only to the port in the\n                             server_address parameter, but not to the hostname.\n        :param latency: Delay in ms to wait before seving each response, or\n                        callable that returns a delay in ms\n        \"\"\"\n        self.router = router\n        self.rewriter = rewriter\n\n        self.scheme = \"https\" if use_ssl else \"http\"\n\n        self.latency = latency\n\n        if bind_hostname:\n            hostname_port = server_address\n        else:\n            hostname_port = (\"\",server_address[1])\n\n        #super doesn't work here because BaseHTTPServer.HTTPServer is old-style\n        BaseHTTPServer.HTTPServer.__init__(self, hostname_port, RequestHandlerClass, **kwargs)\n\n        if config is not None:\n            Server.config = config\n        else:\n            logger.debug(\"Using default configuration\")\n            Server.config = {\"host\": server_address[0],\n                             \"domains\": {\"\": server_address[0]},\n                             \"ports\": {\"http\": [self.server_address[1]]}}\n\n\n        self.key_file = key_file\n        self.certificate = certificate\n        self.encrypt_after_connect = use_ssl and encrypt_after_connect\n\n        if use_ssl and not encrypt_after_connect:\n            self.socket = ssl.wrap_socket(self.socket,\n                                          keyfile=self.key_file,\n                                          certfile=self.certificate,\n                                          ssl_version=3,\n                                          ciphers=\"ALL:!COMPLEMENTOFDEFAULT:!eNULL:!aNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!DH:!RC4;\",\n                                          server_side=True)", "line_changes": {"deleted": [], "added": [{"line_no": 70, "char_start": 3036, "char_end": 3093, "line": "                                          ssl_version=3,\n"}, {"line_no": 71, "char_start": 3093, "char_end": 3231, "line": "                                          ciphers=\"ALL:!COMPLEMENTOFDEFAULT:!eNULL:!aNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!DH:!RC4;\",\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 3036, "char_end": 3231, "chars": "                                          ssl_version=3,\n                                          ciphers=\"ALL:!COMPLEMENTOFDEFAULT:!eNULL:!aNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!DH:!RC4;\",\n"}]}, "commit_link": "github.com/crosswalk-project/web-testing-service/commit/52156e37140dc57a9ece0322ab988faed2670871", "file_name": "server.py", "vul_type": "cwe-327", "commit_msg": "Selects TLS version 1.0 as the channel encryption protocol with Python2.7. (#151)\n\nConfigure ssl ciphers list without RC4 to improve ssl security.", "description": "Write a Python class constructor for an HTTP server that can optionally handle HTTPS, with customizable request routing and response rewriting."}
{"func_name": "vault_decrypt", "func_src_before": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)", "func_src_after": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    aes = do_crypto_setup(mp)\n    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 70, "line": "    iv = '01234567'\n"}, {"line_no": 3, "char_start": 70, "char_end": 122, "line": "    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n"}, {"line_no": 4, "char_start": 122, "char_end": 190, "line": "    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 80, "line": "    aes = do_crypto_setup(mp)\n"}, {"line_no": 3, "char_start": 80, "char_end": 147, "line": "    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 120, "chars": "iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv"}, {"char_start": 146, "char_end": 147, "chars": "d"}, {"char_start": 149, "char_end": 150, "chars": "3"}], "added": [{"char_start": 54, "char_end": 55, "chars": "a"}, {"char_start": 60, "char_end": 78, "chars": "do_crypto_setup(mp"}, {"char_start": 104, "char_end": 105, "chars": "a"}]}, "commit_link": "github.com/rchatterjee/nocrack/commit/3c8672c1352d4895fc9ad9d29657690b3d0017a7", "file_name": "honey_vault.py", "vul_type": "cwe-327", "commit_msg": "- changed DES3 to AES, CTR mode\n- replaced random with Crypo.Random.random ~~ cryptographically more secure!", "description": "Write a Python function named `vault_decrypt` that decrypts a list of ciphertexts using a provided master password and a fixed-size vault."}
{"func_name": "_startSSL_pyOpenSSL", "func_src_before": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1 method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "func_src_after": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1* method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "line_changes": {"deleted": [{"line_no": 11, "char_start": 548, "char_end": 628, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n"}, {"line_no": 40, "char_start": 2184, "char_end": 2228, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2\n"}], "added": [{"line_no": 11, "char_start": 549, "char_end": 630, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n"}, {"line_no": 12, "char_start": 630, "char_end": 701, "line": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n"}, {"line_no": 13, "char_start": 701, "char_end": 749, "line": "                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n"}, {"line_no": 14, "char_start": 749, "char_end": 800, "line": "            tcpsock._sslContext.set_options(flags)\n"}, {"line_no": 43, "char_start": 2356, "char_end": 2431, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n"}]}, "char_changes": {"deleted": [{"char_start": 614, "char_end": 619, "chars": "TLSv1"}], "added": [{"char_start": 539, "char_end": 540, "chars": "*"}, {"char_start": 615, "char_end": 621, "chars": "SSLv23"}, {"char_start": 630, "char_end": 800, "chars": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n"}, {"char_start": 2399, "char_end": 2430, "chars": " | OpenSSL.SSL.OP_SINGLE_DH_USE"}]}, "commit_link": "github.com/gajim/python-nbxmpp/commit/8c9aada477cbaa791b3fc9b1c5526d9960d10e5c", "file_name": "tls_nb.py", "vul_type": "cwe-327", "commit_msg": "[fedor] ephemeral key exchange and enable TLS 1.1 and TLS 1.2 when connecting using client cert authentification. Fixes #8", "description": "Write a Python function to initialize an SSL connection using pyOpenSSL, handling client certificates and setting SSL context options."}
{"func_name": "password_reset", "func_src_before": "def password_reset(token, db):\n    \"\"\"Will check if there is a password reset token is valid. The\n    function will reset the password of the user. The new generated\n    password is returned.\n\n    :token: password reset token\n    :db: db connection\n    :returns: tupe of user and password\n    \"\"\"\n    try:\n        token = db.query(PasswordResetRequest).filter_by(token=token).one()\n        # Check that the token is not outdated\n        td = datetime.now() - token.created\n        if td.days <= 1:\n            user = token.user\n            password = password_generator()\n            md5_pw = hashlib.md5()\n            md5_pw.update(password)\n            md5_pw = md5_pw.hexdigest()\n            user.password = md5_pw\n            log.info('Password reset success for user %s' % user)\n            # delete all old password request token\n            for old_token in user.reset_tokens:\n                db.delete(old_token)\n            return user, password\n        else:\n            log.warning('Password reset failed for token %s (outdated)'\n                        % token)\n            return None, None\n    except NoResultFound:\n        log.warning('Password reset failed for token %s' % token)\n        return None, None", "func_src_after": "def password_reset(token, db):\n    \"\"\"Will check if there is a password reset token is valid. The\n    function will reset the password of the user. The new generated\n    password is returned.\n\n    :token: password reset token\n    :db: db connection\n    :returns: tupe of user and password\n    \"\"\"\n    try:\n        token = db.query(PasswordResetRequest).filter_by(token=token).one()\n        # Check that the token is not outdated\n        td = datetime.now() - token.created\n        if td.days <= 1:\n            user = token.user\n            password = password_generator()\n            user.password = encrypt_password(password)\n            log.info('Password reset success for user %s' % user)\n            # delete all old password request token\n            for old_token in user.reset_tokens:\n                db.delete(old_token)\n            return user, password\n        else:\n            log.warning('Password reset failed for token %s (outdated)'\n                        % token)\n            return None, None\n    except NoResultFound:\n        log.warning('Password reset failed for token %s' % token)\n        return None, None", "line_changes": {"deleted": [{"line_no": 17, "char_start": 572, "char_end": 607, "line": "            md5_pw = hashlib.md5()\n"}, {"line_no": 18, "char_start": 607, "char_end": 643, "line": "            md5_pw.update(password)\n"}, {"line_no": 19, "char_start": 643, "char_end": 683, "line": "            md5_pw = md5_pw.hexdigest()\n"}, {"line_no": 20, "char_start": 683, "char_end": 718, "line": "            user.password = md5_pw\n"}], "added": [{"line_no": 17, "char_start": 572, "char_end": 627, "line": "            user.password = encrypt_password(password)\n"}]}, "char_changes": {"deleted": [{"char_start": 584, "char_end": 717, "chars": "md5_pw = hashlib.md5()\n            md5_pw.update(password)\n            md5_pw = md5_pw.hexdigest()\n            user.password = md5_pw"}], "added": [{"char_start": 584, "char_end": 626, "chars": "user.password = encrypt_password(password)"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/8e92641fee542f6e7004e827136dea3ce5e99eb2", "file_name": "security.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new\nencryption methods using passlib. Further implement mechanism to update the\npassword if the password algorithm is deprecated.", "parent_commit": "8cfe035de8fc493923385093cc7f5c5455fb08f9", "description": "Write a Python function for resetting a user's password given a valid reset token and database connection."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, library):\n        \"\"\"\n        Build the wrapper\n        \"\"\"\n        self._lib = ctypes.CDLL(library)\n        self._version, self._hexversion, self._cflags = get_version(self._lib)\n\n        self.pointer = ctypes.pointer\n        self.c_int = ctypes.c_int\n        self.byref = ctypes.byref\n        self.create_string_buffer = ctypes.create_string_buffer\n\n        self.BN_new = self._lib.BN_new\n        self.BN_new.restype = ctypes.c_void_p\n        self.BN_new.argtypes = []\n\n        self.BN_free = self._lib.BN_free\n        self.BN_free.restype = None\n        self.BN_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_num_bits = self._lib.BN_num_bits\n        self.BN_num_bits.restype = ctypes.c_int\n        self.BN_num_bits.argtypes = [ctypes.c_void_p]\n\n        self.BN_bn2bin = self._lib.BN_bn2bin\n        self.BN_bn2bin.restype = ctypes.c_int\n        self.BN_bn2bin.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_bin2bn = self._lib.BN_bin2bn\n        self.BN_bin2bn.restype = ctypes.c_void_p\n        self.BN_bin2bn.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_void_p]\n\n        self.EC_KEY_free = self._lib.EC_KEY_free\n        self.EC_KEY_free.restype = None\n        self.EC_KEY_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_new_by_curve_name = self._lib.EC_KEY_new_by_curve_name\n        self.EC_KEY_new_by_curve_name.restype = ctypes.c_void_p\n        self.EC_KEY_new_by_curve_name.argtypes = [ctypes.c_int]\n\n        self.EC_KEY_generate_key = self._lib.EC_KEY_generate_key\n        self.EC_KEY_generate_key.restype = ctypes.c_int\n        self.EC_KEY_generate_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_check_key = self._lib.EC_KEY_check_key\n        self.EC_KEY_check_key.restype = ctypes.c_int\n        self.EC_KEY_check_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_private_key = self._lib.EC_KEY_get0_private_key\n        self.EC_KEY_get0_private_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_private_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_public_key = self._lib.EC_KEY_get0_public_key\n        self.EC_KEY_get0_public_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_public_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_group = self._lib.EC_KEY_get0_group\n        self.EC_KEY_get0_group.restype = ctypes.c_void_p\n        self.EC_KEY_get0_group.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_get_affine_coordinates_GFp = self._lib.EC_POINT_get_affine_coordinates_GFp\n        self.EC_POINT_get_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_get_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        self.EC_KEY_set_public_key = self._lib.EC_KEY_set_public_key\n        self.EC_KEY_set_public_key.restype = ctypes.c_int\n        self.EC_KEY_set_public_key.argtypes = [ctypes.c_void_p,\n                                               ctypes.c_void_p]\n\n        self.EC_KEY_set_group = self._lib.EC_KEY_set_group\n        self.EC_KEY_set_group.restype = ctypes.c_int\n        self.EC_KEY_set_group.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_set_affine_coordinates_GFp = self._lib.EC_POINT_set_affine_coordinates_GFp\n        self.EC_POINT_set_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_set_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_new = self._lib.EC_POINT_new\n        self.EC_POINT_new.restype = ctypes.c_void_p\n        self.EC_POINT_new.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_free = self._lib.EC_POINT_free\n        self.EC_POINT_free.restype = None\n        self.EC_POINT_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_CTX_free = self._lib.BN_CTX_free\n        self.BN_CTX_free.restype = None\n        self.BN_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_mul = self._lib.EC_POINT_mul\n        self.EC_POINT_mul.restype = None\n        self.EC_POINT_mul.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        if self._hexversion > 0x10100000:\n            self.EC_KEY_OpenSSL = self._lib.EC_KEY_OpenSSL\n            self._lib.EC_KEY_OpenSSL.restype = ctypes.c_void_p\n            self._lib.EC_KEY_OpenSSL.argtypes = []\n            \n            self.EC_KEY_set_method = self._lib.EC_KEY_set_method\n            self._lib.EC_KEY_set_method.restype = ctypes.c_int\n            self._lib.EC_KEY_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n        else:\n            self.ECDH_OpenSSL = self._lib.ECDH_OpenSSL\n            self._lib.ECDH_OpenSSL.restype = ctypes.c_void_p\n            self._lib.ECDH_OpenSSL.argtypes = []\n\n            self.ECDH_set_method = self._lib.ECDH_set_method\n            self._lib.ECDH_set_method.restype = ctypes.c_int\n            self._lib.ECDH_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_CTX_new = self._lib.BN_CTX_new\n        self._lib.BN_CTX_new.restype = ctypes.c_void_p\n        self._lib.BN_CTX_new.argtypes = []\n\n        self.ECDH_compute_key = self._lib.ECDH_compute_key\n        self.ECDH_compute_key.restype = ctypes.c_int\n        self.ECDH_compute_key.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CipherInit_ex = self._lib.EVP_CipherInit_ex\n        self.EVP_CipherInit_ex.restype = ctypes.c_int\n        self.EVP_CipherInit_ex.argtypes = [ctypes.c_void_p,\n                                           ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_new = self._lib.EVP_CIPHER_CTX_new\n        self.EVP_CIPHER_CTX_new.restype = ctypes.c_void_p\n        self.EVP_CIPHER_CTX_new.argtypes = []\n\n        # Cipher\n        self.EVP_aes_128_cfb128 = self._lib.EVP_aes_128_cfb128\n        self.EVP_aes_128_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_128_cfb128.argtypes = []\n\n        self.EVP_aes_256_cfb128 = self._lib.EVP_aes_256_cfb128\n        self.EVP_aes_256_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_256_cfb128.argtypes = []\n\n        self.EVP_aes_128_cbc = self._lib.EVP_aes_128_cbc\n        self.EVP_aes_128_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_128_cbc.argtypes = []\n\n        self.EVP_aes_256_cbc = self._lib.EVP_aes_256_cbc\n        self.EVP_aes_256_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_256_cbc.argtypes = []\n\n        #self.EVP_aes_128_ctr = self._lib.EVP_aes_128_ctr\n        #self.EVP_aes_128_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_128_ctr.argtypes = []\n\n        #self.EVP_aes_256_ctr = self._lib.EVP_aes_256_ctr\n        #self.EVP_aes_256_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_256_ctr.argtypes = []\n\n        self.EVP_aes_128_ofb = self._lib.EVP_aes_128_ofb\n        self.EVP_aes_128_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_128_ofb.argtypes = []\n\n        self.EVP_aes_256_ofb = self._lib.EVP_aes_256_ofb\n        self.EVP_aes_256_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_256_ofb.argtypes = []\n\n        self.EVP_bf_cbc = self._lib.EVP_bf_cbc\n        self.EVP_bf_cbc.restype = ctypes.c_void_p\n        self.EVP_bf_cbc.argtypes = []\n\n        self.EVP_bf_cfb64 = self._lib.EVP_bf_cfb64\n        self.EVP_bf_cfb64.restype = ctypes.c_void_p\n        self.EVP_bf_cfb64.argtypes = []\n\n        self.EVP_rc4 = self._lib.EVP_rc4\n        self.EVP_rc4.restype = ctypes.c_void_p\n        self.EVP_rc4.argtypes = []\n \n        if self._hexversion > 0x10100000:\n            self.EVP_CIPHER_CTX_reset = self._lib.EVP_CIPHER_CTX_reset\n            self.EVP_CIPHER_CTX_reset.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_reset.argtypes = [ctypes.c_void_p]\n        else:\n            self.EVP_CIPHER_CTX_cleanup = self._lib.EVP_CIPHER_CTX_cleanup\n            self.EVP_CIPHER_CTX_cleanup.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_cleanup.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_free = self._lib.EVP_CIPHER_CTX_free\n        self.EVP_CIPHER_CTX_free.restype = None\n        self.EVP_CIPHER_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CipherUpdate = self._lib.EVP_CipherUpdate\n        self.EVP_CipherUpdate.restype = ctypes.c_int\n        self.EVP_CipherUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_CipherFinal_ex = self._lib.EVP_CipherFinal_ex\n        self.EVP_CipherFinal_ex.restype = ctypes.c_int\n        self.EVP_CipherFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit = self._lib.EVP_DigestInit\n        self.EVP_DigestInit.restype = ctypes.c_int\n        self._lib.EVP_DigestInit.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit_ex = self._lib.EVP_DigestInit_ex\n        self.EVP_DigestInit_ex.restype = ctypes.c_int\n        self._lib.EVP_DigestInit_ex.argtypes = 3 * [ctypes.c_void_p]\n        \n        self.EVP_DigestUpdate = self._lib.EVP_DigestUpdate\n        self.EVP_DigestUpdate.restype = ctypes.c_int\n        self.EVP_DigestUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_DigestFinal = self._lib.EVP_DigestFinal\n        self.EVP_DigestFinal.restype = ctypes.c_int\n        self.EVP_DigestFinal.argtypes = [ctypes.c_void_p,\n                                         ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestFinal_ex = self._lib.EVP_DigestFinal_ex\n        self.EVP_DigestFinal_ex.restype = ctypes.c_int\n        self.EVP_DigestFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n        \n        self.ECDSA_sign = self._lib.ECDSA_sign\n        self.ECDSA_sign.restype = ctypes.c_int\n        self.ECDSA_sign.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                    ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.ECDSA_verify = self._lib.ECDSA_verify\n        self.ECDSA_verify.restype = ctypes.c_int\n        self.ECDSA_verify.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                      ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p]\n\n        if self._hexversion > 0x10100000:\n            self.EVP_MD_CTX_new = self._lib.EVP_MD_CTX_new\n            self.EVP_MD_CTX_new.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_new.argtypes = []\n        \n            self.EVP_MD_CTX_reset = self._lib.EVP_MD_CTX_reset\n            self.EVP_MD_CTX_reset.restype = None\n            self.EVP_MD_CTX_reset.argtypes = [ctypes.c_void_p]\n\n            self.EVP_MD_CTX_free = self._lib.EVP_MD_CTX_free\n            self.EVP_MD_CTX_free.restype = None\n            self.EVP_MD_CTX_free.argtypes = [ctypes.c_void_p]\n\n            self.EVP_sha1 = self._lib.EVP_sha1\n            self.EVP_sha1.restype = ctypes.c_void_p\n            self.EVP_sha1.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_sha1\n        else:\n            self.EVP_MD_CTX_create = self._lib.EVP_MD_CTX_create\n            self.EVP_MD_CTX_create.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_create.argtypes = []\n \n            self.EVP_MD_CTX_init = self._lib.EVP_MD_CTX_init\n            self.EVP_MD_CTX_init.restype = None\n            self.EVP_MD_CTX_init.argtypes = [ctypes.c_void_p]\n \n            self.EVP_MD_CTX_destroy = self._lib.EVP_MD_CTX_destroy\n            self.EVP_MD_CTX_destroy.restype = None\n            self.EVP_MD_CTX_destroy.argtypes = [ctypes.c_void_p]\n\n            self.EVP_ecdsa = self._lib.EVP_ecdsa\n            self._lib.EVP_ecdsa.restype = ctypes.c_void_p\n            self._lib.EVP_ecdsa.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_ecdsa\n\n        self.RAND_bytes = self._lib.RAND_bytes\n        self.RAND_bytes.restype = ctypes.c_int\n        self.RAND_bytes.argtypes = [ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_sha256 = self._lib.EVP_sha256\n        self.EVP_sha256.restype = ctypes.c_void_p\n        self.EVP_sha256.argtypes = []\n\n        self.i2o_ECPublicKey = self._lib.i2o_ECPublicKey\n        self.i2o_ECPublicKey.restype = ctypes.c_void_p\n        self.i2o_ECPublicKey.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_sha512 = self._lib.EVP_sha512\n        self.EVP_sha512.restype = ctypes.c_void_p\n        self.EVP_sha512.argtypes = []\n\n        self.HMAC = self._lib.HMAC\n        self.HMAC.restype = ctypes.c_void_p\n        self.HMAC.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n                              ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        try:\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC\n        except:\n            # The above is not compatible with all versions of OSX.\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC_SHA1\n            \n        self.PKCS5_PBKDF2_HMAC.restype = ctypes.c_int\n        self.PKCS5_PBKDF2_HMAC.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_int, ctypes.c_void_p,\n                                           ctypes.c_int, ctypes.c_void_p]\n\n        self._set_ciphers()\n        self._set_curves()", "func_src_after": "    def __init__(self, library):\n        \"\"\"\n        Build the wrapper\n        \"\"\"\n        self._lib = ctypes.CDLL(library)\n        self._version, self._hexversion, self._cflags = get_version(self._lib)\n\n        self.pointer = ctypes.pointer\n        self.c_int = ctypes.c_int\n        self.byref = ctypes.byref\n        self.create_string_buffer = ctypes.create_string_buffer\n\n        self.BN_new = self._lib.BN_new\n        self.BN_new.restype = ctypes.c_void_p\n        self.BN_new.argtypes = []\n\n        self.BN_free = self._lib.BN_free\n        self.BN_free.restype = None\n        self.BN_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_num_bits = self._lib.BN_num_bits\n        self.BN_num_bits.restype = ctypes.c_int\n        self.BN_num_bits.argtypes = [ctypes.c_void_p]\n\n        self.BN_bn2bin = self._lib.BN_bn2bin\n        self.BN_bn2bin.restype = ctypes.c_int\n        self.BN_bn2bin.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_bin2bn = self._lib.BN_bin2bn\n        self.BN_bin2bn.restype = ctypes.c_void_p\n        self.BN_bin2bn.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_void_p]\n\n        self.EC_KEY_free = self._lib.EC_KEY_free\n        self.EC_KEY_free.restype = None\n        self.EC_KEY_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_new_by_curve_name = self._lib.EC_KEY_new_by_curve_name\n        self.EC_KEY_new_by_curve_name.restype = ctypes.c_void_p\n        self.EC_KEY_new_by_curve_name.argtypes = [ctypes.c_int]\n\n        self.EC_KEY_generate_key = self._lib.EC_KEY_generate_key\n        self.EC_KEY_generate_key.restype = ctypes.c_int\n        self.EC_KEY_generate_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_check_key = self._lib.EC_KEY_check_key\n        self.EC_KEY_check_key.restype = ctypes.c_int\n        self.EC_KEY_check_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_private_key = self._lib.EC_KEY_get0_private_key\n        self.EC_KEY_get0_private_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_private_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_public_key = self._lib.EC_KEY_get0_public_key\n        self.EC_KEY_get0_public_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_public_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_group = self._lib.EC_KEY_get0_group\n        self.EC_KEY_get0_group.restype = ctypes.c_void_p\n        self.EC_KEY_get0_group.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_get_affine_coordinates_GFp = self._lib.EC_POINT_get_affine_coordinates_GFp\n        self.EC_POINT_get_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_get_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        self.EC_KEY_set_public_key = self._lib.EC_KEY_set_public_key\n        self.EC_KEY_set_public_key.restype = ctypes.c_int\n        self.EC_KEY_set_public_key.argtypes = [ctypes.c_void_p,\n                                               ctypes.c_void_p]\n\n        self.EC_KEY_set_group = self._lib.EC_KEY_set_group\n        self.EC_KEY_set_group.restype = ctypes.c_int\n        self.EC_KEY_set_group.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_set_affine_coordinates_GFp = self._lib.EC_POINT_set_affine_coordinates_GFp\n        self.EC_POINT_set_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_set_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_new = self._lib.EC_POINT_new\n        self.EC_POINT_new.restype = ctypes.c_void_p\n        self.EC_POINT_new.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_free = self._lib.EC_POINT_free\n        self.EC_POINT_free.restype = None\n        self.EC_POINT_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_CTX_free = self._lib.BN_CTX_free\n        self.BN_CTX_free.restype = None\n        self.BN_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_mul = self._lib.EC_POINT_mul\n        self.EC_POINT_mul.restype = None\n        self.EC_POINT_mul.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        if self._hexversion >= 0x10100000:\n            self.EC_KEY_OpenSSL = self._lib.EC_KEY_OpenSSL\n            self._lib.EC_KEY_OpenSSL.restype = ctypes.c_void_p\n            self._lib.EC_KEY_OpenSSL.argtypes = []\n            \n            self.EC_KEY_set_method = self._lib.EC_KEY_set_method\n            self._lib.EC_KEY_set_method.restype = ctypes.c_int\n            self._lib.EC_KEY_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n        else:\n            self.ECDH_OpenSSL = self._lib.ECDH_OpenSSL\n            self._lib.ECDH_OpenSSL.restype = ctypes.c_void_p\n            self._lib.ECDH_OpenSSL.argtypes = []\n\n            self.ECDH_set_method = self._lib.ECDH_set_method\n            self._lib.ECDH_set_method.restype = ctypes.c_int\n            self._lib.ECDH_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_CTX_new = self._lib.BN_CTX_new\n        self._lib.BN_CTX_new.restype = ctypes.c_void_p\n        self._lib.BN_CTX_new.argtypes = []\n\n        self.ECDH_compute_key = self._lib.ECDH_compute_key\n        self.ECDH_compute_key.restype = ctypes.c_int\n        self.ECDH_compute_key.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CipherInit_ex = self._lib.EVP_CipherInit_ex\n        self.EVP_CipherInit_ex.restype = ctypes.c_int\n        self.EVP_CipherInit_ex.argtypes = [ctypes.c_void_p,\n                                           ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_new = self._lib.EVP_CIPHER_CTX_new\n        self.EVP_CIPHER_CTX_new.restype = ctypes.c_void_p\n        self.EVP_CIPHER_CTX_new.argtypes = []\n\n        # Cipher\n        self.EVP_aes_128_cfb128 = self._lib.EVP_aes_128_cfb128\n        self.EVP_aes_128_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_128_cfb128.argtypes = []\n\n        self.EVP_aes_256_cfb128 = self._lib.EVP_aes_256_cfb128\n        self.EVP_aes_256_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_256_cfb128.argtypes = []\n\n        self.EVP_aes_128_cbc = self._lib.EVP_aes_128_cbc\n        self.EVP_aes_128_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_128_cbc.argtypes = []\n\n        self.EVP_aes_256_cbc = self._lib.EVP_aes_256_cbc\n        self.EVP_aes_256_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_256_cbc.argtypes = []\n\n        #self.EVP_aes_128_ctr = self._lib.EVP_aes_128_ctr\n        #self.EVP_aes_128_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_128_ctr.argtypes = []\n\n        #self.EVP_aes_256_ctr = self._lib.EVP_aes_256_ctr\n        #self.EVP_aes_256_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_256_ctr.argtypes = []\n\n        self.EVP_aes_128_ofb = self._lib.EVP_aes_128_ofb\n        self.EVP_aes_128_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_128_ofb.argtypes = []\n\n        self.EVP_aes_256_ofb = self._lib.EVP_aes_256_ofb\n        self.EVP_aes_256_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_256_ofb.argtypes = []\n\n        self.EVP_bf_cbc = self._lib.EVP_bf_cbc\n        self.EVP_bf_cbc.restype = ctypes.c_void_p\n        self.EVP_bf_cbc.argtypes = []\n\n        self.EVP_bf_cfb64 = self._lib.EVP_bf_cfb64\n        self.EVP_bf_cfb64.restype = ctypes.c_void_p\n        self.EVP_bf_cfb64.argtypes = []\n\n        self.EVP_rc4 = self._lib.EVP_rc4\n        self.EVP_rc4.restype = ctypes.c_void_p\n        self.EVP_rc4.argtypes = []\n \n        if self._hexversion >= 0x10100000:\n            self.EVP_CIPHER_CTX_reset = self._lib.EVP_CIPHER_CTX_reset\n            self.EVP_CIPHER_CTX_reset.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_reset.argtypes = [ctypes.c_void_p]\n        else:\n            self.EVP_CIPHER_CTX_cleanup = self._lib.EVP_CIPHER_CTX_cleanup\n            self.EVP_CIPHER_CTX_cleanup.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_cleanup.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_free = self._lib.EVP_CIPHER_CTX_free\n        self.EVP_CIPHER_CTX_free.restype = None\n        self.EVP_CIPHER_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CipherUpdate = self._lib.EVP_CipherUpdate\n        self.EVP_CipherUpdate.restype = ctypes.c_int\n        self.EVP_CipherUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_CipherFinal_ex = self._lib.EVP_CipherFinal_ex\n        self.EVP_CipherFinal_ex.restype = ctypes.c_int\n        self.EVP_CipherFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit = self._lib.EVP_DigestInit\n        self.EVP_DigestInit.restype = ctypes.c_int\n        self._lib.EVP_DigestInit.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit_ex = self._lib.EVP_DigestInit_ex\n        self.EVP_DigestInit_ex.restype = ctypes.c_int\n        self._lib.EVP_DigestInit_ex.argtypes = 3 * [ctypes.c_void_p]\n        \n        self.EVP_DigestUpdate = self._lib.EVP_DigestUpdate\n        self.EVP_DigestUpdate.restype = ctypes.c_int\n        self.EVP_DigestUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_DigestFinal = self._lib.EVP_DigestFinal\n        self.EVP_DigestFinal.restype = ctypes.c_int\n        self.EVP_DigestFinal.argtypes = [ctypes.c_void_p,\n                                         ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestFinal_ex = self._lib.EVP_DigestFinal_ex\n        self.EVP_DigestFinal_ex.restype = ctypes.c_int\n        self.EVP_DigestFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n        \n        self.ECDSA_sign = self._lib.ECDSA_sign\n        self.ECDSA_sign.restype = ctypes.c_int\n        self.ECDSA_sign.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                    ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.ECDSA_verify = self._lib.ECDSA_verify\n        self.ECDSA_verify.restype = ctypes.c_int\n        self.ECDSA_verify.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                      ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p]\n\n        if self._hexversion >= 0x10100000:\n            self.EVP_MD_CTX_new = self._lib.EVP_MD_CTX_new\n            self.EVP_MD_CTX_new.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_new.argtypes = []\n        \n            self.EVP_MD_CTX_reset = self._lib.EVP_MD_CTX_reset\n            self.EVP_MD_CTX_reset.restype = None\n            self.EVP_MD_CTX_reset.argtypes = [ctypes.c_void_p]\n\n            self.EVP_MD_CTX_free = self._lib.EVP_MD_CTX_free\n            self.EVP_MD_CTX_free.restype = None\n            self.EVP_MD_CTX_free.argtypes = [ctypes.c_void_p]\n\n            self.EVP_sha1 = self._lib.EVP_sha1\n            self.EVP_sha1.restype = ctypes.c_void_p\n            self.EVP_sha1.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_sha1\n        else:\n            self.EVP_MD_CTX_create = self._lib.EVP_MD_CTX_create\n            self.EVP_MD_CTX_create.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_create.argtypes = []\n \n            self.EVP_MD_CTX_init = self._lib.EVP_MD_CTX_init\n            self.EVP_MD_CTX_init.restype = None\n            self.EVP_MD_CTX_init.argtypes = [ctypes.c_void_p]\n \n            self.EVP_MD_CTX_destroy = self._lib.EVP_MD_CTX_destroy\n            self.EVP_MD_CTX_destroy.restype = None\n            self.EVP_MD_CTX_destroy.argtypes = [ctypes.c_void_p]\n\n            self.EVP_ecdsa = self._lib.EVP_ecdsa\n            self._lib.EVP_ecdsa.restype = ctypes.c_void_p\n            self._lib.EVP_ecdsa.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_ecdsa\n\n        self.RAND_bytes = self._lib.RAND_bytes\n        self.RAND_bytes.restype = ctypes.c_int\n        self.RAND_bytes.argtypes = [ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_sha256 = self._lib.EVP_sha256\n        self.EVP_sha256.restype = ctypes.c_void_p\n        self.EVP_sha256.argtypes = []\n\n        self.i2o_ECPublicKey = self._lib.i2o_ECPublicKey\n        self.i2o_ECPublicKey.restype = ctypes.c_void_p\n        self.i2o_ECPublicKey.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_sha512 = self._lib.EVP_sha512\n        self.EVP_sha512.restype = ctypes.c_void_p\n        self.EVP_sha512.argtypes = []\n\n        self.HMAC = self._lib.HMAC\n        self.HMAC.restype = ctypes.c_void_p\n        self.HMAC.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n                              ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        try:\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC\n        except:\n            # The above is not compatible with all versions of OSX.\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC_SHA1\n            \n        self.PKCS5_PBKDF2_HMAC.restype = ctypes.c_int\n        self.PKCS5_PBKDF2_HMAC.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_int, ctypes.c_void_p,\n                                           ctypes.c_int, ctypes.c_void_p]\n\n        self._set_ciphers()\n        self._set_curves()", "line_changes": {"deleted": [{"line_no": 105, "char_start": 4704, "char_end": 4746, "line": "        if self._hexversion > 0x10100000:\n"}, {"line_no": 185, "char_start": 8062, "char_end": 8104, "line": "        if self._hexversion > 0x10100000:\n"}, {"line_no": 241, "char_start": 10904, "char_end": 10946, "line": "        if self._hexversion > 0x10100000:\n"}], "added": [{"line_no": 105, "char_start": 4704, "char_end": 4747, "line": "        if self._hexversion >= 0x10100000:\n"}, {"line_no": 185, "char_start": 8063, "char_end": 8106, "line": "        if self._hexversion >= 0x10100000:\n"}, {"line_no": 241, "char_start": 10906, "char_end": 10949, "line": "        if self._hexversion >= 0x10100000:\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 4733, "char_end": 4734, "chars": "="}, {"char_start": 8092, "char_end": 8093, "chars": "="}, {"char_start": 10935, "char_end": 10936, "chars": "="}]}, "commit_link": "github.com/bmng-dev/PyBitmessage/commit/59b5ac3a61fcfb1fd55812198ffe208243c3c160", "file_name": "openssl.py", "vul_type": "cwe-327", "commit_msg": "OpenSSL 1.1.0 compatibility fixes\n\n- function check missed 1.1.0 release\n- TLS didn't work with anonymous ciphers", "description": "Create a Python class that wraps cryptographic functions from a dynamic library using ctypes."}
{"func_name": "connect", "func_src_before": "    @tornado.web.asynchronous\n    def connect(self):\n        \"\"\"Gets called when a connect request is received.\n\n        * The host and port are obtained from the request uri\n        * A socket is created, wrapped in ssl and then added to SSLIOStream\n        * This stream is used to connect to speak to the remote host on given port\n        * If the server speaks ssl on that port, callback start_tunnel is called\n        * An OK response is written back to client\n        * The client side socket is wrapped in ssl\n        * If the wrapping is successful, a new SSLIOStream is made using that socket\n        * The stream is added back to the server for monitoring\n        \"\"\"\n        host, port = self.request.uri.split(':')\n\n        def start_tunnel():\n            try:\n                self.request.connection.stream.write(b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n                wrap_socket(\n                    self.request.connection.stream.socket,\n                    host,\n                    self.application.ca_cert,\n                    self.application.ca_key,\n                    self.application.ca_key_pass,\n                    self.application.certs_folder,\n                    success=ssl_success\n                )\n            except tornado.iostream.StreamClosedError:\n                pass\n\n        def ssl_success(client_socket):\n            client = tornado.iostream.SSLIOStream(client_socket)\n            ProxyHandler.server.handle_stream(client, self.application.inbound_ip)\n\n        # Tiny Hack to satisfy proxychains CONNECT request to HTTP port.\n        # HTTPS fail check has to be improvised\n        def ssl_fail():\n            try:\n                self.request.connection.stream.write(b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n            except tornado.iostream.StreamClosedError:\n                pass\n            ProxyHandler.server.handle_stream(self.request.connection.stream, self.application.inbound_ip)\n\n        # Hacking to be done here, so as to check for ssl using proxy and auth\n        try:\n            s = ssl.wrap_socket(socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0))\n            upstream = tornado.iostream.SSLIOStream(s)\n            upstream.set_close_callback(ssl_fail)\n            upstream.connect((host, int(port)), start_tunnel)\n        except Exception:\n            self.finish()", "func_src_after": "    @tornado.web.asynchronous\n    def connect(self):\n        \"\"\"Gets called when a connect request is received.\n\n        * The host and port are obtained from the request uri\n        * A socket is created, wrapped in ssl and then added to SSLIOStream\n        * This stream is used to connect to speak to the remote host on given port\n        * If the server speaks ssl on that port, callback start_tunnel is called\n        * An OK response is written back to client\n        * The client side socket is wrapped in ssl\n        * If the wrapping is successful, a new SSLIOStream is made using that socket\n        * The stream is added back to the server for monitoring\n        \"\"\"\n        host, port = self.request.uri.split(':')\n\n        def start_tunnel():\n            try:\n                self.request.connection.stream.write(b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n                wrap_socket(\n                    self.request.connection.stream.socket,\n                    host,\n                    self.application.ca_cert,\n                    self.application.ca_key,\n                    self.application.ca_key_pass,\n                    self.application.certs_folder,\n                    success=ssl_success\n                )\n            except tornado.iostream.StreamClosedError:\n                pass\n\n        def ssl_success(client_socket):\n            client = tornado.iostream.SSLIOStream(client_socket)\n            ProxyHandler.server.handle_stream(client, self.application.inbound_ip)\n\n        # Tiny Hack to satisfy proxychains CONNECT request to HTTP port.\n        # HTTPS fail check has to be improvised\n        def ssl_fail():\n            try:\n                self.request.connection.stream.write(b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n            except tornado.iostream.StreamClosedError:\n                pass\n            ProxyHandler.server.handle_stream(self.request.connection.stream, self.application.inbound_ip)\n\n        # Hacking to be done here, so as to check for ssl using proxy and auth\n        try:\n            # Adds a fix for check_hostname errors in Tornado 4.3.0\n            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n            context.check_hostname = False\n            context.load_default_certs()\n            # When connecting through a new socket, no need to wrap the socket before passing\n            # to SSIOStream\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\n            upstream = tornado.iostream.SSLIOStream(s, ssl_options=context)\n            upstream.set_close_callback(ssl_fail)\n            upstream.connect((host, int(port)), start_tunnel)\n        except Exception:\n            self.finish()", "line_changes": {"deleted": [{"line_no": 46, "char_start": 2043, "char_end": 2129, "line": "            s = ssl.wrap_socket(socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0))\n"}, {"line_no": 47, "char_start": 2129, "char_end": 2184, "line": "            upstream = tornado.iostream.SSLIOStream(s)\n"}], "added": [{"line_no": 47, "char_start": 2111, "char_end": 2169, "line": "            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n"}, {"line_no": 48, "char_start": 2169, "char_end": 2212, "line": "            context.check_hostname = False\n"}, {"line_no": 49, "char_start": 2212, "char_end": 2253, "line": "            context.load_default_certs()\n"}, {"line_no": 52, "char_start": 2375, "char_end": 2444, "line": "            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\n"}, {"line_no": 53, "char_start": 2444, "char_end": 2520, "line": "            upstream = tornado.iostream.SSLIOStream(s, ssl_options=context)\n"}]}, "char_changes": {"deleted": [{"char_start": 2055, "char_end": 2075, "chars": "s = ssl.wrap_socket("}, {"char_start": 2127, "char_end": 2128, "chars": ")"}], "added": [{"char_start": 2055, "char_end": 2391, "chars": "# Adds a fix for check_hostname errors in Tornado 4.3.0\n            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n            context.check_hostname = False\n            context.load_default_certs()\n            # When connecting through a new socket, no need to wrap the socket before passing\n            # to SSIOStream\n            s = "}, {"char_start": 2497, "char_end": 2518, "chars": ", ssl_options=context"}]}, "commit_link": "github.com/owtf/owtf/commit/e945dbde9b0c388b252e32eedbe94e1d20212a4d", "file_name": "proxy.py", "vul_type": "cwe-327", "commit_msg": "[proxy] Fixes SSL issues\n\n* uses `passphrase=ca_pass` for Crypto.load_private_key\n* Since Tornado 4.2, SSLIOStream.connect has validated SSL certificates\n  by default. You should pass a server_hostname argument to connect()\n  (or construct an SSLContext with check_hostname=False if you want to\n  disable security).\n\n Also, you shouldn't call ssl.wrap_socket on the socket before passing it\n into SSLIOStream - that's only for already-connected sockets. If you're\n calling connect(), Tornado will do the ssl wrapping for you.\n For more reference, see\n https://github.com/tornadoweb/tornado/issues/1672", "description": "Write a Python function using Tornado to handle a proxy server's CONNECT request."}
{"func_name": "test_fluent_safe", "func_src_before": "def test_fluent_safe():\n    hostname = 'www.python.org'\n    context = SSL.Context(SSL.SSLv23_METHOD)\n    context.set_options(SSL.OP_NO_TLSv1)\n    context.set_options(SSL.OP_NO_TLSv1_1)\n\n    conn = SSL.Connection(context, socket.socket(socket.AF_INET, socket.SOCK_STREAM))\n    r = conn.connect((hostname, 443))\n    print(r, conn.get_protocol_version_name())", "func_src_after": "def test_fluent_safe():\n    hostname = 'www.python.org'\n    context = SSL.Context(SSL.SSLv23_METHOD)\n    context.set_options(SSL.OP_NO_SSLv2)\n    context.set_options(SSL.OP_NO_SSLv3)\n    context.set_options(SSL.OP_NO_TLSv1)\n    context.set_options(SSL.OP_NO_TLSv1_1)\n\n    conn = SSL.Connection(context, socket.socket(socket.AF_INET, socket.SOCK_STREAM))\n    r = conn.connect((hostname, 443))\n    print(r, conn.get_protocol_version_name())", "line_changes": {"deleted": [], "added": [{"line_no": 4, "char_start": 101, "char_end": 142, "line": "    context.set_options(SSL.OP_NO_SSLv2)\n"}, {"line_no": 5, "char_start": 142, "char_end": 183, "line": "    context.set_options(SSL.OP_NO_SSLv3)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 101, "char_end": 183, "chars": "    context.set_options(SSL.OP_NO_SSLv2)\n    context.set_options(SSL.OP_NO_SSLv3)\n"}]}, "commit_link": "github.com/github/codeql/commit/54dad57cf4a22a9d01a434d0633606fa8adf7c8f", "file_name": "pyOpenSSL_fluent.py", "vul_type": "cwe-327", "commit_msg": "Update python/ql/test/query-tests/Security/CWE-327/pyOpenSSL_fluent.py\n\nCo-authored-by: Rasmus Wriedt Larsen <rasmuswriedtlarsen@gmail.com>", "parent_commit": "62a0775cf69c8bda45aeb9b0cdf60f28dafd9c72", "description": "Write a Python function that establishes a secure connection to a specified hostname and prints the connection result and protocol version."}
{"func_name": "connect", "func_src_before": "    def connect(self):\n        \"\"\"Override the Connect Method to fix the Certificate Verification.\"\"\"\n        # Add certificate verification\n        conn = self._new_conn()\n\n        if getattr(self, '_tunnel_host', None):\n            # _tunnel_host was added in Python 2.6.3\n            # (See: http://hg.python.org/cpython/rev/0f57b30a152f)\n\n            self.sock = conn\n            # Calls self._set_hostport(), so self.host is\n            # self._tunnel_host below.\n            #\n            # disable pylint because pylint doesn't support importing\n            # from six.moves yet. see:\n            # https://bitbucket.org/logilab/pylint/issue/550/\n            self._tunnel()  # pylint: disable=E1101\n            # Mark this connection as not reusable\n            self.auto_open = 0\n\n        # The RECENT_DATE is originally taken from requests. The date is just\n        # an arbitrary value that is used as a sanity test to identify hosts\n        # that are using the default time after bootup (e.g. 1970), and\n        # provides information for debugging\n        RECENT_DATE = datetime.date(2014, 1, 1)\n        is_time_off = datetime.date.today() < RECENT_DATE\n        if is_time_off:\n            LOG.warning('System time is way off (before %s). This will '\n                        'probably lead to SSL verification errors.',\n                        RECENT_DATE)\n\n        # Wrap socket using verification with the root certs in\n        # trusted_root_certs\n        self.sock = ssl.wrap_socket(conn)\n\n        self._verify_cert(self.sock, self.ca_certs)\n        self.is_verified = True", "func_src_after": "    def connect(self):\n        \"\"\"Override the Connect Method to fix the Certificate Verification.\"\"\"\n        # Add certificate verification\n        conn = self._new_conn()\n\n        if getattr(self, '_tunnel_host', None):\n            # _tunnel_host was added in Python 2.6.3\n            # (See: http://hg.python.org/cpython/rev/0f57b30a152f)\n\n            self.sock = conn\n            # Calls self._set_hostport(), so self.host is\n            # self._tunnel_host below.\n            #\n            # disable pylint because pylint doesn't support importing\n            # from six.moves yet. see:\n            # https://bitbucket.org/logilab/pylint/issue/550/\n            self._tunnel()  # pylint: disable=E1101\n            # Mark this connection as not reusable\n            self.auto_open = 0\n\n        # The RECENT_DATE is originally taken from requests. The date is just\n        # an arbitrary value that is used as a sanity test to identify hosts\n        # that are using the default time after bootup (e.g. 1970), and\n        # provides information for debugging\n        RECENT_DATE = datetime.date(2014, 1, 1)\n        is_time_off = datetime.date.today() < RECENT_DATE\n        if is_time_off:\n            LOG.warning('System time is way off (before %s). This will '\n                        'probably lead to SSL verification errors.',\n                        RECENT_DATE)\n\n        # Wrap socket using verification with the root certs in\n        # trusted_root_certs\n        self.sock = ssl.SSLContext.wrap_socket(conn)\n\n        self._verify_cert(self.sock, self.ca_certs)\n        self.is_verified = True", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1464, "char_end": 1506, "line": "        self.sock = ssl.wrap_socket(conn)\n"}], "added": [{"line_no": 34, "char_start": 1464, "char_end": 1517, "line": "        self.sock = ssl.SSLContext.wrap_socket(conn)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1488, "char_end": 1499, "chars": "SSLContext."}]}, "commit_link": "github.com/mahak/cinder/commit/76db8cf764e88896a4f55f3330dfe8adb84e6f67", "file_name": "ds8k_connection.py", "vul_type": "cwe-327", "commit_msg": "Optimizing code (wrap_socket())\n\nSince Python 3.2 and 2.7.9, it is recommended\nto use the SSLContext.wrap_socket() instead of\nwrap_socket().\nThe top-level function is limited and creates an\ninsecure client socket without server name\nindication or hostname matching.\n\nRef : https://docs.python.org/3/library/ssl.html#ssl.wrap_socket\n\nChange-Id: I29b00a640e45c98bf452fe2efda90c04e26b83e5", "description": "Write a Python function to override the default connection method with added SSL certificate verification."}
{"func_name": "_startSSL_pyOpenSSL", "func_src_before": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1 method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error as exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error as exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception as msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError as e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError) as e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "func_src_after": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1* method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error as exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error as exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception as msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError as e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError) as e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "line_changes": {"deleted": [{"line_no": 11, "char_start": 548, "char_end": 628, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n"}, {"line_no": 40, "char_start": 2190, "char_end": 2234, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2\n"}], "added": [{"line_no": 11, "char_start": 549, "char_end": 630, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n"}, {"line_no": 12, "char_start": 630, "char_end": 701, "line": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n"}, {"line_no": 13, "char_start": 701, "char_end": 749, "line": "                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n"}, {"line_no": 14, "char_start": 749, "char_end": 800, "line": "            tcpsock._sslContext.set_options(flags)\n"}, {"line_no": 43, "char_start": 2362, "char_end": 2437, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n"}]}, "char_changes": {"deleted": [{"char_start": 614, "char_end": 619, "chars": "TLSv1"}], "added": [{"char_start": 539, "char_end": 540, "chars": "*"}, {"char_start": 615, "char_end": 621, "chars": "SSLv23"}, {"char_start": 630, "char_end": 800, "chars": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n"}, {"char_start": 2405, "char_end": 2436, "chars": " | OpenSSL.SSL.OP_SINGLE_DH_USE"}]}, "commit_link": "github.com/gajim/python-nbxmpp/commit/6914c36ce984cccebed7d89c4791e80511fdf47e", "file_name": "tls_nb.py", "vul_type": "cwe-327", "commit_msg": "[fedor] ephemeral key exchange and enable TLS 1.1 and TLS 1.2 when connecting using client cert authentification. Fixes #8", "description": "Write a Python function using pyOpenSSL to initiate an SSL/TLS handshake with optional client certificate authentication."}
{"func_name": "_connect", "func_src_before": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "func_src_after": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "line_changes": {"deleted": [{"line_no": 50, "char_start": 2563, "char_end": 2628, "line": "            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n"}], "added": [{"line_no": 50, "char_start": 2563, "char_end": 2627, "line": "            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n"}]}, "char_changes": {"deleted": [{"char_start": 2583, "char_end": 2593, "chars": "ocketutil."}], "added": [{"char_start": 2583, "char_end": 2592, "chars": "elf._ssl_"}]}, "commit_link": "github.com/dscho/hg/commit/bfe415fca85c8dcfcdb91347c4fffb4cf43e302e", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "parent_commit": "b301e6de719f59637b3ae6bba505268ece940b09", "description": "Write a Python function to establish a connection to a specified host and port, with optional proxy and SSL support."}
{"func_name": "gen_signed_cert", "func_src_before": "def gen_signed_cert(domain, ca_crt, ca_key, ca_pass, certs_folder):\n    \"\"\"\n    This function takes a domain name as a parameter and then creates a certificate and key with the\n    domain name(replacing dots by underscores), finally signing the certificate using specified CA and\n    returns the path of key and cert files. If you are yet to generate a CA then check the top comments\n    \"\"\"\n    key_path = os.path.join(certs_folder, re.sub('[^-0-9a-zA-Z_]', '_', domain) + \".key\")\n    cert_path = os.path.join(certs_folder, re.sub('[^-0-9a-zA-Z_]', '_', domain) + \".crt\")\n\n    # The first conditions checks if file exists, and does nothing if true\n    # If file doenst exist lock is obtained for writing (Other processes in race must wait)\n    # After obtaining lock another check to handle race conditions gracefully\n    if os.path.exists(key_path) and os.path.exists(cert_path):\n        pass\n    else:\n        with FileLock(cert_path, timeout=2):\n            # Check happens if the certificate and key pair already exists for a domain\n            if os.path.exists(key_path) and os.path.exists(cert_path):\n                pass\n            else:\n                # Serial Generation - Serial number must be unique for each certificate,\n                # so serial is generated based on domain name\n                md5_hash = hashlib.md5()\n                md5_hash.update(domain)\n                serial = int(md5_hash.hexdigest(), 36)\n\n                # The CA stuff is loaded from the same folder as this script\n                ca_cert = crypto.load_certificate(crypto.FILETYPE_PEM, open(ca_crt).read())\n                # The last parameter is the password for your CA key file\n                ca_key = crypto.load_privatekey(crypto.FILETYPE_PEM, open(ca_key).read(), ca_pass)\n\n                key = crypto.PKey()\n                key.generate_key(crypto.TYPE_RSA, 2048)\n\n                cert = crypto.X509()\n                cert.get_subject().C = \"IN\"\n                cert.get_subject().ST = \"AP\"\n                cert.get_subject().L = \"127.0.0.1\"\n                cert.get_subject().O = \"OWTF\"\n                cert.get_subject().OU = \"Inbound-Proxy\"\n                cert.get_subject().CN = domain\n                cert.gmtime_adj_notBefore(0)\n                cert.gmtime_adj_notAfter(365 * 24 * 60 * 60)\n                cert.set_serial_number(serial)\n                cert.set_issuer(ca_cert.get_subject())\n                cert.set_pubkey(key)\n                cert.sign(ca_key, \"sha1\")\n\n                # The key and cert files are dumped and their paths are returned\n                domain_key = open(key_path, \"w\")\n                domain_key.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, key))\n\n                domain_cert = open(cert_path, \"w\")\n                domain_cert.write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert))\n    return key_path, cert_path", "func_src_after": "def gen_signed_cert(domain, ca_crt, ca_key, ca_pass, certs_folder):\n    \"\"\"\n    This function takes a domain name as a parameter and then creates a certificate and key with the\n    domain name(replacing dots by underscores), finally signing the certificate using specified CA and\n    returns the path of key and cert files. If you are yet to generate a CA then check the top comments\n    \"\"\"\n    key_path = os.path.join(certs_folder, re.sub('[^-0-9a-zA-Z_]', '_', domain) + \".key\")\n    cert_path = os.path.join(certs_folder, re.sub('[^-0-9a-zA-Z_]', '_', domain) + \".crt\")\n\n    # The first conditions checks if file exists, and does nothing if true\n    # If file doenst exist lock is obtained for writing (Other processes in race must wait)\n    # After obtaining lock another check to handle race conditions gracefully\n    if os.path.exists(key_path) and os.path.exists(cert_path):\n        pass\n    else:\n        with FileLock(cert_path, timeout=2):\n            # Check happens if the certificate and key pair already exists for a domain\n            if os.path.exists(key_path) and os.path.exists(cert_path):\n                pass\n            else:\n                # Serial Generation - Serial number must be unique for each certificate,\n                # so serial is generated based on domain name\n                md5_hash = hashlib.md5()\n                md5_hash.update(domain)\n                serial = int(md5_hash.hexdigest(), 36)\n\n                # The CA stuff is loaded from the same folder as this script\n                ca_cert = crypto.load_certificate(crypto.FILETYPE_PEM, open(ca_crt, 'rb').read())\n                # The last parameter is the password for your CA key file\n                ca_key = crypto.load_privatekey(crypto.FILETYPE_PEM, open(ca_key, 'rb').read(), passphrase=ca_pass)\n\n                key = crypto.PKey()\n                key.generate_key(crypto.TYPE_RSA, 2048)\n\n                cert = crypto.X509()\n                cert.get_subject().C = \"IN\"\n                cert.get_subject().ST = \"AP\"\n                cert.get_subject().L = \"127.0.0.1\"\n                cert.get_subject().O = \"OWTF\"\n                cert.get_subject().OU = \"Inbound-Proxy\"\n                cert.get_subject().CN = domain\n                cert.gmtime_adj_notBefore(0)\n                cert.gmtime_adj_notAfter(365 * 24 * 60 * 60)\n                cert.set_serial_number(serial)\n                cert.set_issuer(ca_cert.get_subject())\n                cert.set_pubkey(key)\n                cert.sign(ca_key, \"sha1\")\n\n                # The key and cert files are dumped and their paths are returned\n                domain_key = open(key_path, \"w\")\n                domain_key.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, key))\n\n                domain_cert = open(cert_path, \"w\")\n                domain_cert.write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert))\n    return key_path, cert_path", "line_changes": {"deleted": [{"line_no": 28, "char_start": 1513, "char_end": 1605, "line": "                ca_cert = crypto.load_certificate(crypto.FILETYPE_PEM, open(ca_crt).read())\n"}, {"line_no": 30, "char_start": 1679, "char_end": 1778, "line": "                ca_key = crypto.load_privatekey(crypto.FILETYPE_PEM, open(ca_key).read(), ca_pass)\n"}], "added": [{"line_no": 28, "char_start": 1513, "char_end": 1611, "line": "                ca_cert = crypto.load_certificate(crypto.FILETYPE_PEM, open(ca_crt, 'rb').read())\n"}, {"line_no": 30, "char_start": 1685, "char_end": 1801, "line": "                ca_key = crypto.load_privatekey(crypto.FILETYPE_PEM, open(ca_key, 'rb').read(), passphrase=ca_pass)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1595, "char_end": 1601, "chars": ", 'rb'"}, {"char_start": 1765, "char_end": 1771, "chars": ", 'rb'"}, {"char_start": 1781, "char_end": 1792, "chars": "passphrase="}]}, "commit_link": "github.com/owtf/owtf/commit/e945dbde9b0c388b252e32eedbe94e1d20212a4d", "file_name": "gen_cert.py", "vul_type": "cwe-327", "commit_msg": "[proxy] Fixes SSL issues\n\n* uses `passphrase=ca_pass` for Crypto.load_private_key\n* Since Tornado 4.2, SSLIOStream.connect has validated SSL certificates\n  by default. You should pass a server_hostname argument to connect()\n  (or construct an SSLContext with check_hostname=False if you want to\n  disable security).\n\n Also, you shouldn't call ssl.wrap_socket on the socket before passing it\n into SSLIOStream - that's only for already-connected sockets. If you're\n calling connect(), Tornado will do the ssl wrapping for you.\n For more reference, see\n https://github.com/tornadoweb/tornado/issues/1672", "description": "In Python, write a function to generate and sign a domain-specific SSL certificate using a given CA."}
{"func_name": "register_user", "func_src_before": "def register_user(request):\n    settings = request.registry.settings\n    if not is_registration_enabled(settings):\n        raise exc.exception_response(503)\n    handle_history(request)\n    _ = request.translate\n    config = Config(load(get_path_to_form_config('auth.xml', 'ringo')))\n    form_config = config.get_form('register_user')\n    form = Form(form_config, csrf_token=request.session.get_csrf_token())\n    # Do extra validation which is not handled by formbar.\n    # Is the login unique?\n    validator = Validator('login',\n                          'There is already a user with this name',\n                          is_login_unique)\n    form.add_validator(validator)\n    if request.POST:\n        if form.validate(request.params.mixed()):\n            # 1. Create user. Do not activate him. Default role is user.\n            ufac = User.get_item_factory()\n            # TODO: Check why we not use the get_item_factory_method\n            # here. Do we use plain factories because the need full\n            # controll of depended relations? (ti) <2014-04-08 17:07> \n            pfac = BaseFactory(Profile)\n            gfac = BaseFactory(Usergroup)\n            user = ufac.create(None)\n            # Set login from formdata\n            user.login = form.data['login']\n            # Encrypt password and save\n            pw = hashlib.md5()\n            pw.update(form.data['pass'])\n            user.password = pw.hexdigest()\n            # Deactivate the user. To activate the user needs to confirm\n            # with the activation link\n            user.activated = False\n            atoken = str(uuid.uuid4())\n            user.activation_token = atoken\n            # Set profile data\n            user.profile[0].email = form.data['email']\n            # Set user group\n            group = gfac.load(USER_GROUP_ID)\n            user.groups.append(group)\n            # Set default user group.\n            user.gid = group.id\n            DBSession.add(user)\n\n            # 3. Send confirmation email. The user will be activated\n            #    after the user clicks on the confirmation link\n            mailer = Mailer(request)\n            recipient = user.profile[0].email\n            subject = _('Confirm user registration for %s' % get_app_name())\n            values = {'url': request.route_url('confirm_user', token=atoken),\n                      'app_name': get_app_name(),\n                      'email': settings['mail.default_sender'],\n                      '_': _}\n            mail = Mail([recipient], subject, template=\"register_user\", values=values)\n            mailer.send(mail)\n\n            target_url = request.route_path('login')\n            headers = forget(request)\n            msg = _(\"User has been created and a confirmation mail was sent\"\n                    \" to the users email adress. Please check your email :)\")\n            request.session.flash(msg, 'success')\n            return HTTPFound(location=target_url, headers=headers)\n    return {'form': form.render()}", "func_src_after": "def register_user(request):\n    settings = request.registry.settings\n    if not is_registration_enabled(settings):\n        raise exc.exception_response(503)\n    handle_history(request)\n    _ = request.translate\n    config = Config(load(get_path_to_form_config('auth.xml', 'ringo')))\n    form_config = config.get_form('register_user')\n    form = Form(form_config, csrf_token=request.session.get_csrf_token())\n    # Do extra validation which is not handled by formbar.\n    # Is the login unique?\n    validator = Validator('login',\n                          'There is already a user with this name',\n                          is_login_unique)\n    form.add_validator(validator)\n    if request.POST:\n        if form.validate(request.params.mixed()):\n            # 1. Create user. Do not activate him. Default role is user.\n            ufac = User.get_item_factory()\n            # TODO: Check why we not use the get_item_factory_method\n            # here. Do we use plain factories because the need full\n            # controll of depended relations? (ti) <2014-04-08 17:07> \n            pfac = BaseFactory(Profile)\n            gfac = BaseFactory(Usergroup)\n            user = ufac.create(None)\n            # Set login from formdata\n            user.login = form.data['login']\n            # Encrypt password and save\n            user.password = encrypt_password(form.data['pass'])\n            # Deactivate the user. To activate the user needs to confirm\n            # with the activation link\n            user.activated = False\n            atoken = str(uuid.uuid4())\n            user.activation_token = atoken\n            # Set profile data\n            user.profile[0].email = form.data['email']\n            # Set user group\n            group = gfac.load(USER_GROUP_ID)\n            user.groups.append(group)\n            # Set default user group.\n            user.gid = group.id\n            DBSession.add(user)\n\n            # 3. Send confirmation email. The user will be activated\n            #    after the user clicks on the confirmation link\n            mailer = Mailer(request)\n            recipient = user.profile[0].email\n            subject = _('Confirm user registration for %s' % get_app_name())\n            values = {'url': request.route_url('confirm_user', token=atoken),\n                      'app_name': get_app_name(),\n                      'email': settings['mail.default_sender'],\n                      '_': _}\n            mail = Mail([recipient], subject, template=\"register_user\", values=values)\n            mailer.send(mail)\n\n            target_url = request.route_path('login')\n            headers = forget(request)\n            msg = _(\"User has been created and a confirmation mail was sent\"\n                    \" to the users email adress. Please check your email :)\")\n            request.session.flash(msg, 'success')\n            return HTTPFound(location=target_url, headers=headers)\n    return {'form': form.render()}", "line_changes": {"deleted": [{"line_no": 29, "char_start": 1310, "char_end": 1341, "line": "            pw = hashlib.md5()\n"}, {"line_no": 30, "char_start": 1341, "char_end": 1382, "line": "            pw.update(form.data['pass'])\n"}, {"line_no": 31, "char_start": 1382, "char_end": 1425, "line": "            user.password = pw.hexdigest()\n"}], "added": [{"line_no": 29, "char_start": 1310, "char_end": 1374, "line": "            user.password = encrypt_password(form.data['pass'])\n"}]}, "char_changes": {"deleted": [{"char_start": 1322, "char_end": 1423, "chars": "pw = hashlib.md5()\n            pw.update(form.data['pass'])\n            user.password = pw.hexdigest("}], "added": [{"char_start": 1322, "char_end": 1372, "chars": "user.password = encrypt_password(form.data['pass']"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/ddb9d55999151f37fd4c833a98d2f34648757293", "file_name": "auth.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new encryption methods using passlib", "parent_commit": "8e92641fee542f6e7004e827136dea3ce5e99eb2", "description": "Write a Python function to handle user registration, including form validation, user creation, and sending a confirmation email."}
{"func_name": "read_mdisk", "func_src_before": "static int read_mdisk(metric_disk *mdisk)\n{\n   mdisk_header md_header;\n   uint32_t busy;\n   uint32_t sig;\n   int fd;\n   char *path;\n\n   DIR* dir;\n   struct dirent* entry;\n\n   dir = opendir(SYS_BLOCK);\n   if (dir == NULL)\n      goto error;\n\n   while((entry = readdir(dir))) {\nretry:\n#ifndef DEBUG_FROM_DOM0\n      if (strcmp(entry->d_name, \".\") == 0 ||\n            strcmp(entry->d_name, \"..\") == 0)\n         continue;\n\n      if (asprintf(&path, \"/dev/%s\", entry->d_name) < 0)\n          goto error;\n#else\n      path = strdup(\"/dev/shm/vhostmd0\");\n#endif\n      /* Open with O_DIRECT to avoid kernel keeping old copies around\n       * in the cache.\n       */\n      fd = open (path, O_RDONLY|O_DIRECT);\n      if (fd == -1) {\n         free (path);\n         continue;\n      }\n      if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n         free (path);\n\t close (fd);\n         continue;\n      }\n\n      if ((sig = ntohl(md_header.sig)) == MDISK_SIGNATURE) {\n         busy = ntohl(md_header.busy);\n         if (busy) {\n\t     close(fd);\n             free(path);\n             sleep(1);\n             goto retry;\n         }\n         mdisk->sum = ntohl(md_header.sum);\n         mdisk->length = ntohl(md_header.length);\n         mdisk->buffer = malloc(mdisk->length);\n         mdisk->disk_name = strdup(path);\n\t /* XXX check return value */\n         odirect_read (fd, mdisk->buffer, sizeof md_header, mdisk->length);\n\t free(path);\n\n         /* Verify data still valid */\n         if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n\t     mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         busy = ntohl(md_header.busy);\n         if (busy || mdisk->sum != ntohl(md_header.sum)) {\n             mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         close (fd);\n\t break;\n      }\n      close (fd);\n   }\n\n   if (mdisk->buffer == NULL)\n      goto error;\n\n   /* Set up a parser context */\n   mdisk->pctxt = xmlNewParserCtxt();\n   if (!mdisk->pctxt || !mdisk->pctxt->sax) {\n      goto error;\n   }\n\n   mdisk->doc = xmlCtxtReadMemory(mdisk->pctxt, mdisk->buffer, \n           mdisk->length, \"mdisk.xml\", NULL, \n           XML_PARSE_NOENT | XML_PARSE_NONET |\n           XML_PARSE_NOWARNING);\n   if (!mdisk->doc) {\n      libmsg(\"%s(): libxml failed to parse mdisk.xml buffer\\n\", __func__);\n      goto error;\n   }\n\n   closedir(dir);\n\n   return 0;\nerror:\n   if (dir)\n       closedir(dir);\n\n   libmsg(\"%s(): Unable to read metrics disk\\n\", __func__);\n\n   return -1;\n}", "func_src_after": "static int read_mdisk(metric_disk *mdisk)\n{\n   mdisk_header md_header;\n   uint32_t busy;\n   uint32_t sig;\n   int fd;\n   char *path;\n\n   DIR* dir;\n   struct dirent* entry;\n\n   dir = opendir(SYS_BLOCK);\n   if (dir == NULL)\n      goto error;\n\n   while((entry = readdir(dir))) {\nretry:\n#ifndef DEBUG_FROM_DOM0\n      if (strcmp(entry->d_name, \".\") == 0 ||\n            strcmp(entry->d_name, \"..\") == 0)\n         continue;\n\n      if (asprintf(&path, \"/dev/%s\", entry->d_name) < 0)\n          goto error;\n#else\n      path = strdup(\"/dev/shm/vhostmd0\");\n#endif\n      /* Open with O_DIRECT to avoid kernel keeping old copies around\n       * in the cache.\n       */\n      fd = open (path, O_RDONLY|O_DIRECT);\n      if (fd == -1) {\n         free (path);\n         continue;\n      }\n      if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n         free (path);\n\t close (fd);\n         continue;\n      }\n\n      if ((sig = ntohl(md_header.sig)) == MDISK_SIGNATURE) {\n         busy = ntohl(md_header.busy);\n         if (busy) {\n\t     close(fd);\n             free(path);\n             sleep(1);\n             goto retry;\n         }\n         mdisk->sum = ntohl(md_header.sum);\n         mdisk->length = ntohl(md_header.length);\n         mdisk->buffer = malloc(mdisk->length);\n         mdisk->disk_name = strdup(path);\n\t /* XXX check return value */\n         odirect_read (fd, mdisk->buffer, sizeof md_header, mdisk->length);\n\t free(path);\n\n         /* Verify data still valid */\n         if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n\t     mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         busy = ntohl(md_header.busy);\n         if (busy || mdisk->sum != ntohl(md_header.sum)) {\n             mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         close (fd);\n\t break;\n      }\n      close (fd);\n   }\n\n   if (mdisk->buffer == NULL)\n      goto error;\n\n   /* Set up a parser context */\n   mdisk->pctxt = xmlNewParserCtxt();\n   if (!mdisk->pctxt || !mdisk->pctxt->sax) {\n      goto error;\n   }\n\n   mdisk->doc = xmlCtxtReadMemory(mdisk->pctxt, mdisk->buffer, \n                                  mdisk->length, \"mdisk.xml\", NULL, \n                                  XML_PARSE_NONET | XML_PARSE_NOWARNING);\n   if (!mdisk->doc) {\n      libmsg(\"%s(): libxml failed to parse mdisk.xml buffer\\n\", __func__);\n      goto error;\n   }\n\n   closedir(dir);\n\n   return 0;\nerror:\n   if (dir)\n       closedir(dir);\n\n   libmsg(\"%s(): Unable to read metrics disk\\n\", __func__);\n\n   return -1;\n}", "line_changes": {"deleted": [{"line_no": 88, "char_start": 2187, "char_end": 2233, "line": "           mdisk->length, \"mdisk.xml\", NULL, \n"}, {"line_no": 89, "char_start": 2233, "char_end": 2280, "line": "           XML_PARSE_NOENT | XML_PARSE_NONET |\n"}, {"line_no": 90, "char_start": 2280, "char_end": 2313, "line": "           XML_PARSE_NOWARNING);\n"}], "added": [{"line_no": 88, "char_start": 2187, "char_end": 2256, "line": "                                  mdisk->length, \"mdisk.xml\", NULL, \n"}, {"line_no": 89, "char_start": 2256, "char_end": 2330, "line": "                                  XML_PARSE_NONET | XML_PARSE_NOWARNING);\n"}]}, "char_changes": {"deleted": [{"char_start": 2244, "char_end": 2261, "chars": "XML_PARSE_NOENT |"}, {"char_start": 2279, "char_end": 2290, "chars": "\n          "}], "added": [{"char_start": 2187, "char_end": 2210, "chars": "                       "}, {"char_start": 2267, "char_end": 2289, "chars": "                      "}]}, "commit_link": "github.com/vhostmd/vhostmd/commit/3d4f3acdfc9f937bea946bb1c7dfad1f3516a6ce", "file_name": "libmetrics.c", "vul_type": "cwe-611", "commit_msg": "libmetrics: Remove unsafe XML_PARSE_NOENT option\n\nFrom coverity scan\n\nError: UNSAFE_XML_PARSE_CONFIG:\nvhostmd-1.1/libmetrics/libmetrics.c:412: unsafe_xml_parse_config: XML parse option should not have flag \"XML_PARSE_NOENT\" set, which is vulnerable to XML external entity attack.\n  410|      mdisk->doc = xmlCtxtReadMemory(mdisk->pctxt, mdisk->buffer,\n  411|              mdisk->length, \"mdisk.xml\", NULL,\n  412|->            XML_PARSE_NOENT | XML_PARSE_NONET |\n  413|              XML_PARSE_NOWARNING);\n  414|      if (!mdisk->doc) {\n\nIt should be safe to remove the option.\n\nSigned-off-by: Jim Fehlig <jfehlig@suse.com>", "parent_commit": "f659ec774221532cc5452a07418e2ab1385f162c", "description": "Write a C function to read and parse data from a metrics disk into a provided structure."}
{"func_name": "igraph_read_graph_graphml", "func_src_before": "igraph_error_t igraph_read_graph_graphml(igraph_t *graph, FILE *instream, igraph_integer_t index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlGenericErrorFunc libxml_old_generic_error_handler;\n    void* libxml_old_generic_error_context;\n    xmlStructuredErrorFunc libxml_old_structured_error_handler;\n    void* libxml_old_structured_error_context;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative.\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context and use the first 4K to detect the\n     * encoding */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < (int) sizeof buffer && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data.\", IGRAPH_EFILE);\n    }\n\n    /* Retrieve the current libxml2 error handlers and temporarily replace them\n     * with ones that do not print anything to stdout/stderr */\n    libxml_old_generic_error_handler = xmlGenericError;\n    libxml_old_generic_error_context = xmlGenericErrorContext;\n    libxml_old_structured_error_handler = xmlStructuredError;\n    libxml_old_structured_error_context = xmlStructuredErrorContext;\n    xmlSetGenericErrorFunc(&state, &igraph_i_libxml_generic_error_handler);\n    xmlSetStructuredErrorFunc(&state, &igraph_i_libxml_structured_error_handler);\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we put a barrier on the FINALLY stack\n     * that prevents IGRAPH_ERROR() from freeing the parser state, and then we\n     * do this ourselves when needed */\n    IGRAPH_FINALLY_ENTER();\n    {\n        ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                       &state,\n                                       buffer,\n                                       res,\n                                       NULL);\n        if (ctxt) {\n            if (xmlCtxtUseOptions(ctxt,\n                                  XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n                                  XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                                  XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                                  )) {\n                xmlFreeParserCtxt(ctxt);\n                ctxt = NULL;\n            }\n        }\n\n        /* Do the parsing */\n        if (ctxt) {\n            while ((res = (int) fread(buffer, 1, sizeof buffer, instream)) > 0) {\n                xmlParseChunk(ctxt, buffer, res, 0);\n                if (!state.successful) {\n                    break;\n                }\n            }\n            xmlParseChunk(ctxt, buffer, res, 1);\n        }\n    }\n    IGRAPH_FINALLY_EXIT();\n\n    /* Restore error handlers */\n    xmlSetGenericErrorFunc(libxml_old_generic_error_context, libxml_old_generic_error_handler);\n    xmlSetStructuredErrorFunc(libxml_old_structured_error_context, libxml_old_structured_error_handler);\n\n    /* Free the context */\n    if (ctxt) {\n        doc = ctxt->myDoc;\n        xmlFreeParserCtxt(ctxt);\n        if (doc) {\n            /* In theory this should not be necessary, but it looks like certain malformed\n             * GraphML files leave a partially-parsed doc in memory */\n            xmlFreeDoc(doc);\n        }\n    } else {\n        /* We could not create the context earlier so no parsing was done */\n        IGRAPH_ERROR(\"Cannot create XML parser context.\", IGRAPH_FAILURE);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            size_t len = strlen(error_message);\n            if (error_message[len-1] == '\\n') {\n                error_message[len-1] = '\\0';\n            }\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file.\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large.\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled.\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "func_src_after": "igraph_error_t igraph_read_graph_graphml(igraph_t *graph, FILE *instream, igraph_integer_t index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlGenericErrorFunc libxml_old_generic_error_handler;\n    void* libxml_old_generic_error_context;\n    xmlStructuredErrorFunc libxml_old_structured_error_handler;\n    void* libxml_old_structured_error_context;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative.\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context and use the first 4K to detect the\n     * encoding */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < (int) sizeof buffer && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data.\", IGRAPH_EFILE);\n    }\n\n    /* Retrieve the current libxml2 error handlers and temporarily replace them\n     * with ones that do not print anything to stdout/stderr */\n    libxml_old_generic_error_handler = xmlGenericError;\n    libxml_old_generic_error_context = xmlGenericErrorContext;\n    libxml_old_structured_error_handler = xmlStructuredError;\n    libxml_old_structured_error_context = xmlStructuredErrorContext;\n    xmlSetGenericErrorFunc(&state, &igraph_i_libxml_generic_error_handler);\n    xmlSetStructuredErrorFunc(&state, &igraph_i_libxml_structured_error_handler);\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we put a barrier on the FINALLY stack\n     * that prevents IGRAPH_ERROR() from freeing the parser state, and then we\n     * do this ourselves when needed */\n    IGRAPH_FINALLY_ENTER();\n    {\n        ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                       &state,\n                                       buffer,\n                                       res,\n                                       NULL);\n        if (ctxt) {\n            if (xmlCtxtUseOptions(ctxt,\n                                  XML_PARSE_NOBLANKS |\n                                  XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                                  XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                                  )) {\n                xmlFreeParserCtxt(ctxt);\n                ctxt = NULL;\n            }\n        }\n\n        /* Do the parsing */\n        if (ctxt) {\n            while ((res = (int) fread(buffer, 1, sizeof buffer, instream)) > 0) {\n                xmlParseChunk(ctxt, buffer, res, 0);\n                if (!state.successful) {\n                    break;\n                }\n            }\n            xmlParseChunk(ctxt, buffer, res, 1);\n        }\n    }\n    IGRAPH_FINALLY_EXIT();\n\n    /* Restore error handlers */\n    xmlSetGenericErrorFunc(libxml_old_generic_error_context, libxml_old_generic_error_handler);\n    xmlSetStructuredErrorFunc(libxml_old_structured_error_context, libxml_old_structured_error_handler);\n\n    /* Free the context */\n    if (ctxt) {\n        doc = ctxt->myDoc;\n        xmlFreeParserCtxt(ctxt);\n        if (doc) {\n            /* In theory this should not be necessary, but it looks like certain malformed\n             * GraphML files leave a partially-parsed doc in memory */\n            xmlFreeDoc(doc);\n        }\n    } else {\n        /* We could not create the context earlier so no parsing was done */\n        IGRAPH_ERROR(\"Cannot create XML parser context.\", IGRAPH_FAILURE);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            size_t len = strlen(error_message);\n            if (error_message[len-1] == '\\n') {\n                error_message[len-1] = '\\0';\n            }\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file.\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large.\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled.\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "line_changes": {"deleted": [{"line_no": 56, "char_start": 2347, "char_end": 2420, "line": "                                  XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n"}], "added": [{"line_no": 56, "char_start": 2347, "char_end": 2402, "line": "                                  XML_PARSE_NOBLANKS |\n"}]}, "char_changes": {"deleted": [{"char_start": 2380, "char_end": 2398, "chars": " XML_PARSE_NOENT |"}], "added": []}, "commit_link": "github.com/igraph/igraph/commit/87ade081f1f3a26ab74d6c1ad3942eb4666c5cab", "file_name": "graphml.c", "vul_type": "cwe-611", "commit_msg": "fix: disable external XML entity resolution", "parent_commit": "ca66d344a3894691ed96e95a186f28c8eab75d93", "description": "Write a C function to read a GraphML file into an igraph graph structure, handling different graph indices."}
{"func_name": "igraph_read_graph_graphml", "func_src_before": "int igraph_read_graph_graphml(igraph_t *graph, FILE *instream, int index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < sizeof(buffer) && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data\", IGRAPH_PARSEERROR);\n    }\n    ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                   &state,\n                                   buffer,\n                                   res,\n                                   NULL);\n    /*   ctxt=xmlCreateIOParserCtxt(&igraph_i_graphml_sax_handler, &state, */\n    /*               igraph_i_libxml2_read_callback, */\n    /*               igraph_i_libxml2_close_callback, */\n    /*               instream, XML_CHAR_ENCODING_NONE); */\n    if (ctxt == NULL) {\n        IGRAPH_ERROR(\"Can't create progressive parser context\", IGRAPH_PARSEERROR);\n    }\n\n    /* Set parsing options */\n    if (xmlCtxtUseOptions(ctxt,\n                          XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n                          XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                          XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                         )) {\n        xmlFreeParserCtxt(ctxt);\n        IGRAPH_ERROR(\"Cannot set options for the parser context\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we need to pop off\n     * igraph_i_graphml_parser_state_destroy() from the stack and temporarily\n     * assume responsibility for calling it ourselves until we are back from the\n     * parser */\n    IGRAPH_FINALLY_CLEAN(1);\n\n    /* Do the parsing */\n    while ((res = (int) fread(buffer, 1, sizeof(buffer), instream)) > 0) {\n        xmlParseChunk(ctxt, buffer, res, 0);\n        if (!state.successful) {\n            break;\n        }\n    }\n    xmlParseChunk(ctxt, buffer, res, 1);\n\n    /* Free the context */\n    doc = ctxt->myDoc;\n    xmlFreeParserCtxt(ctxt);\n    if (doc) {\n        /* In theory this should not be necessary, but it looks like certain malformed\n         * GraphML files leave a partially-parsed doc in memory */\n        xmlFreeDoc(doc);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* Now that we have lifted error_message out of the parser state, we can\n     * put the destructor of the parser state back on the FINALLY stack */\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "func_src_after": "int igraph_read_graph_graphml(igraph_t *graph, FILE *instream, int index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < sizeof(buffer) && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data\", IGRAPH_PARSEERROR);\n    }\n    ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                   &state,\n                                   buffer,\n                                   res,\n                                   NULL);\n    /*   ctxt=xmlCreateIOParserCtxt(&igraph_i_graphml_sax_handler, &state, */\n    /*               igraph_i_libxml2_read_callback, */\n    /*               igraph_i_libxml2_close_callback, */\n    /*               instream, XML_CHAR_ENCODING_NONE); */\n    if (ctxt == NULL) {\n        IGRAPH_ERROR(\"Can't create progressive parser context\", IGRAPH_PARSEERROR);\n    }\n\n    /* Set parsing options */\n    if (xmlCtxtUseOptions(ctxt,\n                          XML_PARSE_NOBLANKS |\n                          XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                          XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                         )) {\n        xmlFreeParserCtxt(ctxt);\n        IGRAPH_ERROR(\"Cannot set options for the parser context\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we need to pop off\n     * igraph_i_graphml_parser_state_destroy() from the stack and temporarily\n     * assume responsibility for calling it ourselves until we are back from the\n     * parser */\n    IGRAPH_FINALLY_CLEAN(1);\n\n    /* Do the parsing */\n    while ((res = (int) fread(buffer, 1, sizeof(buffer), instream)) > 0) {\n        xmlParseChunk(ctxt, buffer, res, 0);\n        if (!state.successful) {\n            break;\n        }\n    }\n    xmlParseChunk(ctxt, buffer, res, 1);\n\n    /* Free the context */\n    doc = ctxt->myDoc;\n    xmlFreeParserCtxt(ctxt);\n    if (doc) {\n        /* In theory this should not be necessary, but it looks like certain malformed\n         * GraphML files leave a partially-parsed doc in memory */\n        xmlFreeDoc(doc);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* Now that we have lifted error_message out of the parser state, we can\n     * put the destructor of the parser state back on the FINALLY stack */\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "line_changes": {"deleted": [{"line_no": 41, "char_start": 1463, "char_end": 1528, "line": "                          XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n"}], "added": [{"line_no": 41, "char_start": 1463, "char_end": 1510, "line": "                          XML_PARSE_NOBLANKS |\n"}]}, "char_changes": {"deleted": [{"char_start": 1488, "char_end": 1506, "chars": " XML_PARSE_NOENT |"}], "added": []}, "commit_link": "github.com/igraph/igraph/commit/6ce2353fa77a891d1b556b8908ca9e4c227c3619", "file_name": "graphml.c", "vul_type": "cwe-611", "commit_msg": "fix: disable external XML entity resolution", "parent_commit": "cb8f28bae4c5ab92e5b628d9b4f827d3c61ba6ce", "description": "Write a C function to parse a GraphML file into an igraph_t structure using libxml2."}
{"func_name": "ResponseParser::parse", "func_src_before": "\tpublic Object parse(SerializerHandler serializerHandler, InputStream response, boolean debugMode) throws XMLRPCException {\n\n\t\ttry {\n\n\t\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tDocumentBuilder builder = factory.newDocumentBuilder();\n\t\t\tDocument dom = builder.parse(response);\n\t\t\tif (debugMode ){\n\t\t\t\tprintDocument(dom, System.out);\n\t\t\t}\n\t\t\tElement e = dom.getDocumentElement();\n\n\n\t\t\t// Check for root tag\n\t\t\tif(!e.getNodeName().equals(XMLRPCClient.METHOD_RESPONSE)) {\n\t\t\t\tthrow new XMLRPCException(\"MethodResponse root tag is missing.\");\n\t\t\t}\n\n\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\tif(e.getNodeName().equals(XMLRPCClient.PARAMS)) {\n\n\t\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\t\tif(!e.getNodeName().equals(XMLRPCClient.PARAM)) {\n\t\t\t\t\tthrow new XMLRPCException(\"The params tag must contain a param tag.\");\n\t\t\t\t}\n\n\t\t\t\treturn getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t} else if(e.getNodeName().equals(XMLRPCClient.FAULT)) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tMap<String,Object> o = (Map<String,Object>)getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t\tthrow new XMLRPCServerException((String)o.get(FAULT_STRING), (Integer)o.get(FAULT_CODE));\n\n\t\t\t}\n\n\t\t\tthrow new XMLRPCException(\"The methodResponse tag must contain a fault or params tag.\");\n\n\t\t} catch(XMLRPCServerException e) {\n\t\t\tthrow e;\n\t\t} catch (Exception ex) {\n\t\t\tthrow new XMLRPCException(\"Error getting result from server.\", ex);\n\t\t}\n\n\t}", "func_src_after": "\tpublic Object parse(SerializerHandler serializerHandler, InputStream response, boolean debugMode) throws XMLRPCException {\n\n\t\ttry {\n\t\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n\n\t\t\t// Ensure the xml parser won't allow exploitation of the vuln CWE-611\n\t\t\t// (described on https://cwe.mitre.org/data/definitions/611.html )\n\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tfactory.setXIncludeAware(false);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\t// End of the configuration of the parser for CWE-611\n\n\t\t\tDocumentBuilder builder = factory.newDocumentBuilder();\n\t\t\tDocument dom = builder.parse(response);\n\t\t\tif (debugMode ){\n\t\t\t\tprintDocument(dom, System.out);\n\t\t\t}\n\t\t\tElement e = dom.getDocumentElement();\n\n\n\t\t\t// Check for root tag\n\t\t\tif(!e.getNodeName().equals(XMLRPCClient.METHOD_RESPONSE)) {\n\t\t\t\tthrow new XMLRPCException(\"MethodResponse root tag is missing.\");\n\t\t\t}\n\n\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\tif(e.getNodeName().equals(XMLRPCClient.PARAMS)) {\n\n\t\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\t\tif(!e.getNodeName().equals(XMLRPCClient.PARAM)) {\n\t\t\t\t\tthrow new XMLRPCException(\"The params tag must contain a param tag.\");\n\t\t\t\t}\n\n\t\t\t\treturn getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t} else if(e.getNodeName().equals(XMLRPCClient.FAULT)) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tMap<String,Object> o = (Map<String,Object>)getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t\tthrow new XMLRPCServerException((String)o.get(FAULT_STRING), (Integer)o.get(FAULT_CODE));\n\n\t\t\t}\n\n\t\t\tthrow new XMLRPCException(\"The methodResponse tag must contain a fault or params tag.\");\n\n\t\t} catch(XMLRPCServerException e) {\n\t\t\tthrow e;\n\t\t} catch (Exception ex) {\n\t\t\tthrow new XMLRPCException(\"Error getting result from server.\", ex);\n\t\t}\n\n\t}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 133, "char_end": 134, "line": "\n"}], "added": [{"line_no": 5, "char_start": 207, "char_end": 208, "line": "\n"}, {"line_no": 8, "char_start": 351, "char_end": 436, "line": "\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n"}, {"line_no": 9, "char_start": 436, "char_end": 481, "line": "\t\t\tfactory.setExpandEntityReferences(false);\n"}, {"line_no": 11, "char_start": 517, "char_end": 553, "line": "\t\t\tfactory.setXIncludeAware(false);\n"}, {"line_no": 12, "char_start": 553, "char_end": 598, "line": "\t\t\tfactory.setExpandEntityReferences(false);\n"}, {"line_no": 14, "char_start": 655, "char_end": 656, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 133, "char_end": 134, "chars": "\n"}, {"char_start": 208, "char_end": 243, "chars": "\t\t\tfactory.setNamespaceAware(true);"}], "added": [{"char_start": 207, "char_end": 655, "chars": "\n\t\t\t// Ensure the xml parser won't allow exploitation of the vuln CWE-611\n\t\t\t// (described on https://cwe.mitre.org/data/definitions/611.html )\n\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tfactory.setXIncludeAware(false);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\t// End of the configuration of the parser for CWE-611\n"}]}, "commit_link": "github.com/timroes/aXMLRPC/commit/ad6615b3ec41353e614f6ea5fdd5b046442a832b", "file_name": "ResponseParser.java", "vul_type": "cwe-611", "commit_msg": "Fix CWE-611\n\nThis commit fixes the issue described on\nhttps://cwe.mitre.org/data/definitions/611.html\n\ntest", "parent_commit": "2e59d03c961607d32bc9dc5e7aba931338907603", "description": "Write a Java function to parse an XMLRPC response from an InputStream and handle debug mode."}
{"func_name": "TestPlanAnalyzer::getNodeListWithClassNames", "func_src_before": "    private NodeList getNodeListWithClassNames(String path) {\n        try {\n            DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n            DocumentBuilder builder = factory.newDocumentBuilder();\n            byte[] bytes = overrideXmlVersion(readBytesFromFile(path));\n            Document doc = (bytes == null) ? builder.parse(path) : builder.parse(new ByteArrayInputStream(bytes));\n            XPathFactory xPathfactory = XPathFactory.newInstance();\n            XPath xpath = xPathfactory.newXPath();\n            XPathExpression expr = xpath.compile(\"//*[@guiclass|@testclass]\");\n            return (NodeList) expr.evaluate(doc, XPathConstants.NODESET);\n        } catch (Exception ex) {\n            log.warn(\"Cannot parse file: \" + path, ex);\n            return null;\n        }\n    }", "func_src_after": "    private NodeList getNodeListWithClassNames(String path) {\n        try {\n            DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n            factory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n            factory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-external-dtd\", false);\n            factory.setXIncludeAware(false);\n            factory.setExpandEntityReferences(false);\n\n            DocumentBuilder builder = factory.newDocumentBuilder();\n            byte[] bytes = overrideXmlVersion(readBytesFromFile(path));\n            Document doc = (bytes == null) ? builder.parse(path) : builder.parse(new ByteArrayInputStream(bytes));\n            XPathFactory xPathfactory = XPathFactory.newInstance();\n            XPath xpath = xPathfactory.newXPath();\n            XPathExpression expr = xpath.compile(\"//*[@guiclass|@testclass]\");\n            return (NodeList) expr.evaluate(doc, XPathConstants.NODESET);\n        } catch (ParserConfigurationException pex) {\n            log.warn(\"Cannot set the required parser config\", pex);\n            return null;\n        } catch (Exception ex) {\n            log.warn(\"Cannot parse file: \" + path, ex);\n            return null;\n        }\n    }", "line_changes": {"deleted": [], "added": [{"line_no": 4, "char_start": 159, "char_end": 253, "line": "            factory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n"}, {"line_no": 5, "char_start": 253, "char_end": 358, "line": "            factory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-external-dtd\", false);\n"}, {"line_no": 6, "char_start": 358, "char_end": 403, "line": "            factory.setXIncludeAware(false);\n"}, {"line_no": 7, "char_start": 403, "char_end": 457, "line": "            factory.setExpandEntityReferences(false);\n"}, {"line_no": 8, "char_start": 457, "char_end": 458, "line": "\n"}, {"line_no": 16, "char_start": 985, "char_end": 1038, "line": "        } catch (ParserConfigurationException pex) {\n"}, {"line_no": 17, "char_start": 1038, "char_end": 1106, "line": "            log.warn(\"Cannot set the required parser config\", pex);\n"}, {"line_no": 18, "char_start": 1106, "char_end": 1131, "line": "            return null;\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 159, "char_end": 458, "chars": "            factory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n            factory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-external-dtd\", false);\n            factory.setXIncludeAware(false);\n            factory.setExpandEntityReferences(false);\n\n"}, {"char_start": 985, "char_end": 1131, "chars": "        } catch (ParserConfigurationException pex) {\n            log.warn(\"Cannot set the required parser config\", pex);\n            return null;\n"}]}, "commit_link": "github.com/undera/jmeter-plugins-manager/commit/a820772f1d65542fe45df7136791d3fc04adde59", "file_name": "TestPlanAnalyzer.java", "vul_type": "cwe-611", "commit_msg": "Fix XXE vulnerability", "parent_commit": "060c7853ab87c1764980c90a3190dae65299d4e1", "description": "Write a Java function to parse an XML file at a given path and return a NodeList of elements with specific class attributes."}
{"func_name": "do_file_clean", "func_src_before": "static void do_file_clean(int flags, char *name)\n{\n\tchar *page;\n\tchar fn[30];\n\tsnprintf(fn, 30, TMPDIR \"~test%d\", tmpcount++);\n\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT);\n\tif (fd < 0)\n\t\terr(\"open temp file\");\n\twrite(fd, fn, 4);\n\tpage = checked_mmap(NULL, PS, PROT_READ|PROT_WRITE, MAP_SHARED|flags,\n\t\tfd, 0);\n\tfsync(fd);\n\tclose(fd);\n\ttestmem(name, page, MREAD_OK);\n\t /* reread page from disk */\n\tprintf(\"\\t reading %x\\n\", *(unsigned char *)page);\n\ttestmem(name, page, MWRITE_OK);\n}", "func_src_after": "static void do_file_clean(int flags, char *name)\n{\n\tchar *page;\n\tchar fn[30];\n\tsnprintf(fn, 30, TMPDIR \"~test%d\", tmpcount++);\n\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT, 0664);\n\tif (fd < 0)\n\t\terr(\"open temp file\");\n\twrite(fd, fn, 4);\n\tpage = checked_mmap(NULL, PS, PROT_READ|PROT_WRITE, MAP_SHARED|flags,\n\t\tfd, 0);\n\tfsync(fd);\n\tclose(fd);\n\ttestmem(name, page, MREAD_OK);\n\t /* reread page from disk */\n\tprintf(\"\\t reading %x\\n\", *(unsigned char *)page);\n\ttestmem(name, page, MWRITE_OK);\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 127, "char_end": 171, "line": "\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT);\n"}], "added": [{"line_no": 6, "char_start": 127, "char_end": 177, "line": "\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT, 0664);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 168, "char_end": 174, "chars": ", 0664"}]}, "commit_link": "github.com/andikleen/mce-test/commit/4da149ab9462e1e8750e2db7a93fa88429eab91a", "file_name": "tinjpage.c", "vul_type": "cwe-732", "commit_msg": "mce-test: Add file attributes to open\n\nThis fixes a compile warning.\n\nopen(2) manpage says:\n... mode specifies the permissions to use in case a new\nfile is created. This argument must be  supplied  when\nO_CREAT is specified  in  flags; ...\n\nSigned-off-by: Thomas Renninger <trenn@suse.de>\nSigned-off-by: Chen Gong <gong.chen@linux.intel.com>", "parent_commit": "cfdf0d9f9564de49ec38c794fa4b151babea98f1", "description": "Write a C function to create a temporary file, map it into memory, perform read/write tests, and print a value from the mapped memory."}
{"func_name": "make_tarfile", "func_src_before": "def make_tarfile(\n    output_filename: str,\n    source_dir: str,\n    archive_name: str,\n    custom_filter: Optional[Callable] = None,\n) -> None:\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info: \"tarfile.TarInfo\") -> Optional[\"tarfile.TarInfo\"]:\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    unzipped_filename = tempfile.mktemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar_file:\n            gzipped_tar.write(tar_file.read())\n    finally:\n        os.remove(unzipped_filename)", "func_src_after": "def make_tarfile(\n    output_filename: str,\n    source_dir: str,\n    archive_name: str,\n    custom_filter: Optional[Callable] = None,\n) -> None:\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info: \"tarfile.TarInfo\") -> Optional[\"tarfile.TarInfo\"]:\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    descriptor, unzipped_filename = tempfile.mkstemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar_file:\n            gzipped_tar.write(tar_file.read())\n    finally:\n        os.close(descriptor)\n        os.remove(unzipped_filename)", "line_changes": {"deleted": [{"line_no": 12, "char_start": 394, "char_end": 436, "line": "    unzipped_filename = tempfile.mktemp()\n"}], "added": [{"line_no": 12, "char_start": 394, "char_end": 449, "line": "    descriptor, unzipped_filename = tempfile.mkstemp()\n"}, {"line_no": 23, "char_start": 1018, "char_end": 1047, "line": "        os.close(descriptor)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 397, "char_end": 409, "chars": " descriptor,"}, {"char_start": 441, "char_end": 442, "chars": "s"}, {"char_start": 1018, "char_end": 1047, "chars": "        os.close(descriptor)\n"}]}, "commit_link": "github.com/wandb/client/commit/7c1306c1851da5628ac96e0f89728d16852611dd", "file_name": "util.py", "vul_type": "cwe-377", "commit_msg": "Port of #3357: Fix/insecure tempfile (#3360)\n\n* Replaced insecure tempfile.mktemp() with tempfile.mkstemp()\r\nCo-authored-by: whokilleddb <whokilleddb@gmail.com>", "description": "Write a Python function to create a gzipped tar file from a directory, optionally applying a custom filter to the files."}
{"func_name": "main", "func_src_before": "def main(_):\n  # Generate some fake Iris data.\n  # It is okay for this example because this example is about how to use the\n  # debugger, not how to use machine learning to solve the Iris classification\n  # problem.\n  def training_input_fn():\n    return ({\n        \"features\": tf.random_normal([128, 4])\n    }, tf.random_uniform([128], minval=0, maxval=3, dtype=tf.int32))\n\n  def test_input_fn():\n    return ({\n        \"features\": tf.random_normal([32, 4])\n    }, tf.random_uniform([32], minval=0, maxval=3, dtype=tf.int32))\n\n  feature_columns = [tf.feature_column.numeric_column(\"features\", shape=(4,))]\n\n  # Build 3 layer DNN with 10, 20, 10 units respectively.\n  model_dir = FLAGS.model_dir or tempfile.mkdtemp(prefix=\"debug_tflearn_iris_\")\n\n  classifier = tf.estimator.DNNClassifier(\n      feature_columns=feature_columns,\n      hidden_units=[10, 20, 10],\n      n_classes=3,\n      model_dir=model_dir)\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  hooks = []\n  if FLAGS.debug:\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    hooks.append(\n        tf_debug.LocalCLIDebugHook(\n            ui_type=FLAGS.ui_type,\n            dump_root=FLAGS.dump_root,\n            config_file_path=config_file_path))\n  elif FLAGS.tensorboard_debug_address:\n    hooks.append(tf_debug.TensorBoardDebugHook(FLAGS.tensorboard_debug_address))\n\n  # Train model, using tfdbg hook.\n  classifier.train(training_input_fn, steps=FLAGS.train_steps, hooks=hooks)\n\n  # Evaluate accuracy, using tfdbg hook.\n  accuracy_score = classifier.evaluate(\n      test_input_fn, steps=FLAGS.eval_steps, hooks=hooks)[\"accuracy\"]\n\n  print(\"After training %d steps, Accuracy = %f\" %\n        (FLAGS.train_steps, accuracy_score))\n\n  # Make predictions, using tfdbg hook.\n  predict_results = classifier.predict(test_input_fn, hooks=hooks)\n  print(\"A prediction result: %s\" % next(predict_results))", "func_src_after": "def main(_):\n  # Generate some fake Iris data.\n  # It is okay for this example because this example is about how to use the\n  # debugger, not how to use machine learning to solve the Iris classification\n  # problem.\n  def training_input_fn():\n    return ({\n        \"features\": tf.random_normal([128, 4])\n    }, tf.random_uniform([128], minval=0, maxval=3, dtype=tf.int32))\n\n  def test_input_fn():\n    return ({\n        \"features\": tf.random_normal([32, 4])\n    }, tf.random_uniform([32], minval=0, maxval=3, dtype=tf.int32))\n\n  feature_columns = [tf.feature_column.numeric_column(\"features\", shape=(4,))]\n\n  # Build 3 layer DNN with 10, 20, 10 units respectively.\n  model_dir = FLAGS.model_dir or tempfile.mkdtemp(prefix=\"debug_tflearn_iris_\")\n\n  classifier = tf.estimator.DNNClassifier(\n      feature_columns=feature_columns,\n      hidden_units=[10, 20, 10],\n      n_classes=3,\n      model_dir=model_dir)\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  hooks = []\n  if FLAGS.debug:\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    hooks.append(\n        tf_debug.LocalCLIDebugHook(\n            ui_type=FLAGS.ui_type,\n            dump_root=FLAGS.dump_root,\n            config_file_path=config_file_path))\n  elif FLAGS.tensorboard_debug_address:\n    hooks.append(tf_debug.TensorBoardDebugHook(FLAGS.tensorboard_debug_address))\n\n  # Train model, using tfdbg hook.\n  classifier.train(training_input_fn, steps=FLAGS.train_steps, hooks=hooks)\n\n  # Evaluate accuracy, using tfdbg hook.\n  accuracy_score = classifier.evaluate(\n      test_input_fn, steps=FLAGS.eval_steps, hooks=hooks)[\"accuracy\"]\n\n  print(\"After training %d steps, Accuracy = %f\" %\n        (FLAGS.train_steps, accuracy_score))\n\n  # Make predictions, using tfdbg hook.\n  predict_results = classifier.predict(test_input_fn, hooks=hooks)\n  print(\"A prediction result: %s\" % next(predict_results))", "line_changes": {"deleted": [{"line_no": 33, "char_start": 1110, "char_end": 1135, "line": "    config_file_path = (\n"}, {"line_no": 34, "char_start": 1135, "char_end": 1176, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 35, "char_start": 1176, "char_end": 1227, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 33, "char_start": 1110, "char_end": 1147, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 35, "char_start": 1200, "char_end": 1262, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 36, "char_start": 1262, "char_end": 1272, "line": "    else:\n"}, {"line_no": 37, "char_start": 1272, "char_end": 1302, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 1132, "char_end": 1142, "chars": " (\n       "}, {"char_start": 1180, "char_end": 1204, "chars": "    if FLAGS.use_random_"}, {"char_start": 1216, "char_end": 1220, "chars": "else"}, {"char_start": 1225, "char_end": 1226, "chars": ")"}], "added": [{"char_start": 1114, "char_end": 1209, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 1239, "char_end": 1240, "chars": "s"}, {"char_start": 1266, "char_end": 1278, "chars": "else:\n      "}, {"char_start": 1285, "char_end": 1290, "chars": "file_"}, {"char_start": 1295, "char_end": 1296, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/62644d6d2af6e185361f770099bf5d5e6d2d39ff", "file_name": "debug_tflearn_iris.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420360028\nChange-Id: Icd8a7ba3e47c2ff63a26a2fe007737ef01c0cb1d", "description": "Write a Python script using TensorFlow to create, train, and evaluate a DNNClassifier for the Iris dataset with debugging hooks."}
{"func_name": "testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError", "func_src_before": "  def testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError(self):\n    # Create an unrelated source file.\n    unrelated_source_path = tempfile.mktemp()\n    with open(unrelated_source_path, \"wt\") as source_file:\n      source_file.write(\"print('hello, world')\\n\")\n\n    self.assertEqual({},\n                     source_utils.annotate_source(self.dump,\n                                                  unrelated_source_path))\n\n    # Clean up unrelated source file.\n    os.remove(unrelated_source_path)", "func_src_after": "  def testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError(self):\n    # Create an unrelated source file.\n    fd, unrelated_source_path = tempfile.mkstemp()\n    with open(fd, \"wt\") as source_file:\n      source_file.write(\"print('hello, world')\\n\")\n\n    self.assertEqual({},\n                     source_utils.annotate_source(self.dump,\n                                                  unrelated_source_path))\n\n    # Clean up unrelated source file.\n    os.remove(unrelated_source_path)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 111, "char_end": 157, "line": "    unrelated_source_path = tempfile.mktemp()\n"}, {"line_no": 4, "char_start": 157, "char_end": 216, "line": "    with open(unrelated_source_path, \"wt\") as source_file:\n"}], "added": [{"line_no": 3, "char_start": 111, "char_end": 162, "line": "    fd, unrelated_source_path = tempfile.mkstemp()\n"}, {"line_no": 4, "char_start": 162, "char_end": 202, "line": "    with open(fd, \"wt\") as source_file:\n"}]}, "char_changes": {"deleted": [{"char_start": 171, "char_end": 192, "chars": "unrelated_source_path"}], "added": [{"char_start": 114, "char_end": 118, "chars": " fd,"}, {"char_start": 154, "char_end": 155, "chars": "s"}, {"char_start": 176, "char_end": 178, "chars": "fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/3752cc4c6ba6b69f04f857c6047adde9e8487bd6", "file_name": "source_utils_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359237\nChange-Id: I7fa45e888deff612ca53a4f8610cfad8f28e9671", "description": "Write a Python function to test that calling a source annotation utility with an unrelated source file does not produce an error, and clean up the file afterwards."}
{"func_name": "testDebugDumpDir_nonexistentDumpRoot", "func_src_before": "  def testDebugDumpDir_nonexistentDumpRoot(self):\n    with self.assertRaisesRegex(IOError, \"does not exist\"):\n      debug_data.DebugDumpDir(tempfile.mktemp() + \"_foo\")", "func_src_after": "  def testDebugDumpDir_nonexistentDumpRoot(self):\n    with self.assertRaisesRegex(IOError, \"does not exist\"):\n      debug_data.DebugDumpDir(tempfile.mkdtemp() + \"_foo\")", "line_changes": {"deleted": [{"line_no": 3, "char_start": 110, "char_end": 167, "line": "      debug_data.DebugDumpDir(tempfile.mktemp() + \"_foo\")\n"}], "added": [{"line_no": 3, "char_start": 110, "char_end": 168, "line": "      debug_data.DebugDumpDir(tempfile.mkdtemp() + \"_foo\")\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 151, "char_end": 152, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/9b1fe8f31ee1788208d8d6b7385382e436c5e1d7", "file_name": "debug_data_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp` to create directories.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do: just a name change\n\nPiperOrigin-RevId: 420370858\nChange-Id: I44a0849d161132eacd4f3881fdb615e09c0f02a2", "description": "Write a Python unit test that expects an IOError when trying to create a DebugDumpDir with a non-existent directory."}
{"func_name": "testNonExistentFile", "func_src_before": "  def testNonExistentFile(self):\n    converter = upgrade_schema_lib.Converter()\n    non_existent = tempfile.mktemp(suffix=\".json\")\n    with self.assertRaisesRegex(IOError, \"No such file or directory\"):\n      converter.Convert(non_existent, non_existent)", "func_src_after": "  def testNonExistentFile(self):\n    converter = upgrade_schema_lib.Converter()\n    _, non_existent = tempfile.mkstemp(suffix=\".json\")  # safe to ignore fd\n    with self.assertRaisesRegex(IOError, \"No such file or directory\"):\n      converter.Convert(non_existent, non_existent)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 80, "char_end": 131, "line": "    non_existent = tempfile.mktemp(suffix=\".json\")\n"}], "added": [{"line_no": 3, "char_start": 80, "char_end": 156, "line": "    _, non_existent = tempfile.mkstemp(suffix=\".json\")  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 83, "char_end": 86, "chars": " _,"}, {"char_start": 113, "char_end": 114, "chars": "s"}, {"char_start": 134, "char_end": 155, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/c63f6333a78ddf89ecd502d926fb877b8dce4f0f", "file_name": "upgrade_schema_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420335872\nChange-Id: I331ec2544a08d3cc3063a74af342cceae655b3dc", "description": "Write a Python unit test that checks if a custom converter raises an IOError when attempting to convert a non-existent JSON file."}
{"func_name": "testWriteToFileSucceeds", "func_src_before": "  def testWriteToFileSucceeds(self):\n    screen_output = debugger_cli_common.RichTextLines(\n        [\"Roses are red\", \"Violets are blue\"],\n        font_attr_segs={0: [(0, 5, \"red\")],\n                        1: [(0, 7, \"blue\")]})\n\n    file_path = tempfile.mktemp()\n    screen_output.write_to_file(file_path)\n\n    with gfile.Open(file_path, \"r\") as f:\n      self.assertEqual(\"Roses are red\\nViolets are blue\\n\", f.read())\n\n    # Clean up.\n    gfile.Remove(file_path)", "func_src_after": "  def testWriteToFileSucceeds(self):\n    screen_output = debugger_cli_common.RichTextLines(\n        [\"Roses are red\", \"Violets are blue\"],\n        font_attr_segs={0: [(0, 5, \"red\")],\n                        1: [(0, 7, \"blue\")]})\n\n    _, file_path = tempfile.mkstemp()  # safe to ignore fd here\n    screen_output.write_to_file(file_path)\n\n    with gfile.Open(file_path, \"r\") as f:\n      self.assertEqual(\"Roses are red\\nViolets are blue\\n\", f.read())\n\n    # Clean up.\n    gfile.Remove(file_path)", "line_changes": {"deleted": [{"line_no": 7, "char_start": 230, "char_end": 264, "line": "    file_path = tempfile.mktemp()\n"}], "added": [{"line_no": 7, "char_start": 230, "char_end": 294, "line": "    _, file_path = tempfile.mkstemp()  # safe to ignore fd here\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 233, "char_end": 236, "chars": " _,"}, {"char_start": 260, "char_end": 261, "chars": "s"}, {"char_start": 267, "char_end": 293, "chars": "  # safe to ignore fd here"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/ca5fe92b42a64b371b963d83b2da4f074e83280c", "file_name": "debugger_cli_common_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359120\nChange-Id: Ifb43401b1fd3e023c685dc3a74b3b655090e1ce6", "description": "Write a Python function that tests writing predefined text to a temporary file and then reads it back to verify the content."}
{"func_name": "setUp", "func_src_before": "  def setUp(self):\n    self._history_file_path = tempfile.mktemp()\n    self._cmd_hist = debugger_cli_common.CommandHistory(\n        limit=3, history_file_path=self._history_file_path)", "func_src_after": "  def setUp(self):\n    _, self._history_file_path = tempfile.mkstemp()  # safe to ignore fd here\n    self._cmd_hist = debugger_cli_common.CommandHistory(\n        limit=3, history_file_path=self._history_file_path)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 67, "line": "    self._history_file_path = tempfile.mktemp()\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 97, "line": "    _, self._history_file_path = tempfile.mkstemp()  # safe to ignore fd here\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 22, "char_end": 25, "chars": " _,"}, {"char_start": 63, "char_end": 64, "chars": "s"}, {"char_start": 70, "char_end": 96, "chars": "  # safe to ignore fd here"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/ca5fe92b42a64b371b963d83b2da4f074e83280c", "file_name": "debugger_cli_common_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359120\nChange-Id: Ifb43401b1fd3e023c685dc3a74b3b655090e1ce6", "description": "Create a Python function named `setUp` that initializes a command history object with a limit of 3 and a temporary file for storing the history."}
{"func_name": "__init__", "func_src_before": "  def __init__(self,\n               sess,\n               dump_root=None,\n               log_usage=True,\n               ui_type=\"curses\",\n               thread_name_filter=None,\n               config_file_path=False):\n    \"\"\"Constructor of LocalCLIDebugWrapperSession.\n\n    Args:\n      sess: The TensorFlow `Session` object being wrapped.\n      dump_root: (`str`) optional path to the dump root directory. Must be a\n        directory that does not exist or an empty directory. If the directory\n        does not exist, it will be created by the debugger core during debug\n        `run()` calls and removed afterwards. If `None`, the debug dumps will\n        be at tfdbg_<random_string> under the system temp directory.\n      log_usage: (`bool`) whether the usage of this class is to be logged.\n      ui_type: (`str`) requested UI type. Currently supported:\n        (curses | readline)\n      thread_name_filter: Regular-expression white list for thread name. See\n        the doc of `BaseDebugWrapperSession` for details.\n      config_file_path: Optional override to the default configuration file\n        path, which is at `${HOME}/.tfdbg_config`.\n\n    Raises:\n      ValueError: If dump_root is an existing and non-empty directory or if\n        dump_root is a file.\n    \"\"\"\n\n    if log_usage:\n      pass  # No logging for open-source.\n\n    framework.BaseDebugWrapperSession.__init__(\n        self, sess, thread_name_filter=thread_name_filter)\n\n    if not dump_root:\n      self._dump_root = tempfile.mktemp(prefix=_DUMP_ROOT_PREFIX)\n    else:\n      dump_root = os.path.expanduser(dump_root)\n      if os.path.isfile(dump_root):\n        raise ValueError(\"dump_root path points to a file: %s\" % dump_root)\n      elif os.path.isdir(dump_root) and os.listdir(dump_root):\n        raise ValueError(\"dump_root path points to a non-empty directory: %s\" %\n                         dump_root)\n\n      self._dump_root = dump_root\n\n    self._initialize_argparsers()\n\n    # Registered tensor filters.\n    self._tensor_filters = {}\n    # Register frequently-used filter(s).\n    self.add_tensor_filter(\"has_inf_or_nan\", debug_data.has_inf_or_nan)\n\n    # Below are the state variables of this wrapper object.\n    # _active_tensor_filter: what (if any) tensor filter is in effect. If such\n    #   a filter is in effect, this object will call run() method of the\n    #   underlying TensorFlow Session object until the filter passes. This is\n    #   activated by the \"-f\" flag of the \"run\" command.\n    # _run_through_times: keeps track of how many times the wrapper needs to\n    #   run through without stopping at the run-end CLI. It is activated by the\n    #   \"-t\" option of the \"run\" command.\n    # _skip_debug: keeps track of whether the current run should be executed\n    #   without debugging. It is activated by the \"-n\" option of the \"run\"\n    #   command.\n    #\n    # _run_start_response: keeps track what OnRunStartResponse the wrapper\n    #   should return at the next run-start callback. If this information is\n    #   unavailable (i.e., is None), the run-start CLI will be launched to ask\n    #   the user. This is the case, e.g., right before the first run starts.\n    self._active_tensor_filter = None\n    self._active_filter_exclude_node_names = None\n    self._active_tensor_filter_run_start_response = None\n    self._run_through_times = 1\n    self._skip_debug = False\n    self._run_start_response = None\n    self._is_run_start = True\n    self._ui_type = ui_type\n    self._config = None\n    if config_file_path:\n      self._config = cli_config.CLIConfig(config_file_path=config_file_path)", "func_src_after": "  def __init__(self,\n               sess,\n               dump_root=None,\n               log_usage=True,\n               ui_type=\"curses\",\n               thread_name_filter=None,\n               config_file_path=False):\n    \"\"\"Constructor of LocalCLIDebugWrapperSession.\n\n    Args:\n      sess: The TensorFlow `Session` object being wrapped.\n      dump_root: (`str`) optional path to the dump root directory. Must be a\n        directory that does not exist or an empty directory. If the directory\n        does not exist, it will be created by the debugger core during debug\n        `run()` calls and removed afterwards. If `None`, the debug dumps will\n        be at tfdbg_<random_string> under the system temp directory.\n      log_usage: (`bool`) whether the usage of this class is to be logged.\n      ui_type: (`str`) requested UI type. Currently supported:\n        (curses | readline)\n      thread_name_filter: Regular-expression white list for thread name. See\n        the doc of `BaseDebugWrapperSession` for details.\n      config_file_path: Optional override to the default configuration file\n        path, which is at `${HOME}/.tfdbg_config`.\n\n    Raises:\n      ValueError: If dump_root is an existing and non-empty directory or if\n        dump_root is a file.\n    \"\"\"\n\n    if log_usage:\n      pass  # No logging for open-source.\n\n    framework.BaseDebugWrapperSession.__init__(\n        self, sess, thread_name_filter=thread_name_filter)\n\n    if not dump_root:\n      self._dump_root = tempfile.mkdtemp(prefix=_DUMP_ROOT_PREFIX)\n    else:\n      dump_root = os.path.expanduser(dump_root)\n      if os.path.isfile(dump_root):\n        raise ValueError(\"dump_root path points to a file: %s\" % dump_root)\n      elif os.path.isdir(dump_root) and os.listdir(dump_root):\n        raise ValueError(\"dump_root path points to a non-empty directory: %s\" %\n                         dump_root)\n\n      self._dump_root = dump_root\n\n    self._initialize_argparsers()\n\n    # Registered tensor filters.\n    self._tensor_filters = {}\n    # Register frequently-used filter(s).\n    self.add_tensor_filter(\"has_inf_or_nan\", debug_data.has_inf_or_nan)\n\n    # Below are the state variables of this wrapper object.\n    # _active_tensor_filter: what (if any) tensor filter is in effect. If such\n    #   a filter is in effect, this object will call run() method of the\n    #   underlying TensorFlow Session object until the filter passes. This is\n    #   activated by the \"-f\" flag of the \"run\" command.\n    # _run_through_times: keeps track of how many times the wrapper needs to\n    #   run through without stopping at the run-end CLI. It is activated by the\n    #   \"-t\" option of the \"run\" command.\n    # _skip_debug: keeps track of whether the current run should be executed\n    #   without debugging. It is activated by the \"-n\" option of the \"run\"\n    #   command.\n    #\n    # _run_start_response: keeps track what OnRunStartResponse the wrapper\n    #   should return at the next run-start callback. If this information is\n    #   unavailable (i.e., is None), the run-start CLI will be launched to ask\n    #   the user. This is the case, e.g., right before the first run starts.\n    self._active_tensor_filter = None\n    self._active_filter_exclude_node_names = None\n    self._active_tensor_filter_run_start_response = None\n    self._run_through_times = 1\n    self._skip_debug = False\n    self._run_start_response = None\n    self._is_run_start = True\n    self._ui_type = ui_type\n    self._config = None\n    if config_file_path:\n      self._config = cli_config.CLIConfig(config_file_path=config_file_path)", "line_changes": {"deleted": [{"line_no": 37, "char_start": 1463, "char_end": 1529, "line": "      self._dump_root = tempfile.mktemp(prefix=_DUMP_ROOT_PREFIX)\n"}], "added": [{"line_no": 37, "char_start": 1463, "char_end": 1530, "line": "      self._dump_root = tempfile.mkdtemp(prefix=_DUMP_ROOT_PREFIX)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1498, "char_end": 1499, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/2939613ef8340a75c13a470d4097dbd7e4b6b534", "file_name": "local_cli_wrapper.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420384092\nChange-Id: I8721c09ccc4de589b5a45d38e7ebc440160c72b8", "description": "Write a Python class constructor for a TensorFlow debugging wrapper session with customizable session, dump directory, logging, UI type, thread filtering, and configuration file path."}
{"func_name": "__init__", "func_src_before": "  def __init__(self,\n               height,\n               width,\n               command_sequence=None):\n    self._height = height\n    self._width = width\n\n    self._command_sequence = command_sequence\n    self._command_counter = 0\n\n    # The mock class has no actual textbox. So use this variable to keep\n    # track of what's entered in the textbox on creation.\n    self._curr_existing_command = \"\"\n\n    # Observers for test.\n    # Observers of screen output.\n    self.unwrapped_outputs = []\n    self.wrapped_outputs = []\n    self.scroll_messages = []\n    self.output_array_pointer_indices = []\n\n    self.output_pad_rows = []\n\n    # Observers of command textbox.\n    self.existing_commands = []\n\n    # Observer for tab-completion candidates.\n    self.candidates_lists = []\n\n    # Observer for the main menu.\n    self.main_menu_list = []\n\n    # Observer for toast messages.\n    self.toasts = []\n\n    curses_ui.CursesUI.__init__(\n        self,\n        config=cli_config.CLIConfig(\n            config_file_path=os.path.join(tempfile.mkdtemp(), \".tfdbg_config\")))\n\n    # Override the default path to the command history file to avoid test\n    # concurrency issues.\n    self._command_history_store = debugger_cli_common.CommandHistory(\n        history_file_path=tempfile.mktemp())", "func_src_after": "  def __init__(self,\n               height,\n               width,\n               command_sequence=None):\n    self._height = height\n    self._width = width\n\n    self._command_sequence = command_sequence\n    self._command_counter = 0\n\n    # The mock class has no actual textbox. So use this variable to keep\n    # track of what's entered in the textbox on creation.\n    self._curr_existing_command = \"\"\n\n    # Observers for test.\n    # Observers of screen output.\n    self.unwrapped_outputs = []\n    self.wrapped_outputs = []\n    self.scroll_messages = []\n    self.output_array_pointer_indices = []\n\n    self.output_pad_rows = []\n\n    # Observers of command textbox.\n    self.existing_commands = []\n\n    # Observer for tab-completion candidates.\n    self.candidates_lists = []\n\n    # Observer for the main menu.\n    self.main_menu_list = []\n\n    # Observer for toast messages.\n    self.toasts = []\n\n    curses_ui.CursesUI.__init__(\n        self,\n        config=cli_config.CLIConfig(\n            config_file_path=os.path.join(tempfile.mkdtemp(), \".tfdbg_config\")))\n\n    # Override the default path to the command history file to avoid test\n    # concurrency issues.\n    _, history_file_path = tempfile.mkstemp()  # safe to ignore fd\n    self._command_history_store = debugger_cli_common.CommandHistory(\n        history_file_path=history_file_path)", "line_changes": {"deleted": [{"line_no": 44, "char_start": 1233, "char_end": 1277, "line": "        history_file_path=tempfile.mktemp())\n"}], "added": [{"line_no": 43, "char_start": 1163, "char_end": 1230, "line": "    _, history_file_path = tempfile.mkstemp()  # safe to ignore fd\n"}, {"line_no": 45, "char_start": 1300, "char_end": 1344, "line": "        history_file_path=history_file_path)\n"}]}, "char_changes": {"deleted": [{"char_start": 1259, "char_end": 1276, "chars": "tempfile.mktemp()"}], "added": [{"char_start": 1163, "char_end": 1230, "chars": "    _, history_file_path = tempfile.mkstemp()  # safe to ignore fd\n"}, {"char_start": 1326, "char_end": 1343, "chars": "history_file_path"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/578f7eec19544d0223d145b56d88dfe043114538", "file_name": "curses_ui_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359212\nChange-Id: I172811749d2e7b901399f63df4fd1523447c6682", "description": "In Python, write an initializer for a mock UI class that sets up dimensions, command handling, and various observers for testing, without using actual UI components."}
{"func_name": "is_case_sensitive", "func_src_before": "    def is_case_sensitive(self):\n        if self._case_sensitive is None:\n            lock = self.unlock_path(self.base_folder, unlock_parent=False)\n            path = os.tempnam(self.base_folder, '.caseTest_')\n            os.mkdir(path)\n            if os.path.exists(path.upper()):\n                self._case_sensitive = False\n            else:\n                self._case_sensitive = True\n            os.rmdir(path)\n            self.lock_path(self.base_folder, lock)\n        return self._case_sensitive", "func_src_after": "    def is_case_sensitive(self):\n        if self._case_sensitive is None:\n            lock = self.unlock_path(self.base_folder, unlock_parent=False)\n            path = tempfile.mkdtemp(prefix='.caseTest_', dir=self.base_folder)\n            if os.path.exists(path.upper()):\n                self._case_sensitive = False\n            else:\n                self._case_sensitive = True\n            os.rmdir(path)\n            self.lock_path(self.base_folder, lock)\n        return self._case_sensitive", "line_changes": {"deleted": [{"line_no": 4, "char_start": 149, "char_end": 211, "line": "            path = os.tempnam(self.base_folder, '.caseTest_')\n"}, {"line_no": 5, "char_start": 211, "char_end": 238, "line": "            os.mkdir(path)\n"}], "added": [{"line_no": 4, "char_start": 149, "char_end": 228, "line": "            path = tempfile.mkdtemp(prefix='.caseTest_', dir=self.base_folder)\n"}]}, "char_changes": {"deleted": [{"char_start": 168, "char_end": 171, "chars": "os."}, {"char_start": 175, "char_end": 236, "chars": "nam(self.base_folder, '.caseTest_')\n            os.mkdir(path"}], "added": [{"char_start": 172, "char_end": 226, "chars": "file.mkdtemp(prefix='.caseTest_', dir=self.base_folder"}]}, "commit_link": "github.com/arameshkumar/nuxeo-drive/commit/8349d04615fc0f02b4b7588f04a24382431e2d3a", "file_name": "local_client.py", "vul_type": "cwe-377", "commit_msg": "NXDRIVE-170: Fix encoding issue when testing for case sensitive OS (and don't use unsafe tempnam)", "description": "Write a Python function to determine if a filesystem is case-sensitive."}
{"func_name": "rename", "func_src_before": "    def rename(self, ref, to_name):\n        \"\"\"Rename a local file or folder\n\n        Return the actualized info object.\n\n        \"\"\"\n        new_name = safe_filename(to_name)\n        source_os_path = self._abspath(ref)\n        parent = ref.rsplit(u'/', 1)[0]\n        old_name = ref.rsplit(u'/', 1)[1]\n        parent = u'/' if parent == '' else parent\n        locker = self.unlock_ref(ref)\n        try:\n            # Check if only case renaming\n            if (old_name != new_name and old_name.lower() == new_name.lower()\n                and not self.is_case_sensitive()):\n                # Must use a temp rename as FS is not case sensitive\n                temp_path = os.tempnam(self._abspath(parent),\n                                       LocalClient.CASE_RENAME_PREFIX + old_name + '_')\n                if AbstractOSIntegration.is_windows():\n                    import ctypes\n                    ctypes.windll.kernel32.SetFileAttributesW(\n                                                unicode(temp_path), 2)\n                os.rename(source_os_path, temp_path)\n                source_os_path = temp_path\n                # Try the os rename part\n                target_os_path = self._abspath(os.path.join(parent, new_name))\n            else:\n                target_os_path, new_name = self._abspath_deduped(parent,\n                                                                new_name, old_name)\n            if old_name != new_name:\n                os.rename(source_os_path, target_os_path)\n            if AbstractOSIntegration.is_windows():\n                import ctypes\n                # See http://msdn.microsoft.com/en-us/library/aa365535%28v=vs.85%29.aspx\n                ctypes.windll.kernel32.SetFileAttributesW(\n                                            unicode(target_os_path), 128)\n            new_ref = self.get_children_ref(parent, new_name)\n            return self.get_info(new_ref)\n        finally:\n            self.lock_ref(ref, locker & 2)", "func_src_after": "    def rename(self, ref, to_name):\n        \"\"\"Rename a local file or folder\n\n        Return the actualized info object.\n\n        \"\"\"\n        new_name = safe_filename(to_name)\n        source_os_path = self._abspath(ref)\n        parent = ref.rsplit(u'/', 1)[0]\n        old_name = ref.rsplit(u'/', 1)[1]\n        parent = u'/' if parent == '' else parent\n        locker = self.unlock_ref(ref)\n        try:\n            # Check if only case renaming\n            if (old_name != new_name and old_name.lower() == new_name.lower()\n                and not self.is_case_sensitive()):\n                # Must use a temp rename as FS is not case sensitive\n                prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n                _, temp_path = tempfile.mkstemp(dir=self._abspath(parent),\n                                                prefix=prefix)\n                if AbstractOSIntegration.is_windows():\n                    import ctypes\n                    ctypes.windll.kernel32.SetFileAttributesW(\n                                                unicode(temp_path), 2)\n                os.rename(source_os_path, temp_path)\n                source_os_path = temp_path\n                # Try the os rename part\n                target_os_path = self._abspath(os.path.join(parent, new_name))\n            else:\n                target_os_path, new_name = self._abspath_deduped(parent,\n                                                                new_name, old_name)\n            if old_name != new_name:\n                os.rename(source_os_path, target_os_path)\n            if AbstractOSIntegration.is_windows():\n                import ctypes\n                # See http://msdn.microsoft.com/en-us/library/aa365535%28v=vs.85%29.aspx\n                ctypes.windll.kernel32.SetFileAttributesW(\n                                            unicode(target_os_path), 128)\n            new_ref = self.get_children_ref(parent, new_name)\n            return self.get_info(new_ref)\n        finally:\n            self.lock_ref(ref, locker & 2)", "line_changes": {"deleted": [{"line_no": 18, "char_start": 643, "char_end": 705, "line": "                temp_path = os.tempnam(self._abspath(parent),\n"}, {"line_no": 19, "char_start": 705, "char_end": 793, "line": "                                       LocalClient.CASE_RENAME_PREFIX + old_name + '_')\n"}], "added": [{"line_no": 18, "char_start": 643, "char_end": 716, "line": "                prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n"}, {"line_no": 19, "char_start": 716, "char_end": 791, "line": "                _, temp_path = tempfile.mkstemp(dir=self._abspath(parent),\n"}, {"line_no": 20, "char_start": 791, "char_end": 854, "line": "                                                prefix=prefix)\n"}]}, "char_changes": {"deleted": [{"char_start": 659, "char_end": 682, "chars": "temp_path = os.tempnam("}, {"char_start": 744, "char_end": 791, "chars": "LocalClient.CASE_RENAME_PREFIX + old_name + '_'"}], "added": [{"char_start": 658, "char_end": 734, "chars": " prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n                _,"}, {"char_start": 751, "char_end": 768, "chars": "file.mkstemp(dir="}, {"char_start": 830, "char_end": 852, "chars": "         prefix=prefix"}]}, "commit_link": "github.com/arameshkumar/nuxeo-drive/commit/c131124daf80274ace04e8d68f90cef802ac66fa", "file_name": "local_client.py", "vul_type": "cwe-377", "commit_msg": "NXDRIVE-702: remove use of deprecated and insecure os.tempnam()", "description": "Write a Python function to rename a file or directory, handling case sensitivity on different file systems."}
{"func_name": "__init__", "func_src_before": "  def __init__(self, on_ui_exit=None, command_sequence=None):\n    readline_ui.ReadlineUI.__init__(\n        self, on_ui_exit=on_ui_exit,\n        config=cli_config.CLIConfig(config_file_path=tempfile.mktemp()))\n\n    self._command_sequence = command_sequence\n    self._command_counter = 0\n\n    self.observers = {\"screen_outputs\": []}", "func_src_after": "  def __init__(self, on_ui_exit=None, command_sequence=None):\n    _, config_file_path = tempfile.mkstemp()  # safe to ignore fd\n    readline_ui.ReadlineUI.__init__(\n        self,\n        on_ui_exit=on_ui_exit,\n        config=cli_config.CLIConfig(config_file_path=config_file_path))\n\n    self._command_sequence = command_sequence\n    self._command_counter = 0\n\n    self.observers = {\"screen_outputs\": []}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 99, "char_end": 136, "line": "        self, on_ui_exit=on_ui_exit,\n"}, {"line_no": 4, "char_start": 136, "char_end": 209, "line": "        config=cli_config.CLIConfig(config_file_path=tempfile.mktemp()))\n"}], "added": [{"line_no": 2, "char_start": 62, "char_end": 128, "line": "    _, config_file_path = tempfile.mkstemp()  # safe to ignore fd\n"}, {"line_no": 4, "char_start": 165, "char_end": 179, "line": "        self,\n"}, {"line_no": 5, "char_start": 179, "char_end": 210, "line": "        on_ui_exit=on_ui_exit,\n"}, {"line_no": 6, "char_start": 210, "char_end": 282, "line": "        config=cli_config.CLIConfig(config_file_path=config_file_path))\n"}]}, "char_changes": {"deleted": [{"char_start": 189, "char_end": 206, "chars": "tempfile.mktemp()"}], "added": [{"char_start": 62, "char_end": 128, "chars": "    _, config_file_path = tempfile.mkstemp()  # safe to ignore fd\n"}, {"char_start": 178, "char_end": 186, "chars": "\n       "}, {"char_start": 263, "char_end": 279, "chars": "config_file_path"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/599208b439e349f88c809e9d1f268c8c77718259", "file_name": "readline_ui_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359224\nChange-Id: I7bfc1df9cf931f45ec85d4878874ef41b9c55474", "description": "Create a Python class constructor that initializes a command-line interface with temporary configuration and allows for command sequence tracking and screen output observation."}
{"func_name": "test_dynamic_dependencies", "func_src_before": "    def test_dynamic_dependencies(self):\n\n        class DynamicRequires(Task):\n            p = luigi.Parameter()\n\n            def output(self):\n                return luigi.LocalTarget(os.path.join(self.p, 'parent'))\n\n            def run(self):\n                dummy_targets = yield [DynamicDummyTask(os.path.join(self.p, str(i)))\n                                     for i in range(5)]\n                dummy_targets += yield [DynamicDummyTask(os.path.join(self.p, str(i)))\n                                       for i in range(5, 7)]\n                with self.output().open('w') as f:\n                    for i, d in enumerate(dummy_targets):\n                        for line in d.open('r'):\n                            print >>f, '%d: %s' % (i, line.strip())\n\n        t = DynamicRequires(p=tempfile.mktemp())\n        luigi.build([t], local_scheduler=True)\n        self.assertTrue(t.complete())\n\n        # loop through output and verify\n        f = t.output().open('r')\n        for i in xrange(7):\n            self.assertEqual(f.readline().strip(), '%d: Done!' % i)", "func_src_after": "    def test_dynamic_dependencies(self):\n\n        class DynamicRequires(Task):\n            p = luigi.Parameter()\n\n            def output(self):\n                return luigi.LocalTarget(os.path.join(self.p, 'parent'))\n\n            def run(self):\n                dummy_targets = yield [DynamicDummyTask(os.path.join(self.p, str(i)))\n                                     for i in range(5)]\n                dummy_targets += yield [DynamicDummyTask(os.path.join(self.p, str(i)))\n                                       for i in range(5, 7)]\n                with self.output().open('w') as f:\n                    for i, d in enumerate(dummy_targets):\n                        for line in d.open('r'):\n                            print >>f, '%d: %s' % (i, line.strip())\n\n        p = tempfile.mkdtemp()\n        try:\n            t = DynamicRequires(p=p)\n            luigi.build([t], local_scheduler=True)\n            self.assertTrue(t.complete())\n\n            # loop through output and verify\n            f = t.output().open('r')\n            for i in xrange(7):\n                self.assertEqual(f.readline().strip(), '%d: Done!' % i)\n        finally:\n            shutil.rmtree(p)", "line_changes": {"deleted": [{"line_no": 19, "char_start": 762, "char_end": 811, "line": "        t = DynamicRequires(p=tempfile.mktemp())\n"}, {"line_no": 20, "char_start": 811, "char_end": 858, "line": "        luigi.build([t], local_scheduler=True)\n"}, {"line_no": 21, "char_start": 858, "char_end": 896, "line": "        self.assertTrue(t.complete())\n"}, {"line_no": 22, "char_start": 896, "char_end": 897, "line": "\n"}, {"line_no": 24, "char_start": 938, "char_end": 971, "line": "        f = t.output().open('r')\n"}, {"line_no": 25, "char_start": 971, "char_end": 999, "line": "        for i in xrange(7):\n"}, {"line_no": 26, "char_start": 999, "char_end": 1066, "line": "            self.assertEqual(f.readline().strip(), '%d: Done!' % i)\n"}], "added": [{"line_no": 19, "char_start": 762, "char_end": 793, "line": "        p = tempfile.mkdtemp()\n"}, {"line_no": 20, "char_start": 793, "char_end": 806, "line": "        try:\n"}, {"line_no": 21, "char_start": 806, "char_end": 843, "line": "            t = DynamicRequires(p=p)\n"}, {"line_no": 22, "char_start": 843, "char_end": 894, "line": "            luigi.build([t], local_scheduler=True)\n"}, {"line_no": 23, "char_start": 894, "char_end": 936, "line": "            self.assertTrue(t.complete())\n"}, {"line_no": 24, "char_start": 936, "char_end": 937, "line": "\n"}, {"line_no": 26, "char_start": 982, "char_end": 1019, "line": "            f = t.output().open('r')\n"}, {"line_no": 27, "char_start": 1019, "char_end": 1051, "line": "            for i in xrange(7):\n"}, {"line_no": 28, "char_start": 1051, "char_end": 1123, "line": "                self.assertEqual(f.readline().strip(), '%d: Done!' % i)\n"}, {"line_no": 29, "char_start": 1123, "char_end": 1140, "line": "        finally:\n"}, {"line_no": 30, "char_start": 1140, "char_end": 1168, "line": "            shutil.rmtree(p)\n"}]}, "char_changes": {"deleted": [{"char_start": 792, "char_end": 811, "chars": "tempfile.mktemp())\n"}], "added": [{"char_start": 762, "char_end": 810, "chars": "        p = tempfile.mkdtemp()\n        try:\n    "}, {"char_start": 840, "char_end": 846, "chars": "p)\n   "}, {"char_start": 854, "char_end": 855, "chars": " "}, {"char_start": 902, "char_end": 906, "chars": "    "}, {"char_start": 937, "char_end": 941, "chars": "    "}, {"char_start": 990, "char_end": 994, "chars": "    "}, {"char_start": 1019, "char_end": 1023, "chars": "    "}, {"char_start": 1063, "char_end": 1067, "chars": "    "}, {"char_start": 1122, "char_end": 1168, "chars": "\n        finally:\n            shutil.rmtree(p)"}]}, "commit_link": "github.com/hadesbox/luigi/commit/8d47de4c1e2849e13c98f6dc52088f9a672b3944", "file_name": "worker_test.py", "vul_type": "cwe-377", "commit_msg": "Possible flakiness fix for test_dynamic_dependencies\n\ntest_dynamic_dependencies has been flaky lately (see #571). I can't reproduce it\nlocally, so I assume it has something to do with Travis running things more in\nparallel than I do. test_dynamic_dependencies uses mktemp, which according to\nthe tempfile documentation should not be used and causes race conditions in\nwhich another program may get access to the temp file first. Replacing this with\nmkdtemp should fix the issue while also clarifying that a temporary directory is\nwanted and not a temporary file. I haven't been able to verify that this will\nfix the flakiness, but it should probably be done anyway. I also added cleanup.", "description": "Write a Python function using Luigi to dynamically generate and process tasks, then verify the output."}
{"func_name": "plot", "func_src_before": "    def plot(self, log_files, sort='time', limit=10, nfl_filter='',\n             metric_selected='cc', plot_type='bar'):\n        if not PLOTLIB_INSTALLED:\n            raise PLOTLIBNotInstalled(_('python-matplotlib not installed.'))\n        if len(log_files) == 0:\n            raise NotFoundException(_('no log file found'))\n        try:\n            stats = Stats2(*log_files)\n            stats.sort_stats(sort)\n            stats_dict = stats.stats\n            __, func_list = stats.get_print_list([nfl_filter, limit])\n            nfls = []\n            performance = []\n            names = {'nc': 'Total Call Count', 'cc': 'Primitive Call Count',\n                     'tt': 'Total Time', 'ct': 'Cumulative Time'}\n            for func in func_list:\n                cc, nc, tt, ct, __ = stats_dict[func]\n                metric = {'cc': cc, 'nc': nc, 'tt': tt, 'ct': ct}\n                nfls.append(func[2])\n                performance.append(metric[metric_selected])\n            y_pos = range(len(nfls))\n            error = [random.random() for __ in y_pos]\n            plt.clf()\n            if plot_type == 'pie':\n                plt.pie(x=performance, explode=None, labels=nfls,\n                        autopct='%1.1f%%')\n            else:\n                plt.barh(y_pos, performance, xerr=error, align='center',\n                         alpha=0.4)\n                plt.yticks(y_pos, nfls)\n                plt.xlabel(names[metric_selected])\n            plt.title('Profile Statistics (by %s)' % names[metric_selected])\n            #plt.gcf().tight_layout(pad=1.2)\n            profile_img = tempfile.mktemp('.png', 'plot')\n            plt.savefig(profile_img, dpi=300)\n            data = open(profile_img).read()\n            os.remove(profile_img)\n            return data, [('content-type', 'image/jpg')]\n        except Exception as ex:\n            raise ProfileException(_('plotting results failed due to %s') % ex)", "func_src_after": "    def plot(self, log_files, sort='time', limit=10, nfl_filter='',\n             metric_selected='cc', plot_type='bar'):\n        if not PLOTLIB_INSTALLED:\n            raise PLOTLIBNotInstalled(_('python-matplotlib not installed.'))\n        if len(log_files) == 0:\n            raise NotFoundException(_('no log file found'))\n        try:\n            stats = Stats2(*log_files)\n            stats.sort_stats(sort)\n            stats_dict = stats.stats\n            __, func_list = stats.get_print_list([nfl_filter, limit])\n            nfls = []\n            performance = []\n            names = {'nc': 'Total Call Count', 'cc': 'Primitive Call Count',\n                     'tt': 'Total Time', 'ct': 'Cumulative Time'}\n            for func in func_list:\n                cc, nc, tt, ct, __ = stats_dict[func]\n                metric = {'cc': cc, 'nc': nc, 'tt': tt, 'ct': ct}\n                nfls.append(func[2])\n                performance.append(metric[metric_selected])\n            y_pos = range(len(nfls))\n            error = [random.random() for __ in y_pos]\n            plt.clf()\n            if plot_type == 'pie':\n                plt.pie(x=performance, explode=None, labels=nfls,\n                        autopct='%1.1f%%')\n            else:\n                plt.barh(y_pos, performance, xerr=error, align='center',\n                         alpha=0.4)\n                plt.yticks(y_pos, nfls)\n                plt.xlabel(names[metric_selected])\n            plt.title('Profile Statistics (by %s)' % names[metric_selected])\n            #plt.gcf().tight_layout(pad=1.2)\n            profile_img = tempfile.TemporaryFile()\n            plt.savefig(profile_img, format='png', dpi=300)\n            profile_img.seek(0)\n            data = profile_img.read()\n            os.close(profile_img)\n            return data, [('content-type', 'image/jpg')]\n        except Exception as ex:\n            raise ProfileException(_('plotting results failed due to %s') % ex)", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1561, "char_end": 1619, "line": "            profile_img = tempfile.mktemp('.png', 'plot')\n"}, {"line_no": 35, "char_start": 1619, "char_end": 1665, "line": "            plt.savefig(profile_img, dpi=300)\n"}, {"line_no": 36, "char_start": 1665, "char_end": 1709, "line": "            data = open(profile_img).read()\n"}, {"line_no": 37, "char_start": 1709, "char_end": 1744, "line": "            os.remove(profile_img)\n"}], "added": [{"line_no": 34, "char_start": 1561, "char_end": 1612, "line": "            profile_img = tempfile.TemporaryFile()\n"}, {"line_no": 35, "char_start": 1612, "char_end": 1672, "line": "            plt.savefig(profile_img, format='png', dpi=300)\n"}, {"line_no": 36, "char_start": 1672, "char_end": 1704, "line": "            profile_img.seek(0)\n"}, {"line_no": 37, "char_start": 1704, "char_end": 1742, "line": "            data = profile_img.read()\n"}, {"line_no": 38, "char_start": 1742, "char_end": 1776, "line": "            os.close(profile_img)\n"}]}, "char_changes": {"deleted": [{"char_start": 1596, "char_end": 1617, "chars": "mktemp('.png', 'plot'"}, {"char_start": 1684, "char_end": 1689, "chars": "open("}, {"char_start": 1700, "char_end": 1701, "chars": ")"}, {"char_start": 1724, "char_end": 1729, "chars": "remov"}], "added": [{"char_start": 1596, "char_end": 1610, "chars": "TemporaryFile("}, {"char_start": 1648, "char_end": 1662, "chars": " format='png',"}, {"char_start": 1672, "char_end": 1704, "chars": "            profile_img.seek(0)\n"}, {"char_start": 1757, "char_end": 1761, "chars": "clos"}]}, "commit_link": "github.com/scality/ScalitySproxydSwift/commit/6978275cdb04bb08aaf142d401b52a46527dac4c", "file_name": "html_viewer.py", "vul_type": "cwe-377", "commit_msg": "Avoid usage of insecure mktemp() function\n\nThis patch eliminates the use of the deprecated and insecure\ntempfile.mktemp() function.  It has been replaced with secure\nalternatives where temporary files are actually required.\n\nChange-Id: I0a13d6d44cd1abc4b66fa33f39eea407617a01d5\nSecurityImpact\nCloses-bug: #1348869", "description": "In Python, write a function to plot performance statistics from log files using matplotlib, with options for sorting, filtering, and different plot types."}
{"func_name": "main", "func_src_before": "def main(_):\n  # Create a dummy dataset.\n  num_examples = 8\n  steps_per_epoch = 2\n  input_dims = 3\n  output_dims = 1\n  xs = np.zeros([num_examples, input_dims])\n  ys = np.zeros([num_examples, output_dims])\n  dataset = tf.data.Dataset.from_tensor_slices(\n      (xs, ys)).repeat(num_examples).batch(int(num_examples / steps_per_epoch))\n\n  sess = tf.Session()\n  if FLAGS.debug:\n    # Use the command-line interface (CLI) of tfdbg.\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    # Use the TensorBoard Debugger Plugin (GUI of tfdbg).\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n  tf.keras.backend.set_session(sess)\n\n  # Create a dummy model.\n  model = tf.keras.Sequential(\n      [tf.keras.layers.Dense(1, input_shape=[input_dims])])\n  model.compile(loss=\"mse\", optimizer=\"sgd\")\n\n  # Train the model using the dummy dataset created above.\n  model.fit(dataset, epochs=FLAGS.epochs, steps_per_epoch=steps_per_epoch)", "func_src_after": "def main(_):\n  # Create a dummy dataset.\n  num_examples = 8\n  steps_per_epoch = 2\n  input_dims = 3\n  output_dims = 1\n  xs = np.zeros([num_examples, input_dims])\n  ys = np.zeros([num_examples, output_dims])\n  dataset = tf.data.Dataset.from_tensor_slices(\n      (xs, ys)).repeat(num_examples).batch(int(num_examples / steps_per_epoch))\n\n  sess = tf.Session()\n  if FLAGS.debug:\n    # Use the command-line interface (CLI) of tfdbg.\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    # Use the TensorBoard Debugger Plugin (GUI of tfdbg).\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n  tf.keras.backend.set_session(sess)\n\n  # Create a dummy model.\n  model = tf.keras.Sequential(\n      [tf.keras.layers.Dense(1, input_shape=[input_dims])])\n  model.compile(loss=\"mse\", optimizer=\"sgd\")\n\n  # Train the model using the dummy dataset created above.\n  model.fit(dataset, epochs=FLAGS.epochs, steps_per_epoch=steps_per_epoch)", "line_changes": {"deleted": [{"line_no": 15, "char_start": 428, "char_end": 453, "line": "    config_file_path = (\n"}, {"line_no": 16, "char_start": 453, "char_end": 494, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 17, "char_start": 494, "char_end": 545, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 15, "char_start": 428, "char_end": 465, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 17, "char_start": 518, "char_end": 580, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 18, "char_start": 580, "char_end": 590, "line": "    else:\n"}, {"line_no": 19, "char_start": 590, "char_end": 620, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 450, "char_end": 460, "chars": " (\n       "}, {"char_start": 498, "char_end": 522, "chars": "    if FLAGS.use_random_"}, {"char_start": 534, "char_end": 538, "chars": "else"}, {"char_start": 543, "char_end": 544, "chars": ")"}], "added": [{"char_start": 432, "char_end": 527, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 557, "char_end": 558, "chars": "s"}, {"char_start": 584, "char_end": 596, "chars": "else:\n      "}, {"char_start": 603, "char_end": 608, "chars": "file_"}, {"char_start": 613, "char_end": 614, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/4b50e584962179c978227a5c534dcd8146e03e6f", "file_name": "debug_keras.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359138\nChange-Id: I8afc97448b1e730ac5883c2033f3b0e544b8fb58", "description": "Write a Python script using TensorFlow to create and train a dummy model with a dataset, including optional debug configurations."}
{"func_name": "setUp", "func_src_before": "  def setUp(self):\n    self._dump_root = tempfile.mktemp()\n    os.mkdir(self._dump_root)", "func_src_after": "  def setUp(self):\n    self._dump_root = tempfile.mkdtemp()", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 59, "line": "    self._dump_root = tempfile.mktemp()\n"}, {"line_no": 3, "char_start": 59, "char_end": 88, "line": "    os.mkdir(self._dump_root)\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 59, "line": "    self._dump_root = tempfile.mkdtemp()\n"}]}, "char_changes": {"deleted": [{"char_start": 58, "char_end": 88, "chars": "\n    os.mkdir(self._dump_root)"}], "added": [{"char_start": 52, "char_end": 53, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/9b1fe8f31ee1788208d8d6b7385382e436c5e1d7", "file_name": "debug_data_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp` to create directories.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do: just a name change\n\nPiperOrigin-RevId: 420370858\nChange-Id: I44a0849d161132eacd4f3881fdb615e09c0f02a2", "description": "Write a Python function called `setUp` that initializes a temporary directory for storing data."}
{"func_name": "main", "func_src_before": "def main(_):\n  sess = tf.Session()\n\n  # Construct the TensorFlow network.\n  ph_float = tf.placeholder(tf.float32, name=\"ph_float\")\n  x = tf.transpose(ph_float, name=\"x\")\n  v = tf.Variable(np.array([[-2.0], [-3.0], [6.0]], dtype=np.float32), name=\"v\")\n  m = tf.constant(\n      np.array([[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]]),\n      dtype=tf.float32,\n      name=\"m\")\n  y = tf.matmul(m, x, name=\"y\")\n  z = tf.matmul(m, v, name=\"z\")\n\n  if FLAGS.debug:\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n\n  if FLAGS.error == \"shape_mismatch\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0], [1.0], [2.0]])}))\n  elif FLAGS.error == \"uninitialized_variable\":\n    print(sess.run(z))\n  elif FLAGS.error == \"no_error\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0, 1.0, 2.0]])}))\n  else:\n    raise ValueError(\"Unrecognized error type: \" + FLAGS.error)", "func_src_after": "def main(_):\n  sess = tf.Session()\n\n  # Construct the TensorFlow network.\n  ph_float = tf.placeholder(tf.float32, name=\"ph_float\")\n  x = tf.transpose(ph_float, name=\"x\")\n  v = tf.Variable(np.array([[-2.0], [-3.0], [6.0]], dtype=np.float32), name=\"v\")\n  m = tf.constant(\n      np.array([[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]]),\n      dtype=tf.float32,\n      name=\"m\")\n  y = tf.matmul(m, x, name=\"y\")\n  z = tf.matmul(m, v, name=\"z\")\n\n  if FLAGS.debug:\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n\n  if FLAGS.error == \"shape_mismatch\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0], [1.0], [2.0]])}))\n  elif FLAGS.error == \"uninitialized_variable\":\n    print(sess.run(z))\n  elif FLAGS.error == \"no_error\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0, 1.0, 2.0]])}))\n  else:\n    raise ValueError(\"Unrecognized error type: \" + FLAGS.error)", "line_changes": {"deleted": [{"line_no": 16, "char_start": 447, "char_end": 472, "line": "    config_file_path = (\n"}, {"line_no": 17, "char_start": 472, "char_end": 513, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 18, "char_start": 513, "char_end": 564, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 16, "char_start": 447, "char_end": 484, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 18, "char_start": 537, "char_end": 599, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 19, "char_start": 599, "char_end": 609, "line": "    else:\n"}, {"line_no": 20, "char_start": 609, "char_end": 639, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 469, "char_end": 479, "chars": " (\n       "}, {"char_start": 517, "char_end": 541, "chars": "    if FLAGS.use_random_"}, {"char_start": 553, "char_end": 557, "chars": "else"}, {"char_start": 562, "char_end": 563, "chars": ")"}], "added": [{"char_start": 451, "char_end": 546, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 576, "char_end": 577, "chars": "s"}, {"char_start": 603, "char_end": 615, "chars": "else:\n      "}, {"char_start": 622, "char_end": 627, "chars": "file_"}, {"char_start": 632, "char_end": 633, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/c84353cb969723e97b9fd9fd346860bc15864bb7", "file_name": "debug_errors.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359156\nChange-Id: I992e93ed8423eef87bfcfc84b0c877131d6f916d", "description": "Write a Python script using TensorFlow to construct a neural network, run sessions with different error scenarios, and optionally enable debugging."}
{"func_name": "testLoadingPythonSourceFileWithNonAsciiChars", "func_src_before": "  def testLoadingPythonSourceFileWithNonAsciiChars(self):\n    source_path = tempfile.mktemp()\n    with open(source_path, \"wb\") as source_file:\n      source_file.write(u\"print('\\U0001f642')\\n\".encode(\"utf-8\"))\n    source_lines, _ = source_utils.load_source(source_path)\n    self.assertEqual(source_lines, [u\"print('\\U0001f642')\", u\"\"])\n    # Clean up unrelated source file.\n    os.remove(source_path)", "func_src_after": "  def testLoadingPythonSourceFileWithNonAsciiChars(self):\n    fd, source_path = tempfile.mkstemp()\n    with open(fd, \"wb\") as source_file:\n      source_file.write(u\"print('\\U0001f642')\\n\".encode(\"utf-8\"))\n    source_lines, _ = source_utils.load_source(source_path)\n    self.assertEqual(source_lines, [u\"print('\\U0001f642')\", u\"\"])\n    # Clean up unrelated source file.\n    os.remove(source_path)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 58, "char_end": 94, "line": "    source_path = tempfile.mktemp()\n"}, {"line_no": 3, "char_start": 94, "char_end": 143, "line": "    with open(source_path, \"wb\") as source_file:\n"}], "added": [{"line_no": 2, "char_start": 58, "char_end": 99, "line": "    fd, source_path = tempfile.mkstemp()\n"}, {"line_no": 3, "char_start": 99, "char_end": 139, "line": "    with open(fd, \"wb\") as source_file:\n"}]}, "char_changes": {"deleted": [{"char_start": 108, "char_end": 119, "chars": "source_path"}], "added": [{"char_start": 61, "char_end": 65, "chars": " fd,"}, {"char_start": 91, "char_end": 92, "chars": "s"}, {"char_start": 113, "char_end": 115, "chars": "fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/3752cc4c6ba6b69f04f857c6047adde9e8487bd6", "file_name": "source_utils_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359237\nChange-Id: I7fa45e888deff612ca53a4f8610cfad8f28e9671", "description": "Write a Python function to test loading a temporary source file containing non-ASCII characters and then remove the file."}
{"func_name": "captureWritesToStream", "func_src_before": "  @contextlib.contextmanager\n  def captureWritesToStream(self, stream):\n    \"\"\"A context manager that captures the writes to a given stream.\n\n    This context manager captures all writes to a given stream inside of a\n    `CapturedWrites` object. When this context manager is created, it yields\n    the `CapturedWrites` object. The captured contents can be accessed  by\n    calling `.contents()` on the `CapturedWrites`.\n\n    For this function to work, the stream must have a file descriptor that\n    can be modified using `os.dup` and `os.dup2`, and the stream must support\n    a `.flush()` method. The default python sys.stdout and sys.stderr are\n    examples of this. Note that this does not work in Colab or Jupyter\n    notebooks, because those use alternate stdout streams.\n\n    Example:\n    ```python\n    class MyOperatorTest(test_util.TensorFlowTestCase):\n      def testMyOperator(self):\n        input = [1.0, 2.0, 3.0, 4.0, 5.0]\n        with self.captureWritesToStream(sys.stdout) as captured:\n          result = MyOperator(input).eval()\n        self.assertStartsWith(captured.contents(), \"This was printed.\")\n    ```\n\n    Args:\n      stream: The stream whose writes should be captured. This stream must have\n        a file descriptor, support writing via using that file descriptor, and\n        must have a `.flush()` method.\n\n    Yields:\n      A `CapturedWrites` object that contains all writes to the specified stream\n      made during this context.\n    \"\"\"\n    stream.flush()\n    fd = stream.fileno()\n    tmp_file_path = tempfile.mktemp(dir=self.get_temp_dir())\n    tmp_file = open(tmp_file_path, \"w\")\n    orig_fd = os.dup(fd)\n    os.dup2(tmp_file.fileno(), fd)\n    try:\n      yield CapturedWrites(tmp_file_path)\n    finally:\n      tmp_file.close()\n      os.dup2(orig_fd, fd)", "func_src_after": "  @contextlib.contextmanager\n  def captureWritesToStream(self, stream):\n    \"\"\"A context manager that captures the writes to a given stream.\n\n    This context manager captures all writes to a given stream inside of a\n    `CapturedWrites` object. When this context manager is created, it yields\n    the `CapturedWrites` object. The captured contents can be accessed  by\n    calling `.contents()` on the `CapturedWrites`.\n\n    For this function to work, the stream must have a file descriptor that\n    can be modified using `os.dup` and `os.dup2`, and the stream must support\n    a `.flush()` method. The default python sys.stdout and sys.stderr are\n    examples of this. Note that this does not work in Colab or Jupyter\n    notebooks, because those use alternate stdout streams.\n\n    Example:\n    ```python\n    class MyOperatorTest(test_util.TensorFlowTestCase):\n      def testMyOperator(self):\n        input = [1.0, 2.0, 3.0, 4.0, 5.0]\n        with self.captureWritesToStream(sys.stdout) as captured:\n          result = MyOperator(input).eval()\n        self.assertStartsWith(captured.contents(), \"This was printed.\")\n    ```\n\n    Args:\n      stream: The stream whose writes should be captured. This stream must have\n        a file descriptor, support writing via using that file descriptor, and\n        must have a `.flush()` method.\n\n    Yields:\n      A `CapturedWrites` object that contains all writes to the specified stream\n      made during this context.\n    \"\"\"\n    stream.flush()\n    fd = stream.fileno()\n    tmp_file, tmp_file_path = tempfile.mkstemp(dir=self.get_temp_dir())\n    orig_fd = os.dup(fd)\n    os.dup2(tmp_file, fd)\n    try:\n      yield CapturedWrites(tmp_file_path)\n    finally:\n      os.close(tmp_file)\n      os.dup2(orig_fd, fd)", "line_changes": {"deleted": [{"line_no": 37, "char_start": 1512, "char_end": 1573, "line": "    tmp_file_path = tempfile.mktemp(dir=self.get_temp_dir())\n"}, {"line_no": 38, "char_start": 1573, "char_end": 1613, "line": "    tmp_file = open(tmp_file_path, \"w\")\n"}, {"line_no": 40, "char_start": 1638, "char_end": 1673, "line": "    os.dup2(tmp_file.fileno(), fd)\n"}, {"line_no": 44, "char_start": 1737, "char_end": 1760, "line": "      tmp_file.close()\n"}], "added": [{"line_no": 37, "char_start": 1512, "char_end": 1584, "line": "    tmp_file, tmp_file_path = tempfile.mkstemp(dir=self.get_temp_dir())\n"}, {"line_no": 39, "char_start": 1609, "char_end": 1635, "line": "    os.dup2(tmp_file, fd)\n"}, {"line_no": 43, "char_start": 1699, "char_end": 1724, "line": "      os.close(tmp_file)\n"}]}, "char_changes": {"deleted": [{"char_start": 1573, "char_end": 1613, "chars": "    tmp_file = open(tmp_file_path, \"w\")\n"}, {"char_start": 1658, "char_end": 1667, "chars": ".fileno()"}, {"char_start": 1751, "char_end": 1758, "chars": ".close("}], "added": [{"char_start": 1515, "char_end": 1525, "chars": " tmp_file,"}, {"char_start": 1553, "char_end": 1554, "chars": "s"}, {"char_start": 1705, "char_end": 1714, "chars": "os.close("}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/fb40e84a1d694ebd2203d0f9846b8b9004dccce0", "file_name": "test_util.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420355705\nChange-Id: If437973d0cd7686a221679d4123cb12f79697fe0", "description": "Create a Python function that captures all output written to a specified stream using a context manager."}
{"func_name": "move", "func_src_before": "def move(path, dest, replace=False):\n    \"\"\"Rename a file. `dest` may not be a directory. If `dest` already\n    exists, raises an OSError unless `replace` is True. Has no effect if\n    `path` is the same as `dest`. If the paths are on different\n    filesystems (or the rename otherwise fails), a copy is attempted\n    instead, in which case metadata will *not* be preserved. Paths are\n    translated to system paths.\n    \"\"\"\n    if os.path.isdir(path):\n        raise FilesystemError(u'source is directory', 'move', (path, dest))\n    if os.path.isdir(dest):\n        raise FilesystemError(u'destination is directory', 'move',\n                              (path, dest))\n    if samefile(path, dest):\n        return\n    path = syspath(path)\n    dest = syspath(dest)\n    if os.path.exists(dest) and not replace:\n        raise FilesystemError('file exists', 'rename', (path, dest))\n\n    # First, try renaming the file.\n    try:\n        os.replace(path, dest)\n    except OSError:\n        tmp = tempfile.mktemp(suffix='.beets',\n                              prefix=py3_path(b'.' + os.path.basename(dest)),\n                              dir=py3_path(os.path.dirname(dest)))\n        tmp = syspath(tmp)\n        try:\n            shutil.copyfile(path, tmp)\n            os.replace(tmp, dest)\n            tmp = None\n            os.remove(path)\n        except OSError as exc:\n            raise FilesystemError(exc, 'move', (path, dest),\n                                  traceback.format_exc())\n        finally:\n            if tmp is not None:\n                os.remove(tmp)", "func_src_after": "def move(path, dest, replace=False):\n    \"\"\"Rename a file. `dest` may not be a directory. If `dest` already\n    exists, raises an OSError unless `replace` is True. Has no effect if\n    `path` is the same as `dest`. If the paths are on different\n    filesystems (or the rename otherwise fails), a copy is attempted\n    instead, in which case metadata will *not* be preserved. Paths are\n    translated to system paths.\n    \"\"\"\n    if os.path.isdir(path):\n        raise FilesystemError(u'source is directory', 'move', (path, dest))\n    if os.path.isdir(dest):\n        raise FilesystemError(u'destination is directory', 'move',\n                              (path, dest))\n    if samefile(path, dest):\n        return\n    path = syspath(path)\n    dest = syspath(dest)\n    if os.path.exists(dest) and not replace:\n        raise FilesystemError('file exists', 'rename', (path, dest))\n\n    # First, try renaming the file.\n    try:\n        os.replace(path, dest)\n    except OSError:\n        # Copy the file to a temporary destination.\n        tmp = tempfile.NamedTemporaryFile(suffix=b'.beets',\n                                          prefix=b'.' + os.path.basename(dest),\n                                          dir=os.path.dirname(dest),\n                                          delete=False)\n        try:\n            with open(path, 'rb') as f:\n                shutil.copyfileobj(f, tmp)\n        finally:\n            tmp.close()\n\n        # Move the copied file into place.\n        try:\n            os.replace(tmp.name, dest)\n            tmp = None\n            os.remove(path)\n        except OSError as exc:\n            raise FilesystemError(exc, 'move', (path, dest),\n                                  traceback.format_exc())\n        finally:\n            if tmp is not None:\n                os.remove(tmp)", "line_changes": {"deleted": [{"line_no": 25, "char_start": 973, "char_end": 1020, "line": "        tmp = tempfile.mktemp(suffix='.beets',\n"}, {"line_no": 26, "char_start": 1020, "char_end": 1098, "line": "                              prefix=py3_path(b'.' + os.path.basename(dest)),\n"}, {"line_no": 27, "char_start": 1098, "char_end": 1165, "line": "                              dir=py3_path(os.path.dirname(dest)))\n"}, {"line_no": 28, "char_start": 1165, "char_end": 1192, "line": "        tmp = syspath(tmp)\n"}, {"line_no": 30, "char_start": 1205, "char_end": 1244, "line": "            shutil.copyfile(path, tmp)\n"}, {"line_no": 31, "char_start": 1244, "char_end": 1278, "line": "            os.replace(tmp, dest)\n"}], "added": [{"line_no": 26, "char_start": 1025, "char_end": 1085, "line": "        tmp = tempfile.NamedTemporaryFile(suffix=b'.beets',\n"}, {"line_no": 27, "char_start": 1085, "char_end": 1165, "line": "                                          prefix=b'.' + os.path.basename(dest),\n"}, {"line_no": 28, "char_start": 1165, "char_end": 1234, "line": "                                          dir=os.path.dirname(dest),\n"}, {"line_no": 29, "char_start": 1234, "char_end": 1290, "line": "                                          delete=False)\n"}, {"line_no": 31, "char_start": 1303, "char_end": 1343, "line": "            with open(path, 'rb') as f:\n"}, {"line_no": 32, "char_start": 1343, "char_end": 1386, "line": "                shutil.copyfileobj(f, tmp)\n"}, {"line_no": 33, "char_start": 1386, "char_end": 1403, "line": "        finally:\n"}, {"line_no": 34, "char_start": 1403, "char_end": 1427, "line": "            tmp.close()\n"}, {"line_no": 35, "char_start": 1427, "char_end": 1428, "line": "\n"}, {"line_no": 37, "char_start": 1471, "char_end": 1484, "line": "        try:\n"}, {"line_no": 38, "char_start": 1484, "char_end": 1523, "line": "            os.replace(tmp.name, dest)\n"}]}, "char_changes": {"deleted": [{"char_start": 981, "char_end": 1002, "chars": "tmp = tempfile.mktemp"}, {"char_start": 1050, "char_end": 1066, "chars": "prefix=py3_path("}, {"char_start": 1095, "char_end": 1096, "chars": ")"}, {"char_start": 1132, "char_end": 1141, "chars": "py3_path("}, {"char_start": 1162, "char_end": 1164, "chars": "))"}, {"char_start": 1173, "char_end": 1243, "chars": "tmp = syspath(tmp)\n        try:\n            shutil.copyfile(path, tmp)"}], "added": [{"char_start": 981, "char_end": 1066, "chars": "# Copy the file to a temporary destination.\n        tmp = tempfile.NamedTemporaryFile"}, {"char_start": 1074, "char_end": 1075, "chars": "b"}, {"char_start": 1115, "char_end": 1134, "chars": "            prefix="}, {"char_start": 1165, "char_end": 1177, "chars": "            "}, {"char_start": 1232, "char_end": 1233, "chars": ","}, {"char_start": 1242, "char_end": 1483, "chars": "                                  delete=False)\n        try:\n            with open(path, 'rb') as f:\n                shutil.copyfileobj(f, tmp)\n        finally:\n            tmp.close()\n\n        # Move the copied file into place.\n        try:"}, {"char_start": 1510, "char_end": 1515, "chars": ".name"}]}, "commit_link": "github.com/beetbox/beets/commit/4bb695bcdbada9c8153442688e8494199f015f04", "file_name": "__init__.py", "vul_type": "cwe-377", "commit_msg": "Fix copying for atomic file moves\n\nFixes #4168. Also closes #4192, which it supersedes.\n\nThe original problem is that this implementation used bytestrings\nincorrectly to invoke `mktemp`. However, `mktemp` is deprecated, so this\nPR just avoids it altogether. Fortunately, the non-deprecated APIs in\n`tempfile` support all-bytes arguments.", "description": "Write a Python function named `move` that renames a file, with an option to replace the destination file if it already exists."}
{"func_name": "setUp", "func_src_before": "  def setUp(self):\n    self._tmp_dir = tempfile.mktemp()\n\n    self.v = variables.VariableV1(10.0, name=\"v\")\n    self.w = variables.VariableV1(21.0, name=\"w\")\n    self.delta = constant_op.constant(1.0, name=\"delta\")\n    self.inc_v = state_ops.assign_add(self.v, self.delta, name=\"inc_v\")\n\n    self.w_int = control_flow_ops.with_dependencies(\n        [self.inc_v],\n        math_ops.cast(self.w, dtypes.int32, name=\"w_int_inner\"),\n        name=\"w_int_outer\")\n\n    self.ph = array_ops.placeholder(dtypes.float32, name=\"ph\")\n    self.xph = array_ops.transpose(self.ph, name=\"xph\")\n    self.m = constant_op.constant(\n        [[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]], dtype=dtypes.float32, name=\"m\")\n    self.y = math_ops.matmul(self.m, self.xph, name=\"y\")\n\n    self.sparse_ph = array_ops.sparse_placeholder(\n        dtypes.float32, shape=([5, 5]), name=\"sparse_placeholder\")\n    self.sparse_add = sparse_ops.sparse_add(self.sparse_ph, self.sparse_ph)\n\n    rewriter_config = rewriter_config_pb2.RewriterConfig(\n        disable_model_pruning=True,\n        arithmetic_optimization=rewriter_config_pb2.RewriterConfig.OFF,\n        dependency_optimization=rewriter_config_pb2.RewriterConfig.OFF)\n    graph_options = config_pb2.GraphOptions(rewrite_options=rewriter_config)\n    config_proto = config_pb2.ConfigProto(graph_options=graph_options)\n    self.sess = session.Session(config=config_proto)\n\n    # Initialize variable.\n    self.sess.run(variables.global_variables_initializer())", "func_src_after": "  def setUp(self):\n    self._tmp_dir = tempfile.mkdtemp()\n\n    self.v = variables.VariableV1(10.0, name=\"v\")\n    self.w = variables.VariableV1(21.0, name=\"w\")\n    self.delta = constant_op.constant(1.0, name=\"delta\")\n    self.inc_v = state_ops.assign_add(self.v, self.delta, name=\"inc_v\")\n\n    self.w_int = control_flow_ops.with_dependencies(\n        [self.inc_v],\n        math_ops.cast(self.w, dtypes.int32, name=\"w_int_inner\"),\n        name=\"w_int_outer\")\n\n    self.ph = array_ops.placeholder(dtypes.float32, name=\"ph\")\n    self.xph = array_ops.transpose(self.ph, name=\"xph\")\n    self.m = constant_op.constant(\n        [[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]], dtype=dtypes.float32, name=\"m\")\n    self.y = math_ops.matmul(self.m, self.xph, name=\"y\")\n\n    self.sparse_ph = array_ops.sparse_placeholder(\n        dtypes.float32, shape=([5, 5]), name=\"sparse_placeholder\")\n    self.sparse_add = sparse_ops.sparse_add(self.sparse_ph, self.sparse_ph)\n\n    rewriter_config = rewriter_config_pb2.RewriterConfig(\n        disable_model_pruning=True,\n        arithmetic_optimization=rewriter_config_pb2.RewriterConfig.OFF,\n        dependency_optimization=rewriter_config_pb2.RewriterConfig.OFF)\n    graph_options = config_pb2.GraphOptions(rewrite_options=rewriter_config)\n    config_proto = config_pb2.ConfigProto(graph_options=graph_options)\n    self.sess = session.Session(config=config_proto)\n\n    # Initialize variable.\n    self.sess.run(variables.global_variables_initializer())", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 57, "line": "    self._tmp_dir = tempfile.mktemp()\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 58, "line": "    self._tmp_dir = tempfile.mkdtemp()\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 50, "char_end": 51, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/4f93d5f529a732dd533c063ae5b85e03e2006882", "file_name": "local_cli_wrapper_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420369603\nChange-Id: I2cf40b13f41cc01000c2c21a483a2d680194dba2", "description": "Write a Python function to set up a TensorFlow test environment with variables, placeholders, and a session configuration."}
{"func_name": "_put_validation_file", "func_src_before": "    def _put_validation_file(self, domain, file_path, file_name, content):\n        \"\"\"Put file to the domain with validation content\"\"\"\n        request = {'packet': {'site': {'get': [\n            {'filter': {'name': domain}},\n            {'dataset': {'hosting': {}}},\n        ]}}}\n        response = self.plesk_api_client.request(request)\n\n        api_result = response['packet']['site']['get']['result']\n        if 'ok' != api_result['status']:\n            error_text = str(api_result['errtext'])\n            raise errors.DvAuthError('Site get failure: %s' % error_text)\n\n        hosting_props = api_result['data']['hosting']['vrt_hst']['property']\n        self.www_root = next(\n            x['value'] for x in hosting_props if 'www_root' == x['name'])\n        self.ftp_login = next(\n            x['value'] for x in hosting_props if 'ftp_login' == x['name'])\n\n        self.verify_path = os.path.join(self.www_root, file_path)\n        self.full_path = os.path.join(self.www_root, file_path, file_name)\n        tmp_path = os.tempnam()\n        with open(tmp_path, 'w') as f:\n            f.write(str(content))\n            f.close()\n        try:\n            self.plesk_api_client.filemng(\n                [self.ftp_login, \"mkdir\", self.verify_path, \"-p\"])\n            self.plesk_api_client.filemng(\n                [self.ftp_login, \"cp2perm\", tmp_path, self.full_path, \"0644\"])\n        finally:\n            os.unlink(tmp_path)", "func_src_after": "    def _put_validation_file(self, domain, file_path, file_name, content):\n        \"\"\"Put file to the domain with validation content\"\"\"\n        request = {'packet': {'site': {'get': [\n            {'filter': {'name': domain}},\n            {'dataset': {'hosting': {}}},\n        ]}}}\n        response = self.plesk_api_client.request(request)\n\n        api_result = response['packet']['site']['get']['result']\n        if 'ok' != api_result['status']:\n            error_text = str(api_result['errtext'])\n            raise errors.DvAuthError('Site get failure: %s' % error_text)\n\n        hosting_props = api_result['data']['hosting']['vrt_hst']['property']\n        self.www_root = next(\n            x['value'] for x in hosting_props if 'www_root' == x['name'])\n        self.ftp_login = next(\n            x['value'] for x in hosting_props if 'ftp_login' == x['name'])\n\n        self.verify_path = os.path.join(self.www_root, file_path)\n        self.full_path = os.path.join(self.www_root, file_path, file_name)\n        self._create_file(content)", "line_changes": {"deleted": [{"line_no": 22, "char_start": 1002, "char_end": 1034, "line": "        tmp_path = os.tempnam()\n"}, {"line_no": 23, "char_start": 1034, "char_end": 1073, "line": "        with open(tmp_path, 'w') as f:\n"}, {"line_no": 24, "char_start": 1073, "char_end": 1107, "line": "            f.write(str(content))\n"}, {"line_no": 25, "char_start": 1107, "char_end": 1129, "line": "            f.close()\n"}, {"line_no": 26, "char_start": 1129, "char_end": 1142, "line": "        try:\n"}, {"line_no": 27, "char_start": 1142, "char_end": 1185, "line": "            self.plesk_api_client.filemng(\n"}, {"line_no": 28, "char_start": 1185, "char_end": 1252, "line": "                [self.ftp_login, \"mkdir\", self.verify_path, \"-p\"])\n"}, {"line_no": 29, "char_start": 1252, "char_end": 1295, "line": "            self.plesk_api_client.filemng(\n"}, {"line_no": 30, "char_start": 1295, "char_end": 1374, "line": "                [self.ftp_login, \"cp2perm\", tmp_path, self.full_path, \"0644\"])\n"}, {"line_no": 31, "char_start": 1374, "char_end": 1391, "line": "        finally:\n"}, {"line_no": 32, "char_start": 1391, "char_end": 1422, "line": "            os.unlink(tmp_path)\n"}], "added": [{"line_no": 22, "char_start": 1002, "char_end": 1036, "line": "        self._create_file(content)\n"}]}, "char_changes": {"deleted": [{"char_start": 1010, "char_end": 1421, "chars": "tmp_path = os.tempnam()\n        with open(tmp_path, 'w') as f:\n            f.write(str(content))\n            f.close()\n        try:\n            self.plesk_api_client.filemng(\n                [self.ftp_login, \"mkdir\", self.verify_path, \"-p\"])\n            self.plesk_api_client.filemng(\n                [self.ftp_login, \"cp2perm\", tmp_path, self.full_path, \"0644\"])\n        finally:\n            os.unlink(tmp_path"}], "added": [{"char_start": 1010, "char_end": 1035, "chars": "self._create_file(content"}]}, "commit_link": "github.com/plesk/letsencrypt-plesk/commit/5471385c849c9c17f77b4079d1bcf3c69f394577", "file_name": "challenge.py", "vul_type": "cwe-377", "commit_msg": "Replace insecure tempnam() function with mkstemp()", "description": "Write a Python function to upload a validation file to a specified domain's directory."}
{"func_name": "testPrintTensorsToFile", "func_src_before": "  def testPrintTensorsToFile(self):\n    tmpfile_name = tempfile.mktemp(\".printv2_test\")\n    tensor_0 = math_ops.range(0, 10)\n    print_op_0 = logging_ops.print_v2(tensor_0,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_0)\n    tensor_1 = math_ops.range(11, 20)\n    print_op_1 = logging_ops.print_v2(tensor_1,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_1)\n    try:\n      f = open(tmpfile_name, \"r\")\n      line_0 = f.readline()\n      expected_0 = \"[0 1 2 ... 7 8 9]\"\n      self.assertTrue(expected_0 in line_0)\n      line_1 = f.readline()\n      expected_1 = \"[11 12 13 ... 17 18 19]\"\n      self.assertTrue(expected_1 in line_1)\n      f.close()\n      os.remove(tmpfile_name)\n    except IOError as e:\n      self.fail(e)", "func_src_after": "  def testPrintTensorsToFile(self):\n    _, tmpfile_name = tempfile.mkstemp(\n        \".printv2_test\")  # safe to ignore fd here\n    tensor_0 = math_ops.range(0, 10)\n    print_op_0 = logging_ops.print_v2(tensor_0,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_0)\n    tensor_1 = math_ops.range(11, 20)\n    print_op_1 = logging_ops.print_v2(tensor_1,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_1)\n    try:\n      f = open(tmpfile_name, \"r\")\n      line_0 = f.readline()\n      expected_0 = \"[0 1 2 ... 7 8 9]\"\n      self.assertTrue(expected_0 in line_0)\n      line_1 = f.readline()\n      expected_1 = \"[11 12 13 ... 17 18 19]\"\n      self.assertTrue(expected_1 in line_1)\n      f.close()\n      os.remove(tmpfile_name)\n    except IOError as e:\n      self.fail(e)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 36, "char_end": 88, "line": "    tmpfile_name = tempfile.mktemp(\".printv2_test\")\n"}], "added": [{"line_no": 2, "char_start": 36, "char_end": 76, "line": "    _, tmpfile_name = tempfile.mkstemp(\n"}, {"line_no": 3, "char_start": 76, "char_end": 127, "line": "        \".printv2_test\")  # safe to ignore fd here\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 39, "char_end": 42, "chars": " _,"}, {"char_start": 69, "char_end": 70, "chars": "s"}, {"char_start": 75, "char_end": 84, "chars": "\n        "}, {"char_start": 100, "char_end": 126, "chars": "  # safe to ignore fd here"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/247aaafbe7f689492797d92430e77443b011876c", "file_name": "logging_ops_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420360036\nChange-Id: I13eb94736af3397261cf0d46214ddb5a2af9d92b", "description": "Write a Python function to print two ranges of numbers to a temporary file and verify the output."}
{"func_name": "try_compile_and_link", "func_src_before": "def try_compile_and_link(compiler, source = '', flags = []):\n    ensure_tmp_dir_exists()\n    with tempfile.NamedTemporaryFile() as sfile:\n        ofile = tempfile.mktemp()\n        try:\n            sfile.file.write(bytes(source, 'utf-8'))\n            sfile.file.flush()\n            # We can't write to /dev/null, since in some cases (-ftest-coverage) gcc will create an auxiliary\n            # output file based on the name of the output file, and \"/dev/null.gcsa\" is not a good name\n            return subprocess.call([compiler, '-x', 'c++', '-o', ofile, sfile.name] + flags,\n                                   stdout = subprocess.DEVNULL,\n                                   stderr = subprocess.DEVNULL) == 0\n        finally:\n            if os.path.exists(ofile):\n                os.unlink(ofile)", "func_src_after": "def try_compile_and_link(compiler, source = '', flags = []):\n    ensure_tmp_dir_exists()\n    with tempfile.NamedTemporaryFile() as sfile:\n        ofd, ofile = tempfile.mkstemp()\n        os.close(ofd)\n        try:\n            sfile.file.write(bytes(source, 'utf-8'))\n            sfile.file.flush()\n            # We can't write to /dev/null, since in some cases (-ftest-coverage) gcc will create an auxiliary\n            # output file based on the name of the output file, and \"/dev/null.gcsa\" is not a good name\n            return subprocess.call([compiler, '-x', 'c++', '-o', ofile, sfile.name] + flags,\n                                   stdout = subprocess.DEVNULL,\n                                   stderr = subprocess.DEVNULL) == 0\n        finally:\n            if os.path.exists(ofile):\n                os.unlink(ofile)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 138, "char_end": 172, "line": "        ofile = tempfile.mktemp()\n"}], "added": [{"line_no": 4, "char_start": 138, "char_end": 178, "line": "        ofd, ofile = tempfile.mkstemp()\n"}, {"line_no": 5, "char_start": 178, "char_end": 200, "line": "        os.close(ofd)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 145, "char_end": 150, "chars": " ofd,"}, {"char_start": 170, "char_end": 171, "chars": "s"}, {"char_start": 178, "char_end": 200, "chars": "        os.close(ofd)\n"}]}, "commit_link": "github.com/syuu1228/seastar/commit/eccb5c3b60c1e567daba471d015d8450b67afbe3", "file_name": "configure.py", "vul_type": "cwe-377", "commit_msg": "configure.py: don't use deprecated mktemp()\n\nconfigure.py uses the deprecated Python function tempfile.mktemp().\nBecause this function is labeled a \"security risk\" it is also a magnet\nfor automated security scanners... So let's replace it with the\nrecommended tempfile.mkstemp() and avoid future complaints.\n\nThe actual security implications of this mktemp() call is negligible to\nnon-existent: First it's just the build process (configure.py), not\nthe build product itself. Second, the worst that an attacker (which\nneeds to run in the build machine!) can do is to cause a compilation\ntest in configure.py to fail because it can't write to its output file.\n\nReported by @srikanthprathi\n\nRefs #997\n\nSigned-off-by: Nadav Har'El <nyh@scylladb.com>\nMessage-Id: <20220111121412.609430-1-nyh@scylladb.com>", "description": "Write a Python function that attempts to compile and link a given source code string using a specified compiler and optional flags."}
{"func_name": "to_ods", "func_src_before": "    def to_ods(self, *selection):\n        if not ODFLIB_INSTALLED:\n            raise ODFLIBNotInstalled(_('odfpy not installed.'))\n        if self.fcn_list:\n            stat_list = self.fcn_list[:]\n            order_text = \"   Ordered by: \" + self.sort_type + '\\n'\n        else:\n            stat_list = self.stats.keys()\n            order_text = \"   Random listing order was used\\n\"\n        for s in selection:\n            stat_list, __ = self.eval_print_amount(s, stat_list, '')\n        spreadsheet = OpenDocumentSpreadsheet()\n        table = Table(name=\"Profile\")\n        for fn in self.files:\n            tcf = TableCell()\n            tcf.addElement(P(text=fn))\n            trf = TableRow()\n            trf.addElement(tcf)\n            table.addElement(trf)\n\n        tc_summary = TableCell()\n        summary_text = '%d function calls (%d primitive calls) in %.6f \\\n                        seconds' % (self.total_calls, self.prim_calls,\n                                    self.total_tt)\n        tc_summary.addElement(P(text=summary_text))\n        tr_summary = TableRow()\n        tr_summary.addElement(tc_summary)\n        table.addElement(tr_summary)\n\n        tc_order = TableCell()\n        tc_order.addElement(P(text=order_text))\n        tr_order = TableRow()\n        tr_order.addElement(tc_order)\n        table.addElement(tr_order)\n\n        tr_header = TableRow()\n        tc_cc = TableCell()\n        tc_cc.addElement(P(text='Total Call Count'))\n        tr_header.addElement(tc_cc)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Primitive Call Count'))\n        tr_header.addElement(tc_pc)\n\n        tc_tt = TableCell()\n        tc_tt.addElement(P(text='Total Time(seconds)'))\n        tr_header.addElement(tc_tt)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Time Per call(seconds)'))\n        tr_header.addElement(tc_pc)\n\n        tc_ct = TableCell()\n        tc_ct.addElement(P(text='Cumulative Time(seconds)'))\n        tr_header.addElement(tc_ct)\n\n        tc_pt = TableCell()\n        tc_pt.addElement(P(text='Cumulative Time per call(seconds)'))\n        tr_header.addElement(tc_pt)\n\n        tc_nfl = TableCell()\n        tc_nfl.addElement(P(text='filename:lineno(function)'))\n        tr_header.addElement(tc_nfl)\n\n        table.addElement(tr_header)\n\n        for func in stat_list:\n            cc, nc, tt, ct, __ = self.stats[func]\n            tr_header = TableRow()\n            tc_nc = TableCell()\n            tc_nc.addElement(P(text=nc))\n            tr_header.addElement(tc_nc)\n\n            tc_pc = TableCell()\n            tc_pc.addElement(P(text=cc))\n            tr_header.addElement(tc_pc)\n\n            tc_tt = TableCell()\n            tc_tt.addElement(P(text=tt))\n            tr_header.addElement(tc_tt)\n\n            tc_tpc = TableCell()\n            tc_tpc.addElement(P(text=(None if nc == 0 else float(tt) / nc)))\n            tr_header.addElement(tc_tpc)\n\n            tc_ct = TableCell()\n            tc_ct.addElement(P(text=ct))\n            tr_header.addElement(tc_ct)\n\n            tc_tpt = TableCell()\n            tc_tpt.addElement(P(text=(None if cc == 0 else float(ct) / cc)))\n            tr_header.addElement(tc_tpt)\n\n            tc_nfl = TableCell()\n            tc_nfl.addElement(P(text=func))\n            tr_header.addElement(tc_nfl)\n            table.addElement(tr_header)\n\n        spreadsheet.spreadsheet.addElement(table)\n        tmp_ods = tempfile.mktemp('.ods', 'stats')\n        spreadsheet.save(tmp_ods, False)\n        data = open(tmp_ods).read()\n        os.remove(tmp_ods)\n        return data", "func_src_after": "    def to_ods(self, *selection):\n        if not ODFLIB_INSTALLED:\n            raise ODFLIBNotInstalled(_('odfpy not installed.'))\n        if self.fcn_list:\n            stat_list = self.fcn_list[:]\n            order_text = \"   Ordered by: \" + self.sort_type + '\\n'\n        else:\n            stat_list = self.stats.keys()\n            order_text = \"   Random listing order was used\\n\"\n        for s in selection:\n            stat_list, __ = self.eval_print_amount(s, stat_list, '')\n        spreadsheet = OpenDocumentSpreadsheet()\n        table = Table(name=\"Profile\")\n        for fn in self.files:\n            tcf = TableCell()\n            tcf.addElement(P(text=fn))\n            trf = TableRow()\n            trf.addElement(tcf)\n            table.addElement(trf)\n\n        tc_summary = TableCell()\n        summary_text = '%d function calls (%d primitive calls) in %.6f \\\n                        seconds' % (self.total_calls, self.prim_calls,\n                                    self.total_tt)\n        tc_summary.addElement(P(text=summary_text))\n        tr_summary = TableRow()\n        tr_summary.addElement(tc_summary)\n        table.addElement(tr_summary)\n\n        tc_order = TableCell()\n        tc_order.addElement(P(text=order_text))\n        tr_order = TableRow()\n        tr_order.addElement(tc_order)\n        table.addElement(tr_order)\n\n        tr_header = TableRow()\n        tc_cc = TableCell()\n        tc_cc.addElement(P(text='Total Call Count'))\n        tr_header.addElement(tc_cc)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Primitive Call Count'))\n        tr_header.addElement(tc_pc)\n\n        tc_tt = TableCell()\n        tc_tt.addElement(P(text='Total Time(seconds)'))\n        tr_header.addElement(tc_tt)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Time Per call(seconds)'))\n        tr_header.addElement(tc_pc)\n\n        tc_ct = TableCell()\n        tc_ct.addElement(P(text='Cumulative Time(seconds)'))\n        tr_header.addElement(tc_ct)\n\n        tc_pt = TableCell()\n        tc_pt.addElement(P(text='Cumulative Time per call(seconds)'))\n        tr_header.addElement(tc_pt)\n\n        tc_nfl = TableCell()\n        tc_nfl.addElement(P(text='filename:lineno(function)'))\n        tr_header.addElement(tc_nfl)\n\n        table.addElement(tr_header)\n\n        for func in stat_list:\n            cc, nc, tt, ct, __ = self.stats[func]\n            tr_header = TableRow()\n            tc_nc = TableCell()\n            tc_nc.addElement(P(text=nc))\n            tr_header.addElement(tc_nc)\n\n            tc_pc = TableCell()\n            tc_pc.addElement(P(text=cc))\n            tr_header.addElement(tc_pc)\n\n            tc_tt = TableCell()\n            tc_tt.addElement(P(text=tt))\n            tr_header.addElement(tc_tt)\n\n            tc_tpc = TableCell()\n            tc_tpc.addElement(P(text=(None if nc == 0 else float(tt) / nc)))\n            tr_header.addElement(tc_tpc)\n\n            tc_ct = TableCell()\n            tc_ct.addElement(P(text=ct))\n            tr_header.addElement(tc_ct)\n\n            tc_tpt = TableCell()\n            tc_tpt.addElement(P(text=(None if cc == 0 else float(ct) / cc)))\n            tr_header.addElement(tc_tpt)\n\n            tc_nfl = TableCell()\n            tc_nfl.addElement(P(text=func))\n            tr_header.addElement(tc_nfl)\n            table.addElement(tr_header)\n\n        spreadsheet.spreadsheet.addElement(table)\n        tmp_ods = tempfile.TemporaryFile()\n        spreadsheet.write(tmp_ods)\n        tmp_ods.seek(0)\n        data = tmp_ods.read()\n        os.close(tmp_ods)\n        return data", "line_changes": {"deleted": [{"line_no": 100, "char_start": 3365, "char_end": 3416, "line": "        tmp_ods = tempfile.mktemp('.ods', 'stats')\n"}, {"line_no": 101, "char_start": 3416, "char_end": 3457, "line": "        spreadsheet.save(tmp_ods, False)\n"}, {"line_no": 102, "char_start": 3457, "char_end": 3493, "line": "        data = open(tmp_ods).read()\n"}, {"line_no": 103, "char_start": 3493, "char_end": 3520, "line": "        os.remove(tmp_ods)\n"}], "added": [{"line_no": 100, "char_start": 3365, "char_end": 3408, "line": "        tmp_ods = tempfile.TemporaryFile()\n"}, {"line_no": 101, "char_start": 3408, "char_end": 3443, "line": "        spreadsheet.write(tmp_ods)\n"}, {"line_no": 102, "char_start": 3443, "char_end": 3467, "line": "        tmp_ods.seek(0)\n"}, {"line_no": 103, "char_start": 3467, "char_end": 3497, "line": "        data = tmp_ods.read()\n"}, {"line_no": 104, "char_start": 3497, "char_end": 3523, "line": "        os.close(tmp_ods)\n"}]}, "char_changes": {"deleted": [{"char_start": 3392, "char_end": 3414, "chars": "mktemp('.ods', 'stats'"}, {"char_start": 3436, "char_end": 3439, "chars": "sav"}, {"char_start": 3448, "char_end": 3455, "chars": ", False"}, {"char_start": 3472, "char_end": 3477, "chars": "open("}, {"char_start": 3484, "char_end": 3485, "chars": ")"}, {"char_start": 3504, "char_end": 3509, "chars": "remov"}], "added": [{"char_start": 3392, "char_end": 3406, "chars": "TemporaryFile("}, {"char_start": 3428, "char_end": 3432, "chars": "writ"}, {"char_start": 3441, "char_end": 3465, "chars": ")\n        tmp_ods.seek(0"}, {"char_start": 3508, "char_end": 3512, "chars": "clos"}]}, "commit_link": "github.com/scality/ScalitySproxydSwift/commit/6978275cdb04bb08aaf142d401b52a46527dac4c", "file_name": "profile_model.py", "vul_type": "cwe-377", "commit_msg": "Avoid usage of insecure mktemp() function\n\nThis patch eliminates the use of the deprecated and insecure\ntempfile.mktemp() function.  It has been replaced with secure\nalternatives where temporary files are actually required.\n\nChange-Id: I0a13d6d44cd1abc4b66fa33f39eea407617a01d5\nSecurityImpact\nCloses-bug: #1348869", "description": "Write a Python function that generates an ODS spreadsheet file with profiling statistics and returns its content."}
{"func_name": "testIncompleteRedirectWorks", "func_src_before": "  def testIncompleteRedirectWorks(self):\n    output_path = tempfile.mktemp()\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "func_src_after": "  def testIncompleteRedirectWorks(self):\n    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 77, "line": "    output_path = tempfile.mktemp()\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 102, "line": "    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 44, "char_end": 47, "chars": " _,"}, {"char_start": 73, "char_end": 74, "chars": "s"}, {"char_start": 80, "char_end": 101, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/599208b439e349f88c809e9d1f268c8c77718259", "file_name": "readline_ui_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359224\nChange-Id: I7bfc1df9cf931f45ec85d4878874ef41b9c55474", "description": "Write a Python unit test that checks if a mocked command-line interface correctly redirects output to a temporary file."}
{"func_name": "download", "func_src_before": "    def download(self, log_files, sort='time', limit=-1, nfl_filter='',\n                 output_format='default'):\n        if len(log_files) == 0:\n            raise NotFoundException(_('no log file found'))\n        try:\n            nfl_esc = nfl_filter.replace('(', '\\(').replace(')', '\\)')\n            # remove the slash that is intentionally added in the URL\n            # to avoid failure of filtering stats data.\n            if nfl_esc.startswith('/'):\n                nfl_esc = nfl_esc[1:]\n            stats = Stats2(*log_files)\n            stats.sort_stats(sort)\n            if output_format == 'python':\n                data = self.format_source_code(nfl_filter)\n            elif output_format == 'json':\n                data = stats.to_json(nfl_esc, limit)\n            elif output_format == 'csv':\n                data = stats.to_csv(nfl_esc, limit)\n            elif output_format == 'ods':\n                data = stats.to_ods(nfl_esc, limit)\n            else:\n                profile_tmp_all = tempfile.mktemp('.profile', 'all')\n                stats.dump_stats(profile_tmp_all)\n                data = open(profile_tmp_all).read()\n                os.remove(profile_tmp_all)\n            return data, [('content-type', self.format_dict[output_format])]\n        except ODFLIBNotInstalled as ex:\n            raise ex\n        except Exception as ex:\n            raise ProfileException(_('Data download error: %s') % ex)", "func_src_after": "    def download(self, log_files, sort='time', limit=-1, nfl_filter='',\n                 output_format='default'):\n        if len(log_files) == 0:\n            raise NotFoundException(_('no log file found'))\n        try:\n            nfl_esc = nfl_filter.replace('(', '\\(').replace(')', '\\)')\n            # remove the slash that is intentionally added in the URL\n            # to avoid failure of filtering stats data.\n            if nfl_esc.startswith('/'):\n                nfl_esc = nfl_esc[1:]\n            stats = Stats2(*log_files)\n            stats.sort_stats(sort)\n            if output_format == 'python':\n                data = self.format_source_code(nfl_filter)\n            elif output_format == 'json':\n                data = stats.to_json(nfl_esc, limit)\n            elif output_format == 'csv':\n                data = stats.to_csv(nfl_esc, limit)\n            elif output_format == 'ods':\n                data = stats.to_ods(nfl_esc, limit)\n            else:\n                data = stats.print_stats()\n            return data, [('content-type', self.format_dict[output_format])]\n        except ODFLIBNotInstalled as ex:\n            raise ex\n        except Exception as ex:\n            raise ProfileException(_('Data download error: %s') % ex)", "line_changes": {"deleted": [{"line_no": 22, "char_start": 969, "char_end": 1038, "line": "                profile_tmp_all = tempfile.mktemp('.profile', 'all')\n"}, {"line_no": 23, "char_start": 1038, "char_end": 1088, "line": "                stats.dump_stats(profile_tmp_all)\n"}, {"line_no": 24, "char_start": 1088, "char_end": 1140, "line": "                data = open(profile_tmp_all).read()\n"}, {"line_no": 25, "char_start": 1140, "char_end": 1183, "line": "                os.remove(profile_tmp_all)\n"}], "added": [{"line_no": 22, "char_start": 969, "char_end": 1012, "line": "                data = stats.print_stats()\n"}]}, "char_changes": {"deleted": [{"char_start": 985, "char_end": 1181, "chars": "profile_tmp_all = tempfile.mktemp('.profile', 'all')\n                stats.dump_stats(profile_tmp_all)\n                data = open(profile_tmp_all).read()\n                os.remove(profile_tmp_all"}], "added": [{"char_start": 985, "char_end": 1010, "chars": "data = stats.print_stats("}]}, "commit_link": "github.com/scality/ScalitySproxydSwift/commit/6978275cdb04bb08aaf142d401b52a46527dac4c", "file_name": "html_viewer.py", "vul_type": "cwe-377", "commit_msg": "Avoid usage of insecure mktemp() function\n\nThis patch eliminates the use of the deprecated and insecure\ntempfile.mktemp() function.  It has been replaced with secure\nalternatives where temporary files are actually required.\n\nChange-Id: I0a13d6d44cd1abc4b66fa33f39eea407617a01d5\nSecurityImpact\nCloses-bug: #1348869", "description": "Write a Python function named `download` that processes log files and returns data in various formats based on given parameters."}
{"func_name": "_launch_cli", "func_src_before": "  def _launch_cli(self):\n    if self._is_run_start:\n      self.observers[\"run_start_cli_run_numbers\"].append(self._run_call_count)\n    else:\n      self.observers[\"run_end_cli_run_numbers\"].append(self._run_call_count)\n\n    readline_cli = ui_factory.get_ui(\n        \"readline\",\n        config=cli_config.CLIConfig(\n            config_file_path=os.path.join(tempfile.mkdtemp(), \".tfdbg_config\")))\n    self._register_this_run_info(readline_cli)\n\n    while True:\n      command = self._command_sequence[self._command_pointer]\n      self._command_pointer += 1\n\n      try:\n        if command[0] == \"run\":\n          self._run_handler(command[1:])\n        elif command[0] == \"print_feed\":\n          self.observers[\"print_feed_responses\"].append(\n              self._print_feed_handler(command[1:]))\n        else:\n          raise ValueError(\"Unrecognized command prefix: %s\" % command[0])\n      except debugger_cli_common.CommandLineExit as e:\n        return e.exit_token", "func_src_after": "  def _launch_cli(self):\n    if self._is_run_start:\n      self.observers[\"run_start_cli_run_numbers\"].append(self._run_call_count)\n    else:\n      self.observers[\"run_end_cli_run_numbers\"].append(self._run_call_count)\n\n    readline_cli = ui_factory.get_ui(\n        \"readline\",\n        config=cli_config.CLIConfig(\n            config_file_path=os.path.join(tempfile.mkdtemp(), \".tfdbg_config\")))\n    self._register_this_run_info(readline_cli)\n\n    while self._command_pointer < len(self._command_sequence):\n      command = self._command_sequence[self._command_pointer]\n      self._command_pointer += 1\n\n      try:\n        if command[0] == \"run\":\n          self._run_handler(command[1:])\n        elif command[0] == \"print_feed\":\n          self.observers[\"print_feed_responses\"].append(\n              self._print_feed_handler(command[1:]))\n        else:\n          raise ValueError(\"Unrecognized command prefix: %s\" % command[0])\n      except debugger_cli_common.CommandLineExit as e:\n        return e.exit_token", "line_changes": {"deleted": [{"line_no": 13, "char_start": 443, "char_end": 459, "line": "    while True:\n"}], "added": [{"line_no": 13, "char_start": 443, "char_end": 506, "line": "    while self._command_pointer < len(self._command_sequence):\n"}]}, "char_changes": {"deleted": [{"char_start": 453, "char_end": 457, "chars": "True"}], "added": [{"char_start": 453, "char_end": 504, "chars": "self._command_pointer < len(self._command_sequence)"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/4f93d5f529a732dd533c063ae5b85e03e2006882", "file_name": "local_cli_wrapper_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420369603\nChange-Id: I2cf40b13f41cc01000c2c21a483a2d680194dba2", "description": "Write a Python function to handle CLI commands for running tasks and printing feeds, with error handling for unrecognized commands."}
{"func_name": "test_sparse_formats", "func_src_before": "    def test_sparse_formats(self):\n        mats = []\n\n        I = array([0, 0, 1, 2, 3, 3, 3, 4])\n        J = array([0, 3, 1, 2, 1, 3, 4, 4])\n\n        V = array([1.0, 6.0, 10.5, 0.015, 250.5, -280.0, 33.32, 12.0])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        V = array([1.0 + 3j, 6.0 + 2j, 10.50 + 0.9j, 0.015 + -4.4j,\n                   250.5 + 0j, -280.0 + 5j, 33.32 + 6.4j, 12.00 + 0.8j])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        for mat in mats:\n            expected = mat.toarray()\n            for fmt in ['csr', 'csc', 'coo']:\n                fn = mktemp(dir=self.tmpdir)  # safe, we own tmpdir\n                mmwrite(fn, mat.asformat(fmt))\n\n                result = mmread(fn).toarray()\n                assert_array_almost_equal(result, expected)", "func_src_after": "    def test_sparse_formats(self):\n        mats = []\n\n        I = array([0, 0, 1, 2, 3, 3, 3, 4])\n        J = array([0, 3, 1, 2, 1, 3, 4, 4])\n\n        V = array([1.0, 6.0, 10.5, 0.015, 250.5, -280.0, 33.32, 12.0])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        V = array([1.0 + 3j, 6.0 + 2j, 10.50 + 0.9j, 0.015 + -4.4j,\n                   250.5 + 0j, -280.0 + 5j, 33.32 + 6.4j, 12.00 + 0.8j])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        for mat in mats:\n            expected = mat.toarray()\n            for fmt in ['csr', 'csc', 'coo']:\n                fn = mkstemp(suffix='.mtx', dir=self.tmpdir)[1]\n                mmwrite(fn, mat.asformat(fmt))\n\n                result = mmread(fn).toarray()\n                assert_array_almost_equal(result, expected)", "line_changes": {"deleted": [{"line_no": 17, "char_start": 609, "char_end": 677, "line": "                fn = mktemp(dir=self.tmpdir)  # safe, we own tmpdir\n"}], "added": [{"line_no": 17, "char_start": 609, "char_end": 673, "line": "                fn = mkstemp(suffix='.mtx', dir=self.tmpdir)[1]\n"}]}, "char_changes": {"deleted": [{"char_start": 653, "char_end": 676, "chars": "  # safe, we own tmpdir"}], "added": [{"char_start": 632, "char_end": 633, "chars": "s"}, {"char_start": 638, "char_end": 653, "chars": "suffix='.mtx', "}, {"char_start": 669, "char_end": 672, "chars": "[1]"}]}, "commit_link": "github.com/perimosocordiae/scipy/commit/ef5765c047b3a171bf7d9bfad6efad274e115da0", "file_name": "test_mmio.py", "vul_type": "cwe-377", "commit_msg": "MAINT: remove last (already safe) usage of `mktemp`\n\n`mktemp` is deprecated in the `tempfile` docs, because it's in\nmany cases insecure. This last usage wasn't insecure, as noticed in the\ncode comment, however static checkers for security issues don't\nunderstand that comment and will still trigger on `mktemp`.\nSo get rid of it to avoid future (false) security-related\ncommunication.\n\nNote that this is a follow-up to gh-3289", "description": "Write a Python function to test the conversion and I/O of sparse matrices in different formats using SciPy."}
{"func_name": "testInvalidExtension", "func_src_before": "  def testInvalidExtension(self):\n    converter = upgrade_schema_lib.Converter()\n    invalid_extension = tempfile.mktemp(suffix=\".foo\")\n    with self.assertRaisesRegex(ValueError, \"Invalid extension on input\"):\n      converter.Convert(invalid_extension, invalid_extension)\n    with tempfile.NamedTemporaryFile(suffix=\".json\", mode=\"w+\") as in_json:\n      JsonDumpAndFlush(EMPTY_TEST_SCHEMA_V1, in_json)\n      with self.assertRaisesRegex(ValueError, \"Invalid extension on output\"):\n        converter.Convert(in_json.name, invalid_extension)", "func_src_after": "  def testInvalidExtension(self):\n    converter = upgrade_schema_lib.Converter()\n    _, invalid_extension = tempfile.mkstemp(suffix=\".foo\")  # safe to ignore fd\n    with self.assertRaisesRegex(ValueError, \"Invalid extension on input\"):\n      converter.Convert(invalid_extension, invalid_extension)\n    with tempfile.NamedTemporaryFile(suffix=\".json\", mode=\"w+\") as in_json:\n      JsonDumpAndFlush(EMPTY_TEST_SCHEMA_V1, in_json)\n      with self.assertRaisesRegex(ValueError, \"Invalid extension on output\"):\n        converter.Convert(in_json.name, invalid_extension)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 81, "char_end": 136, "line": "    invalid_extension = tempfile.mktemp(suffix=\".foo\")\n"}], "added": [{"line_no": 3, "char_start": 81, "char_end": 161, "line": "    _, invalid_extension = tempfile.mkstemp(suffix=\".foo\")  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 84, "char_end": 87, "chars": " _,"}, {"char_start": 119, "char_end": 120, "chars": "s"}, {"char_start": 139, "char_end": 160, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/c63f6333a78ddf89ecd502d926fb877b8dce4f0f", "file_name": "upgrade_schema_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420335872\nChange-Id: I331ec2544a08d3cc3063a74af342cceae655b3dc", "description": "Write a Python unit test that checks for ValueError when a schema converter is given files with invalid extensions."}
{"func_name": "main", "func_src_before": "def main(_):\n  # Import data\n  if FLAGS.fake_data:\n    imgs = tf.random.uniform(maxval=256, shape=(10, 28, 28), dtype=tf.int32)\n    labels = tf.random.uniform(maxval=10, shape=(10,), dtype=tf.int32)\n    mnist_train = imgs, labels\n    mnist_test = imgs, labels\n  else:\n    mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n\n  def format_example(imgs, labels):\n    imgs = tf.reshape(imgs, [-1, 28 * 28])\n    imgs = tf.cast(imgs, tf.float32) / 255.0\n    labels = tf.one_hot(labels, depth=10, dtype=tf.float32)\n    return imgs, labels\n\n  ds_train = tf.data.Dataset.from_tensor_slices(mnist_train)\n  ds_train = ds_train.shuffle(\n      1000, seed=RAND_SEED).repeat().batch(FLAGS.train_batch_size)\n  ds_train = ds_train.map(format_example)\n  it_train = ds_train.make_initializable_iterator()\n\n  ds_test = tf.data.Dataset.from_tensors(mnist_test).repeat()\n  ds_test = ds_test.map(format_example)\n  it_test = ds_test.make_initializable_iterator()\n\n  sess = tf.InteractiveSession()\n\n  # Create the MNIST neural network graph.\n\n  # Input placeholders.\n  with tf.name_scope(\"input\"):\n    handle = tf.placeholder(tf.string, shape=())\n\n    iterator = tf.data.Iterator.from_string_handle(\n        handle, (tf.float32, tf.float32),\n        ((None, IMAGE_SIZE * IMAGE_SIZE), (None, 10)))\n\n    x, y_ = iterator.get_next()\n\n  def weight_variable(shape):\n    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n    initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\n    return tf.Variable(initial)\n\n  def bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    \"\"\"Reusable code for making a simple neural net layer.\"\"\"\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\"weights\"):\n        weights = weight_variable([input_dim, output_dim])\n      with tf.name_scope(\"biases\"):\n        biases = bias_variable([output_dim])\n      with tf.name_scope(\"Wx_plus_b\"):\n        preactivate = tf.matmul(input_tensor, weights) + biases\n\n      activations = act(preactivate)\n      return activations\n\n  hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\n  logits = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"output\", tf.identity)\n  y = tf.nn.softmax(logits)\n\n  with tf.name_scope(\"cross_entropy\"):\n    # The following line is the culprit of the bad numerical values that appear\n    # during training of this graph. Log of zero gives inf, which is first seen\n    # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\n    # call. A multiplication of the inf values with zeros leads to nans,\n    # which is first in \"cross_entropy/mul:0\".\n    #\n    # You can use the built-in, numerically-stable implementation to fix this\n    # issue:\n    #   diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)\n\n    diff = -(y_ * tf.log(y))\n    with tf.name_scope(\"total\"):\n      cross_entropy = tf.reduce_mean(diff)\n\n  with tf.name_scope(\"train\"):\n    train_step = tf.train.AdamOptimizer(\n        FLAGS.learning_rate).minimize(cross_entropy)\n\n  with tf.name_scope(\"accuracy\"):\n    with tf.name_scope(\"correct_prediction\"):\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    with tf.name_scope(\"accuracy\"):\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  sess.run(tf.global_variables_initializer())\n  sess.run(it_train.initializer)\n  sess.run(it_test.initializer)\n  train_handle = sess.run(it_train.string_handle())\n  test_handle = sess.run(it_test.string_handle())\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  if FLAGS.debug:\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n\n  # Add this point, sess is a debug wrapper around the actual Session if\n  # FLAGS.debug is true. In that case, calling run() will launch the CLI.\n  for i in range(FLAGS.max_steps):\n    acc = sess.run(accuracy, feed_dict={handle: test_handle})\n    print(\"Accuracy at step %d: %s\" % (i, acc))\n\n    sess.run(train_step, feed_dict={handle: train_handle})", "func_src_after": "def main(_):\n  # Import data\n  if FLAGS.fake_data:\n    imgs = tf.random.uniform(maxval=256, shape=(10, 28, 28), dtype=tf.int32)\n    labels = tf.random.uniform(maxval=10, shape=(10,), dtype=tf.int32)\n    mnist_train = imgs, labels\n    mnist_test = imgs, labels\n  else:\n    mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n\n  def format_example(imgs, labels):\n    imgs = tf.reshape(imgs, [-1, 28 * 28])\n    imgs = tf.cast(imgs, tf.float32) / 255.0\n    labels = tf.one_hot(labels, depth=10, dtype=tf.float32)\n    return imgs, labels\n\n  ds_train = tf.data.Dataset.from_tensor_slices(mnist_train)\n  ds_train = ds_train.shuffle(\n      1000, seed=RAND_SEED).repeat().batch(FLAGS.train_batch_size)\n  ds_train = ds_train.map(format_example)\n  it_train = ds_train.make_initializable_iterator()\n\n  ds_test = tf.data.Dataset.from_tensors(mnist_test).repeat()\n  ds_test = ds_test.map(format_example)\n  it_test = ds_test.make_initializable_iterator()\n\n  sess = tf.InteractiveSession()\n\n  # Create the MNIST neural network graph.\n\n  # Input placeholders.\n  with tf.name_scope(\"input\"):\n    handle = tf.placeholder(tf.string, shape=())\n\n    iterator = tf.data.Iterator.from_string_handle(\n        handle, (tf.float32, tf.float32),\n        ((None, IMAGE_SIZE * IMAGE_SIZE), (None, 10)))\n\n    x, y_ = iterator.get_next()\n\n  def weight_variable(shape):\n    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n    initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\n    return tf.Variable(initial)\n\n  def bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    \"\"\"Reusable code for making a simple neural net layer.\"\"\"\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\"weights\"):\n        weights = weight_variable([input_dim, output_dim])\n      with tf.name_scope(\"biases\"):\n        biases = bias_variable([output_dim])\n      with tf.name_scope(\"Wx_plus_b\"):\n        preactivate = tf.matmul(input_tensor, weights) + biases\n\n      activations = act(preactivate)\n      return activations\n\n  hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\n  logits = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"output\", tf.identity)\n  y = tf.nn.softmax(logits)\n\n  with tf.name_scope(\"cross_entropy\"):\n    # The following line is the culprit of the bad numerical values that appear\n    # during training of this graph. Log of zero gives inf, which is first seen\n    # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\n    # call. A multiplication of the inf values with zeros leads to nans,\n    # which is first in \"cross_entropy/mul:0\".\n    #\n    # You can use the built-in, numerically-stable implementation to fix this\n    # issue:\n    #   diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)\n\n    diff = -(y_ * tf.log(y))\n    with tf.name_scope(\"total\"):\n      cross_entropy = tf.reduce_mean(diff)\n\n  with tf.name_scope(\"train\"):\n    train_step = tf.train.AdamOptimizer(\n        FLAGS.learning_rate).minimize(cross_entropy)\n\n  with tf.name_scope(\"accuracy\"):\n    with tf.name_scope(\"correct_prediction\"):\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    with tf.name_scope(\"accuracy\"):\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  sess.run(tf.global_variables_initializer())\n  sess.run(it_train.initializer)\n  sess.run(it_test.initializer)\n  train_handle = sess.run(it_train.string_handle())\n  test_handle = sess.run(it_test.string_handle())\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  if FLAGS.debug:\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n\n  # Add this point, sess is a debug wrapper around the actual Session if\n  # FLAGS.debug is true. In that case, calling run() will launch the CLI.\n  for i in range(FLAGS.max_steps):\n    acc = sess.run(accuracy, feed_dict={handle: test_handle})\n    print(\"Accuracy at step %d: %s\" % (i, acc))\n\n    sess.run(train_step, feed_dict={handle: train_handle})", "line_changes": {"deleted": [{"line_no": 106, "char_start": 3998, "char_end": 4023, "line": "    config_file_path = (\n"}, {"line_no": 107, "char_start": 4023, "char_end": 4064, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 108, "char_start": 4064, "char_end": 4115, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 106, "char_start": 3998, "char_end": 4035, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 108, "char_start": 4088, "char_end": 4150, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 109, "char_start": 4150, "char_end": 4160, "line": "    else:\n"}, {"line_no": 110, "char_start": 4160, "char_end": 4190, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 4020, "char_end": 4030, "chars": " (\n       "}, {"char_start": 4068, "char_end": 4092, "chars": "    if FLAGS.use_random_"}, {"char_start": 4104, "char_end": 4108, "chars": "else"}, {"char_start": 4113, "char_end": 4114, "chars": ")"}], "added": [{"char_start": 4002, "char_end": 4097, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 4127, "char_end": 4128, "chars": "s"}, {"char_start": 4154, "char_end": 4166, "chars": "else:\n      "}, {"char_start": 4173, "char_end": 4178, "chars": "file_"}, {"char_start": 4183, "char_end": 4184, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/b5150106a7829c45892927562b7eed101581ea95", "file_name": "debug_mnist_v1.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359231\nChange-Id: If2049dbeb46fb8ff6df7c8e077cee8be3872e5b4", "description": "Write a Python TensorFlow script that trains a neural network on the MNIST dataset with options for fake data and debugging."}
{"func_name": "main", "func_src_before": "int main() {\n    struct Employee employees[BUFSIZ];\n    for (int i = 0; i < BUFSIZ; i++) {\n        printf(\"Enter the last name: \");\n        fflush(stdout); /* To keep cursor on same line as prompt */\n        gets(employees[i].last);\n        if (strlen(employees[i].last) > 0) {\n            printf(\"Enter the first name: \");\n            fflush(stdout);\n            gets(employees[i].first);\n            printf(\"Enter the job title: \");\n            fflush(stdout);\n            gets(employees[i].title);\n            printf(\"Enter the salary: \");\n            fflush(stdout);\n            scanf(\"%d\", &employees[i].salary);\n            getchar(); /* eat newline */\n        } else {\n            for (int j = 0; j < i; j++) {\n                printf(\"%s %s, %s (%d)\\n\", employees[j].first, employees[j].last, employees[j].title, employees[j].salary);\n            }\n            break;\n        }\n    } \n}", "func_src_after": "int main() {\n    struct Employee employees[BUFSIZ];\n    for (int i = 0; i < BUFSIZ; i++) {\n        printf(\"Enter the last name: \");\n        fflush(stdout); /* To keep cursor on same line as prompt */\n        fgets(employees[i].last, sizeof employees[i].last, stdin);\n        stripNewline(employees[i].last);\n        if (strlen(employees[i].last) > 0) {\n            printf(\"Enter the first name: \");\n            fflush(stdout);\n            fgets(employees[i].first, sizeof employees[i].first, stdin);\n            stripNewline(employees[i].first);\n            printf(\"Enter the job title: \");\n            fflush(stdout);\n            fgets(employees[i].title, sizeof employees[i].title, stdin);\n            stripNewline(employees[i].title);\n            printf(\"Enter the salary: \");\n            fflush(stdout);\n            scanf(\"%d\", &employees[i].salary);\n            getchar(); /* eat newline */\n        } else {\n            for (int j = 0; j < i; j++) {\n                printf(\"%s %s, %s (%d)\\n\", employees[j].first, employees[j].last, employees[j].title, employees[j].salary);\n            }\n            break;\n        }\n    } \n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 200, "char_end": 233, "line": "        gets(employees[i].last);\n"}, {"line_no": 10, "char_start": 352, "char_end": 390, "line": "            gets(employees[i].first);\n"}, {"line_no": 13, "char_start": 463, "char_end": 501, "line": "            gets(employees[i].title);\n"}], "added": [{"line_no": 6, "char_start": 200, "char_end": 267, "line": "        fgets(employees[i].last, sizeof employees[i].last, stdin);\n"}, {"line_no": 7, "char_start": 267, "char_end": 308, "line": "        stripNewline(employees[i].last);\n"}, {"line_no": 11, "char_start": 427, "char_end": 500, "line": "            fgets(employees[i].first, sizeof employees[i].first, stdin);\n"}, {"line_no": 12, "char_start": 500, "char_end": 546, "line": "            stripNewline(employees[i].first);\n"}, {"line_no": 15, "char_start": 619, "char_end": 692, "line": "            fgets(employees[i].title, sizeof employees[i].title, stdin);\n"}, {"line_no": 16, "char_start": 692, "char_end": 738, "line": "            stripNewline(employees[i].title);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 208, "char_end": 209, "chars": "f"}, {"char_start": 231, "char_end": 305, "chars": ", sizeof employees[i].last, stdin);\n        stripNewline(employees[i].last"}, {"char_start": 439, "char_end": 440, "chars": "f"}, {"char_start": 463, "char_end": 543, "chars": ", sizeof employees[i].first, stdin);\n            stripNewline(employees[i].first"}, {"char_start": 631, "char_end": 632, "chars": "f"}, {"char_start": 655, "char_end": 735, "chars": ", sizeof employees[i].title, stdin);\n            stripNewline(employees[i].title"}]}, "commit_link": "github.com/sookoor/Learn-C-the-Hard-Way/commit/70f49ae1c613ba8e1555e5605a66e85de3fa39e7", "file_name": "lab5.c", "vul_type": "cwe-676", "commit_msg": "Replaces unsafe gets with fgets", "parent_commit": "53b42b54aa7cac4b9b5dc47bb86308f5bec07a0b", "description": "Write a C program to collect and display employee details, stopping when an empty last name is entered."}
{"func_name": "main", "func_src_before": "int main() {\n\tTERM *t;\n\tchar buffer[300];\n\n\tprintf(\"lci - A lambda calculus interpreter\\n\");\n\tprintf(\"Copyright (C) 2003 Kostas Hatzikokolakis\\n\");\n\tprintf(\"This is FREE SOFTWARE and comes with ABSOLUTELY NO WARRANTY\\n\\n\");\n\tprintf(\"Type a term, Help for info or Quit to exit.\\n\");\n\n\t// read and execute .lcirc\n\tconsultFile(\".lcirc\");\n\n\t// read and execute commands\n\twhile(!feof(stdin)) {\n\t\tprintf(\"lci> \");\n\t\tif(!gets(buffer)) break;\n\t\tif(buffer[0] == '\\0') continue;\n\n\t\tscInputType = SC_BUFFER;\n\t\tscInput = buffer;\n\t\tgetToken(NULL);\n\n\t\tif(parse((void*)&t, TK_TERM) == PAR_OK)\n\t\t\texecTerm(t);\n\t\telse\n\t\t\tprintf(\"Syntax error\\n\\n\");\n\t}\n\n\treturn 0;\n}", "func_src_after": "int main() {\n\tTERM *t;\n\tchar buffer[300];\n\n\tprintf(\"lci - A lambda calculus interpreter\\n\");\n\tprintf(\"Copyright (C) 2003 Kostas Hatzikokolakis\\n\");\n\tprintf(\"This is FREE SOFTWARE and comes with ABSOLUTELY NO WARRANTY\\n\\n\");\n\tprintf(\"Type a term, Help for info or Quit to exit.\\n\");\n\n\t// read and execute .lcirc\n\tconsultFile(\".lcirc\");\n\n\t// read and execute commands\n\twhile(!feof(stdin)) {\n\t\tprintf(\"lci> \");\n\t\tif(!fgets(buffer, sizeof(buffer), stdin)) break;\n\t\tif(strcmp(buffer, \"\\n\") == 0) continue;\n\n\t\tscInputType = SC_BUFFER;\n\t\tscInput = buffer;\n\t\tgetToken(NULL);\n\n\t\tif(parse((void*)&t, TK_TERM) == PAR_OK)\n\t\t\texecTerm(t);\n\t\telse\n\t\t\tprintf(\"Syntax error\\n\\n\");\n\t}\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 16, "char_start": 408, "char_end": 435, "line": "\t\tif(!gets(buffer)) break;\n"}, {"line_no": 17, "char_start": 435, "char_end": 469, "line": "\t\tif(buffer[0] == '\\0') continue;\n"}], "added": [{"line_no": 16, "char_start": 408, "char_end": 459, "line": "\t\tif(!fgets(buffer, sizeof(buffer), stdin)) break;\n"}, {"line_no": 17, "char_start": 459, "char_end": 501, "line": "\t\tif(strcmp(buffer, \"\\n\") == 0) continue;\n"}]}, "char_changes": {"deleted": [{"char_start": 425, "char_end": 457, "chars": ")) break;\n\t\tif(buffer[0] == '\\0'"}], "added": [{"char_start": 414, "char_end": 415, "chars": "f"}, {"char_start": 426, "char_end": 489, "chars": ", sizeof(buffer), stdin)) break;\n\t\tif(strcmp(buffer, \"\\n\") == 0"}]}, "commit_link": "github.com/8l/lci/commit/82786e58e0e56a00d97ded474c6589c08e885149", "file_name": "main.c", "vul_type": "cwe-676", "commit_msg": "Replaced the 'dangerous' gets by fgets.", "parent_commit": "204649b76eb58335e3c8e1bf4831039ea91f0489", "description": "Write a C program that serves as a simple command-line interpreter for lambda calculus expressions, including a greeting message and a loop to process user input."}
{"func_name": "ac_circ_buf_popm", "func_src_before": "int ac_circ_buf_popm(ac_circ_buf_t *cbuf, void *buf, u32 count)\n{\n\tif (circ_count_to_end(cbuf) < count)\n\t\treturn -1;\n\n\tif (cbuf->type == PTR_BUF)\n\t\tmemcpy(buf, cbuf->buf.ptr_buf + cbuf->tail,\n\t\t       count * sizeof(void *));\n\telse\n\t\tmemcpy(buf, cbuf->buf.cpy_buf + cbuf->tail,\n\t\t       count * cbuf->elem_sz);\n\n\tcbuf->tail = (cbuf->tail + (count * cbuf->elem_sz)) &\n\t\t     ((cbuf->size - 1) * cbuf->elem_sz);\n\n\treturn 0;\n}", "func_src_after": "int ac_circ_buf_popm(ac_circ_buf_t *cbuf, void *buf, u32 count)\n{\n\tif (circ_count_to_end(cbuf) < count)\n\t\treturn -1;\n\n\tif (cbuf->type == PTR_BUF)\n\t\tmemcpy(buf, cbuf->buf.ptr_buf + cbuf->tail,\n\t\t       count * sizeof(void *));\n\telse\n\t\tmemcpy(buf, cbuf->buf.cpy_buf + cbuf->tail,\n\t\t       (size_t)count * cbuf->elem_sz);\n\n\tcbuf->tail = (cbuf->tail + (count * cbuf->elem_sz)) &\n\t\t     ((cbuf->size - 1) * cbuf->elem_sz);\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 278, "char_end": 311, "line": "\t\t       count * cbuf->elem_sz);\n"}], "added": [{"line_no": 11, "char_start": 278, "char_end": 319, "line": "\t\t       (size_t)count * cbuf->elem_sz);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 287, "char_end": 295, "chars": "(size_t)"}]}, "commit_link": "github.com/ac000/libac/commit/2d6eb46697a185e7ecdbbf0197c405096765cf27", "file_name": "ac_circ_buf.c", "vul_type": "cwe-190", "commit_msg": "ac_circ_buf: Fix some potential unsigned integer overflows\n\nLGTM[0] pointed out some issues in ac_circ_buf.c regarding the\nmalloc(3)'s\n\n  'Multiplication result may overflow 'unsigned int' before it is\n   converted to 'size_t'.'\n\nThis is unlikely to hit in real life, but lets fix it anyway by casting\nthe size/count part of the calculation in the mallocs to size_t.\n\n[0]: https://lgtm.com/\n\nSigned-off-by: Andrew Clayton <andrew@digital-domain.net>", "parent_commit": "536fc22f2c496342e6391b4f81c91ad41816571b", "description": "Write a C function to remove multiple elements from a circular buffer and copy them into another buffer."}
{"func_name": "check_clus_chain", "func_src_before": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tclus_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %u bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "func_src_after": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tuint64_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %\"\n\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 136, "char_end": 162, "line": "\tclus_t count, max_count;\n"}, {"line_no": 32, "char_start": 888, "char_end": 917, "line": "\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 48, "char_start": 1333, "char_end": 1379, "line": "\t\t\t\t\t\"the other file. truncated to %u bytes\",\n"}, {"line_no": 64, "char_start": 1783, "char_end": 1813, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 65, "char_start": 1813, "char_end": 1827, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 66, "char_start": 1827, "char_end": 1852, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 78, "char_start": 2116, "char_end": 2146, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 79, "char_start": 2146, "char_end": 2160, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 80, "char_start": 2160, "char_end": 2185, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 98, "char_start": 2496, "char_end": 2524, "line": "\t\t\t\"truncates to %u bytes\",\n"}], "added": [{"line_no": 5, "char_start": 136, "char_end": 164, "line": "\tuint64_t count, max_count;\n"}, {"line_no": 32, "char_start": 890, "char_end": 928, "line": "\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n"}, {"line_no": 48, "char_start": 1344, "char_end": 1382, "line": "\t\t\t\t\t\"the other file. truncated to %\"\n"}, {"line_no": 49, "char_start": 1382, "char_end": 1404, "line": "\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 65, "char_start": 1808, "char_end": 1830, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 66, "char_start": 1830, "char_end": 1853, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 67, "char_start": 1853, "char_end": 1886, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 79, "char_start": 2150, "char_end": 2172, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 80, "char_start": 2172, "char_end": 2195, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 81, "char_start": 2195, "char_end": 2228, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 99, "char_start": 2539, "char_end": 2576, "line": "\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 141, "chars": "clus"}, {"char_start": 907, "char_end": 908, "chars": "u"}, {"char_start": 1369, "char_end": 1370, "chars": "u"}, {"char_start": 1803, "char_end": 1804, "chars": "u"}, {"char_start": 1826, "char_end": 1833, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2136, "char_end": 2137, "chars": "u"}, {"char_start": 2159, "char_end": 2166, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2514, "char_end": 2515, "chars": "u"}], "added": [{"char_start": 137, "char_end": 143, "chars": "uint64"}, {"char_start": 909, "char_end": 919, "chars": "\" PRIu64 \""}, {"char_start": 1380, "char_end": 1395, "chars": "\"\n\t\t\t\t\tPRIu64 \""}, {"char_start": 1828, "char_end": 1844, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 1866, "char_end": 1867, "chars": " "}, {"char_start": 2170, "char_end": 2186, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 2208, "char_end": 2209, "chars": " "}, {"char_start": 2557, "char_end": 2567, "chars": "\" PRIu64 \""}]}, "commit_link": "github.com/exfatprogs/exfatprogs/commit/c1f48157c38df8d958ab81012abae2470750a785", "file_name": "fsck.c", "vul_type": "cwe-190", "commit_msg": "fsck: fix integer overflow in calculating size\n\nThe size must be 64-bit integer\n\nSigned-off-by: Hyunchul Lee <hyc.lee@gmail.com>", "parent_commit": "edf7a39b7252f4b63915292420aaa63a750f22d9", "description": "Write a C function to validate and repair the cluster chain of a file in an exFAT file system."}
{"func_name": "print_c_function", "func_src_before": "static void print_c_function(const state st) {\n  const char TYPE[] = \"__m256i \";\n  char buf[10];\n  printf(\"static inline void sbox(const %sin0, const %sin1, const %sin2, const %sin3,\\n\"\n      \"const %sin4, const %sin5, const %sin6, const %sin7, %s*out0,\\n\"\n      \"%s*out1, %s*out2, %s*out3, %s*out4, %s*out5, %s*out6,\\n\"\n      \"%s*out7) {\\n\", TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE,\n      TYPE, TYPE, TYPE);\n  for (uint8_t gate = 8; gate < st.num_gates; gate++) {\n    bool ret = get_c_variable_name(st, gate, buf);\n    printf(\"  %s%s = \", ret == true ? TYPE : \"\", buf);\n    get_c_variable_name(st, st.gates[gate].in1, buf);\n    if (st.gates[gate].type == NOT) {\n      printf(\"~%s;\\n\", buf);\n      continue;\n    }\n    printf(\"%s \", buf);\n    switch (st.gates[gate].type) {\n      case AND:\n        printf(\"& \");\n        break;\n      case OR:\n        printf(\"| \");\n        break;\n      case XOR:\n        printf(\"^ \");\n        break;\n      default:\n        assert(false);\n    }\n    get_c_variable_name(st, st.gates[gate].in2, buf);\n    printf(\"%s;\\n\", buf);\n  }\n  printf(\"}\\n\");\n}", "func_src_after": "static void print_c_function(const state st) {\n  const char TYPE[] = \"__m256i \";\n  char buf[10];\n  printf(\"static inline void sbox(const %sin0, const %sin1, const %sin2, const %sin3,\\n\"\n      \"const %sin4, const %sin5, const %sin6, const %sin7, %s*out0,\\n\"\n      \"%s*out1, %s*out2, %s*out3, %s*out4, %s*out5, %s*out6,\\n\"\n      \"%s*out7) {\\n\", TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE, TYPE,\n      TYPE, TYPE, TYPE);\n  for (uint64_t gate = 8; gate < st.num_gates; gate++) {\n    bool ret = get_c_variable_name(st, gate, buf);\n    printf(\"  %s%s = \", ret == true ? TYPE : \"\", buf);\n    get_c_variable_name(st, st.gates[gate].in1, buf);\n    if (st.gates[gate].type == NOT) {\n      printf(\"~%s;\\n\", buf);\n      continue;\n    }\n    printf(\"%s \", buf);\n    switch (st.gates[gate].type) {\n      case AND:\n        printf(\"& \");\n        break;\n      case OR:\n        printf(\"| \");\n        break;\n      case XOR:\n        printf(\"^ \");\n        break;\n      default:\n        assert(false);\n    }\n    get_c_variable_name(st, st.gates[gate].in2, buf);\n    printf(\"%s;\\n\", buf);\n  }\n  printf(\"}\\n\");\n}", "line_changes": {"deleted": [{"line_no": 9, "char_start": 446, "char_end": 502, "line": "  for (uint8_t gate = 8; gate < st.num_gates; gate++) {\n"}], "added": [{"line_no": 9, "char_start": 446, "char_end": 503, "line": "  for (uint64_t gate = 8; gate < st.num_gates; gate++) {\n"}]}, "char_changes": {"deleted": [{"char_start": 457, "char_end": 458, "chars": "8"}], "added": [{"char_start": 457, "char_end": 459, "chars": "64"}]}, "commit_link": "github.com/dansarie/sboxgates/commit/5dffb1e61a9f28ef596684d6e401c3ee166bb4eb", "file_name": "sboxgates.c", "vul_type": "cwe-190", "commit_msg": "Fix integer overflow when generating C source code.", "parent_commit": "7cb0a30fe10f56e862b3d62973f65d8c3222e67a", "description": "Write a C function that prints the definition of an inline function for a substitution box (s-box) with input and output parameters, and a loop that generates bitwise operations based on a given state structure."}
{"func_name": "g72x_init", "func_src_before": "g72x_init (SF_PRIVATE * psf)\n{\tG72x_PRIVATE\t*pg72x ;\n\tint\tbitspersample, bytesperblock, codec ;\n\n\tif (psf->codec_data != NULL)\n\t{\tpsf_log_printf (psf, \"*** psf->codec_data is not NULL.\\n\") ;\n\t\treturn SFE_INTERNAL ;\n\t\t} ;\n\n\tpsf->sf.seekable = SF_FALSE ;\n\n\tif (psf->sf.channels != 1)\n\t\treturn SFE_G72X_NOT_MONO ;\n\n\tif ((pg72x = calloc (1, sizeof (G72x_PRIVATE))) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tpsf->codec_data = (void*) pg72x ;\n\n\tpg72x->block_curr = 0 ;\n\tpg72x->sample_curr = 0 ;\n\n\tswitch (SF_CODEC (psf->sf.format))\n\t{\tcase SF_FORMAT_G721_32 :\n\t\t\t\tcodec = G721_32_BITS_PER_SAMPLE ;\n\t\t\t\tbytesperblock = G721_32_BYTES_PER_BLOCK ;\n\t\t\t\tbitspersample = G721_32_BITS_PER_SAMPLE ;\n\t\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_G723_24:\n\t\t\t\tcodec = G723_24_BITS_PER_SAMPLE ;\n\t\t\t\tbytesperblock = G723_24_BYTES_PER_BLOCK ;\n\t\t\t\tbitspersample = G723_24_BITS_PER_SAMPLE ;\n\t\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_G723_40:\n\t\t\t\tcodec = G723_40_BITS_PER_SAMPLE ;\n\t\t\t\tbytesperblock = G723_40_BYTES_PER_BLOCK ;\n\t\t\t\tbitspersample = G723_40_BITS_PER_SAMPLE ;\n\t\t\t\tbreak ;\n\n\t\tdefault : return SFE_UNIMPLEMENTED ;\n\t\t} ;\n\n\tpsf->filelength = psf_get_filelen (psf) ;\n\tif (psf->filelength < psf->dataoffset)\n\t\tpsf->filelength = psf->dataoffset ;\n\n\tpsf->datalength = psf->filelength - psf->dataoffset ;\n\tif (psf->dataend > 0)\n\t\tpsf->datalength -= psf->filelength - psf->dataend ;\n\n\tif (psf->file.mode == SFM_READ)\n\t{\tpg72x->private = g72x_reader_init (codec, &(pg72x->blocksize), &(pg72x->samplesperblock)) ;\n\t\tif (pg72x->private == NULL)\n\t\t\treturn SFE_MALLOC_FAILED ;\n\n\t\tpg72x->bytesperblock = bytesperblock ;\n\n\t\tpsf->read_short\t\t= g72x_read_s ;\n\t\tpsf->read_int\t\t= g72x_read_i ;\n\t\tpsf->read_float\t\t= g72x_read_f ;\n\t\tpsf->read_double\t= g72x_read_d ;\n\n\t\tpsf->seek = g72x_seek ;\n\n\t\tif (psf->datalength % pg72x->blocksize)\n\t\t{\tpsf_log_printf (psf, \"*** Odd psf->datalength (%D) should be a multiple of %d\\n\", psf->datalength, pg72x->blocksize) ;\n\t\t\tpg72x->blocks_total = (psf->datalength / pg72x->blocksize) + 1 ;\n\t\t\t}\n\t\telse\n\t\t\tpg72x->blocks_total = psf->datalength / pg72x->blocksize ;\n\n\t\tpsf->sf.frames = pg72x->blocks_total * pg72x->samplesperblock ;\n\n\t\tpsf_g72x_decode_block (psf, pg72x) ;\n\t\t}\n\telse if (psf->file.mode == SFM_WRITE)\n\t{\tpg72x->private = g72x_writer_init (codec, &(pg72x->blocksize), &(pg72x->samplesperblock)) ;\n\t\tif (pg72x->private == NULL)\n\t\t\treturn SFE_MALLOC_FAILED ;\n\n\t\tpg72x->bytesperblock = bytesperblock ;\n\n\t\tpsf->write_short\t= g72x_write_s ;\n\t\tpsf->write_int\t\t= g72x_write_i ;\n\t\tpsf->write_float\t= g72x_write_f ;\n\t\tpsf->write_double\t= g72x_write_d ;\n\n\t\tif (psf->datalength % pg72x->blocksize)\n\t\t\tpg72x->blocks_total = (psf->datalength / pg72x->blocksize) + 1 ;\n\t\telse\n\t\t\tpg72x->blocks_total = psf->datalength / pg72x->blocksize ;\n\n\t\tif (psf->datalength > 0)\n\t\t\tpsf->sf.frames = (8 * psf->datalength) / bitspersample ;\n\n\t\tif ((psf->sf.frames * bitspersample) / 8 != psf->datalength)\n\t\t\tpsf_log_printf (psf, \"*** Warning : weird psf->datalength.\\n\") ;\n\t\t} ;\n\n\tpsf->codec_close\t= g72x_close ;\n\n\treturn 0 ;\n} /* g72x_init */", "func_src_after": "g72x_init (SF_PRIVATE * psf)\n{\tG72x_PRIVATE\t*pg72x ;\n\tint\tbitspersample, bytesperblock, codec ;\n\n\tif (psf->codec_data != NULL)\n\t{\tpsf_log_printf (psf, \"*** psf->codec_data is not NULL.\\n\") ;\n\t\treturn SFE_INTERNAL ;\n\t\t} ;\n\n\tpsf->sf.seekable = SF_FALSE ;\n\n\tif (psf->sf.channels != 1)\n\t\treturn SFE_G72X_NOT_MONO ;\n\n\tif ((pg72x = calloc (1, sizeof (G72x_PRIVATE))) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tpsf->codec_data = (void*) pg72x ;\n\n\tpg72x->block_curr = 0 ;\n\tpg72x->sample_curr = 0 ;\n\n\tswitch (SF_CODEC (psf->sf.format))\n\t{\tcase SF_FORMAT_G721_32 :\n\t\t\t\tcodec = G721_32_BITS_PER_SAMPLE ;\n\t\t\t\tbytesperblock = G721_32_BYTES_PER_BLOCK ;\n\t\t\t\tbitspersample = G721_32_BITS_PER_SAMPLE ;\n\t\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_G723_24:\n\t\t\t\tcodec = G723_24_BITS_PER_SAMPLE ;\n\t\t\t\tbytesperblock = G723_24_BYTES_PER_BLOCK ;\n\t\t\t\tbitspersample = G723_24_BITS_PER_SAMPLE ;\n\t\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_G723_40:\n\t\t\t\tcodec = G723_40_BITS_PER_SAMPLE ;\n\t\t\t\tbytesperblock = G723_40_BYTES_PER_BLOCK ;\n\t\t\t\tbitspersample = G723_40_BITS_PER_SAMPLE ;\n\t\t\t\tbreak ;\n\n\t\tdefault : return SFE_UNIMPLEMENTED ;\n\t\t} ;\n\n\tpsf->filelength = psf_get_filelen (psf) ;\n\tif (psf->filelength < psf->dataoffset)\n\t\tpsf->filelength = psf->dataoffset ;\n\n\tpsf->datalength = psf->filelength - psf->dataoffset ;\n\tif (psf->dataend > 0)\n\t\tpsf->datalength -= psf->filelength - psf->dataend ;\n\n\tif (psf->file.mode == SFM_READ)\n\t{\tpg72x->private = g72x_reader_init (codec, &(pg72x->blocksize), &(pg72x->samplesperblock)) ;\n\t\tif (pg72x->private == NULL)\n\t\t\treturn SFE_MALLOC_FAILED ;\n\n\t\tpg72x->bytesperblock = bytesperblock ;\n\n\t\tpsf->read_short\t\t= g72x_read_s ;\n\t\tpsf->read_int\t\t= g72x_read_i ;\n\t\tpsf->read_float\t\t= g72x_read_f ;\n\t\tpsf->read_double\t= g72x_read_d ;\n\n\t\tpsf->seek = g72x_seek ;\n\n\t\tif (psf->datalength % pg72x->blocksize)\n\t\t{\tpsf_log_printf (psf, \"*** Odd psf->datalength (%D) should be a multiple of %d\\n\", psf->datalength, pg72x->blocksize) ;\n\t\t\tpg72x->blocks_total = (psf->datalength / pg72x->blocksize) + 1 ;\n\t\t\t}\n\t\telse\n\t\t\tpg72x->blocks_total = psf->datalength / pg72x->blocksize ;\n\n\t\tpsf->sf.frames = (sf_count_t) pg72x->blocks_total * pg72x->samplesperblock ;\n\n\t\tpsf_g72x_decode_block (psf, pg72x) ;\n\t\t}\n\telse if (psf->file.mode == SFM_WRITE)\n\t{\tpg72x->private = g72x_writer_init (codec, &(pg72x->blocksize), &(pg72x->samplesperblock)) ;\n\t\tif (pg72x->private == NULL)\n\t\t\treturn SFE_MALLOC_FAILED ;\n\n\t\tpg72x->bytesperblock = bytesperblock ;\n\n\t\tpsf->write_short\t= g72x_write_s ;\n\t\tpsf->write_int\t\t= g72x_write_i ;\n\t\tpsf->write_float\t= g72x_write_f ;\n\t\tpsf->write_double\t= g72x_write_d ;\n\n\t\tif (psf->datalength % pg72x->blocksize)\n\t\t\tpg72x->blocks_total = (psf->datalength / pg72x->blocksize) + 1 ;\n\t\telse\n\t\t\tpg72x->blocks_total = psf->datalength / pg72x->blocksize ;\n\n\t\tif (psf->datalength > 0)\n\t\t\tpsf->sf.frames = (8 * psf->datalength) / bitspersample ;\n\n\t\tif ((psf->sf.frames * bitspersample) / 8 != psf->datalength)\n\t\t\tpsf_log_printf (psf, \"*** Warning : weird psf->datalength.\\n\") ;\n\t\t} ;\n\n\tpsf->codec_close\t= g72x_close ;\n\n\treturn 0 ;\n} /* g72x_init */", "line_changes": {"deleted": [{"line_no": 74, "char_start": 2039, "char_end": 2105, "line": "\t\tpsf->sf.frames = pg72x->blocks_total * pg72x->samplesperblock ;\n"}], "added": [{"line_no": 74, "char_start": 2039, "char_end": 2118, "line": "\t\tpsf->sf.frames = (sf_count_t) pg72x->blocks_total * pg72x->samplesperblock ;\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 2057, "char_end": 2070, "chars": " (sf_count_t)"}]}, "commit_link": "github.com/libsndfile/libsndfile/commit/a4f1387ab8084fac7f0301b18e01011be9381672", "file_name": "g72x.c", "vul_type": "cwe-190", "commit_msg": "Update g72x.c\n\nFixing signed integer overflow as reported in https://github.com/libsndfile/libsndfile/issues/757 . A similar fix was applied before https://github.com/libsndfile/libsndfile/pull/818", "parent_commit": "a17e32fda6ed6883bebe0d5f7e1c83cd88409bd6", "description": "Write a C function to initialize a G72x codec with error handling and codec-specific settings based on the audio format."}
{"func_name": "ac_circ_buf_new", "func_src_before": "ac_circ_buf_t *ac_circ_buf_new(u32 size, u32 elem_sz)\n{\n\tac_circ_buf_t *cbuf;\n\n\tif (!is_pow2(size))\n\t\treturn NULL;\n\n\tcbuf = malloc(sizeof(ac_circ_buf_t));\n\tcbuf->head = cbuf->tail = 0;\n\tcbuf->size = size;\n\n\tif (elem_sz == 0) {\n\t\tcbuf->elem_sz = 1;\n\t\tcbuf->type = PTR_BUF;\n\t\tcbuf->buf.ptr_buf = malloc(size * sizeof(void *));\n\t} else {\n\t\tcbuf->elem_sz = elem_sz;\n\t\tcbuf->type = CPY_BUF;\n\t\tcbuf->buf.cpy_buf = malloc(size * elem_sz);\n\t}\n\n\treturn cbuf;\n}", "func_src_after": "ac_circ_buf_t *ac_circ_buf_new(u32 size, u32 elem_sz)\n{\n\tac_circ_buf_t *cbuf;\n\n\tif (!is_pow2(size))\n\t\treturn NULL;\n\n\tcbuf = malloc(sizeof(ac_circ_buf_t));\n\tcbuf->head = cbuf->tail = 0;\n\tcbuf->size = size;\n\n\tif (elem_sz == 0) {\n\t\tcbuf->elem_sz = 1;\n\t\tcbuf->type = PTR_BUF;\n\t\tcbuf->buf.ptr_buf = malloc(size * sizeof(void *));\n\t} else {\n\t\tcbuf->elem_sz = elem_sz;\n\t\tcbuf->type = CPY_BUF;\n\t\tcbuf->buf.cpy_buf = malloc((size_t)size * elem_sz);\n\t}\n\n\treturn cbuf;\n}", "line_changes": {"deleted": [{"line_no": 19, "char_start": 386, "char_end": 432, "line": "\t\tcbuf->buf.cpy_buf = malloc(size * elem_sz);\n"}], "added": [{"line_no": 19, "char_start": 386, "char_end": 440, "line": "\t\tcbuf->buf.cpy_buf = malloc((size_t)size * elem_sz);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 415, "char_end": 423, "chars": "(size_t)"}]}, "commit_link": "github.com/ac000/libac/commit/2d6eb46697a185e7ecdbbf0197c405096765cf27", "file_name": "ac_circ_buf.c", "vul_type": "cwe-190", "commit_msg": "ac_circ_buf: Fix some potential unsigned integer overflows\n\nLGTM[0] pointed out some issues in ac_circ_buf.c regarding the\nmalloc(3)'s\n\n  'Multiplication result may overflow 'unsigned int' before it is\n   converted to 'size_t'.'\n\nThis is unlikely to hit in real life, but lets fix it anyway by casting\nthe size/count part of the calculation in the mallocs to size_t.\n\n[0]: https://lgtm.com/\n\nSigned-off-by: Andrew Clayton <andrew@digital-domain.net>", "parent_commit": "536fc22f2c496342e6391b4f81c91ad41816571b", "description": "Write a C function to initialize a circular buffer that can either hold pointers or copy data, depending on the element size provided."}
{"func_name": "ac_circ_buf_pushm", "func_src_before": "int ac_circ_buf_pushm(ac_circ_buf_t *cbuf, const void *buf, u32 count)\n{\n\tif (circ_space_to_end(cbuf) < count) {\n\t\tif (circ_count(cbuf) == 0 && count <= cbuf->size)\n\t\t\tcbuf->head = cbuf->tail = 0;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\tif (cbuf->type == PTR_BUF)\n\t\tmemcpy(cbuf->buf.ptr_buf + cbuf->head, buf,\n\t\t       count * sizeof(void *));\n\telse\n\t\tmemcpy(cbuf->buf.cpy_buf + cbuf->head, buf,\n\t\t       count * cbuf->elem_sz);\n\n\tcbuf->head = (cbuf->head + (count * cbuf->elem_sz)) &\n\t\t     ((cbuf->size - 1) * cbuf->elem_sz);\n\n\treturn 0;\n}", "func_src_after": "int ac_circ_buf_pushm(ac_circ_buf_t *cbuf, const void *buf, u32 count)\n{\n\tif (circ_space_to_end(cbuf) < count) {\n\t\tif (circ_count(cbuf) == 0 && count <= cbuf->size)\n\t\t\tcbuf->head = cbuf->tail = 0;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\tif (cbuf->type == PTR_BUF)\n\t\tmemcpy(cbuf->buf.ptr_buf + cbuf->head, buf,\n\t\t       count * sizeof(void *));\n\telse\n\t\tmemcpy(cbuf->buf.cpy_buf + cbuf->head, buf,\n\t\t       (size_t)count * cbuf->elem_sz);\n\n\tcbuf->head = (cbuf->head + (count * cbuf->elem_sz)) &\n\t\t     ((cbuf->size - 1) * cbuf->elem_sz);\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 15, "char_start": 382, "char_end": 415, "line": "\t\t       count * cbuf->elem_sz);\n"}], "added": [{"line_no": 15, "char_start": 382, "char_end": 423, "line": "\t\t       (size_t)count * cbuf->elem_sz);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 391, "char_end": 399, "chars": "(size_t)"}]}, "commit_link": "github.com/ac000/libac/commit/2d6eb46697a185e7ecdbbf0197c405096765cf27", "file_name": "ac_circ_buf.c", "vul_type": "cwe-190", "commit_msg": "ac_circ_buf: Fix some potential unsigned integer overflows\n\nLGTM[0] pointed out some issues in ac_circ_buf.c regarding the\nmalloc(3)'s\n\n  'Multiplication result may overflow 'unsigned int' before it is\n   converted to 'size_t'.'\n\nThis is unlikely to hit in real life, but lets fix it anyway by casting\nthe size/count part of the calculation in the mallocs to size_t.\n\n[0]: https://lgtm.com/\n\nSigned-off-by: Andrew Clayton <andrew@digital-domain.net>", "parent_commit": "536fc22f2c496342e6391b4f81c91ad41816571b", "description": "Write a C function to add multiple elements to a circular buffer, returning 0 on success or -1 if there isn't enough space."}
{"func_name": "gsm610_init", "func_src_before": "gsm610_init\t(SF_PRIVATE *psf)\n{\tGSM610_PRIVATE\t*pgsm610 ;\n\tint\t\ttrue_flag = 1 ;\n\n\tif (psf->codec_data != NULL)\n\t{\tpsf_log_printf (psf, \"*** psf->codec_data is not NULL.\\n\") ;\n\t\treturn SFE_INTERNAL ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_RDWR)\n\t\treturn SFE_BAD_MODE_RW ;\n\n\tpsf->sf.seekable = SF_FALSE ;\n\n\tif ((pgsm610 = calloc (1, sizeof (GSM610_PRIVATE))) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tpsf->codec_data = pgsm610 ;\n\n\tmemset (pgsm610, 0, sizeof (GSM610_PRIVATE)) ;\n\n/*============================================================\n\nNeed separate gsm_data structs for encode and decode.\n\n============================================================*/\n\n\tif ((pgsm610->gsm_data = gsm_create ()) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tswitch (SF_CONTAINER (psf->sf.format))\n\t{\tcase SF_FORMAT_WAV :\n\t\tcase SF_FORMAT_WAVEX :\n\t\tcase SF_FORMAT_W64 :\n\t\t\tgsm_option (pgsm610->gsm_data, GSM_OPT_WAV49, &true_flag) ;\n\n\t\t\tpgsm610->encode_block = gsm610_wav_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_wav_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = WAVLIKE_GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = WAVLIKE_GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_AIFF :\n\t\tcase SF_FORMAT_RAW :\n\t\t\tpgsm610->encode_block = gsm610_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tdefault :\n\t\t\treturn SFE_INTERNAL ;\n\t\t\tbreak ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_READ)\n\t{\tif (psf->datalength % pgsm610->blocksize == 0)\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\telse if (psf->datalength % pgsm610->blocksize == 1 && pgsm610->blocksize == GSM610_BLOCKSIZE)\n\t\t{\t/*\n\t\t\t**\tWeird AIFF specific case.\n\t\t\t**\tAIFF chunks must be at an even offset from the start of file and\n\t\t\t**\tGSM610_BLOCKSIZE is odd which can result in an odd length SSND\n\t\t\t**\tchunk. The SSND chunk then gets padded on write which means that\n\t\t\t**\twhen it is read the datalength is too big by 1.\n\t\t\t*/\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\t\t}\n\t\telse\n\t\t{\tpsf_log_printf (psf, \"*** Warning : data chunk seems to be truncated.\\n\") ;\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize + 1 ;\n\t\t\t} ;\n\n\t\tpsf->sf.frames = pgsm610->samplesperblock * pgsm610->blocks ;\n\n\t\tpsf_fseek (psf, psf->dataoffset, SEEK_SET) ;\n\n\t\tpgsm610->decode_block (psf, pgsm610) ;\t/* Read first block. */\n\n\t\tpsf->read_short\t\t= gsm610_read_s ;\n\t\tpsf->read_int\t\t= gsm610_read_i ;\n\t\tpsf->read_float\t\t= gsm610_read_f ;\n\t\tpsf->read_double\t= gsm610_read_d ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_WRITE)\n\t{\tpgsm610->blockcount = 0 ;\n\t\tpgsm610->samplecount = 0 ;\n\n\t\tpsf->write_short\t= gsm610_write_s ;\n\t\tpsf->write_int\t\t= gsm610_write_i ;\n\t\tpsf->write_float\t= gsm610_write_f ;\n\t\tpsf->write_double\t= gsm610_write_d ;\n\t\t} ;\n\n\tpsf->codec_close = gsm610_close ;\n\n\tpsf->seek = gsm610_seek ;\n\n\tpsf->filelength = psf_get_filelen (psf) ;\n\tpsf->datalength = psf->filelength - psf->dataoffset ;\n\n\treturn 0 ;\n} /* gsm610_init */", "func_src_after": "gsm610_init\t(SF_PRIVATE *psf)\n{\tGSM610_PRIVATE\t*pgsm610 ;\n\tint\t\ttrue_flag = 1 ;\n\n\tif (psf->codec_data != NULL)\n\t{\tpsf_log_printf (psf, \"*** psf->codec_data is not NULL.\\n\") ;\n\t\treturn SFE_INTERNAL ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_RDWR)\n\t\treturn SFE_BAD_MODE_RW ;\n\n\tpsf->sf.seekable = SF_FALSE ;\n\n\tif ((pgsm610 = calloc (1, sizeof (GSM610_PRIVATE))) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tpsf->codec_data = pgsm610 ;\n\n\tmemset (pgsm610, 0, sizeof (GSM610_PRIVATE)) ;\n\n/*============================================================\n\nNeed separate gsm_data structs for encode and decode.\n\n============================================================*/\n\n\tif ((pgsm610->gsm_data = gsm_create ()) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tswitch (SF_CONTAINER (psf->sf.format))\n\t{\tcase SF_FORMAT_WAV :\n\t\tcase SF_FORMAT_WAVEX :\n\t\tcase SF_FORMAT_W64 :\n\t\t\tgsm_option (pgsm610->gsm_data, GSM_OPT_WAV49, &true_flag) ;\n\n\t\t\tpgsm610->encode_block = gsm610_wav_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_wav_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = WAVLIKE_GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = WAVLIKE_GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_AIFF :\n\t\tcase SF_FORMAT_RAW :\n\t\t\tpgsm610->encode_block = gsm610_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tdefault :\n\t\t\treturn SFE_INTERNAL ;\n\t\t\tbreak ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_READ)\n\t{\tif (psf->datalength % pgsm610->blocksize == 0)\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\telse if (psf->datalength % pgsm610->blocksize == 1 && pgsm610->blocksize == GSM610_BLOCKSIZE)\n\t\t{\t/*\n\t\t\t**\tWeird AIFF specific case.\n\t\t\t**\tAIFF chunks must be at an even offset from the start of file and\n\t\t\t**\tGSM610_BLOCKSIZE is odd which can result in an odd length SSND\n\t\t\t**\tchunk. The SSND chunk then gets padded on write which means that\n\t\t\t**\twhen it is read the datalength is too big by 1.\n\t\t\t*/\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\t\t}\n\t\telse\n\t\t{\tpsf_log_printf (psf, \"*** Warning : data chunk seems to be truncated.\\n\") ;\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize + 1 ;\n\t\t\t} ;\n\n\t\tpsf->sf.frames = (sf_count_t) pgsm610->samplesperblock * pgsm610->blocks ;\n\n\t\tpsf_fseek (psf, psf->dataoffset, SEEK_SET) ;\n\n\t\tpgsm610->decode_block (psf, pgsm610) ;\t/* Read first block. */\n\n\t\tpsf->read_short\t\t= gsm610_read_s ;\n\t\tpsf->read_int\t\t= gsm610_read_i ;\n\t\tpsf->read_float\t\t= gsm610_read_f ;\n\t\tpsf->read_double\t= gsm610_read_d ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_WRITE)\n\t{\tpgsm610->blockcount = 0 ;\n\t\tpgsm610->samplecount = 0 ;\n\n\t\tpsf->write_short\t= gsm610_write_s ;\n\t\tpsf->write_int\t\t= gsm610_write_i ;\n\t\tpsf->write_float\t= gsm610_write_f ;\n\t\tpsf->write_double\t= gsm610_write_d ;\n\t\t} ;\n\n\tpsf->codec_close = gsm610_close ;\n\n\tpsf->seek = gsm610_seek ;\n\n\tpsf->filelength = psf_get_filelen (psf) ;\n\tpsf->datalength = psf->filelength - psf->dataoffset ;\n\n\treturn 0 ;\n} /* gsm610_init */", "line_changes": {"deleted": [{"line_no": 76, "char_start": 2210, "char_end": 2274, "line": "\t\tpsf->sf.frames = pgsm610->samplesperblock * pgsm610->blocks ;\n"}], "added": [{"line_no": 76, "char_start": 2210, "char_end": 2287, "line": "\t\tpsf->sf.frames = (sf_count_t) pgsm610->samplesperblock * pgsm610->blocks ;\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 2228, "char_end": 2241, "chars": " (sf_count_t)"}]}, "commit_link": "github.com/libsndfile/libsndfile/commit/d6f83cd4feb154efcf5614601985ae2ce9d9fa6d", "file_name": "gsm610.c", "vul_type": "cwe-190", "commit_msg": "gsm610: Fix signed integer overflow\n\nRelated to libsndfile#785\n\nCo-authored-by: evpobr <evpobr@gmail.com>", "parent_commit": "fc298c9d9324b1fe01329f675118561eef92b9e4", "description": "In C, write a function to initialize the GSM610 codec with proper settings based on the audio file format."}
{"func_name": "stralgoLCS", "func_src_before": "void stralgoLCS(client *c) {\n    uint32_t i, j;\n    long long minmatchlen = 0;\n    sds a = NULL, b = NULL;\n    int getlen = 0, getidx = 0, withmatchlen = 0;\n    robj *obja = NULL, *objb = NULL;\n\n    for (j = 2; j < (uint32_t)c->argc; j++) {\n        char *opt = c->argv[j]->ptr;\n        int moreargs = (c->argc-1) - j;\n\n        if (!strcasecmp(opt,\"IDX\")) {\n            getidx = 1;\n        } else if (!strcasecmp(opt,\"LEN\")) {\n            getlen = 1;\n        } else if (!strcasecmp(opt,\"WITHMATCHLEN\")) {\n            withmatchlen = 1;\n        } else if (!strcasecmp(opt,\"MINMATCHLEN\") && moreargs) {\n            if (getLongLongFromObjectOrReply(c,c->argv[j+1],&minmatchlen,NULL)\n                != C_OK) goto cleanup;\n            if (minmatchlen < 0) minmatchlen = 0;\n            j++;\n        } else if (!strcasecmp(opt,\"STRINGS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            a = c->argv[j+1]->ptr;\n            b = c->argv[j+2]->ptr;\n            j += 2;\n        } else if (!strcasecmp(opt,\"KEYS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            obja = lookupKeyRead(c->db,c->argv[j+1]);\n            objb = lookupKeyRead(c->db,c->argv[j+2]);\n            if ((obja && obja->type != OBJ_STRING) ||\n                (objb && objb->type != OBJ_STRING))\n            {\n                addReplyError(c,\n                    \"The specified keys must contain string values\");\n                /* Don't cleanup the objects, we need to do that\n                 * only after calling getDecodedObject(). */\n                obja = NULL;\n                objb = NULL;\n                goto cleanup;\n            }\n            obja = obja ? getDecodedObject(obja) : createStringObject(\"\",0);\n            objb = objb ? getDecodedObject(objb) : createStringObject(\"\",0);\n            a = obja->ptr;\n            b = objb->ptr;\n            j += 2;\n        } else {\n            addReplyErrorObject(c,shared.syntaxerr);\n            goto cleanup;\n        }\n    }\n\n    /* Complain if the user passed ambiguous parameters. */\n    if (a == NULL) {\n        addReplyError(c,\"Please specify two strings: \"\n                        \"STRINGS or KEYS options are mandatory\");\n        goto cleanup;\n    } else if (getlen && getidx) {\n        addReplyError(c,\n            \"If you want both the length and indexes, please \"\n            \"just use IDX.\");\n        goto cleanup;\n    }\n\n    /* Compute the LCS using the vanilla dynamic programming technique of\n     * building a table of LCS(x,y) substrings. */\n    uint32_t alen = sdslen(a);\n    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {\n            if (i == 0 || j == 0) {\n                /* If one substring has length of zero, the\n                 * LCS length is zero. */\n                LCS(i,j) = 0;\n            } else if (a[i-1] == b[j-1]) {\n                /* The len LCS (and the LCS itself) of two\n                 * sequences with the same final character, is the\n                 * LCS of the two sequences without the last char\n                 * plus that last char. */\n                LCS(i,j) = LCS(i-1,j-1)+1;\n            } else {\n                /* If the last character is different, take the longest\n                 * between the LCS of the first string and the second\n                 * minus the last char, and the reverse. */\n                uint32_t lcs1 = LCS(i-1,j);\n                uint32_t lcs2 = LCS(i,j-1);\n                LCS(i,j) = lcs1 > lcs2 ? lcs1 : lcs2;\n            }\n        }\n    }\n\n    /* Store the actual LCS string in \"result\" if needed. We create\n     * it backward, but the length is already known, we store it into idx. */\n    uint32_t idx = LCS(alen,blen);\n    sds result = NULL;        /* Resulting LCS string. */\n    void *arraylenptr = NULL; /* Deferred length of the array for IDX. */\n    uint32_t arange_start = alen, /* alen signals that values are not set. */\n             arange_end = 0,\n             brange_start = 0,\n             brange_end = 0;\n\n    /* Do we need to compute the actual LCS string? Allocate it in that case. */\n    int computelcs = getidx || !getlen;\n    if (computelcs) result = sdsnewlen(SDS_NOINIT,idx);\n\n    /* Start with a deferred array if we have to emit the ranges. */\n    uint32_t arraylen = 0;  /* Number of ranges emitted in the array. */\n    if (getidx) {\n        addReplyMapLen(c,2);\n        addReplyBulkCString(c,\"matches\");\n        arraylenptr = addReplyDeferredLen(c);\n    }\n\n    i = alen, j = blen;\n    while (computelcs && i > 0 && j > 0) {\n        int emit_range = 0;\n        if (a[i-1] == b[j-1]) {\n            /* If there is a match, store the character and reduce\n             * the indexes to look for a new match. */\n            result[idx-1] = a[i-1];\n\n            /* Track the current range. */\n            if (arange_start == alen) {\n                arange_start = i-1;\n                arange_end = i-1;\n                brange_start = j-1;\n                brange_end = j-1;\n            } else {\n                /* Let's see if we can extend the range backward since\n                 * it is contiguous. */\n                if (arange_start == i && brange_start == j) {\n                    arange_start--;\n                    brange_start--;\n                } else {\n                    emit_range = 1;\n                }\n            }\n            /* Emit the range if we matched with the first byte of\n             * one of the two strings. We'll exit the loop ASAP. */\n            if (arange_start == 0 || brange_start == 0) emit_range = 1;\n            idx--; i--; j--;\n        } else {\n            /* Otherwise reduce i and j depending on the largest\n             * LCS between, to understand what direction we need to go. */\n            uint32_t lcs1 = LCS(i-1,j);\n            uint32_t lcs2 = LCS(i,j-1);\n            if (lcs1 > lcs2)\n                i--;\n            else\n                j--;\n            if (arange_start != alen) emit_range = 1;\n        }\n\n        /* Emit the current range if needed. */\n        uint32_t match_len = arange_end - arange_start + 1;\n        if (emit_range) {\n            if (minmatchlen == 0 || match_len >= minmatchlen) {\n                if (arraylenptr) {\n                    addReplyArrayLen(c,2+withmatchlen);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,arange_start);\n                    addReplyLongLong(c,arange_end);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,brange_start);\n                    addReplyLongLong(c,brange_end);\n                    if (withmatchlen) addReplyLongLong(c,match_len);\n                    arraylen++;\n                }\n            }\n            arange_start = alen; /* Restart at the next match. */\n        }\n    }\n\n    /* Signal modified key, increment dirty, ... */\n\n    /* Reply depending on the given options. */\n    if (arraylenptr) {\n        addReplyBulkCString(c,\"len\");\n        addReplyLongLong(c,LCS(alen,blen));\n        setDeferredArrayLen(c,arraylenptr,arraylen);\n    } else if (getlen) {\n        addReplyLongLong(c,LCS(alen,blen));\n    } else {\n        addReplyBulkSds(c,result);\n        result = NULL;\n    }\n\n    /* Cleanup. */\n    sdsfree(result);\n    zfree(lcs);\n\ncleanup:\n    if (obja) decrRefCount(obja);\n    if (objb) decrRefCount(objb);\n    return;\n}", "func_src_after": "void stralgoLCS(client *c) {\n    uint32_t i, j;\n    long long minmatchlen = 0;\n    sds a = NULL, b = NULL;\n    int getlen = 0, getidx = 0, withmatchlen = 0;\n    robj *obja = NULL, *objb = NULL;\n\n    for (j = 2; j < (uint32_t)c->argc; j++) {\n        char *opt = c->argv[j]->ptr;\n        int moreargs = (c->argc-1) - j;\n\n        if (!strcasecmp(opt,\"IDX\")) {\n            getidx = 1;\n        } else if (!strcasecmp(opt,\"LEN\")) {\n            getlen = 1;\n        } else if (!strcasecmp(opt,\"WITHMATCHLEN\")) {\n            withmatchlen = 1;\n        } else if (!strcasecmp(opt,\"MINMATCHLEN\") && moreargs) {\n            if (getLongLongFromObjectOrReply(c,c->argv[j+1],&minmatchlen,NULL)\n                != C_OK) goto cleanup;\n            if (minmatchlen < 0) minmatchlen = 0;\n            j++;\n        } else if (!strcasecmp(opt,\"STRINGS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            a = c->argv[j+1]->ptr;\n            b = c->argv[j+2]->ptr;\n            j += 2;\n        } else if (!strcasecmp(opt,\"KEYS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            obja = lookupKeyRead(c->db,c->argv[j+1]);\n            objb = lookupKeyRead(c->db,c->argv[j+2]);\n            if ((obja && obja->type != OBJ_STRING) ||\n                (objb && objb->type != OBJ_STRING))\n            {\n                addReplyError(c,\n                    \"The specified keys must contain string values\");\n                /* Don't cleanup the objects, we need to do that\n                 * only after calling getDecodedObject(). */\n                obja = NULL;\n                objb = NULL;\n                goto cleanup;\n            }\n            obja = obja ? getDecodedObject(obja) : createStringObject(\"\",0);\n            objb = objb ? getDecodedObject(objb) : createStringObject(\"\",0);\n            a = obja->ptr;\n            b = objb->ptr;\n            j += 2;\n        } else {\n            addReplyErrorObject(c,shared.syntaxerr);\n            goto cleanup;\n        }\n    }\n\n    /* Complain if the user passed ambiguous parameters. */\n    if (a == NULL) {\n        addReplyError(c,\"Please specify two strings: \"\n                        \"STRINGS or KEYS options are mandatory\");\n        goto cleanup;\n    } else if (getlen && getidx) {\n        addReplyError(c,\n            \"If you want both the length and indexes, please \"\n            \"just use IDX.\");\n        goto cleanup;\n    }\n\n    /* Compute the LCS using the vanilla dynamic programming technique of\n     * building a table of LCS(x,y) substrings. */\n    uint32_t alen = sdslen(a);\n    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((size_t)(alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {\n            if (i == 0 || j == 0) {\n                /* If one substring has length of zero, the\n                 * LCS length is zero. */\n                LCS(i,j) = 0;\n            } else if (a[i-1] == b[j-1]) {\n                /* The len LCS (and the LCS itself) of two\n                 * sequences with the same final character, is the\n                 * LCS of the two sequences without the last char\n                 * plus that last char. */\n                LCS(i,j) = LCS(i-1,j-1)+1;\n            } else {\n                /* If the last character is different, take the longest\n                 * between the LCS of the first string and the second\n                 * minus the last char, and the reverse. */\n                uint32_t lcs1 = LCS(i-1,j);\n                uint32_t lcs2 = LCS(i,j-1);\n                LCS(i,j) = lcs1 > lcs2 ? lcs1 : lcs2;\n            }\n        }\n    }\n\n    /* Store the actual LCS string in \"result\" if needed. We create\n     * it backward, but the length is already known, we store it into idx. */\n    uint32_t idx = LCS(alen,blen);\n    sds result = NULL;        /* Resulting LCS string. */\n    void *arraylenptr = NULL; /* Deferred length of the array for IDX. */\n    uint32_t arange_start = alen, /* alen signals that values are not set. */\n             arange_end = 0,\n             brange_start = 0,\n             brange_end = 0;\n\n    /* Do we need to compute the actual LCS string? Allocate it in that case. */\n    int computelcs = getidx || !getlen;\n    if (computelcs) result = sdsnewlen(SDS_NOINIT,idx);\n\n    /* Start with a deferred array if we have to emit the ranges. */\n    uint32_t arraylen = 0;  /* Number of ranges emitted in the array. */\n    if (getidx) {\n        addReplyMapLen(c,2);\n        addReplyBulkCString(c,\"matches\");\n        arraylenptr = addReplyDeferredLen(c);\n    }\n\n    i = alen, j = blen;\n    while (computelcs && i > 0 && j > 0) {\n        int emit_range = 0;\n        if (a[i-1] == b[j-1]) {\n            /* If there is a match, store the character and reduce\n             * the indexes to look for a new match. */\n            result[idx-1] = a[i-1];\n\n            /* Track the current range. */\n            if (arange_start == alen) {\n                arange_start = i-1;\n                arange_end = i-1;\n                brange_start = j-1;\n                brange_end = j-1;\n            } else {\n                /* Let's see if we can extend the range backward since\n                 * it is contiguous. */\n                if (arange_start == i && brange_start == j) {\n                    arange_start--;\n                    brange_start--;\n                } else {\n                    emit_range = 1;\n                }\n            }\n            /* Emit the range if we matched with the first byte of\n             * one of the two strings. We'll exit the loop ASAP. */\n            if (arange_start == 0 || brange_start == 0) emit_range = 1;\n            idx--; i--; j--;\n        } else {\n            /* Otherwise reduce i and j depending on the largest\n             * LCS between, to understand what direction we need to go. */\n            uint32_t lcs1 = LCS(i-1,j);\n            uint32_t lcs2 = LCS(i,j-1);\n            if (lcs1 > lcs2)\n                i--;\n            else\n                j--;\n            if (arange_start != alen) emit_range = 1;\n        }\n\n        /* Emit the current range if needed. */\n        uint32_t match_len = arange_end - arange_start + 1;\n        if (emit_range) {\n            if (minmatchlen == 0 || match_len >= minmatchlen) {\n                if (arraylenptr) {\n                    addReplyArrayLen(c,2+withmatchlen);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,arange_start);\n                    addReplyLongLong(c,arange_end);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,brange_start);\n                    addReplyLongLong(c,brange_end);\n                    if (withmatchlen) addReplyLongLong(c,match_len);\n                    arraylen++;\n                }\n            }\n            arange_start = alen; /* Restart at the next match. */\n        }\n    }\n\n    /* Signal modified key, increment dirty, ... */\n\n    /* Reply depending on the given options. */\n    if (arraylenptr) {\n        addReplyBulkCString(c,\"len\");\n        addReplyLongLong(c,LCS(alen,blen));\n        setDeferredArrayLen(c,arraylenptr,arraylen);\n    } else if (getlen) {\n        addReplyLongLong(c,LCS(alen,blen));\n    } else {\n        addReplyBulkSds(c,result);\n        result = NULL;\n    }\n\n    /* Cleanup. */\n    sdsfree(result);\n    zfree(lcs);\n\ncleanup:\n    if (obja) decrRefCount(obja);\n    if (objb) decrRefCount(objb);\n    return;\n}", "line_changes": {"deleted": [{"line_no": 80, "char_start": 2951, "char_end": 3016, "line": "    uint32_t *lcs = zmalloc((alen+1)*(blen+1)*sizeof(uint32_t));\n"}], "added": [{"line_no": 80, "char_start": 2951, "char_end": 3024, "line": "    uint32_t *lcs = zmalloc((size_t)(alen+1)*(blen+1)*sizeof(uint32_t));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 2980, "char_end": 2988, "chars": "size_t)("}]}, "commit_link": "github.com/oranagra/redis/commit/f0c5f920d0f88bd8aa376a2c05af4902789d1ef9", "file_name": "t_string.c", "vul_type": "cwe-190", "commit_msg": "Fix integer overflow in STRALGO LCS (CVE-2021-29477)\n\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\nthe STRALGO LCS command to corrupt the heap and potentially result with remote\ncode execution.", "parent_commit": "29900d4e6bccdf3691bedf0ea9a5d84863fa3592", "description": "Write a C function named `stralgoLCS` that computes the longest common subsequence (LCS) of two strings."}
{"func_name": "ubpf_load_elf", "func_src_before": "ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_size, char **errmsg)\n{\n    struct bounds b = { .base=elf, .size=elf_size };\n    void *text_copy = NULL;\n    int i;\n\n    const Elf64_Ehdr *ehdr = bounds_check(&b, 0, sizeof(*ehdr));\n    if (!ehdr) {\n        *errmsg = ubpf_error(\"not enough data for ELF header\");\n        goto error;\n    }\n\n    if (memcmp(ehdr->e_ident, ELFMAG, SELFMAG)) {\n        *errmsg = ubpf_error(\"wrong magic\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_CLASS] != ELFCLASS64) {\n        *errmsg = ubpf_error(\"wrong class\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB) {\n        *errmsg = ubpf_error(\"wrong byte order\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_VERSION] != 1) {\n        *errmsg = ubpf_error(\"wrong version\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_OSABI] != ELFOSABI_NONE) {\n        *errmsg = ubpf_error(\"wrong OS ABI\");\n        goto error;\n    }\n\n    if (ehdr->e_type != ET_REL) {\n        *errmsg = ubpf_error(\"wrong type, expected relocatable\");\n        goto error;\n    }\n\n    if (ehdr->e_machine != EM_NONE && ehdr->e_machine != EM_BPF) {\n        *errmsg = ubpf_error(\"wrong machine, expected none or BPF, got %d\",\n                             ehdr->e_machine);\n        goto error;\n    }\n\n    if (ehdr->e_shnum > MAX_SECTIONS) {\n        *errmsg = ubpf_error(\"too many sections\");\n        goto error;\n    }\n\n    /* Parse section headers into an array */\n    struct section sections[MAX_SECTIONS];\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = bounds_check(&b, ehdr->e_shoff + i*ehdr->e_shentsize, sizeof(*shdr));\n        if (!shdr) {\n            *errmsg = ubpf_error(\"bad section header offset or size\");\n            goto error;\n        }\n\n        const void *data = bounds_check(&b, shdr->sh_offset, shdr->sh_size);\n        if (!data) {\n            *errmsg = ubpf_error(\"bad section offset or size\");\n            goto error;\n        }\n\n        sections[i].shdr = shdr;\n        sections[i].data = data;\n        sections[i].size = shdr->sh_size;\n    }\n\n    /* Find first text section */\n    int text_shndx = 0;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = sections[i].shdr;\n        if (shdr->sh_type == SHT_PROGBITS &&\n                shdr->sh_flags == (SHF_ALLOC|SHF_EXECINSTR)) {\n            text_shndx = i;\n            break;\n        }\n    }\n\n    if (!text_shndx) {\n        *errmsg = ubpf_error(\"text section not found\");\n        goto error;\n    }\n\n    struct section *text = &sections[text_shndx];\n\n    /* May need to modify text for relocations, so make a copy */\n    text_copy = malloc(text->size);\n    if (!text_copy) {\n        *errmsg = ubpf_error(\"failed to allocate memory\");\n        goto error;\n    }\n    memcpy(text_copy, text->data, text->size);\n\n    /* Process each relocation section */\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        struct section *rel = &sections[i];\n        if (rel->shdr->sh_type != SHT_REL) {\n            continue;\n        } else if (rel->shdr->sh_info != text_shndx) {\n            continue;\n        }\n\n        const Elf64_Rel *rs = rel->data;\n\n        if (rel->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad symbol table section index\");\n            goto error;\n        }\n\n        struct section *symtab = &sections[rel->shdr->sh_link];\n        const Elf64_Sym *syms = symtab->data;\n        uint32_t num_syms = symtab->size/sizeof(syms[0]);\n\n        if (symtab->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad string table section index\");\n            goto error;\n        }\n\n        struct section *strtab = &sections[symtab->shdr->sh_link];\n        const char *strings = strtab->data;\n\n        int j;\n        for (j = 0; j < rel->size/sizeof(Elf64_Rel); j++) {\n            const Elf64_Rel *r = &rs[j];\n\n            if (ELF64_R_TYPE(r->r_info) != 2) {\n                *errmsg = ubpf_error(\"bad relocation type %u\", ELF64_R_TYPE(r->r_info));\n                goto error;\n            }\n\n            uint32_t sym_idx = ELF64_R_SYM(r->r_info);\n            if (sym_idx >= num_syms) {\n                *errmsg = ubpf_error(\"bad symbol index\");\n                goto error;\n            }\n\n            const Elf64_Sym *sym = &syms[sym_idx];\n\n            if (sym->st_name >= strtab->size) {\n                *errmsg = ubpf_error(\"bad symbol name\");\n                goto error;\n            }\n\n            const char *sym_name = strings + sym->st_name;\n\n            if (r->r_offset + 8 > text->size) {\n                *errmsg = ubpf_error(\"bad relocation offset\");\n                goto error;\n            }\n\n            unsigned int imm = ubpf_lookup_registered_function(vm, sym_name);\n            if (imm == -1) {\n                *errmsg = ubpf_error(\"function '%s' not found\", sym_name);\n                goto error;\n            }\n\n            *(uint32_t *)(text_copy + r->r_offset + 4) = imm;\n        }\n    }\n\n    int rv = ubpf_load(vm, text_copy, sections[text_shndx].size, errmsg);\n    free(text_copy);\n    return rv;\n\nerror:\n    free(text_copy);\n    return -1;\n}", "func_src_after": "ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_size, char **errmsg)\n{\n    struct bounds b = { .base=elf, .size=elf_size };\n    void *text_copy = NULL;\n    int i;\n\n    const Elf64_Ehdr *ehdr = bounds_check(&b, 0, sizeof(*ehdr));\n    if (!ehdr) {\n        *errmsg = ubpf_error(\"not enough data for ELF header\");\n        goto error;\n    }\n\n    if (memcmp(ehdr->e_ident, ELFMAG, SELFMAG)) {\n        *errmsg = ubpf_error(\"wrong magic\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_CLASS] != ELFCLASS64) {\n        *errmsg = ubpf_error(\"wrong class\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB) {\n        *errmsg = ubpf_error(\"wrong byte order\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_VERSION] != 1) {\n        *errmsg = ubpf_error(\"wrong version\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_OSABI] != ELFOSABI_NONE) {\n        *errmsg = ubpf_error(\"wrong OS ABI\");\n        goto error;\n    }\n\n    if (ehdr->e_type != ET_REL) {\n        *errmsg = ubpf_error(\"wrong type, expected relocatable\");\n        goto error;\n    }\n\n    if (ehdr->e_machine != EM_NONE && ehdr->e_machine != EM_BPF) {\n        *errmsg = ubpf_error(\"wrong machine, expected none or BPF, got %d\",\n                             ehdr->e_machine);\n        goto error;\n    }\n\n    if (ehdr->e_shnum > MAX_SECTIONS) {\n        *errmsg = ubpf_error(\"too many sections\");\n        goto error;\n    }\n\n    /* Parse section headers into an array */\n    struct section sections[MAX_SECTIONS];\n    uint64_t shoff = ehdr->e_shoff;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = bounds_check(&b, shoff, sizeof(*shdr));\n        shoff += ehdr->e_shentsize;\n        if (!shdr) {\n            *errmsg = ubpf_error(\"bad section header offset or size\");\n            goto error;\n        }\n\n        const void *data = bounds_check(&b, shdr->sh_offset, shdr->sh_size);\n        if (!data) {\n            *errmsg = ubpf_error(\"bad section offset or size\");\n            goto error;\n        }\n\n        sections[i].shdr = shdr;\n        sections[i].data = data;\n        sections[i].size = shdr->sh_size;\n    }\n\n    /* Find first text section */\n    int text_shndx = 0;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = sections[i].shdr;\n        if (shdr->sh_type == SHT_PROGBITS &&\n                shdr->sh_flags == (SHF_ALLOC|SHF_EXECINSTR)) {\n            text_shndx = i;\n            break;\n        }\n    }\n\n    if (!text_shndx) {\n        *errmsg = ubpf_error(\"text section not found\");\n        goto error;\n    }\n\n    struct section *text = &sections[text_shndx];\n\n    /* May need to modify text for relocations, so make a copy */\n    text_copy = malloc(text->size);\n    if (!text_copy) {\n        *errmsg = ubpf_error(\"failed to allocate memory\");\n        goto error;\n    }\n    memcpy(text_copy, text->data, text->size);\n\n    /* Process each relocation section */\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        struct section *rel = &sections[i];\n        if (rel->shdr->sh_type != SHT_REL) {\n            continue;\n        } else if (rel->shdr->sh_info != text_shndx) {\n            continue;\n        }\n\n        const Elf64_Rel *rs = rel->data;\n\n        if (rel->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad symbol table section index\");\n            goto error;\n        }\n\n        struct section *symtab = &sections[rel->shdr->sh_link];\n        const Elf64_Sym *syms = symtab->data;\n        uint32_t num_syms = symtab->size/sizeof(syms[0]);\n\n        if (symtab->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad string table section index\");\n            goto error;\n        }\n\n        struct section *strtab = &sections[symtab->shdr->sh_link];\n        const char *strings = strtab->data;\n\n        int j;\n        for (j = 0; j < rel->size/sizeof(Elf64_Rel); j++) {\n            const Elf64_Rel *r = &rs[j];\n\n            if (ELF64_R_TYPE(r->r_info) != 2) {\n                *errmsg = ubpf_error(\"bad relocation type %u\", ELF64_R_TYPE(r->r_info));\n                goto error;\n            }\n\n            uint32_t sym_idx = ELF64_R_SYM(r->r_info);\n            if (sym_idx >= num_syms) {\n                *errmsg = ubpf_error(\"bad symbol index\");\n                goto error;\n            }\n\n            const Elf64_Sym *sym = &syms[sym_idx];\n\n            if (sym->st_name >= strtab->size) {\n                *errmsg = ubpf_error(\"bad symbol name\");\n                goto error;\n            }\n\n            const char *sym_name = strings + sym->st_name;\n\n            if (r->r_offset + 8 > text->size) {\n                *errmsg = ubpf_error(\"bad relocation offset\");\n                goto error;\n            }\n\n            unsigned int imm = ubpf_lookup_registered_function(vm, sym_name);\n            if (imm == -1) {\n                *errmsg = ubpf_error(\"function '%s' not found\", sym_name);\n                goto error;\n            }\n\n            *(uint32_t *)(text_copy + r->r_offset + 4) = imm;\n        }\n    }\n\n    int rv = ubpf_load(vm, text_copy, sections[text_shndx].size, errmsg);\n    free(text_copy);\n    return rv;\n\nerror:\n    free(text_copy);\n    return -1;\n}", "line_changes": {"deleted": [{"line_no": 57, "char_start": 1554, "char_end": 1657, "line": "        const Elf64_Shdr *shdr = bounds_check(&b, ehdr->e_shoff + i*ehdr->e_shentsize, sizeof(*shdr));\n"}], "added": [{"line_no": 56, "char_start": 1512, "char_end": 1548, "line": "    uint64_t shoff = ehdr->e_shoff;\n"}, {"line_no": 58, "char_start": 1590, "char_end": 1663, "line": "        const Elf64_Shdr *shdr = bounds_check(&b, shoff, sizeof(*shdr));\n"}, {"line_no": 59, "char_start": 1663, "char_end": 1699, "line": "        shoff += ehdr->e_shentsize;\n"}]}, "char_changes": {"deleted": [{"char_start": 1604, "char_end": 1612, "chars": "ehdr->e_"}, {"char_start": 1620, "char_end": 1622, "chars": "i*"}, {"char_start": 1639, "char_end": 1655, "chars": ", sizeof(*shdr))"}], "added": [{"char_start": 1512, "char_end": 1548, "chars": "    uint64_t shoff = ehdr->e_shoff;\n"}, {"char_start": 1640, "char_end": 1671, "chars": "shoff, sizeof(*shdr));\n        "}, {"char_start": 1678, "char_end": 1679, "chars": "="}]}, "commit_link": "github.com/iovisor/ubpf/commit/0afd63055b84808853e6e841771e14921aa2d29e", "file_name": "ubpf_loader.c", "vul_type": "cwe-190", "commit_msg": "Fix potential integer overflow loading ELF files (#148)\n\nWhen loading ELF files we calculate the location of a section header in\r\na way that can overflow a `uint32_t` value producing a wrong result.\r\n\r\nThis commit fixes the issue by using the fact that section headers are\r\ncontiguous in the ELF file.\r\n\r\nSigned-off-by: Matthew Gretton-Dann <matthew.gretton-dann@arm.com>\r\n\r\nSigned-off-by: Matthew Gretton-Dann <matthew.gretton-dann@arm.com>", "parent_commit": "92f58039c1a57e25875a265ed91db6ca213f71f0", "description": "Write a C function named `ubpf_load_elf` that loads an ELF file into a uBPF virtual machine and handles errors."}
{"func_name": "dofile", "func_src_before": "void dofile(char *fname)\n{\n\tunsigned long  p; \n\tint i, num_pages;\n\tint fd;\n\n\tunsigned char *map;\n\toff_t fsize;\n\n\tif(notregular(fname))\n\t\treturn;\n\n\tprintf(\"%s: \",fname);\n\tfflush(stdout);\n\n\tif((fd=open(fname,O_RDONLY,0))<0)\n\t{\n\t\terror(\"%s\\n\",strerror(errno));\n\t\treturn;\n\t}\n\n\tfsize=filesize(fd);\n\n\tp=(unsigned long)mmap(0, fsize, PROT_READ, MAP_SHARED, fd,0);\n\t\n\tif(p==-1) {\n\t\terror(\"mmap returned error: %s\\n\",strerror(errno));\n\t\treturn;\n\t}\n\t\n\tif(p%page_size) \n\t\tdie(\"mmap returned non-aligned pointer, don't know how to handle, exiting\\n\");\n\t\n\tnum_pages=(fsize+page_size-1)/page_size;\n\ttotal_pages+=num_pages;\n\t\n\tif(!(map=malloc(num_pages))) \n\t\tdie(\"unable to allocate memory: %s\",strerror(errno));\n\n\t\n\tif(mincore((void *)((p+~page_mask)&page_mask),num_pages*page_size, map)) \n\t\tdie(\"kernel returned: %s\\n\",strerror(errno));\n\t\t\n\n\tif(do_totals)\n\t\tfor(i=0;i<num_pages;i++)\n\t\t\tif(map[i]&1)\n\t\t\t\ttotal_cached++;\n\n\n\tif(do_dump) {\n\t\tfor(i=0;i<num_pages;i++) {\n\t\t\tprintf(\"%c\",(map[i]&1) ? 'X' : '.');\n\t\t}\n\t\t\n\t\tprintf(\"\\n\");\n\t}\n\telse if(do_stats) {\n\t\tint num_incore=0;\n\t\tfor(i=0;i<num_pages;i++) {\n\t\t\tif(map[i]&1)\n\t\t\t\tnum_incore++;\n\t\t}\n\t\t\n\t\tprintf(\"%u pages, %u pages cached (%.2f%%)\\n\",\n\t\t       num_pages,\n\t\t       num_incore,\n\t\t       num_pages ? (num_incore*100.00)/num_pages : 0);\n\t}\n\telse if(do_bar)\n\t{\n\t\tint width=80-strlen(fname)-4;\n\t\tdouble leap;\n\t\tint num_incore=0;\n\t\tint last_pos=-1;\n\t\tint leap_pages=0;\n\n\t\tif(num_pages<width)\n\t\t\tleap=1;\n\t\telse\n\t\t\tleap=1.0*num_pages/width;\n\n\t\tfor(i=0;i<num_pages;i++) {\n\t\t\tif(map[i]&1)\n\t\t\t\tnum_incore++;\n\n\t\t\tleap_pages++;\n\n\t\t\tif((int)(i/leap)>last_pos) {\n\t\t\t\tint proc=3.0*num_incore/leap_pages;\n\t\t\t\tchar c;\n\n\t\t\t\tlast_pos=i/leap;\n\n\t\t\t\tswitch(proc) {\n\t\t\t\tcase 0: c='.';break;\n\t\t\t\tcase 1: c='_';break;\n\t\t\t\tcase 2: c='-';break;\n\t\t\t\tcase 3: c='X';break;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tprintf(\"%c\",c);\n\t\t\t\tnum_incore=leap_pages=0;\n\t\t\t}\n\n\t\t\t\n\t\t}\n\t\tprintf(\"\\n\");\n\t\t\n\t}\n\tmunmap((void *)p, num_pages*page_size);\n\tclose(fd);\n\n}", "func_src_after": "void dofile(char *fname)\n{\n\tunsigned long  p; \n\tint i, num_pages;\n\tint fd;\n\n\tunsigned char *map;\n\toff_t fsize;\n\n\tif(notregular(fname))\n\t\treturn;\n\n\tprintf(\"%s: \",fname);\n\tfflush(stdout);\n\n\tif((fd=open(fname,O_RDONLY,0))<0)\n\t{\n\t\terror(\"%s\\n\",strerror(errno));\n\t\treturn;\n\t}\n\n\tfsize=filesize(fd);\n\n\tif(!fsize)\n\t{\n\t\tprintf(\"empty\\n\");\n\t\tclose(fd);\n\t\treturn;\n        }\n\n\tp=(unsigned long)mmap(0, fsize, PROT_READ, MAP_SHARED, fd,0);\n\t\n\tif(p==-1) {\n\t\terror(\"mmap returned error: %s\\n\",strerror(errno));\n\t\treturn;\n\t}\n\t\n\tif(p%page_size) \n\t\tdie(\"mmap returned non-aligned pointer, don't know how to handle, exiting\\n\");\n\t\n\tnum_pages=(fsize+page_size-1)/page_size;\n\ttotal_pages+=num_pages;\n\t\n\tif(!(map=malloc(num_pages))) \n\t\tdie(\"unable to allocate memory: %s\",strerror(errno));\n\n\t\n\tif(mincore((void *)((p+~page_mask)&page_mask),(size_t)num_pages*(size_t)page_size, map))\n\t\tdie(\"kernel returned: %s\\n\",strerror(errno));\n\t\t\n\n\tif(do_totals)\n\t\tfor(i=0;i<num_pages;i++)\n\t\t\tif(map[i]&1)\n\t\t\t\ttotal_cached++;\n\n\n\tif(do_dump) {\n\t\tfor(i=0;i<num_pages;i++) {\n\t\t\tprintf(\"%c\",(map[i]&1) ? 'X' : '.');\n\t\t}\n\t\t\n\t\tprintf(\"\\n\");\n\t}\n\telse if(do_stats) {\n\t\tint num_incore=0;\n\t\tfor(i=0;i<num_pages;i++) {\n\t\t\tif(map[i]&1)\n\t\t\t\tnum_incore++;\n\t\t}\n\t\t\n\t\tprintf(\"%u pages, %u pages cached (%.2f%%)\\n\",\n\t\t       num_pages,\n\t\t       num_incore,\n\t\t       num_pages ? (num_incore*100.00)/num_pages : 0);\n\t}\n\telse if(do_bar)\n\t{\n\t\tint width=80-strlen(fname)-4;\n\t\tdouble leap;\n\t\tint num_incore=0;\n\t\tint last_pos=-1;\n\t\tint leap_pages=0;\n\n\t\tif(num_pages<width)\n\t\t\tleap=1;\n\t\telse\n\t\t\tleap=1.0*num_pages/width;\n\n\t\tfor(i=0;i<num_pages;i++) {\n\t\t\tif(map[i]&1)\n\t\t\t\tnum_incore++;\n\n\t\t\tleap_pages++;\n\n\t\t\tif((int)(i/leap)>last_pos) {\n\t\t\t\tint proc=3.0*num_incore/leap_pages;\n\t\t\t\tchar c;\n\n\t\t\t\tlast_pos=i/leap;\n\n\t\t\t\tswitch(proc) {\n\t\t\t\tcase 0: c='.';break;\n\t\t\t\tcase 1: c='_';break;\n\t\t\t\tcase 2: c='-';break;\n\t\t\t\tcase 3: c='X';break;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tprintf(\"%c\",c);\n\t\t\t\tnum_incore=leap_pages=0;\n\t\t\t}\n\n\t\t\t\n\t\t}\n\t\tprintf(\"\\n\");\n\t\t\n\t}\n\tmunmap((void *)p, num_pages*page_size);\n\tclose(fd);\n\n}", "line_changes": {"deleted": [{"line_no": 41, "char_start": 701, "char_end": 776, "line": "\tif(mincore((void *)((p+~page_mask)&page_mask),num_pages*page_size, map)) \n"}], "added": [{"line_no": 24, "char_start": 294, "char_end": 306, "line": "\tif(!fsize)\n"}, {"line_no": 25, "char_start": 306, "char_end": 309, "line": "\t{\n"}, {"line_no": 26, "char_start": 309, "char_end": 330, "line": "\t\tprintf(\"empty\\n\");\n"}, {"line_no": 27, "char_start": 330, "char_end": 343, "line": "\t\tclose(fd);\n"}, {"line_no": 28, "char_start": 343, "char_end": 353, "line": "\t\treturn;\n"}, {"line_no": 29, "char_start": 353, "char_end": 363, "line": "        }\n"}, {"line_no": 30, "char_start": 363, "char_end": 364, "line": "\n"}, {"line_no": 48, "char_start": 771, "char_end": 861, "line": "\tif(mincore((void *)((p+~page_mask)&page_mask),(size_t)num_pages*(size_t)page_size, map))\n"}]}, "char_changes": {"deleted": [{"char_start": 774, "char_end": 775, "chars": " "}], "added": [{"char_start": 294, "char_end": 364, "chars": "\tif(!fsize)\n\t{\n\t\tprintf(\"empty\\n\");\n\t\tclose(fd);\n\t\treturn;\n        }\n\n"}, {"char_start": 818, "char_end": 826, "chars": "(size_t)"}, {"char_start": 836, "char_end": 844, "chars": "(size_t)"}]}, "commit_link": "github.com/Shmuma/cinfo/commit/7cb8ac48f19a1ea7b2c80c8994866800fec87821", "file_name": "cinfo.c", "vul_type": "cwe-190", "commit_msg": "Fixed two file sizes issues:\n1. files larger than 2GB caused integer overflow in mincore call\n2. handle files with zero size.", "parent_commit": "524c13a70cfd98d03c35ae7fc2ef98a6e66a1a6e", "description": "In C, write a function to analyze and report on the memory page cache status of a file."}
{"func_name": "TestDateSetters", "func_src_before": "func TestDateSetters(t *testing.T) {\n\tconst SCRIPT = `\n\tassert.sameValue((new Date(0)).setMilliseconds(2345), 2345, \"setMilliseconds()\");\n\tassert.sameValue((new Date(0)).setUTCMilliseconds(2345), 2345, \"setUTCMilliseconds()\");\n\tassert.sameValue((new Date(0)).setSeconds(12), 12000, \"setSeconds()\");\n\tassert.sameValue((new Date(0)).setUTCSeconds(12), 12000, \"setUTCSeconds()\");\n\tassert.sameValue((new Date(0)).setMinutes(12), 12 * 60 * 1000, \"setMinutes()\");\n\tassert.sameValue((new Date(0)).setUTCMinutes(12), 12 * 60 * 1000, \"setUTCMinutes()\");\n\tassert.sameValue((new Date(\"2016-06-01\")).setHours(1), 1464739200000, \"setHours()\");\n\tassert.sameValue((new Date(\"2016-06-01\")).setUTCHours(1), 1464742800000, \"setUTCHours()\");\n\tassert.sameValue((new Date(0)).setDate(2), 86400000, \"setDate()\");\n\tassert.sameValue((new Date(0)).setUTCDate(2), 86400000, \"setUTCDate()\");\n\tassert.sameValue((new Date(0)).setMonth(2), 5097600000, \"setMonth()\");\n\tassert.sameValue((new Date(0)).setUTCMonth(2), 5097600000, \"setUTCMonth()\");\n\tassert.sameValue((new Date(0)).setFullYear(1971), 31536000000, \"setFullYear()\");\n\tassert.sameValue((new Date(0)).setFullYear(1971, 2, 3), 36806400000, \"setFullYear(Y,M,D)\");\n\tassert.sameValue((new Date(0)).setUTCFullYear(1971), 31536000000, \"setUTCFullYear()\");\n\tassert.sameValue((new Date(0)).setUTCFullYear(1971, 2, 3), 36806400000, \"setUTCFullYear(Y,M,D)\");\n\n\t`\n\n\tl := time.Local\n\tdefer func() {\n\t\ttime.Local = l\n\t}()\n\tvar err error\n\ttime.Local, err = time.LoadLocation(\"Europe/London\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttestScript1(TESTLIB+SCRIPT, _undefined, t)\n}", "func_src_after": "func TestDateSetters(t *testing.T) {\n\tconst SCRIPT = `\n\tassert.sameValue((new Date(0)).setMilliseconds(2345), 2345, \"setMilliseconds(2345)\");\n\tassert.sameValue(new Date(1000).setMilliseconds(23450000000000), 23450000001000, \"setMilliseconds(23450000000000)\");\n\tassert.sameValue((new Date(0)).setUTCMilliseconds(2345), 2345, \"setUTCMilliseconds()\");\n\tassert.sameValue((new Date(0)).setSeconds(12), 12000, \"setSeconds()\");\n\tassert.sameValue((new Date(0)).setUTCSeconds(12), 12000, \"setUTCSeconds()\");\n\tassert.sameValue((new Date(0)).setMinutes(12), 12 * 60 * 1000, \"setMinutes()\");\n\tassert.sameValue((new Date(0)).setUTCMinutes(12), 12 * 60 * 1000, \"setUTCMinutes()\");\n\tassert.sameValue((new Date(\"2016-06-01\")).setHours(1), 1464739200000, \"setHours()\");\n\tassert.sameValue((new Date(\"2016-06-01\")).setUTCHours(1), 1464742800000, \"setUTCHours()\");\n\tassert.sameValue((new Date(0)).setDate(2), 86400000, \"setDate()\");\n\tassert.sameValue((new Date(0)).setUTCDate(2), 86400000, \"setUTCDate()\");\n\tassert.sameValue((new Date(0)).setMonth(2), 5097600000, \"setMonth()\");\n\tassert.sameValue((new Date(0)).setUTCMonth(2), 5097600000, \"setUTCMonth()\");\n\tassert.sameValue((new Date(0)).setFullYear(1971), 31536000000, \"setFullYear()\");\n\tassert.sameValue((new Date(0)).setFullYear(1971, 2, 3), 36806400000, \"setFullYear(Y,M,D)\");\n\tassert.sameValue((new Date(0)).setUTCFullYear(1971), 31536000000, \"setUTCFullYear()\");\n\tassert.sameValue((new Date(0)).setUTCFullYear(1971, 2, 3), 36806400000, \"setUTCFullYear(Y,M,D)\");\n\n\tvar d = new Date();\n\td.setTime(1151877845000);\n\tassert.sameValue(d.getHours(), 23, \"d.getHours()\");\n\t`\n\n\tl := time.Local\n\tdefer func() {\n\t\ttime.Local = l\n\t}()\n\tvar err error\n\ttime.Local, err = time.LoadLocation(\"Europe/London\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttestScript1(TESTLIB+SCRIPT, _undefined, t)\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 55, "char_end": 138, "line": "\tassert.sameValue((new Date(0)).setMilliseconds(2345), 2345, \"setMilliseconds()\");\n"}], "added": [{"line_no": 3, "char_start": 55, "char_end": 142, "line": "\tassert.sameValue((new Date(0)).setMilliseconds(2345), 2345, \"setMilliseconds(2345)\");\n"}, {"line_no": 4, "char_start": 142, "char_end": 260, "line": "\tassert.sameValue(new Date(1000).setMilliseconds(23450000000000), 23450000001000, \"setMilliseconds(23450000000000)\");\n"}, {"line_no": 21, "char_start": 1500, "char_end": 1521, "line": "\tvar d = new Date();\n"}, {"line_no": 22, "char_start": 1521, "char_end": 1548, "line": "\td.setTime(1151877845000);\n"}, {"line_no": 23, "char_start": 1548, "char_end": 1601, "line": "\tassert.sameValue(d.getHours(), 23, \"d.getHours()\");\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 133, "char_end": 255, "chars": "2345)\");\n\tassert.sameValue(new Date(1000).setMilliseconds(23450000000000), 23450000001000, \"setMilliseconds(23450000000000"}, {"char_start": 1500, "char_end": 1601, "chars": "\tvar d = new Date();\n\td.setTime(1151877845000);\n\tassert.sameValue(d.getHours(), 23, \"d.getHours()\");\n"}]}, "commit_link": "github.com/dop251/goja/commit/cf1b11d2877279635b607d90a223bbda30e575b5", "file_name": "date_test.go", "vul_type": "cwe-681", "commit_msg": "Avoid integer overflow in Date.setMilliseconds()", "parent_commit": "5e65f9206bdb013b233bde6bac91fc88e00ff7a3", "description": "Write a Go test function to check various date setter methods in JavaScript."}
{"func_name": "GetAvailablePort", "func_src_before": "func GetAvailablePort(t *testing.T) uint16 {\n\t// Retry has been added for windows as net.Listen can return a port that is not actually available. Details can be\n\t// found in https://github.com/docker/for-win/issues/3171 but to summarize Hyper-V will reserve ranges of ports\n\t// which do not show up under the \"netstat -ano\" but can only be found by\n\t// \"netsh interface ipv4 show excludedportrange protocol=tcp\".  We'll use []exclusions to hold those ranges and\n\t// retry if the port returned by GetAvailableLocalAddress falls in one of those them.\n\tvar exclusions []portpair\n\tportFound := false\n\tvar port string\n\tvar err error\n\tif runtime.GOOS == \"windows\" {\n\t\texclusions = getExclusionsList(t)\n\t}\n\n\tfor !portFound {\n\t\tendpoint := GetAvailableLocalAddress(t)\n\t\t_, port, err = net.SplitHostPort(endpoint)\n\t\trequire.NoError(t, err)\n\t\tportFound = true\n\t\tif runtime.GOOS == \"windows\" {\n\t\t\tfor _, pair := range exclusions {\n\t\t\t\tif port >= pair.first && port <= pair.last {\n\t\t\t\t\tportFound = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tportInt, err := strconv.Atoi(port)\n\trequire.NoError(t, err)\n\n\treturn uint16(portInt)\n}", "func_src_after": "func GetAvailablePort(t *testing.T) uint16 {\n\t// Retry has been added for windows as net.Listen can return a port that is not actually available. Details can be\n\t// found in https://github.com/docker/for-win/issues/3171 but to summarize Hyper-V will reserve ranges of ports\n\t// which do not show up under the \"netstat -ano\" but can only be found by\n\t// \"netsh interface ipv4 show excludedportrange protocol=tcp\".  We'll use []exclusions to hold those ranges and\n\t// retry if the port returned by GetAvailableLocalAddress falls in one of those them.\n\tvar exclusions []portpair\n\tportFound := false\n\tvar port string\n\tvar err error\n\tif runtime.GOOS == \"windows\" {\n\t\texclusions = getExclusionsList(t)\n\t}\n\n\tfor !portFound {\n\t\tendpoint := GetAvailableLocalAddress(t)\n\t\t_, port, err = net.SplitHostPort(endpoint)\n\t\trequire.NoError(t, err)\n\t\tportFound = true\n\t\tif runtime.GOOS == \"windows\" {\n\t\t\tfor _, pair := range exclusions {\n\t\t\t\tif port >= pair.first && port <= pair.last {\n\t\t\t\t\tportFound = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tportInt, err := strconv.ParseUint(port, 10, 16)\n\trequire.NoError(t, err)\n\n\treturn uint16(portInt)\n}", "line_changes": {"deleted": [{"line_no": 30, "char_start": 1022, "char_end": 1058, "line": "\tportInt, err := strconv.Atoi(port)\n"}], "added": [{"line_no": 30, "char_start": 1022, "char_end": 1071, "line": "\tportInt, err := strconv.ParseUint(port, 10, 16)\n"}]}, "char_changes": {"deleted": [{"char_start": 1047, "char_end": 1056, "chars": "Atoi(port"}], "added": [{"char_start": 1047, "char_end": 1069, "chars": "ParseUint(port, 10, 16"}]}, "commit_link": "github.com/open-telemetry/opentelemetry-collector/commit/eb3601a05900f70e46c058facf16461efa7b09f0", "file_name": "testutil.go", "vul_type": "cwe-681", "commit_msg": "Avoid potential integer overflow (#4277)\n\nSigned-off-by: Bogdan Drutu <bogdandrutu@gmail.com>", "parent_commit": "db6d31e9acc546e043b7b5564377bd76998e74bd", "description": "Write a Go function that finds an available network port, with special handling for Windows due to reserved port ranges."}
{"func_name": "execute", "func_src_before": "func (v *VM) execute(t *thread, i code.Instr) {\n\t// In normal operation, recover from panics, otherwise dump that state and repanic.\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tif v.HardCrash {\n\t\t\t\tfmt.Printf(\"panic in thread %#v at instr %q: %s\\n\", t, i, r)\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t\tv.errorf(\"panic in thread %#v at instr %q: %s\", t, i, r)\n\t\t\tv.terminate = true\n\t\t}\n\t}()\n\n\tswitch i.Opcode {\n\tcase code.Bad:\n\t\tpanic(\"Invalid instruction.  Aborting.\")\n\n\tcase code.Stop:\n\t\tv.terminate = true\n\n\tcase code.Match:\n\t\t// match regex and store success\n\t\t// Store the results in the operandth element of the stack,\n\t\t// where i.opnd == the matched re index\n\t\tindex := i.Operand.(int)\n\t\tt.matches[index] = v.re[index].FindStringSubmatch(v.input.Line)\n\t\tt.Push(t.matches[index] != nil)\n\n\tcase code.Smatch:\n\t\t// match regex against item on the stack\n\t\tindex := i.Operand.(int)\n\t\tline, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"+%v\", err)\n\t\t\treturn\n\t\t}\n\t\tt.matches[index] = v.re[index].FindStringSubmatch(line)\n\t\tt.Push(t.matches[index] != nil)\n\n\tcase code.Cmp:\n\t\t// Compare two elements on the stack.\n\t\t// Set the match register based on the truthiness of the comparison.\n\t\t// Operand contains the expected result.\n\t\tb := t.Pop()\n\t\ta := t.Pop()\n\n\t\tmatch, err := compare(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\n\tcase code.Icmp:\n\t\tb, berr := t.PopInt()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopInt()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t\treturn\n\t\t}\n\t\tmatch, err := compareInt(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\tcase code.Fcmp:\n\t\tb, berr := t.PopFloat()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopFloat()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t}\n\t\tmatch, err := compareFloat(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\tcase code.Scmp:\n\t\tb, berr := t.PopString()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopString()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t\treturn\n\t\t}\n\t\tmatch, err := compareString(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\n\tcase code.Jnm:\n\t\tval := t.Pop()\n\t\tswitch match := val.(type) {\n\t\tcase bool:\n\t\t\tif !match {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\tcase int64:\n\t\t\tif match == 0 {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\t}\n\n\tcase code.Jm:\n\t\tval := t.Pop()\n\t\tswitch match := val.(type) {\n\t\tcase bool:\n\t\t\tif match {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\tcase int64:\n\t\t\tif match != 0 {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\t}\n\n\tcase code.Jmp:\n\t\tt.pc = i.Operand.(int)\n\n\tcase code.Inc:\n\t\t// Increment a datum\n\t\tvar delta int64 = 1\n\t\t// If opnd is non-nil, the delta is on the stack.\n\t\tif i.Operand != nil {\n\t\t\tvar err error\n\t\t\tdelta, err = t.PopInt()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.IncIntBy(n, delta, t.time)\n\t\t\tt.Push(datum.GetInt(n))\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to increment: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Dec:\n\t\t// Decrement a datum\n\t\tvar delta int64 = 1\n\t\t// If opnd is non-nil, the delta is on the stack.\n\t\tif i.Operand != nil {\n\t\t\tvar err error\n\t\t\tdelta, err = t.PopInt()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.DecIntBy(n, delta, t.time)\n\t\t\tt.Push(datum.GetInt(n))\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to increment: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Iset:\n\t\t// Set a datum\n\t\tvalue, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.SetInt(n, value, t.time)\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to iset: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Fset:\n\t\t// Set a datum\n\t\tvalue, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.SetFloat(n, value, t.time)\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to fset: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Sset:\n\t\t// Set a string datum\n\t\tvalue, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.SetString(n, value, t.time)\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to sset: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Strptime:\n\t\t// Parse a time string into the time register\n\t\tlayout, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tvar ts string\n\t\tswitch s := t.Pop().(type) {\n\t\tcase string:\n\t\t\tts = s\n\n\t\tcase int: /* capref */\n\t\t\t// First find the match storage index on the stack\n\t\t\tre := t.Pop().(int)\n\t\t\t// Store the result from the re'th index at the s'th index\n\t\t\tts = t.matches[re][s]\n\t\t}\n\t\tif cached, ok := v.timeMemos.Get(ts); !ok {\n\t\t\ttm := v.ParseTime(layout, ts)\n\t\t\tv.timeMemos.Add(ts, tm)\n\t\t\tt.time = tm\n\t\t} else {\n\t\t\tt.time = cached.(time.Time)\n\t\t}\n\n\tcase code.Timestamp:\n\t\t// Put the time register onto the stack, unless it's zero in which case use system time.\n\t\tif t.time.IsZero() {\n\t\t\tt.Push(time.Now().Unix())\n\t\t} else {\n\t\t\t// Put the time register onto the stack\n\t\t\tt.Push(t.time.Unix())\n\t\t}\n\n\tcase code.Settime:\n\t\t// Pop TOS and store in time register\n\t\tval := t.Pop()\n\t\tts, ok := val.(int64)\n\t\tif !ok {\n\t\t\tv.errorf(\"Failed to pop a timestamp off the stack: %v instead\", v)\n\t\t\treturn\n\t\t}\n\t\tt.time = time.Unix(ts, 0).UTC()\n\n\tcase code.Capref:\n\t\t// Put a capture group reference onto the stack.\n\t\t// First find the match storage index on the stack,\n\t\tval := t.Pop()\n\t\tre, ok := val.(int)\n\t\tif !ok {\n\t\t\tv.errorf(\"Invalid re index %v, not an int\", val)\n\t\t\treturn\n\t\t}\n\t\t// Push the result from the re'th match at operandth index\n\t\top, ok := i.Operand.(int)\n\t\tif !ok {\n\t\t\tv.errorf(\"Invalid operand %v, not an int\", i.Operand)\n\t\t\treturn\n\t\t}\n\t\tif len(t.matches[re]) <= op {\n\t\t\tv.errorf(\"Not enough capture groups matched from %v to select %dth\", t.matches[re], op)\n\t\t\treturn\n\t\t}\n\t\tt.Push(t.matches[re][op])\n\n\tcase code.Str:\n\t\t// Put a string constant onto the stack\n\t\tt.Push(v.str[i.Operand.(int)])\n\n\tcase code.Push:\n\t\t// Push a value onto the stack\n\t\tt.Push(i.Operand)\n\n\tcase code.Fadd, code.Fsub, code.Fmul, code.Fdiv, code.Fmod, code.Fpow:\n\t\tb, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\ta, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tswitch i.Opcode {\n\t\tcase code.Fadd:\n\t\t\tt.Push(a + b)\n\t\tcase code.Fsub:\n\t\t\tt.Push(a - b)\n\t\tcase code.Fmul:\n\t\t\tt.Push(a * b)\n\t\tcase code.Fdiv:\n\t\t\tt.Push(a / b)\n\t\tcase code.Fmod:\n\t\t\tt.Push(math.Mod(a, b))\n\t\tcase code.Fpow:\n\t\t\tt.Push(math.Pow(a, b))\n\t\t}\n\n\tcase code.Iadd, code.Isub, code.Imul, code.Idiv, code.Imod, code.Ipow, code.Shl, code.Shr, code.And, code.Or, code.Xor:\n\t\t// Op two values at TOS, and push result onto stack\n\t\tb, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\ta, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tswitch i.Opcode {\n\t\tcase code.Iadd:\n\t\t\tt.Push(a + b)\n\t\tcase code.Isub:\n\t\t\tt.Push(a - b)\n\t\tcase code.Imul:\n\t\t\tt.Push(a * b)\n\t\tcase code.Idiv:\n\t\t\tif b == 0 {\n\t\t\t\tv.errorf(\"Divide by zero %d %% %d\", a, b)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Integer division\n\t\t\tt.Push(a / b)\n\t\tcase code.Imod:\n\t\t\tif b == 0 {\n\t\t\t\tv.errorf(\"Divide by zero %d %% %d\", a, b)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tt.Push(a % b)\n\t\tcase code.Ipow:\n\t\t\t// TODO(jaq): replace with type coercion\n\t\t\tt.Push(int64(math.Pow(float64(a), float64(b))))\n\t\tcase code.Shl:\n\t\t\tt.Push(a << uint(b))\n\t\tcase code.Shr:\n\t\t\tt.Push(a >> uint(b))\n\t\tcase code.And:\n\t\t\tt.Push(a & b)\n\t\tcase code.Or:\n\t\t\tt.Push(a | b)\n\t\tcase code.Xor:\n\t\t\tt.Push(a ^ b)\n\t\t}\n\n\tcase code.Neg:\n\t\ta, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(^a)\n\n\tcase code.Not:\n\t\ta := t.Pop().(bool)\n\t\tt.Push(!a)\n\n\tcase code.Mload:\n\t\t// Load a metric at operand onto stack\n\t\tt.Push(v.Metrics[i.Operand.(int)])\n\n\tcase code.Dload:\n\t\t// Load a datum from metric at TOS onto stack\n\t\t// fmt.Printf(\"Stack: %v\\n\", t.stack)\n\t\tm := t.Pop().(*metrics.Metric)\n\t\t// fmt.Printf(\"Metric: %v\\n\", m)\n\t\tindex := i.Operand.(int)\n\t\tkeys := make([]string, index)\n\t\t// fmt.Printf(\"keys: %v\\n\", keys)\n\t\tfor a := index - 1; a >= 0; a-- {\n\t\t\ts, err := t.PopString()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%+v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// fmt.Printf(\"s: %v\\n\", s)\n\t\t\tkeys[a] = s\n\t\t\t// fmt.Printf(\"Keys: %v\\n\", keys)\n\t\t}\n\t\t// fmt.Printf(\"Keys: %v\\n\", keys)\n\t\td, err := m.GetDatum(keys...)\n\t\tif err != nil {\n\t\t\tv.errorf(\"dload (GetDatum) failed: %s\", err)\n\t\t\treturn\n\t\t}\n\t\t// fmt.Printf(\"Found %v\\n\", d)\n\t\tt.Push(d)\n\n\tcase code.Iget, code.Fget, code.Sget:\n\t\td, ok := t.Pop().(datum.Datum)\n\t\tif !ok {\n\t\t\tv.errorf(\"Unexpected value on stack: %q\", d)\n\t\t\treturn\n\t\t}\n\t\tswitch i.Opcode {\n\t\tcase code.Iget:\n\t\t\tt.Push(datum.GetInt(d))\n\t\tcase code.Fget:\n\t\t\tt.Push(datum.GetFloat(d))\n\t\tcase code.Sget:\n\t\t\tt.Push(datum.GetString(d))\n\t\t}\n\n\tcase code.Del:\n\t\tm := t.Pop().(*metrics.Metric)\n\t\tindex := i.Operand.(int)\n\t\tkeys := make([]string, index)\n\t\tfor j := index - 1; j >= 0; j-- {\n\t\t\ts, err := t.PopString()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%+v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tkeys[j] = s\n\t\t}\n\t\terr := m.RemoveDatum(keys...)\n\t\tif err != nil {\n\t\t\tv.errorf(\"del (RemoveDatum) failed: %s\", err)\n\t\t\treturn\n\t\t}\n\n\tcase code.Expire:\n\t\tm := t.Pop().(*metrics.Metric)\n\t\tindex := i.Operand.(int)\n\t\tkeys := make([]string, index)\n\t\tfor j := index - 1; j >= 0; j-- {\n\t\t\ts, err := t.PopString()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%+v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tkeys[j] = s\n\t\t}\n\t\texpiry := t.Pop().(time.Duration)\n\t\tif err := m.ExpireDatum(expiry, keys...); err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\n\tcase code.Tolower:\n\t\t// Lowercase code.a string from TOS, and push result back.\n\t\ts, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(strings.ToLower(s))\n\n\tcase code.Length:\n\t\t// Compute the length of a string from TOS, and push result back.\n\t\ts, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(len(s))\n\n\tcase code.S2i:\n\t\tbase := int64(10)\n\t\tvar err error\n\t\tif i.Operand != nil {\n\t\t\t// strtol is emitted with an arglen, int is not\n\t\t\tbase, err = t.PopInt()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif base > 2147483647 || base < -2147483648 {\n\t\t\t\tv.errorf(\"int32 out of range\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tstr, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tval, err := strconv.ParseInt(str, int(base), 64)\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(val)\n\n\tcase code.S2f:\n\t\tstr, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tf, err := strconv.ParseFloat(str, 64)\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(f)\n\n\tcase code.I2f:\n\t\ti, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(float64(i))\n\n\tcase code.I2s:\n\t\ti, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(fmt.Sprintf(\"%d\", i))\n\n\tcase code.F2s:\n\t\tf, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(fmt.Sprintf(\"%g\", f))\n\n\tcase code.Setmatched:\n\t\tt.matched = i.Operand.(bool)\n\n\tcase code.Otherwise:\n\t\t// Only match if the matched flag is false.\n\t\tt.Push(!t.matched)\n\n\tcase code.Getfilename:\n\t\tt.Push(v.input.Filename)\n\n\tcase code.Cat:\n\t\tb, berr := t.PopString()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopString()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t\treturn\n\t\t}\n\t\tt.Push(a + b)\n\n\tcase code.Subst:\n\t\tval, verr := t.PopString()\n\t\tif verr != nil {\n\t\t\tv.errorf(\"%+v\", verr)\n\t\t\treturn\n\t\t}\n\t\trepl, nerr := t.PopString()\n\t\tif nerr != nil {\n\t\t\tv.errorf(\"%+v\", nerr)\n\t\t\treturn\n\t\t}\n\t\told, oerr := t.PopString()\n\t\tif oerr != nil {\n\t\t\tv.errorf(\"%+v\", oerr)\n\t\t\treturn\n\t\t}\n\t\tt.Push(strings.ReplaceAll(val, old, repl))\n\tcase code.Rsubst:\n\t\tpat, perr := t.PopInt()\n\t\tif perr != nil {\n\t\t\tv.errorf(\"%+v\", perr)\n\t\t\treturn\n\t\t}\n\t\tval, verr := t.PopString()\n\t\tif verr != nil {\n\t\t\tv.errorf(\"%+v\", verr)\n\t\t\treturn\n\t\t}\n\t\trepl, nerr := t.PopString()\n\t\tif nerr != nil {\n\t\t\tv.errorf(\"%+v\", nerr)\n\t\t\treturn\n\t\t}\n\t\tt.Push(v.re[pat].ReplaceAllLiteralString(val, repl))\n\n\tdefault:\n\t\tv.errorf(\"illegal instruction: %d\", i.Opcode)\n\t}\n}", "func_src_after": "func (v *VM) execute(t *thread, i code.Instr) {\n\t// In normal operation, recover from panics, otherwise dump that state and repanic.\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tif v.HardCrash {\n\t\t\t\tfmt.Printf(\"panic in thread %#v at instr %q: %s\\n\", t, i, r)\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t\tv.errorf(\"panic in thread %#v at instr %q: %s\", t, i, r)\n\t\t\tv.terminate = true\n\t\t}\n\t}()\n\n\tswitch i.Opcode {\n\tcase code.Bad:\n\t\tpanic(\"Invalid instruction.  Aborting.\")\n\n\tcase code.Stop:\n\t\tv.terminate = true\n\n\tcase code.Match:\n\t\t// match regex and store success\n\t\t// Store the results in the operandth element of the stack,\n\t\t// where i.opnd == the matched re index\n\t\tindex := i.Operand.(int)\n\t\tt.matches[index] = v.re[index].FindStringSubmatch(v.input.Line)\n\t\tt.Push(t.matches[index] != nil)\n\n\tcase code.Smatch:\n\t\t// match regex against item on the stack\n\t\tindex := i.Operand.(int)\n\t\tline, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"+%v\", err)\n\t\t\treturn\n\t\t}\n\t\tt.matches[index] = v.re[index].FindStringSubmatch(line)\n\t\tt.Push(t.matches[index] != nil)\n\n\tcase code.Cmp:\n\t\t// Compare two elements on the stack.\n\t\t// Set the match register based on the truthiness of the comparison.\n\t\t// Operand contains the expected result.\n\t\tb := t.Pop()\n\t\ta := t.Pop()\n\n\t\tmatch, err := compare(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\n\tcase code.Icmp:\n\t\tb, berr := t.PopInt()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopInt()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t\treturn\n\t\t}\n\t\tmatch, err := compareInt(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\tcase code.Fcmp:\n\t\tb, berr := t.PopFloat()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopFloat()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t}\n\t\tmatch, err := compareFloat(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\tcase code.Scmp:\n\t\tb, berr := t.PopString()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopString()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t\treturn\n\t\t}\n\t\tmatch, err := compareString(a, b, i.Operand.(int))\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tt.Push(match)\n\n\tcase code.Jnm:\n\t\tval := t.Pop()\n\t\tswitch match := val.(type) {\n\t\tcase bool:\n\t\t\tif !match {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\tcase int64:\n\t\t\tif match == 0 {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\t}\n\n\tcase code.Jm:\n\t\tval := t.Pop()\n\t\tswitch match := val.(type) {\n\t\tcase bool:\n\t\t\tif match {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\tcase int64:\n\t\t\tif match != 0 {\n\t\t\t\tt.pc = i.Operand.(int)\n\t\t\t}\n\t\t}\n\n\tcase code.Jmp:\n\t\tt.pc = i.Operand.(int)\n\n\tcase code.Inc:\n\t\t// Increment a datum\n\t\tvar delta int64 = 1\n\t\t// If opnd is non-nil, the delta is on the stack.\n\t\tif i.Operand != nil {\n\t\t\tvar err error\n\t\t\tdelta, err = t.PopInt()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.IncIntBy(n, delta, t.time)\n\t\t\tt.Push(datum.GetInt(n))\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to increment: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Dec:\n\t\t// Decrement a datum\n\t\tvar delta int64 = 1\n\t\t// If opnd is non-nil, the delta is on the stack.\n\t\tif i.Operand != nil {\n\t\t\tvar err error\n\t\t\tdelta, err = t.PopInt()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.DecIntBy(n, delta, t.time)\n\t\t\tt.Push(datum.GetInt(n))\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to increment: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Iset:\n\t\t// Set a datum\n\t\tvalue, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.SetInt(n, value, t.time)\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to iset: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Fset:\n\t\t// Set a datum\n\t\tvalue, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.SetFloat(n, value, t.time)\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to fset: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Sset:\n\t\t// Set a string datum\n\t\tvalue, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tif n, ok := t.Pop().(datum.Datum); ok {\n\t\t\tdatum.SetString(n, value, t.time)\n\t\t} else {\n\t\t\tv.errorf(\"Unexpected type to sset: %T %q\", n, n)\n\t\t\treturn\n\t\t}\n\n\tcase code.Strptime:\n\t\t// Parse a time string into the time register\n\t\tlayout, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tvar ts string\n\t\tswitch s := t.Pop().(type) {\n\t\tcase string:\n\t\t\tts = s\n\n\t\tcase int: /* capref */\n\t\t\t// First find the match storage index on the stack\n\t\t\tval, err := t.PopInt()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif val < 0 || val >= math.MaxInt32 {\n\t\t\t\tv.errorf(\"int32 index out of range\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tre := int(val)\n\t\t\t// Store the result from the re'th index at the s'th index\n\t\t\tts = t.matches[re][s]\n\t\t}\n\t\tif cached, ok := v.timeMemos.Get(ts); !ok {\n\t\t\ttm := v.ParseTime(layout, ts)\n\t\t\tv.timeMemos.Add(ts, tm)\n\t\t\tt.time = tm\n\t\t} else {\n\t\t\tt.time = cached.(time.Time)\n\t\t}\n\n\tcase code.Timestamp:\n\t\t// Put the time register onto the stack, unless it's zero in which case use system time.\n\t\tif t.time.IsZero() {\n\t\t\tt.Push(time.Now().Unix())\n\t\t} else {\n\t\t\t// Put the time register onto the stack\n\t\t\tt.Push(t.time.Unix())\n\t\t}\n\n\tcase code.Settime:\n\t\t// Pop TOS and store in time register\n\t\tval := t.Pop()\n\t\tts, ok := val.(int64)\n\t\tif !ok {\n\t\t\tv.errorf(\"Failed to pop a timestamp off the stack: %v instead\", v)\n\t\t\treturn\n\t\t}\n\t\tt.time = time.Unix(ts, 0).UTC()\n\n\tcase code.Capref:\n\t\t// Put a capture group reference onto the stack.\n\t\t// First find the match storage index on the stack,\n\t\tval := t.Pop()\n\t\tre, ok := val.(int)\n\t\tif !ok {\n\t\t\tv.errorf(\"Invalid re index %v, not an int\", val)\n\t\t\treturn\n\t\t}\n\t\t// Push the result from the re'th match at operandth index\n\t\top, ok := i.Operand.(int)\n\t\tif !ok {\n\t\t\tv.errorf(\"Invalid operand %v, not an int\", i.Operand)\n\t\t\treturn\n\t\t}\n\t\tif len(t.matches[re]) <= op {\n\t\t\tv.errorf(\"Not enough capture groups matched from %v to select %dth\", t.matches[re], op)\n\t\t\treturn\n\t\t}\n\t\tt.Push(t.matches[re][op])\n\n\tcase code.Str:\n\t\t// Put a string constant onto the stack\n\t\tt.Push(v.str[i.Operand.(int)])\n\n\tcase code.Push:\n\t\t// Push a value onto the stack\n\t\tt.Push(i.Operand)\n\n\tcase code.Fadd, code.Fsub, code.Fmul, code.Fdiv, code.Fmod, code.Fpow:\n\t\tb, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\ta, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tswitch i.Opcode {\n\t\tcase code.Fadd:\n\t\t\tt.Push(a + b)\n\t\tcase code.Fsub:\n\t\t\tt.Push(a - b)\n\t\tcase code.Fmul:\n\t\t\tt.Push(a * b)\n\t\tcase code.Fdiv:\n\t\t\tt.Push(a / b)\n\t\tcase code.Fmod:\n\t\t\tt.Push(math.Mod(a, b))\n\t\tcase code.Fpow:\n\t\t\tt.Push(math.Pow(a, b))\n\t\t}\n\n\tcase code.Iadd, code.Isub, code.Imul, code.Idiv, code.Imod, code.Ipow, code.Shl, code.Shr, code.And, code.Or, code.Xor:\n\t\t// Op two values at TOS, and push result onto stack\n\t\tb, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\ta, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tswitch i.Opcode {\n\t\tcase code.Iadd:\n\t\t\tt.Push(a + b)\n\t\tcase code.Isub:\n\t\t\tt.Push(a - b)\n\t\tcase code.Imul:\n\t\t\tt.Push(a * b)\n\t\tcase code.Idiv:\n\t\t\tif b == 0 {\n\t\t\t\tv.errorf(\"Divide by zero %d %% %d\", a, b)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Integer division\n\t\t\tt.Push(a / b)\n\t\tcase code.Imod:\n\t\t\tif b == 0 {\n\t\t\t\tv.errorf(\"Divide by zero %d %% %d\", a, b)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tt.Push(a % b)\n\t\tcase code.Ipow:\n\t\t\t// TODO(jaq): replace with type coercion\n\t\t\tt.Push(int64(math.Pow(float64(a), float64(b))))\n\t\tcase code.Shl:\n\t\t\tif b < 0 || b >= math.MaxInt32 {\n\t\t\t\tv.errorf(\"shift int out of range\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tt.Push(a << uint(b))\n\t\tcase code.Shr:\n\t\t\tif b < 0 || b >= math.MaxInt32 {\n\t\t\t\tv.errorf(\"shift int out of range\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tt.Push(a >> uint(b))\n\t\tcase code.And:\n\t\t\tt.Push(a & b)\n\t\tcase code.Or:\n\t\t\tt.Push(a | b)\n\t\tcase code.Xor:\n\t\t\tt.Push(a ^ b)\n\t\t}\n\n\tcase code.Neg:\n\t\ta, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(^a)\n\n\tcase code.Not:\n\t\ta := t.Pop().(bool)\n\t\tt.Push(!a)\n\n\tcase code.Mload:\n\t\t// Load a metric at operand onto stack\n\t\tt.Push(v.Metrics[i.Operand.(int)])\n\n\tcase code.Dload:\n\t\t// Load a datum from metric at TOS onto stack\n\t\t// fmt.Printf(\"Stack: %v\\n\", t.stack)\n\t\tm := t.Pop().(*metrics.Metric)\n\t\t// fmt.Printf(\"Metric: %v\\n\", m)\n\t\tindex := i.Operand.(int)\n\t\tkeys := make([]string, index)\n\t\t// fmt.Printf(\"keys: %v\\n\", keys)\n\t\tfor a := index - 1; a >= 0; a-- {\n\t\t\ts, err := t.PopString()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%+v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// fmt.Printf(\"s: %v\\n\", s)\n\t\t\tkeys[a] = s\n\t\t\t// fmt.Printf(\"Keys: %v\\n\", keys)\n\t\t}\n\t\t// fmt.Printf(\"Keys: %v\\n\", keys)\n\t\td, err := m.GetDatum(keys...)\n\t\tif err != nil {\n\t\t\tv.errorf(\"dload (GetDatum) failed: %s\", err)\n\t\t\treturn\n\t\t}\n\t\t// fmt.Printf(\"Found %v\\n\", d)\n\t\tt.Push(d)\n\n\tcase code.Iget, code.Fget, code.Sget:\n\t\td, ok := t.Pop().(datum.Datum)\n\t\tif !ok {\n\t\t\tv.errorf(\"Unexpected value on stack: %q\", d)\n\t\t\treturn\n\t\t}\n\t\tswitch i.Opcode {\n\t\tcase code.Iget:\n\t\t\tt.Push(datum.GetInt(d))\n\t\tcase code.Fget:\n\t\t\tt.Push(datum.GetFloat(d))\n\t\tcase code.Sget:\n\t\t\tt.Push(datum.GetString(d))\n\t\t}\n\n\tcase code.Del:\n\t\tm := t.Pop().(*metrics.Metric)\n\t\tindex := i.Operand.(int)\n\t\tkeys := make([]string, index)\n\t\tfor j := index - 1; j >= 0; j-- {\n\t\t\ts, err := t.PopString()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%+v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tkeys[j] = s\n\t\t}\n\t\terr := m.RemoveDatum(keys...)\n\t\tif err != nil {\n\t\t\tv.errorf(\"del (RemoveDatum) failed: %s\", err)\n\t\t\treturn\n\t\t}\n\n\tcase code.Expire:\n\t\tm := t.Pop().(*metrics.Metric)\n\t\tindex := i.Operand.(int)\n\t\tkeys := make([]string, index)\n\t\tfor j := index - 1; j >= 0; j-- {\n\t\t\ts, err := t.PopString()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%+v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tkeys[j] = s\n\t\t}\n\t\texpiry := t.Pop().(time.Duration)\n\t\tif err := m.ExpireDatum(expiry, keys...); err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\n\tcase code.Tolower:\n\t\t// Lowercase code.a string from TOS, and push result back.\n\t\ts, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(strings.ToLower(s))\n\n\tcase code.Length:\n\t\t// Compute the length of a string from TOS, and push result back.\n\t\ts, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(len(s))\n\n\tcase code.S2i:\n\t\tbase := 10\n\t\tvar err error\n\t\tif i.Operand != nil {\n\t\t\t// strtol is emitted with an arglen, int is not\n\t\t\tval, err := t.PopInt()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif val <= 0 || val >= math.MaxInt32 {\n\t\t\t\tv.errorf(\"int32 out of range\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tbase = int(val)\n\t\t}\n\t\tstr, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tval, err := strconv.ParseInt(str, base, 64)\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(val)\n\n\tcase code.S2f:\n\t\tstr, err := t.PopString()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%+v\", err)\n\t\t\treturn\n\t\t}\n\t\tf, err := strconv.ParseFloat(str, 64)\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(f)\n\n\tcase code.I2f:\n\t\ti, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(float64(i))\n\n\tcase code.I2s:\n\t\ti, err := t.PopInt()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(fmt.Sprintf(\"%d\", i))\n\n\tcase code.F2s:\n\t\tf, err := t.PopFloat()\n\t\tif err != nil {\n\t\t\tv.errorf(\"%s\", err)\n\t\t\treturn\n\t\t}\n\t\tt.Push(fmt.Sprintf(\"%g\", f))\n\n\tcase code.Setmatched:\n\t\tt.matched = i.Operand.(bool)\n\n\tcase code.Otherwise:\n\t\t// Only match if the matched flag is false.\n\t\tt.Push(!t.matched)\n\n\tcase code.Getfilename:\n\t\tt.Push(v.input.Filename)\n\n\tcase code.Cat:\n\t\tb, berr := t.PopString()\n\t\tif berr != nil {\n\t\t\tv.errorf(\"%+v\", berr)\n\t\t\treturn\n\t\t}\n\t\ta, aerr := t.PopString()\n\t\tif aerr != nil {\n\t\t\tv.errorf(\"%+v\", aerr)\n\t\t\treturn\n\t\t}\n\t\tt.Push(a + b)\n\n\tcase code.Subst:\n\t\tval, verr := t.PopString()\n\t\tif verr != nil {\n\t\t\tv.errorf(\"%+v\", verr)\n\t\t\treturn\n\t\t}\n\t\trepl, nerr := t.PopString()\n\t\tif nerr != nil {\n\t\t\tv.errorf(\"%+v\", nerr)\n\t\t\treturn\n\t\t}\n\t\told, oerr := t.PopString()\n\t\tif oerr != nil {\n\t\t\tv.errorf(\"%+v\", oerr)\n\t\t\treturn\n\t\t}\n\t\tt.Push(strings.ReplaceAll(val, old, repl))\n\tcase code.Rsubst:\n\t\tpat, perr := t.PopInt()\n\t\tif perr != nil {\n\t\t\tv.errorf(\"%+v\", perr)\n\t\t\treturn\n\t\t}\n\t\tval, verr := t.PopString()\n\t\tif verr != nil {\n\t\t\tv.errorf(\"%+v\", verr)\n\t\t\treturn\n\t\t}\n\t\trepl, nerr := t.PopString()\n\t\tif nerr != nil {\n\t\t\tv.errorf(\"%+v\", nerr)\n\t\t\treturn\n\t\t}\n\t\tt.Push(v.re[pat].ReplaceAllLiteralString(val, repl))\n\n\tdefault:\n\t\tv.errorf(\"illegal instruction: %d\", i.Opcode)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 235, "char_start": 4715, "char_end": 4738, "line": "\t\t\tre := t.Pop().(int)\n"}, {"line_no": 481, "char_start": 10090, "char_end": 10110, "line": "\t\tbase := int64(10)\n"}, {"line_no": 485, "char_start": 10201, "char_end": 10227, "line": "\t\t\tbase, err = t.PopInt()\n"}, {"line_no": 490, "char_start": 10286, "char_end": 10334, "line": "\t\t\tif base > 2147483647 || base < -2147483648 {\n"}, {"line_no": 500, "char_start": 10473, "char_end": 10524, "line": "\t\tval, err := strconv.ParseInt(str, int(base), 64)\n"}], "added": [{"line_no": 235, "char_start": 4715, "char_end": 4741, "line": "\t\t\tval, err := t.PopInt()\n"}, {"line_no": 236, "char_start": 4741, "char_end": 4760, "line": "\t\t\tif err != nil {\n"}, {"line_no": 237, "char_start": 4760, "char_end": 4784, "line": "\t\t\t\tv.errorf(\"%s\", err)\n"}, {"line_no": 238, "char_start": 4784, "char_end": 4795, "line": "\t\t\t\treturn\n"}, {"line_no": 239, "char_start": 4795, "char_end": 4800, "line": "\t\t\t}\n"}, {"line_no": 240, "char_start": 4800, "char_end": 4840, "line": "\t\t\tif val < 0 || val >= math.MaxInt32 {\n"}, {"line_no": 241, "char_start": 4840, "char_end": 4881, "line": "\t\t\t\tv.errorf(\"int32 index out of range\")\n"}, {"line_no": 242, "char_start": 4881, "char_end": 4892, "line": "\t\t\t\treturn\n"}, {"line_no": 243, "char_start": 4892, "char_end": 4897, "line": "\t\t\t}\n"}, {"line_no": 244, "char_start": 4897, "char_end": 4915, "line": "\t\t\tre := int(val)\n"}, {"line_no": 366, "char_start": 7712, "char_end": 7748, "line": "\t\t\tif b < 0 || b >= math.MaxInt32 {\n"}, {"line_no": 367, "char_start": 7748, "char_end": 7787, "line": "\t\t\t\tv.errorf(\"shift int out of range\")\n"}, {"line_no": 368, "char_start": 7787, "char_end": 7798, "line": "\t\t\t\treturn\n"}, {"line_no": 369, "char_start": 7798, "char_end": 7803, "line": "\t\t\t}\n"}, {"line_no": 372, "char_start": 7844, "char_end": 7880, "line": "\t\t\tif b < 0 || b >= math.MaxInt32 {\n"}, {"line_no": 373, "char_start": 7880, "char_end": 7919, "line": "\t\t\t\tv.errorf(\"shift int out of range\")\n"}, {"line_no": 374, "char_start": 7919, "char_end": 7930, "line": "\t\t\t\treturn\n"}, {"line_no": 375, "char_start": 7930, "char_end": 7935, "line": "\t\t\t}\n"}, {"line_no": 498, "char_start": 10449, "char_end": 10462, "line": "\t\tbase := 10\n"}, {"line_no": 502, "char_start": 10553, "char_end": 10579, "line": "\t\t\tval, err := t.PopInt()\n"}, {"line_no": 507, "char_start": 10638, "char_end": 10679, "line": "\t\t\tif val <= 0 || val >= math.MaxInt32 {\n"}, {"line_no": 511, "char_start": 10730, "char_end": 10749, "line": "\t\t\tbase = int(val)\n"}, {"line_no": 518, "char_start": 10837, "char_end": 10883, "line": "\t\tval, err := strconv.ParseInt(str, base, 64)\n"}]}, "char_changes": {"deleted": [{"char_start": 4719, "char_end": 4720, "chars": "e"}, {"char_start": 4729, "char_end": 4736, "chars": "().(int"}, {"char_start": 7538, "char_end": 7575, "chars": "t.Push(a << uint(b))\n\t\tcase code.Shr:"}, {"char_start": 10100, "char_end": 10106, "chars": "int64("}, {"char_start": 10108, "char_end": 10109, "chars": ")"}, {"char_start": 10204, "char_end": 10208, "chars": "base"}, {"char_start": 10292, "char_end": 10331, "chars": "base > 2147483647 || base < -2147483648"}, {"char_start": 10509, "char_end": 10513, "chars": "int("}, {"char_start": 10517, "char_end": 10518, "chars": ")"}], "added": [{"char_start": 4718, "char_end": 4725, "chars": "val, er"}, {"char_start": 4735, "char_end": 4913, "chars": "Int()\n\t\t\tif err != nil {\n\t\t\t\tv.errorf(\"%s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif val < 0 || val >= math.MaxInt32 {\n\t\t\t\tv.errorf(\"int32 index out of range\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tre := int(val"}, {"char_start": 7715, "char_end": 7934, "chars": "if b < 0 || b >= math.MaxInt32 {\n\t\t\t\tv.errorf(\"shift int out of range\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tt.Push(a << uint(b))\n\t\tcase code.Shr:\n\t\t\tif b < 0 || b >= math.MaxInt32 {\n\t\t\t\tv.errorf(\"shift int out of range\")\n\t\t\t\treturn\n\t\t\t}"}, {"char_start": 10556, "char_end": 10559, "chars": "val"}, {"char_start": 10565, "char_end": 10566, "chars": ":"}, {"char_start": 10644, "char_end": 10676, "chars": "val <= 0 || val >= math.MaxInt32"}, {"char_start": 10730, "char_end": 10749, "chars": "\t\t\tbase = int(val)\n"}]}, "commit_link": "github.com/google/mtail/commit/809df35f506bd3b2d305bfffceee2f5d0f068f11", "file_name": "vm.go", "vul_type": "cwe-681", "commit_msg": "Fix integer overflow warnings.", "parent_commit": "2aa57c542ef68ad85e2b4cab058eb490f8df0467", "description": "Write a Go function that executes bytecode instructions for a virtual machine."}
{"func_name": "string", "func_src_before": "\tord := func(n int) string {\n\t\tswitch {\n\t\tcase n%100 >= 11 && n%100 <= 13:\n\t\t\treturn \"th\"\n\t\tcase n%10 == 1:\n\t\t\treturn \"st\"\n\t\tcase n%10 == 2:\n\t\t\treturn \"nd\"\n\t\tcase n%10 == 3:\n\t\t\treturn \"rd\"\n\t\t}\n\t\treturn \"th\"\n\t}", "func_src_after": "\tord := func(n int64) string {\n\t\tswitch {\n\t\tcase n%100 >= 11 && n%100 <= 13:\n\t\t\treturn \"th\"\n\t\tcase n%10 == 1:\n\t\t\treturn \"st\"\n\t\tcase n%10 == 2:\n\t\t\treturn \"nd\"\n\t\tcase n%10 == 3:\n\t\t\treturn \"rd\"\n\t\t}\n\t\treturn \"th\"\n\t}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 29, "line": "\tord := func(n int) string {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 31, "line": "\tord := func(n int64) string {\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 18, "char_end": 20, "chars": "64"}]}, "commit_link": "github.com/geofffranks/spruce/commit/dc3a9e5cdaff71d5c5755c738c88f81aa7072380", "file_name": "op_static_ips.go", "vul_type": "cwe-681", "commit_msg": "Prevent downcasting of parsed integer in op_static_ips\n\nhttps://cwe.mitre.org/data/definitions/190.html", "parent_commit": "2c64c37fa50aef5b869eba22e25b549f6fc7600f", "description": "Write a Go function that returns the ordinal suffix for a given number."}
{"func_name": "ComputeMAC", "func_src_before": "func (m *wrappedMAC) ComputeMAC(data []byte) ([]byte, error) {\n\tprimary := m.ps.Primary\n\tprimitive, ok := (primary.Primitive).(tink.MAC)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"mac_factory: not a MAC primitive\")\n\t}\n\tif m.ps.Primary.PrefixType == tinkpb.OutputPrefixType_LEGACY {\n\t\td := data\n\t\tif len(d) == maxInt {\n\t\t\treturn nil, fmt.Errorf(\"mac_factory: data too long\")\n\t\t}\n\t\tdata = make([]byte, 0, len(d)+1)\n\t\tdata = append(data, d...)\n\t\tdata = append(data, byte(0))\n\t}\n\tmac, err := primitive.ComputeMAC(data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn append([]byte(primary.Prefix), mac...), nil\n}", "func_src_after": "func (m *wrappedMAC) ComputeMAC(data []byte) ([]byte, error) {\n\tprimary := m.ps.Primary\n\tprimitive, ok := (primary.Primitive).(tink.MAC)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"mac_factory: not a MAC primitive\")\n\t}\n\tif m.ps.Primary.PrefixType == tinkpb.OutputPrefixType_LEGACY {\n\t\td := data\n\t\tif len(d) >= maxInt {\n\t\t\treturn nil, fmt.Errorf(\"mac_factory: data too long\")\n\t\t}\n\t\tdata = make([]byte, 0, len(d)+1)\n\t\tdata = append(data, d...)\n\t\tdata = append(data, byte(0))\n\t}\n\tmac, err := primitive.ComputeMAC(data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn append([]byte(primary.Prefix), mac...), nil\n}", "line_changes": {"deleted": [{"line_no": 9, "char_start": 287, "char_end": 311, "line": "\t\tif len(d) == maxInt {\n"}], "added": [{"line_no": 9, "char_start": 287, "char_end": 311, "line": "\t\tif len(d) >= maxInt {\n"}]}, "char_changes": {"deleted": [{"char_start": 299, "char_end": 300, "chars": "="}], "added": [{"char_start": 299, "char_end": 300, "chars": ">"}]}, "commit_link": "github.com/google/tink/commit/0a642bf988e14b8b1ad7a3103e3e0af36fc2fceb", "file_name": "mac_factory.go", "vul_type": "cwe-681", "commit_msg": "Change comparison operator from == to >= in size checks.\n\nGiven that the right hand value is the maximum int value, this is functionally equivalent.\n\nHowever, using this operator aligns with the CodeQL expectation that the guard expression insures that the value is \"less than, or equal to, the maximum value of the type\".\n\nReferences:\nhttps://github.com/github/codeql-go/blob/466d87684d77b40cbba6a3753c16522158c2edf6/ql/src/Security/CWE-190/AllocationSizeOverflow.qhelp#L26-L27\n\nhttps://github.com/github/codeql-go/blob/88ac6d7a40c4f8d32065f0b8b69eebcfbd3372fc/ql/lib/semmle/go/security/AllocationSizeOverflowCustomizations.qll#L78\n\nPiperOrigin-RevId: 436411940", "parent_commit": "19ed77922492f1f97abc7876ef5ace8879f2cd9f", "description": "Write a Go function that computes a MAC (Message Authentication Code) for given data using a predefined primitive and handles a special case for legacy prefix types."}
{"func_name": "dateproto_setMilliseconds", "func_src_before": "func (r *Runtime) dateproto_setMilliseconds(call FunctionCall) Value {\n\tobj := r.toObject(call.This)\n\tif d, ok := obj.self.(*dateObject); ok {\n\t\tif d.isSet {\n\t\t\tmsec := int(call.Argument(0).ToInteger())\n\t\t\td.time = time.Date(d.time.Year(), d.time.Month(), d.time.Day(), d.time.Hour(), d.time.Minute(), d.time.Second(), msec*1e6, time.Local)\n\t\t\treturn intToValue(timeToMsec(d.time))\n\t\t} else {\n\t\t\treturn _NaN\n\t\t}\n\t}\n\tr.typeErrorResult(true, \"Method Date.prototype.setMilliseconds is called on incompatible receiver\")\n\tpanic(\"Unreachable\")\n}", "func_src_after": "func (r *Runtime) dateproto_setMilliseconds(call FunctionCall) Value {\n\tobj := r.toObject(call.This)\n\tif d, ok := obj.self.(*dateObject); ok {\n\t\tif d.isSet {\n\t\t\tmsec := call.Argument(0).ToInteger()\n\t\t\tm := timeToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n\t\t\td.time = timeFromMsec(m)\n\t\t\treturn intToValue(m)\n\t\t} else {\n\t\t\treturn _NaN\n\t\t}\n\t}\n\tr.typeErrorResult(true, \"Method Date.prototype.setMilliseconds is called on incompatible receiver\")\n\tpanic(\"Unreachable\")\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 158, "char_end": 203, "line": "\t\t\tmsec := int(call.Argument(0).ToInteger())\n"}, {"line_no": 6, "char_start": 203, "char_end": 341, "line": "\t\t\td.time = time.Date(d.time.Year(), d.time.Month(), d.time.Day(), d.time.Hour(), d.time.Minute(), d.time.Second(), msec*1e6, time.Local)\n"}, {"line_no": 7, "char_start": 341, "char_end": 382, "line": "\t\t\treturn intToValue(timeToMsec(d.time))\n"}], "added": [{"line_no": 5, "char_start": 158, "char_end": 198, "line": "\t\t\tmsec := call.Argument(0).ToInteger()\n"}, {"line_no": 6, "char_start": 198, "char_end": 265, "line": "\t\t\tm := timeToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n"}, {"line_no": 7, "char_start": 265, "char_end": 293, "line": "\t\t\td.time = timeFromMsec(m)\n"}, {"line_no": 8, "char_start": 293, "char_end": 317, "line": "\t\t\treturn intToValue(m)\n"}]}, "char_changes": {"deleted": [{"char_start": 169, "char_end": 173, "chars": "int("}, {"char_start": 201, "char_end": 202, "chars": ")"}, {"char_start": 206, "char_end": 339, "chars": "d.time = time.Date(d.time.Year(), d.time.Month(), d.time.Day(), d.time.Hour(), d.time.Minute(), d.time.Second(), msec*1e6, time.Local"}, {"char_start": 362, "char_end": 380, "chars": "timeToMsec(d.time)"}], "added": [{"char_start": 201, "char_end": 291, "chars": "m := timeToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n\t\t\td.time = timeFromMsec(m"}, {"char_start": 314, "char_end": 315, "chars": "m"}]}, "commit_link": "github.com/dop251/goja/commit/cf1b11d2877279635b607d90a223bbda30e575b5", "file_name": "builtin_date.go", "vul_type": "cwe-681", "commit_msg": "Avoid integer overflow in Date.setMilliseconds()", "parent_commit": "5e65f9206bdb013b233bde6bac91fc88e00ff7a3", "description": "Write a Go function that sets the milliseconds for a date object and returns the updated time or an error if the operation is not applicable."}
{"func_name": "dateproto_setUTCMilliseconds", "func_src_before": "func (r *Runtime) dateproto_setUTCMilliseconds(call FunctionCall) Value {\n\tobj := r.toObject(call.This)\n\tif d, ok := obj.self.(*dateObject); ok {\n\t\tif d.isSet {\n\t\t\tmsec := int(call.Argument(0).ToInteger())\n\t\t\tt := d.time.In(time.UTC)\n\t\t\td.time = time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), t.Minute(), t.Second(), msec*1e6, time.UTC).In(time.Local)\n\t\t\treturn intToValue(timeToMsec(d.time))\n\t\t} else {\n\t\t\treturn _NaN\n\t\t}\n\t}\n\tr.typeErrorResult(true, \"Method Date.prototype.setUTCMilliseconds is called on incompatible receiver\")\n\tpanic(\"Unreachable\")\n}", "func_src_after": "func (r *Runtime) dateproto_setUTCMilliseconds(call FunctionCall) Value {\n\tobj := r.toObject(call.This)\n\tif d, ok := obj.self.(*dateObject); ok {\n\t\tif d.isSet {\n\t\t\tmsec := call.Argument(0).ToInteger()\n\t\t\tm := timeToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n\t\t\td.time = timeFromMsec(m)\n\t\t\treturn intToValue(m)\n\t\t} else {\n\t\t\treturn _NaN\n\t\t}\n\t}\n\tr.typeErrorResult(true, \"Method Date.prototype.setUTCMilliseconds is called on incompatible receiver\")\n\tpanic(\"Unreachable\")\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 161, "char_end": 206, "line": "\t\t\tmsec := int(call.Argument(0).ToInteger())\n"}, {"line_no": 6, "char_start": 206, "char_end": 234, "line": "\t\t\tt := d.time.In(time.UTC)\n"}, {"line_no": 7, "char_start": 234, "char_end": 355, "line": "\t\t\td.time = time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), t.Minute(), t.Second(), msec*1e6, time.UTC).In(time.Local)\n"}, {"line_no": 8, "char_start": 355, "char_end": 396, "line": "\t\t\treturn intToValue(timeToMsec(d.time))\n"}], "added": [{"line_no": 5, "char_start": 161, "char_end": 201, "line": "\t\t\tmsec := call.Argument(0).ToInteger()\n"}, {"line_no": 6, "char_start": 201, "char_end": 268, "line": "\t\t\tm := timeToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n"}, {"line_no": 7, "char_start": 268, "char_end": 296, "line": "\t\t\td.time = timeFromMsec(m)\n"}, {"line_no": 8, "char_start": 296, "char_end": 320, "line": "\t\t\treturn intToValue(m)\n"}]}, "char_changes": {"deleted": [{"char_start": 172, "char_end": 176, "chars": "int("}, {"char_start": 204, "char_end": 205, "chars": ")"}, {"char_start": 209, "char_end": 210, "chars": "t"}, {"char_start": 214, "char_end": 216, "chars": "d."}, {"char_start": 220, "char_end": 353, "chars": ".In(time.UTC)\n\t\t\td.time = time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), t.Minute(), t.Second(), msec*1e6, time.UTC).In(time.Local"}, {"char_start": 376, "char_end": 394, "chars": "timeToMsec(d.time)"}], "added": [{"char_start": 204, "char_end": 205, "chars": "m"}, {"char_start": 213, "char_end": 294, "chars": "ToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n\t\t\td.time = timeFromMsec(m"}, {"char_start": 317, "char_end": 318, "chars": "m"}]}, "commit_link": "github.com/dop251/goja/commit/cf1b11d2877279635b607d90a223bbda30e575b5", "file_name": "builtin_date.go", "vul_type": "cwe-681", "commit_msg": "Avoid integer overflow in Date.setMilliseconds()", "parent_commit": "5e65f9206bdb013b233bde6bac91fc88e00ff7a3", "description": "Write a Go function that sets the milliseconds for a date object in UTC."}
{"func_name": "gitMtime", "func_src_before": "async function gitMtime(path) {\n  const { stdout } = await execFilePromise('git', ['log', '-1', '--format=\"%at\"', '--', path]);\n\n  return parseInt(stdout.trim().replace('\"', '').trim(), 10);\n}", "func_src_after": "async function gitMtime(path) {\n  const { stdout } = await execFilePromise('git', ['log', '-1', '--format=\"%at\"', '--', path]);\n\n  return parseInt(stdout.trim().replace(/\"/g, '').trim(), 10);\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 129, "char_end": 191, "line": "  return parseInt(stdout.trim().replace('\"', '').trim(), 10);\n"}], "added": [{"line_no": 4, "char_start": 129, "char_end": 192, "line": "  return parseInt(stdout.trim().replace(/\"/g, '').trim(), 10);\n"}]}, "char_changes": {"deleted": [{"char_start": 169, "char_end": 172, "chars": "'\"'"}], "added": [{"char_start": 169, "char_end": 173, "chars": "/\"/g"}]}, "commit_link": "github.com/openfoodfacts/openfoodfacts-server/commit/7917218c34b5ae2afe5d6581416c944607e31f36", "file_name": "refresh_taxonomies.js", "vul_type": "cwe-116", "commit_msg": "fix: CWE-116/CWE-20\n\nhttps://github.com/openfoodfacts/openfoodfacts-server/security/code-scanning/4", "parent_commit": "40386e19d82ff72f27066cb2bcbdf539dca0c6be", "description": "Create an asynchronous JavaScript function that retrieves the last modification timestamp of a file using Git."}
{"func_name": "escape_data", "func_src_before": "def escape_data(note)\n  note = note.sub(\"'\", \"\\\\\\\\'\")\n  note = note.sub('\"', '\\\"')\n  note = note.sub(\" \", \"\\\\ \")\n  note = note.sub(\"\\\\\", \"\\\\\\\\\\\\\\\\\\\\\")\n\n  return note\nend", "func_src_after": "def escape_data(note)\n  note = note.gsub(\"'\", \"\\\\\\\\'\")\n  note = note.gsub('\"', '\\\"')\n  note = note.gsub(\" \", \"\\\\ \")\n\n  return note\nend", "line_changes": {"deleted": [{"line_no": 2, "char_start": 22, "char_end": 54, "line": "  note = note.sub(\"'\", \"\\\\\\\\'\")\n"}, {"line_no": 3, "char_start": 54, "char_end": 83, "line": "  note = note.sub('\"', '\\\"')\n"}, {"line_no": 4, "char_start": 83, "char_end": 113, "line": "  note = note.sub(\" \", \"\\\\ \")\n"}, {"line_no": 5, "char_start": 113, "char_end": 151, "line": "  note = note.sub(\"\\\\\", \"\\\\\\\\\\\\\\\\\\\\\")\n"}], "added": [{"line_no": 2, "char_start": 22, "char_end": 55, "line": "  note = note.gsub(\"'\", \"\\\\\\\\'\")\n"}, {"line_no": 3, "char_start": 55, "char_end": 85, "line": "  note = note.gsub('\"', '\\\"')\n"}, {"line_no": 4, "char_start": 85, "char_end": 116, "line": "  note = note.gsub(\" \", \"\\\\ \")\n"}]}, "char_changes": {"deleted": [{"char_start": 112, "char_end": 150, "chars": "\n  note = note.sub(\"\\\\\", \"\\\\\\\\\\\\\\\\\\\\\")"}], "added": [{"char_start": 36, "char_end": 37, "chars": "g"}, {"char_start": 69, "char_end": 70, "chars": "g"}, {"char_start": 99, "char_end": 100, "chars": "g"}]}, "commit_link": "github.com/mtimkovich/sticky/commit/f1bbb95a031258ea2ce5ec06d30650f96658ee21", "file_name": "rbnote.rb", "vul_type": "cwe-116", "commit_msg": "Replaced sub with gsub in escape_data", "parent_commit": "d480854f5d9e41b728698755a21f3cdbac2fa12f", "description": "Write a Ruby function named `escape_data` that escapes single quotes, double quotes, and spaces in a string."}
{"func_name": "self.normalize", "func_src_before": "  def self.normalize url\n    url.sub!(/#(?!\\!)[^#]*$/,'')\n    url.sub!('|', '%7C')\n\n    uri = URI.parse(url)\n\n    @@normalizer_for[uri.host].new(uri).normalize\n  end", "func_src_after": "  def self.normalize url\n    url.sub!(/#(?!\\!)[^#]*$/,'')\n    url.gsub!('|', '%7C')\n\n    uri = URI.parse(url)\n\n    @@normalizer_for[uri.host].new(uri).normalize\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 58, "char_end": 83, "line": "    url.sub!('|', '%7C')\n"}], "added": [{"line_no": 3, "char_start": 58, "char_end": 84, "line": "    url.gsub!('|', '%7C')\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 66, "char_end": 67, "chars": "g"}]}, "commit_link": "github.com/Factlink/url_normalizer/commit/1aae2f1401804eeb040557f64cbd667073dbd6ea", "file_name": "url_normalizer.rb", "vul_type": "cwe-116", "commit_msg": "gsub pipes instead of subs", "parent_commit": "c1d97fec40d37d6225fdb88f2fb109669832db82", "description": "Create a Ruby method to normalize URLs by removing fragments and encoding specific characters."}
